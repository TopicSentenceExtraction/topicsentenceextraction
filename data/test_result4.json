{"columns":["predict_title","actual_title","actual_abstract"],"data":[["Electride materials offer attractive physical properties due to their loosely bound electrons.","Mechanical, optoelectronic and transport properties of single-layer Ca2N and Sr2N electrides","summarize: Electride materials offer attractive physical properties due to their loosely bound electrons. Ca2N, an electride in the two-dimensional form was successfully recently synthesized. We conducted extensive first-principles calculations to explore the mechanical, electronic, optical and transport response of single-layer and free-standing Ca2N and Sr2N electrides to external strain. We show that Ca2N and Sr2N sheets present isotropic elastic properties with positive Poisson's ratios, however, they yield around 50% higher tensile strength along the zigzag direction as compared with armchair. We also showed that the strain has negligible effect on the conductivity of the materials; the current in the system reduces by less than 32% for the structure under ultimate uniaxial strain along the armchair direction. Compressive strain always increases the electronic transport in the systems due to stronger overlap of the atomic orbitals. Our results show that the optical spectra are anisotropic for light polarization parallel and perpendicular to the plane. Interband transition contributions along in-plane polarization are not negligible, by considering this effect the optical properties of Ca2N and Sr2N sheets in the low frequency regime significantly changed. The insight provided by this study can be useful for the future application of Ca2N and Sr2N in nanodevices."],["the aim of this paper is to show how a conjectural lower bound on the can","Bornes sur le nombre de points rationnels des courbes -- en quete d'uniformite","summarize: The aim of this paper is to show how a conjectural lower bound on the canonical height function in the spirit of Lang and Silverman leads to an explicit uniform bound on the number of rational points on curves of genus "],["four degenerate electromagnetic modes arise in a slow-wave structure and electron beam","Giant Amplification in Degenerate Band Edge Slow-Wave Structures Interacting with an Electron Beam","summarize: We propose a new amplification regime based on synchronous operation of four degenerate electromagnetic modes in a slow-wave structure and the electron beam, referred to as super synchronization. These four EM modes arise in a Fabry-Perot cavity when degenerate band edge condition is satisfied. The modes interact constructively with the electron beam resulting in superior amplification. In particular, much larger gains are achieved for smaller beam currents compared to conventional structures based on synchronization with only a single EM mode. We demonstrate giant gain scaling with respect to the length of the slow-wave structure compared to conventional Pierce type single mode traveling wave tube amplifiers. We construct a coupled transmission line model for a loaded waveguide slow-wave structure exhibiting a DBE, and investigate the phenomenon of giant gain via super synchronization using the Pierce model generalized to multimode interaction."],["magnetic field of a homogeneously magnetized cylindrical tile geometry is found. the","The magnetic field from a homogeneously magnetized cylindrical tile","summarize: The magnetic field of a homogeneously magnetized cylindrical tile geometry, i.e. an angular section of a finite hollow cylinder, is found. The field is expressed as the product between a tensor field describing the geometrical part of the problem and a column vector holding the magnetization of the tile. Outside the tile, the tensor is identical to the demagnetization tensor. We find that four components of the tensor, "],["heavy hitters formulation generates solutions that are more accurate and effective than clustering formulation","Finding Frequent Entities in Continuous Data","summarize: In many applications that involve processing high-dimensional data, it is important to identify a small set of entities that account for a significant fraction of detections. Rather than formalize this as a clustering problem, in which all detections must be grouped into hard or soft categories, we formalize it as an instance of the frequent items or heavy hitters problem, which finds groups of tightly clustered objects that have a high density in the feature space. We show that the heavy hitters formulation generates solutions that are more accurate and effective than the clustering formulation. In addition, we present a novel online algorithm for heavy hitters, called HAC, which addresses problems in continuous space, and demonstrate its effectiveness on real video and household domains."],["cloud computing has already succeeded in the web based application. the demand for context aware services","A Context Aware and Self Adaptation Strategy for Cloud Service Selection and Configuration in Run Time","summarize: Day after day, the number of mobile applications deployed on cloud computing continues in increasing because o f smartphone capabilities improvement. Cloud computing has already succeeded in the web based application, for that reason, the demand for context aware services provided by cloud computing increases. To customize a cloud service that takes into account th e consumer requirements, which depend on information change, it brings to light many recent challenges to cloud computing about environment aware, location aware, time aware. The cloud provider, moreover, has to manage personalized applications and the con straints of mobile devices in matters of interaction abilities and communication restrictions. This paper proposes a strategy for selecting automatically an appropriate cloud environment that runs out whole requirements, defines a configuration for the ass ociated cloud environment and able to easily adapt to the change of the environment on either the user or the cloud side or both. This process builds on the principles of dynamic software product lines, Agent oriented software engineering, and the MAPE k m odel to select and configure cloud environments according to the consumer needs and the context change."],["the discretization for the tempered fractional substantial derivative is derived. the finite","Numerical schemes of the time tempered fractional Feynman-Kac equation","summarize: This paper focuses on providing the computation methods for the backward time tempered fractional Feynman-Kac equation, being one of the models recently proposed in . The discretization for the tempered fractional substantial derivative is derived, and the corresponding finite difference and finite element schemes are designed with well established stability and convergence. The performed numerical experiments show the effectiveness of the presented schemes."],["RT Cru belongs to the rare class of hard X-ray emitting sy","Long-term X-ray Variability of the Symbiotic System RT Cru based on Chandra Spectroscopy","summarize: RT Cru belongs to the rare class of hard X-ray emitting symbiotics, whose origin is not yet fully understood. In this work, we have conducted a detailed spectroscopic analysis of X-ray emission from RT Cru based on observations taken by the Chandra Observatory using the Low Energy Transmission Grating on the High-Resolution Camera Spectrometer in 2015 and the High Energy Transmission Grating on the Advanced CCD Imaging Spectrometer S-array in 2005. Our thermal plasma modeling of the time-averaged HRC-S\/LETG spectrum suggests a mean temperature of "],["we present results of high-resolution numerical simulations of compressible 2D turbul","Energy Transfer and Spectra in Simulations of Two-dimensional Compressible Turbulence","summarize: We present results of high-resolution numerical simulations of compressible 2D turbulence forced at intermediate spatial scales with a solenoidal white-in-time external acceleration. A case with an isothermal equation of state, low energy injection rate, and turbulent Mach number "],["boundary control input is subject to constant delay while the open loop system might exhibit a finite","Feedback Stabilization of a Class of Diagonal Infinite-Dimensional Systems with Delay Boundary Control","summarize: This paper studies the boundary feedback stabilization of a class of diagonal infinite-dimensional boundary control systems. In the studied setting, the boundary control input is subject to a constant delay while the open loop system might exhibit a finite number of unstable modes. The proposed control design strategy consists in two main steps. First, a finite-dimensional subsystem is obtained by truncation of the original Infinite-Dimensional System via modal decomposition. It includes the unstable components of the infinite-dimensional system and allows the design of a finite-dimensional delay controller by means of the Artstein transformation and the pole-shifting theorem. Second, it is shown via the selection of an adequate Lyapunov function that 1) the finite-dimensional delay controller successfully stabilizes the original infinite-dimensional system; 2) the closed-loop system is exponentially Input-to-State Stable with respect to distributed disturbances. Finally, the obtained ISS property is used to derive a small gain condition ensuring the stability of an IDS-ODE interconnection."],["we study two counter-propagating electromagnetic waves in the quantum vacuum. we discuss the","Properties of Finite Amplitude Electromagnetic Waves propagating in the Quantum Vacuum","summarize: We study two counter-propagating electromagnetic waves in the vacuum within the framework of the Heisenberg-Euler formalism in quantum electrodynamics. We show that the non-linear field equations decouple for ordinary wave case and can be solved exactly. We solve the non-linear field equations assuming the solution in a form of a Riemann wave. We discuss the properties of the nonlinear electromagnetic wave propagating in the quantum vacuum, such as the wave steepening, subsequent generation of high order harmonics and electromagnetic shock wave formation with electron-positron pair generation at the shock wave front."],["we show each time the conjunction coincides with a suitable Frank t-norm","Conjunction of Conditional Events and T-norms","summarize: We study the relationship between a notion of conjunction among conditional events, introduced in recent papers, and the notion of Frank t-norm. By examining different cases, in the setting of coherence, we show each time that the conjunction coincides with a suitable Frank t-norm. In particular, the conjunction may coincide with the Product t-norm, the Minimum t-norm, and Lukasiewicz t-norm. We show by a counterexample, that the prevision assessments obtained by Lukasiewicz t-norm may be not coherent. Then, we give some conditions of coherence when using Lukasiewicz t-norm."],["a new method to determine the breakdown voltage of SiPMs is presented. it is","A model based DC analysis of SiPM breakdown voltages","summarize: A new method to determine the breakdown voltage of SiPMs is presented. It is backed up by a DC model which describes the breakdown phenomenon by distinct avalanche turn-on and turn off voltages. It is shown that "],["work presents robust and efficient sharp interface immersed boundary framework. the work deploys an in","A Novel Sharp Interface Immersed Boundary Framework for Viscous Flow Simulations at Arbitrary Mach Number Involving Complex and Moving Boundaries","summarize: This work presents a robust and efficient sharp interface immersed boundary framework, which is applicable for all-speed flow regimes and is capable of handling arbitrarily complex bodies . The work deploys an in-house, parallel, multi-block structured finite volume flow solver, which employs a 3D unsteady Favre averaged Navier Stokes equations in a generalized curvilinear coordinate system; while we employ a combination of HCIB method and GC for solution reconstruction near immersed boundary interface. A significant difficulty for these sharp interface approaches is of handling sharp features\/edges of complex geometries. In this study, we observe that apart from the need for robust node classification strategy and higher order boundary formulations, the direction in which the reconstruction procedures are performed plays an important role in handling sharp edges. Taking this into account we present a versatile interface tracking procedure based on ray tracing algorithm and a novel three step solution reconstruction procedure that computes pseudo-normals in the regions where the normal is not well-defined and reconstructs the flow field along those directions. We demonstrate that this procedure enables solver to efficiently handle and accurately represent sharp-edged regions. A fifth-order weighted essentially non-oscillatory scheme is used for capturing shock-induced discontinuities and complex fluid-solid interactions with high resolution. The developed IBM framework is applied to a wide range of flow phenomena encompassing all-speed regimes . A total of seven benchmark cases are presented involving various geometries and the predictions are found to be in excellent agreement with the published results."],["TM is a new modeling technique that can be used to understand UML. it","UML Modeling to TM Modeling and Back","summarize: Certainly, the success of the Unified Modeling Language as the de facto standard for modeling software systems does not imply closing the door on scientific exploration or experimentation with modeling in the field. Continuing studies in this area can produce theoretical results that strengthen UML as the leading modeling language. Recently, a new modeling technique has been proposed called thinging machine modeling. This paper utilizes TM to further understand UML, with two objectives: Fine issues in UML are studied, including theoretical notions such as events, objects, actions, activities, etc. Specifically, TM can be used to solve problems related to internal cross-diagram integration. TM applies a different method of conceptualization, including building a model on one-category ontology in contrast to the object-oriented paradigm. The long-term objective of this study is to explore the possibility of TM complementing certain aspects in the UML methodology to develop and design software systems. Accordingly, we alternate between UML and TM modeling. A sample UML model is redesigned in TM, and then UML diagrams are extracted from TM. The results clarify many notions in both models. Particularly, the TM behavioral specification seems to be applicable in UML."],["random initial conditions can be transformed during the passage near a delayed\/dynamic Hopf bifur","Uncertainty Transformation via Hopf Bifurcation in Fast-Slow Systems","summarize: Propagation of uncertainty in dynamical systems is a significant challenge. Here we focus on random multiscale ordinary differential equation models. In particular, we study Hopf bifurcation in the fast subsystem for random initial conditions. We show that a random initial condition distribution can be transformed during the passage near a delayed\/dynamic Hopf bifurcation: to certain classes of symmetric copies, to an almost deterministic output, to a mixture distribution with differing moments, and to a very restricted class of general distributions. We prove under which conditions the cases - occur in certain classes vector fields."],["our aim is to establish new integral representations for the Fox--Wright function.","New integral representations for the Fox-Wright functions and its applications II","summarize: In this paper our aim is to establish new integral representations for the Fox--Wright function "],["design and architecture are becoming increasingly increasingly popular in the digital world. the technology is being taught","Hybrid design tools - making of a digitally augmented blackboard","summarize: The way that design is being taught is continuously changing under the pressure of the transition from analogical to digital environments. This becomes even more important as the novelty and the alleged superiority of the digital world is used as a marketing tool by competing universities. Even though in some fields of application this approach is desirable, some particular aspects of teaching design and architecture make this transition debatable. The advantages of drawing on blackboards over drawing on whiteboard surfaces in regards of line aesthetic and expression possibilities were previously identified, along with the complementary necessary features for improvement. This study showcases a proof of concept in digitally augmenting a blackboard surface. The system allows the capturing, processing and making real time projections of images over the blackboard surface as trace references. Such a hybrid system, along with providing support for design and architecture related presentations and discussions could also mediate the contradictory relation towards technology that students and teachers have."],["reinforcement learning is a popular paradigm for addressing sequential decision tasks. learning in many domain","Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey","summarize: Reinforcement learning is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research."],["we developed ensemble models using RoBERTa-based neural architectures. we achieved si","aschern at SemEval-2020 Task 11: It Takes Three to Tango: RoBERTa, CRF, and Transfer Learning","summarize: We describe our system for SemEval-2020 Task 11 on Detection of Propaganda Techniques in News Articles. We developed ensemble models using RoBERTa-based neural architectures, additional CRF layers, transfer learning between the two subtasks, and advanced post-processing to handle the multi-label nature of the task, the consistency between nested spans, repetitions, and labels from similar spans in training. We achieved sizable improvements over baseline fine-tuned RoBERTa models, and the official evaluation ranked our system 3rd out of 36 teams on the span identification subtask with an F1 score of 0.491, and 2nd out of 31 teams on the technique classification subtask with an F1 score of 0.62."],["discrete Gaussian Gaussian Gaussian is the discrete Gaussian Gau","Discrete Gaussian Sampling Reduces to CVP and SVP","summarize: The discrete Gaussian "],["training deep neural networks to learn such data-driven partial differential operators requires extensive spatiotemporal","Linking Machine Learning with Multiscale Numerics: Data-Driven Discovery of Homogenized Equations","summarize: The data-driven discovery of partial differential equations consistent with spatiotemporal data is experiencing a rebirth in machine learning research. Training deep neural networks to learn such data-driven partial differential operators requires extensive spatiotemporal data. For learning coarse-scale PDEs from computational fine-scale simulation data, the training data collection process can be prohibitively expensive. We propose to transformatively facilitate this training data collection process by linking machine learning with modern multiscale scientific computation . These equation-free techniques operate over sparse collections of small, appropriately coupled, space-time subdomains , parsimoniously producing the required macro-scale training data. Our illustrative example involves the discovery of effective homogenized equations in one and two dimensions, for problems with fine-scale material property variations. The approach holds promise towards making the discovery of accurate, macro-scale effective materials PDE models possible by efficiently summarizing the physics embodied in the best fine-scale simulation models available."],["Lieb-Liniger model for bosonic gas of bosonic gas.","Density form factors of the 1D Bose gas for finite entropy states","summarize: We consider the Lieb-Liniger model for a gas of bosonic "],["a macroscopic theory of interfacial interactions targeting terrestrial applications has been developed","Exploratory numerical experiments with a macroscopic theory of interfacial interactions","summarize: Phenomenological theories of interfacial interactions have targeted terrestrial applications since long time and their exploitation has inspired our research programme to build up a macroscopic theory of gas-surface interactions targeting the complex phenomenology of hypersonic reentry flows as alternative to standard methods based on accommodation coefficients. The objective of this paper is the description of methods employed and results achieved in an exploratory study, that is, the unsteady heat transfer between two solids in contact with and without interface. It is a simple numerical-demonstrator test case designed to facilitate quick numerical calculations and to bring forth already sufficiently meaningful aspects relevant to thermal protection due to the formation of the interface. The paper begins with a brief introduction on the subject matter and a review of relevant literature. Then the case is considered in which the interface is absent. The importance of tension continuity as boundary condition on the same footing of heat-flux continuity is recognised and the role of the former in governing the establishment of the temperature-difference distribution over the separation surface is explicitly shown. Evidence is given that the standard temperature-continuity boundary condition is just a particular case. Subsequently the case in which the interface is formed between the solids is analysed. The coupling among the heat-transfer equations applicable in the solids and the balance equation for the surface thermodynamic energy formulated in terms of the surface temperature is discussed. Results are illustrated for planar and cylindrical configuration; they show unequivocally that the thermal-protection action of the interface turns out to be driven exclusively by thermophysical properties of the solids and of the interface; accommodation coefficients are not needed."],["the theory was initiated by Foias and Saut in the early 1990s.","Asymptotic expansions with exponential, power, and logarithmic functions for non-autonomous nonlinear differential equations","summarize: This paper develops further and systematically the asymptotic expansion theory that was initiated by Foias and Saut in . We study the long-time dynamics of a large class of dissipative systems of nonlinear ordinary differential equations with time-decaying forcing functions. The nonlinear term can be, but not restricted to, any smooth vector field which, together with its first derivative, vanishes at the origin. The forcing function can be approximated, as time tends to infinity, by a series of functions which are coherent combinations of exponential, power and iterated logarithmic functions. We prove that any decaying solution admits an asymptotic expansion, as time tends to infinity, corresponding to the asymptotic structure of the forcing function. Moreover, these expansions can be generated by more than two base functions and go beyond the polynomial formulation imposed in previous work."],["pulsating members define the subgroup of rapidly oscillating Ap stars. Alpha","New BRITE-Constellation observations of the roAp star Alpha Circini","summarize: Chemically peculiar stars with a measurable magnetic field comprise the group of mCP stars. The pulsating members define the subgroup of rapidly oscillating Ap stars, of which Alpha Circini is the brightest member. Hence, Alpha Circini allows the application of challenging techniques, such as interferometry, very high temporal and spectral resolution photometry, and spectroscopy in a wide wavelength range, that have the potential to provide unique information about the structure and evolution of a star. Based on new photometry from BRITE-Constellation, obtained with blue and red filters, and on photometry from WIRE, SMEI, and TESS we attempt to determine the surface spot structure of Alpha Circini and investigate pulsation frequencies. We used photometric surface imaging and frequency analyses and Bayesian techniques in order to quantitatively compare the probability of different models. BRITE-Constellation photometry obtained from 2014 to 2016 is put in the context of space photometry obtained by WIRE, SMEI, and TESS. This provides improvements in the determination of the rotation period and surface features . The main pulsation frequencies indicate two consecutive radial modes and one intermediate dipolar mode. Advantages and problems of the applied Bayesian technique are discussed."],["OBJECTIVE: We propose using bad smells to help develop software analytics studies","Bad Smells in Software Analytics Papers","summarize: CONTEXT: There has been a rapid growth in the use of data analytics to underpin evidence-based software engineering. However the combination of complex techniques, diverse reporting standards and poorly understood underlying phenomena are causing some concern as to the reliability of studies. OBJECTIVE: Our goal is to provide guidance for producers and consumers of software analytics studies . METHOD: We propose using bad smells, i.e., surface indications of deeper problems and popular in the agile software community and consider how they may be manifest in software analytics studies. RESULTS: We list 12 bad smells in software analytics papers . CONCLUSIONS: We believe the metaphor of bad smell is a useful device. Therefore we encourage more debate on what contributes to the validty of software analytics studies ."],["we construct a classification of dispersion relations for the electromagnetic waves non-minimally","Non-minimal Einstein-Maxwell theory: the Fresnel equation and the Petrov classification of a trace-free susceptibility tensor","summarize: We construct a classification of dispersion relations for the electromagnetic waves non-minimally coupled to the space-time curvature, based on the analysis of the susceptibility tensor, which appears in the non-minimal Einstein-Maxwell theory. We classify solutions to the Fresnel equation for the model with a trace-free non-minimal susceptibility tensor according to the Petrov scheme. For all Petrov types we discuss specific features of the dispersion relations and plot the corresponding wave surfaces."],["Riemannian manifolds of positive Ricci curvature admit an Eucli","A vanishing result for the supersymmetric nonlinear sigma model in higher dimensions","summarize: We prove a vanishing result for critical points of the supersymmetric nonlinear sigma model on complete non-compact Riemannian manifolds of positive Ricci curvature that admit an Euclidean type Sobolev inequality, assuming that the dimension of the domain is bigger than two and that a certain energy is sufficiently small."],["deep learning model is capable of forecasting glucose levels with leading accuracy for simulated patient cases and","Convolutional Recurrent Neural Networks for Glucose Prediction","summarize: Control of blood glucose is essential for diabetes management. Current digital therapeutic approaches for subjects with Type 1 diabetes mellitus such as the artificial pancreas and insulin bolus calculators leverage machine learning techniques for predicting subcutaneous glucose for improved control. Deep learning has recently been applied in healthcare and medical research to achieve state-of-the-art results in a range of tasks including disease diagnosis, and patient state prediction among others. In this work, we present a deep learning model that is capable of forecasting glucose levels with leading accuracy for simulated patient cases and real patient cases . In addition, the model provides competitive performance in providing effective prediction horizon with minimal time lag both in a simulated patient dataset and in a real patient dataset . This approach is evaluated on a dataset of 10 simulated cases generated from the UVa\/Padova simulator and a clinical dataset of 10 real cases each containing glucose readings, insulin bolus, and meal data. Performance of the recurrent convolutional neural network is benchmarked against four algorithms. The proposed algorithm is implemented on an Android mobile phone, with an execution time of "],["the ITC-Irst dataset is based on the most common detection schemes","What Makes Audio Event Detection Harder than Classification?","summarize: There is a common observation that audio event classification is easier to deal with than detection. So far, this observation has been accepted as a fact and we lack of a careful analysis. In this paper, we reason the rationale behind this fact and, more importantly, leverage them to benefit the audio event detection task. We present an improved detection pipeline in which a verification step is appended to augment a detection system. This step employs a high-quality event classifier to postprocess the benign event hypotheses outputted by the detection system and reject false alarms. To demonstrate the effectiveness of the proposed pipeline, we implement and pair up different event detectors based on the most common detection schemes and various event classifiers, ranging from the standard bag-of-words model to the state-of-the-art bank-of-regressors one. Experimental results on the ITC-Irst dataset show significant improvements to detection performance. More importantly, these improvements are consistent for all detector-classifier combinations."],["the 15th conference on theoretical aspects of rationality and knowledge took place in Pittsburgh","Proceedings Fifteenth Conference on Theoretical Aspects of Rationality and Knowledge","summarize: The 15th Conference on Theoretical Aspects of Rationality and Knowledge took place in Carnegie Mellon University, Pittsburgh, USA from June 4 to 6, 2015. The mission of the TARK conferences is to bring together researchers from a wide variety of fields, including Artificial Intelligence, Cryptography, Distributed Computing, Economics and Game Theory, Linguistics, Philosophy, and Psychology, in order to further our understanding of interdisciplinary issues involving reasoning about rationality and knowledge. These proceedings consist of a subset of the papers \/ abstracts presented at the TARK conference."],["crude oil is a multicomponent liquid fuel and threatens marine life. the use","Effect of carbon-based nanoparticles on the ignition, combustion and flame characteristics of crude oil droplets","summarize: The use of in-situ burning as a clean-up response in the event of an oil spill has generated controversy because of unburned hydrocarbons and products of incomplete combustion left behind on an ISB site. These substances threaten marine life, both in the ocean and on the ocean floor. Treating crude oil as a multicomponent liquid fuel, this manuscript investigates the effect of carbon-based nanomaterials, acetylene black and multi-walled carbon nanotube , on the combustion and flame characteristics of crude sourced from the Bakken formation . Sub-millimeter droplets of colloidal suspensions of Bakken crude and nanomaterials at various particle loadings were burned, and the process was captured with CMOS and CCD cameras. The resulting images were post-processed to generate burning rate, ignition delay, total combustion time, and flame stand-off ratio data for the various crude suspensions. A maximum combustion rate enhancement of 39.5% and 31.1% was observed at a particle loading of 0.5% w\/w acetylene black nanoparticles and 0.5% w\/w multi-walled carbon nanotubes, respectively. Generally, FSR for pure Bakken was noted as larger than for Bakken with nanoparticle additives. These results are expected to spur further investigations into the use of nanomaterials for ISB crude oil clean-ups."],["facility location problems aim to identify the best locations to set up new services. majority of","TIPS: Mining Top-K Locations to Minimize User-Inconvenience for Trajectory-Aware Services","summarize: Facility location problems aim to identify the best locations to set up new services. Majority of the existing works typically assume that the users are static. However, there exists a wide array of services such as fuel stations, ATMs, food joints, etc., that are widely accessed by mobile users besides the static ones. Such trajectory-aware services should, therefore, factor in the trajectories of its users rather than simply their static locations. In this work, we introduce the problem of optimal placement of facility locations for such trajectory-aware services that minimize the user inconvenience. The inconvenience of a user is the extra distance traveled by her from her regular path to avail a service. We call this the TIPS problem and consider two variants of it. The goal of the first variant, MAXTIPS, is to minimize the maximum inconvenience faced by any user, while that of the second, AVGTIPS, is to minimize the average inconvenience over all the users. We show that both these problems are NP-hard, and propose multiple efficient heuristics to solve them. Empirical evaluation on real urban-scale road networks validate the efficiency and effectiveness of the proposed heuristics."],["sparse control inputs arise naturally in networked systems. derived conditions can","Controllability of Linear Dynamical Systems Under Input Sparsity Constraints","summarize: In this work, we consider the controllability of a discrete-time linear dynamical system with sparse control inputs. Sparsity constraints on the input arises naturally in networked systems, where activating each input variable adds to the cost of control. We derive algebraic necessary and sufficient conditions for ensuring controllability of a system with an arbitrary transfer matrix. The derived conditions can be verified in polynomial time complexity, unlike the more traditional Kalman-type rank tests. Further, we characterize the minimum number of input vectors required to satisfy the derived conditions for controllability. Finally, we present a generalized Kalman decomposition-like procedure that separates the state-space into subspaces corresponding to sparse-controllable and sparse-uncontrollable parts. These results form a theoretical basis for designing networked linear control systems with sparse inputs."],["new method of multi-cell tracking is based on a study of brain-wide 4","SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter","summarize: Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: cells are imaged as globular-like objects, it is difficult to distinguish cells based only on shape and size, the number of imaged cells ranges in several hundreds, moves of nearly-located cells are strongly correlated and cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. ."],["a newly developed 1-bit local most powerful test detector requires 3.3Q sensors to achieve the","Distributed Detection of Sparse Stochastic Signals via Fusion of 1-bit Local Likelihood Ratios","summarize: In this letter, we consider the detection of sparse stochastic signals with sensor networks , where the fusion center collects 1-bit data from the local sensors and then performs global detection. For this problem, a newly developed 1-bit locally most powerful test detector requires 3.3Q sensors to asymptotically achieve the same detection performance as the centralized LMPT detector with Q sensors. This 1-bit LMPT detector is based on 1-bit quantized observations without any additional processing at the local sensors. However, direct quantization of observations is not the most efficient processing strategy at the sensors since it incurs unnecessary information loss. In this letter, we propose an improved-1-bit LMPT detector that fuses local 1-bit quantized likelihood ratios instead of directly quantized local observations. In addition, we design the quantization thresholds at the local sensors to ensure asymptotically optimal detection performance of the proposed detector. It is shown theoretically and numerically that, with the designed quantization thresholds, the proposed Im-1-bit LMPT detector for the detection of sparse signals requires less number of sensor nodes to compensate for the performance loss caused by 1-bit quantization."],["an analytic approach is initiated by Fayolle, Iasno","Analytic approach for reflected Brownian motion in the quadrant","summarize: Random walks in the quarter plane are an important object both of combinatorics and probability theory. Of particular interest for their study, there is an analytic approach initiated by Fayolle, Iasnogorodski and Malyshev, and further developed by the last two authors of this note. The outcomes of this method are explicit expressions for the generating functions of interest, asymptotic analysis of their coefficients, etc. Although there is an important literature on reflected Brownian motion in the quarter plane , an analogue of the analytic approach has not been fully developed to that context. The aim of this note is twofold: it is first an extended abstract of two recent articles of the authors of this paper, which propose such an approach; we further compare various aspects of the discrete and continuous analytic approaches."],["we study two counter-propagating electromagnetic waves in the quantum vacuum. we discuss the","Properties of Finite Amplitude Electromagnetic Waves propagating in the Quantum Vacuum","summarize: We study two counter-propagating electromagnetic waves in the vacuum within the framework of the Heisenberg-Euler formalism in quantum electrodynamics. We show that the non-linear field equations decouple for ordinary wave case and can be solved exactly. We solve the non-linear field equations assuming the solution in a form of a Riemann wave. We discuss the properties of the nonlinear electromagnetic wave propagating in the quantum vacuum, such as the wave steepening, subsequent generation of high order harmonics and electromagnetic shock wave formation with electron-positron pair generation at the shock wave front."],["wireless sensor networks are being deployed for different applications. each protocol has its own structure, goals","A General Model for MAC Protocol Selection in Wireless Sensor Networks","summarize: Wireless Sensor Networks are being deployed for different applications, each having its own structure, goals and requirements. Medium access control protocols play a significant role in WSNs and hence should be tuned to the applications. However, there is no for selecting MAC protocols for different situations. Therefore, it is hard to decide which MAC protocol is good for a given situation. Having a precise model for each MAC protocol, on the other hand, is almost impossible. Using the intuition that the protocols in the same behavioral category perform similarly, our goal in this paper is to introduce a general model that selects the protocol that satisfy the given requirements from the category that performs better for a given context. We define the Combined Performance Function to demonstrate the performance of different categories protocols for different contexts. Having the general model, we then discuss the model scalability for adding new protocols, categories, requirements, and performance criteria. Considering energy consumption and delay as the initial performance criteria of the model, we focus on deriving mathematical models for them. The results extracted from CPF are the same as the well-known rule of thumb for the MAC protocols that verifies our model. We validate our models with the help of simulation study. We also implemented the current CPF model in a web page to make the model online and useful."],["state-of-the-art approaches perform semantic segmentation or refine object bounding boxes obtained","Instance Segmentation of Biomedical Images with an Object-aware Embedding Learned with Local Constraints","summarize: Automatic instance segmentation is a problem that occurs in many biomedical applications. State-of-the-art approaches either perform semantic segmentation or refine object bounding boxes obtained from detection methods. Both suffer from crowded objects to varying degrees, merging adjacent objects or suppressing a valid object. In this work, we assign an embedding vector to each pixel through a deep neural network. The network is trained to output embedding vectors of similar directions for pixels from the same object, while adjacent objects are orthogonal in the embedding space, which effectively avoids the fusion of objects in a crowd. Our method yields state-of-the-art results even with a light-weighted backbone network on a cell segmentation and a leaf segmentation data set . The code and model weights are public available."],["real-world tasks would benefit from using multiagent reinforcement learning algorithms. if agents","R-MADDPG for Partially Observable Environments and Limited Communication","summarize: There are several real-world tasks that would benefit from applying multiagent reinforcement learning algorithms, including the coordination among self-driving cars. The real world has challenging conditions for multiagent learning systems, such as its partial observable and nonstationary nature. Moreover, if agents must share a limited resource they must all learn how to coordinate resource use. This paper introduces a deep recurrent multiagent actor-critic framework for handling multiagent coordination under partial observable set-tings and limited communication. We investigate recurrency effects on performance and communication use of a team of agents. We demonstrate that the resulting framework learns time dependencies for sharing missing observations, handling resource limitations, and developing different communication patterns among agents."],["standard 1D-LTE model atmosphere analysis provides an in-depth investigation of iron abundance","On the Iron Abundance Anomaly in K-dwarf and Hyades Stars","summarize: Using standard 1D-LTE model atmosphere analysis, we provide an in-depth investigation of iron abundance as derived from neutral and singly ionization iron lines in nearby star clusters. Specifically, we replicate the discrepancy regarding "],["fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change","Delayed Impact of Fair Machine Learning","summarize: Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs."],["fine-tuned transductively, this outperforms the current state-of","A Baseline for Few-Shot Image Classification","summarize: Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the hardness of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way."],["oblivious algorithms are non-trivial to implement obliviously without","Efficient Oblivious Database Joins","summarize: A major algorithmic challenge in designing applications intended for secure remote execution is ensuring that they are oblivious to their inputs, in the sense that their memory access patterns do not leak sensitive information to the server. This problem is particularly relevant to cloud databases that wish to allow queries over the client's encrypted data. One of the major obstacles to such a goal is the join operator, which is non-trivial to implement obliviously without resorting to generic but inefficient solutions like Oblivious RAM . We present an oblivious algorithm for equi-joins which matches the optimal "],["we consider the problem of describing excursion sets of a real-valued function","Profile extrema for visualizing and quantifying uncertainties on excursion regions. Application to coastal flooding","summarize: We consider the problem of describing excursion sets of a real-valued function "],["new HDP based online review rating regression model. combines topics, word sentiment","Unifying Topic, Sentiment & Preference in an HDP-Based Rating Regression Model for Online Reviews","summarize: This paper proposes a new HDP based online review rating regression model named Topic-Sentiment-Preference Regression Analysis . TSPRA combines topics , word sentiment and user preference as regression factors, and is able to perform topic clustering, review rating prediction, sentiment analysis and what we invent as critical aspect analysis altogether in one framework. TSPRA extends sentiment approaches by integrating the key concept user preference in collaborative filtering models into consideration, while it is distinct from current CF models by decoupling user preference and sentiment as independent factors. Our experiments conducted on 22 Amazon datasets show overwhelming better performance in rating predication against a state-of-art model FLAME in terms of error, Pearson's Correlation and number of inverted pairs. For sentiment analysis, we compare the derived word sentiments against a public sentiment resource SenticNet3 and our sentiment estimations clearly make more sense in the context of online reviews. Last, as a result of the de-correlation of user preference from sentiment, TSPRA is able to evaluate a new concept critical aspects, defined as the product aspects seriously concerned by users but negatively commented in reviews. Improvement to such critical aspects could be most effective to enhance user experience."],["review of current state of the art in remote sensing based monitoring of forest disturbances and","Methods for Mapping Forest Disturbance and Degradation from Optical Earth Observation Data: a Review","summarize: Purpose of review: This paper presents a review of the current state of the art in remote sensing based monitoring of forest disturbances and forest degradation from optical Earth Observation data. Part one comprises an overview of currently available optical remote sensing sensors, which can be used for forest disturbance and degradation mapping. Part two reviews the two main categories of existing approaches: classical image-to-image change detection and time series analysis. Recent findings: With the launch of the Sentinel-2a satellite and available Landsat imagery, time series analysis has become the most promising but also most demanding category of degradation mapping approaches. Four time series classification methods are distinguished. The methods are explained and their benefits and drawbacks are discussed. A separate chapter presents a number of recent forest degradation mapping studies for two different ecosystems: temperate forests with a geographical focus on Europe and tropical forests with a geographical focus on Africa. Summary: The review revealed that a wide variety of methods for the detection of forest degradation is already available. Today, the main challenge is to transfer these approaches to high resolution time series data from multiple sensors. Future research should also focus on the classification of disturbance types and the development of robust up-scalable methods to enable near real time disturbance mapping in support of operational reactive measures."],["simulated galaxy groups and clusters have simulated gas properties. we analyse the effect of","Cosmological Simulation of Galaxy Groups and Clusters-I: Global Effect of Feedback from Active Galactic Nuclei","summarize: In this study we quantify the properties of the gas and dark matter around active galactic nuclei in simulated galaxy groups and clusters and analyze the effect of AGN feedback on the surrounding intra-cluster medium. Our results suggest downsizing of AGN luminosity with host halo mass, supporting the results obtained from clustering studies of AGN. By examining the temperature and density distribution of the gas in the vicinity of AGN we show that due to feedback from the central engine, the gas gets displaced from the centre of the group\/cluster resulting in a reduction of the density but an enhancement of temperature. We show that these effects are pronounced at both high and low redshifts and propose new observables to study the effect of feedback in higher redshift galaxies. We also show that the average stellar mass is decreased in halos in the presence of AGN feedback confirming claims from previous studies. Our work for the first time uses a fully cosmological-hydrodynamic simulation to evaluate the global effects of AGN feedback on their host dark matter halos as well as galaxies at scales of galaxy groups and clusters."],["automorphisms of maximum Salem degree 22 do not lift to any characteristic zero model","Dynamics on supersingular K3 surfaces and automorphisms of Salem degree 22","summarize: In this note we exhibit explicit automorphisms of maximal Salem degree 22 on the supersingular K3 surface of Artin invariant one for all primes p congruent 3 mod 4 in a systematic way. Automorphisms of Salem degree 22 do not lift to any characteristic zero model."],["slice Dirac operator over octonions involves a new theory of stem functions","Slice Dirac operator over octonions","summarize: The slice Dirac operator over octonions is a slice counterpart of the Dirac operator over quaternions. It involves a new theory of stem functions, which is the extension from the commutative "],["the profile development in the transition from hydraulically smooth to fully rough flow displays a propagating","Turbulence intensity and the friction factor for smooth- and rough-wall pipe flow","summarize: Turbulence intensity profiles are compared for smooth- and rough-wall pipe flow measurements made in the Princeton Superpipe. The profile development in the transition from hydraulically smooth to fully rough flow displays a propagating sequence from the pipe wall towards the pipe axis. The scaling of turbulence intensity with Reynolds number shows that the smooth- and rough wall level deviates with increasing Reynolds number. We quantify the correspondence between turbulence intensity and the friction factor."],["a method based on time-sensitive distant supervision exploits emerging entities. the method","Early Discovery of Emerging Entities in Microblogs","summarize: Keeping up to date on emerging entities that appear every day is indispensable for various applications, such as social-trend analysis and marketing research. Previous studies have attempted to detect unseen entities that are not registered in a particular knowledge base as emerging entities and consequently find non-emerging entities since the absence of entities in knowledge bases does not guarantee their emergence. We therefore introduce a novel task of discovering truly emerging entities when they have just been introduced to the public through microblogs and propose an effective method based on time-sensitive distant supervision, which exploits distinctive early-stage contexts of emerging entities. Experimental results with a large-scale Twitter archive show that the proposed method achieves 83.2% precision of the top 500 discovered emerging entities, which outperforms baselines based on unseen entity recognition with burst detection. Besides notable emerging entities, our method can discover massive long-tail and homographic emerging entities. An evaluation of relative recall shows that the method detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of them are discovered earlier than their registration in Wikipedia, and the average lead-time is more than one year ."],["theoretical work has derived optimal strategies for when to leave a patch.","Foraging as an evidence accumulation process","summarize: A canonical foraging task is the patch-leaving problem, in which a forager must decide to leave a current resource in search for another. Theoretical work has derived optimal strategies for when to leave a patch, and experiments have tested for conditions where animals do or do not follow an optimal strategy. Nevertheless, models of patch-leaving decisions do not consider the imperfect and noisy sampling process through which an animal gathers information, and how this process is constrained by neurobiological mechanisms. In this theoretical study, we formulate an evidence accumulation model of patch-leaving decisions where the animal averages over noisy measurements to estimate the state of the current patch and the overall environment. Evidence accumulation models belong to the class of drift diffusion processes and have been used to model decision making in different contexts. We solve the model for conditions where foraging decisions are optimal and equivalent to the marginal value theorem, and perform simulations to analyze deviations from optimal when these conditions are not met. By adjusting the drift rate and decision threshold, the model can represent different strategies, for example an increment-decrement or counting strategy. These strategies yield identical decisions in the limiting case but differ in how patch residence times adapt when the foraging environment is uncertain. To account for sub-optimal decisions, we introduce an energy-dependent utility function that predicts longer than optimal patch residence times when food is plentiful. Our model provides a quantitative connection between ecological models of foraging behavior and evidence accumulation models of decision making. Moreover, it provides a theoretical framework for potential experiments which seek to identify neural circuits underlying patch leaving decisions."],["we focus on the case when rewards are piecewise i.i.d. and","Distribution-dependent and Time-uniform Bounds for Piecewise i.i.d Bandits","summarize: We consider the setup of stochastic multi-armed bandits in the case when reward distributions are piecewise i.i.d. and bounded with unknown changepoints. We focus on the case when changes happen simultaneously on all arms, and in stark contrast with the existing literature, we target gap-dependent regret bounds involving the magnitude of changes "],["Lorentzian manifold is a time-oriented manifold.","Reconstruction of Lorentzian manifolds from boundary light observation sets","summarize: On a time-oriented Lorentzian manifold "],["the MapReduce is the most powerful programming paradigm in distributed computing. the enhancement of the","MapReduce Scheduler: A 360-degree view","summarize: Undoubtedly, the MapReduce is the most powerful programming paradigm in distributed computing. The enhancement of the MapReduce is essential and it can lead the computing faster. Therefore, here are many scheduling algorithms to discuss based on their characteristics. Moreover, there are many shortcoming to discover in this field. In this article, we present the state-of-the-art scheduling algorithm to enhance the understanding of the algorithms. The algorithms are presented systematically such that there can be many future possibilities in scheduling algorithm through this article. In this paper, we provide in-depth insight on the MapReduce scheduling algorithm. In addition, we discuss various issues of MapReduce scheduler developed for large-scale computing as well as heterogeneous environment."],["quadrature by expansion solves the problem by locally approximating the potential using","Adaptive quadrature by expansion for layer potential evaluation in two dimensions","summarize: When solving partial differential equations using boundary integral equation methods, accurate evaluation of singular and nearly singular integrals in layer potentials is crucial. A recent scheme for this is quadrature by expansion , which solves the problem by locally approximating the potential using a local expansion centered at some distance from the source boundary. In this paper we introduce an extension of the QBX scheme in 2D denoted AQBX - adaptive quadrature by expansion - which combines QBX with an algorithm for automated selection of parameters, based on a target error tolerance. A key component in this algorithm is the ability to accurately estimate the numerical errors in the coefficients of the expansion. Combining previous results for flat panels with a procedure for taking the panel shape into account, we derive such error estimates for arbitrarily shaped boundaries in 2D that are discretized using panel-based Gauss-Legendre quadrature. Applying our scheme to numerical solutions of Dirichlet problems for the Laplace and Helmholtz equations, and also for solving these equations, we find that the scheme is able to satisfy a given target tolerance to within an order of magnitude, making it useful for practical applications. This represents a significant simplification over the original QBX algorithm, in which choosing a good set of parameters can be hard."],["model generalizes mean-field electron-vibrational theory developed earlier. model","Study of Electron-Vibrational Interaction in Molecular Aggregates Using Mean-Field Theory: From Exciton Absorption and Luminescence to Exciton-Polariton Dispersion in Nanofibers","summarize: We have developed a model in order to account for electron-vibrational effects on absorption, luminescence of molecular aggregates and exciton-polaritons in nanofibers. The model generalizes the mean-field electron-vibrational theory developed by us earlier to the systems with spatial symmetry, exciton luminescence and the exciton-polaritons with spatial dispersion. The correspondence between manifestation of electron-vibrational interaction in monomers, molecular aggregates and exciton-polariton dispersion in nanofibers is obtained by introducing the aggregate line-shape functions in terms of the monomer line-shape functions. With the same description of material parameters we have calculated both the absorption and luminescence of molecular aggregates and the exciton-polariton dispersion in nanofibers. We apply the theory to experiment on fraction of a millimeter propagation of Frenkel exciton polaritons in photoexcited organic nanofibers made of thiacyanine dye."],["remote sensing image scene classification has broad applications in a range of fields. the paper","Remote Sensing Image Scene Classification Meets Deep Learning: Challenges, Methods, Benchmarks, and Opportunities","summarize: Remote sensing image scene classification, which aims at labeling remote sensing images with a set of semantic categories based on their contents, has broad applications in a range of fields. Propelled by the powerful feature learning capabilities of deep neural networks, remote sensing image scene classification driven by deep learning has drawn remarkable attention and achieved significant breakthroughs. However, to the best of our knowledge, a comprehensive review of recent achievements regarding deep learning for scene classification of remote sensing images is still lacking. Considering the rapid evolution of this field, this paper provides a systematic survey of deep learning methods for remote sensing image scene classification by covering more than 160 papers. To be specific, we discuss the main challenges of remote sensing image scene classification and survey Autoencoder-based remote sensing image scene classification methods, Convolutional Neural Network-based remote sensing image scene classification methods, and Generative Adversarial Network-based remote sensing image scene classification methods. In addition, we introduce the benchmarks used for remote sensing image scene classification and summarize the performance of more than two dozen of representative algorithms on three commonly-used benchmark data sets. Finally, we discuss the promising opportunities for further research."],["affine connections on manifolds are determined by the 1-connection form and the fundamental","Flat Affine Manifolds And Their Transformations","summarize: We give a characterization of flat affine connections on manifolds by means of a natural affine representation of the universal covering of the Lie group of diffeomorphisms preserving the connection. From the infinitesimal point of view, this representation is determined by the 1-connection form and the fundamental form of the bundle of linear frames of the manifold. We show that the group of affine transformations of a real flat affine "],["the priority of a non-atomic customer is a function of their class and waiting time","Lowest priority waiting time distribution in an accumulating priority L\\'evy queue","summarize: This paper presents a new method for deriving the waiting time distribution of the lowest class in an accumulating priority queue with positive L\\'evy input. The priority of a non-atomic customer is a function of their class and waiting time in the system, and the particles with the highest AP are the next to be processed. The method relies on the construction of a workload overtaking process and solving a first-passage time problem using an appropriate stopping time."],["first-principles calculations show that all compounds are very stable. most of compounds present","Atomically thin binary V-V compound semiconductor: a first-principles study","summarize: Searching the novel 2D semiconductor is crucial to develop the next-generation low-dimensional electronic device. Using first-principles calculations, we propose a class of unexplored binary V-V compound semiconductor with monolayer black phosphorene and blue phosphorene structure. Our phonon spectra and room-temperature molecular dynamics calculations indicate that all compounds are very stable. Moreover, most of compounds are found to present a moderate energy gap in the visible frequency range, which can be tuned gradually by in-plane strain. Especially, "],["branch star is the star of the asymptotic giant branch.","The extended molecular envelope of the asymptotic giant branch star ","summarize: The S-type asymptotic giant branch star "],["multiwavelength monitoring of the blazar 3C 279 observed a very bright,","Analyzing the December 2013 Orphan Gamma-Ray Flare From 3C 279","summarize: Multiwavelength monitoring of the blazar 3C 279 observed a very bright, 12-hour, orphan gamma-ray flare on 20 Dec 2013 with a uniquely hard Fermi-LAT spectrum and high Compton dominance. We work with a one-zone, leptonic model with both first- and second-order Fermi acceleration, which now reproduces the unique flaring behavior. We present a simplified analytic electron energy distribution to provide intuition about how particle acceleration shapes multi-wavelength blazar jet emission spectra. The contributions of individual processes in relativistic jets is fundamental to understanding the particle energy budget in the formation and propagation of astrophysical jets. We show that first- and second-order Fermi acceleration are sufficient to explain the flare, and that magnetic reconnection is not needed. Our analysis suggests that the flare is initiated by an increase in the particle energies due to shock acceleration, which also increases the stochastic acceleration. The higher energy particle preferentially occupy the outer jet, along the sheath, which decreases the apparent magnetic field and synchrotron radiation, while increasing electron exposure to the broad line region photon fields, driving up the external Compton emission."],["modular Politics would enable platform operators to build bottom-up governance processes from computational components that","Modular Politics: Toward a Governance Layer for Online Communities","summarize: Governance in online communities is an increasingly high-stakes challenge, and yet many basic features of offline governance legacies--juries, political parties, term limits, and formal debates, to name a few--are not in the feature-sets of the software most community platforms use. Drawing on the paradigm of Institutional Analysis and Development, this paper proposes a strategy for addressing this lapse by specifying basic features of a generalizable paradigm for online governance called Modular Politics. Whereas classical governance typologies tend to present a choice among wholesale ideologies, such as democracy or oligarchy, Modular Politics would enable platform operators and their users to build bottom-up governance processes from computational components that are modular and composable, highly versatile in their expressiveness, portable from one context to another, and interoperable across platforms. This kind of approach could implement pre-digital governance systems as well as accelerate innovation in uniquely digital techniques. As diverse communities share and connect their components and data, governance could occur through a ubiquitous network layer. To that end, this paper proposes the development of an open standard for networked governance."],["the thermodynamic properties of the XXZ spin chain are investigated. the contribution of","Surface energy and elementary excitations of the XXZ spin chain with arbitrary boundary fields","summarize: The thermodynamic properties of the XXZ spin chain with integrable open boundary conditions at the gaped region are investigated.It is shown that the contribution of the inhomogeneous term in the "],["a previously undiscovered extension of the radio continuum emission was found. the population of","Probing the magnetic field of the nearby galaxy pair Arp 269","summarize: We present a multiwavelength radio study of the nearby galaxy pair Arp 269 . High sensitivity to extended structures gained by using the merged interferometric and single- dish maps allowed us to reveal a previously undiscovered extension of the radio continuum emission. Its direction is significantly different from that of the neutral gas tail, suggesting that different physical processes might be involved in their creation. The population of radio- emitting electrons is generally young, signifying an ongoing, vigorous star formation -- this claim is supported by strong magnetic fields , similar to the ones found in much larger spiral galaxies. From the study of the spectral energy distribution, we conclude that the electron population in the intergalactic bridge between member galaxies originates from the disc areas, and therefore its age reflects the time-scale of the interaction. We have also discovered an angularly near compact steep source -- which is a member of a different galaxy pair -- at a redshift of approximately 0.125."],["we use the suite of AbacusCosmos Lamda-CDM simulations","Can Assembly Bias Explain the Lensing Amplitude of the BOSS CMASS Sample in a Planck Cosmology?","summarize: In this paper, we investigate whether galaxy assembly bias can reconcile the 20-40% disagreement between the observed galaxy projected clustering signal and the galaxy-galaxy lensing signal in the BOSS CMASS galaxy sample reported in Leauthaud et al. . We use the suite of AbacusCosmos Lamda-CDM simulations at Planck best-fit cosmology and two flexible implementations of extended halo occupation distribution models that incorporate galaxy assembly bias to build forward models and produce joint fits of the observed galaxy clustering signal and the galaxy-galaxy lensing signal. We find that our models using the standard HODs without any assembly bias generalizations continue to show a 20-40% over-prediction of the observed galaxy-galaxy lensing signal. We find that our implementations of galaxy assembly bias do not reconcile the two measurements at Planck best-fit cosmology. In fact, despite incorporating galaxy assembly bias, the satellite distribution parameter, and the satellite velocity bias parameter into our extended HOD model, our fits still strongly suggest a 31-34% discrepancy between the observed projected clustering and galaxy-galaxy lensing measurements. It remains to be seen whether a combination of other galaxy assembly bias models, alternative cosmological parameters, or baryonic effects can explain the amplitude difference between the two signals."],["the profile development in the transition from hydraulically smooth to fully rough flow displays a propagating","Turbulence intensity and the friction factor for smooth- and rough-wall pipe flow","summarize: Turbulence intensity profiles are compared for smooth- and rough-wall pipe flow measurements made in the Princeton Superpipe. The profile development in the transition from hydraulically smooth to fully rough flow displays a propagating sequence from the pipe wall towards the pipe axis. The scaling of turbulence intensity with Reynolds number shows that the smooth- and rough wall level deviates with increasing Reynolds number. We quantify the correspondence between turbulence intensity and the friction factor."],["a dynamo is possible with anisotropic electrical conductivity. the","Dynamo action in sliding plates of anisotropic electrical conductivity","summarize: With materials of anisotropic electrical conductivity, it is possible to generate a dynamo with a simple velocity field, of the type precluded by Cowling's theorems with isotropic materials. Following a previous study by Ruderman and Ruzmaikin who considered the dynamo effect induced by a uniform shear flow, we determine the conditions for the dynamo threshold when a solid plate is sliding over another one, both with anisotropic electrical conductivity. We obtain numerical solutions for a general class of anisotropy and obtain the conditions for the lowest magnetic Reynolds number, using a collocation Chebyshev method. In a particular geometry of anisotropy and wavenumber, we also derive an analytical solution, where the eigenvectors are just combinations of four exponential functions. An explicit analytical expression is obtained for the critical magnetic Reynold number. Above the critical magnetic Reynold number, we have also derived an analytical expression for the growth rate showing that this is a 'very fast' dynamo, extrapolating on the 'slow' and 'fast' terminology introduced by Vainshtein and Zeldovich ."],["the exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data","When Simpler Data Does Not Imply Less Information: A Study of User Profiling Scenarios with Constrained View of Mobile HTTP Traffic","summarize: The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services which are often free in exchange for the use of its customers' data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this paper, we unfold profiling scenarios around mobile HTTP traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in-the-wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights."],["circumbinary accretion discs are a problem in restricted three-body problem","A transition in circumbinary accretion discs at a binary mass ratio of 1:25","summarize: We study circumbinary accretion discs in the framework of the restricted three-body problem and via numerically solving the height-integrated equations of viscous hydrodynamics. Varying the mass ratio of the binary, we find a pronounced change in the behaviour of the disc near mass ratio "],["experiments show a massive acceleration of the annealing of a monolayer of passive","Activity-controlled Annealing of Colloidal Monolayers","summarize: Molecular motors are essential to the living, they generate additional fluctuations that boost transport and assist assembly. Self-propelled colloids, that consume energy to move, hold similar potential for the man-made assembly of microparticles. Yet, experiments showing their use as a powerhouse in materials science lack. Our work explores the design of man-made materials controlled by fluctuations, arising from the internal forces generated by active colloids. Here we show a massive acceleration of the annealing of a monolayer of passive beads by moderate addition of self-propelled microparticles. We rationalize our observations with a model of collisions that drive active fluctuations to overcome kinetic barriers and activate the annealing. The experiment is quantitatively compared with Brownian dynamic simulations that further unveil a dynamical transition in the mechanism of annealing. Active dopants travel uniformly in the system or co-localize at the grain boundaries as a result of the persistence of their motion. Our findings uncover the potential of man-made materials controlled by internal activity and lay the groundwork for the rise of materials science beyond equilibrium."],["social media is a cyber-security risk for every business. it could become a","Protect Against Unintentional Insider Threats: The risk of an employee's cyber misconduct on a Social Media Site","summarize: Social Media is a cyber-security risk for every business. What do people share on the Internet? Almost everything about oneself is shared: friendship, demographics, family, activities, and work-related information. This could become a potential risk in every business if the organization's policies, training and technology fail to properly address these issues. In many cases, it is the employees' behaviour that can put key company information at danger. Social media has turned into a reconnaissance tool for malicious actors and users accounts are now seen as a goldmine for cyber criminals. Investigation of Social Media is in the embryonic stage and thus, is not yet well understood. This research project aims to collect and analyse open-source data from LinkedIn, discover data leakage and analyse personality types through software as a service . The final aim of the study is to understand if there are behavioral factors that can predicting one's attitude toward disclosing sensitive data."],["online learning algorithms update models via one sample per iteration. this is useful to detect","Dual Averaging Method for Online Graph-structured Sparsity","summarize: Online learning algorithms update models via one sample per iteration, thus efficient to process large-scale datasets and useful to detect malicious events for social benefits, such as disease outbreak and traffic congestion on the fly. However, existing algorithms for graph-structured models focused on the offline setting and the least square loss, incapable for online setting, while methods designed for online setting cannot be directly applied to the problem of complex graph-structured sparsity model. To address these limitations, in this paper we propose a new algorithm for graph-structured sparsity constraint problems under online setting, which we call \\textsc. The key part in \\textsc is to project both averaging gradient and primal variables onto lower dimensional subspaces, thus capturing the graph-structured sparsity effectively. Furthermore, the objective functions assumed here are generally convex so as to handle different losses for online learning settings. To the best of our knowledge, \\textsc is the first online learning algorithm for graph-structure constrained optimization problems. To validate our method, we conduct extensive experiments on both benchmark graph and real-world graph datasets. Our experiment results show that, compared to other baseline methods, \\textsc not only improves classification performance, but also successfully captures graph-structured features more effectively, hence stronger interpretability."],["MCTS is one of the most widely used methods for planning. it is used to","Static and Dynamic Values of Computation in MCTS","summarize: Monte-Carlo Tree Search is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the myopic limitations of existing computation-value-based methods in two senses: we are able to account for the impact of non-immediate computations on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art."],["spin-transfer-torque random access memory is a promising candidate. the","Write error rate of spin-transfer-torque random access memory including micromagnetic effects using rare event enhancement","summarize: Spin-transfer-torque random access memory is a promising candidate for the next-generation of random-access-memory due to improved scalability, read-write speeds and endurance. However, the write pulse duration must be long enough to ensure a low write error rate , the probability that a bit will remain unswitched after the write pulse is turned off, in the presence of stochastic thermal effects. WERs on the scale of 10"],["paper is concerned with the existence and uniqueness of the solutions of nonlinear impulsive","Analysis of Impulsive ","summarize: This paper is concerned with the existence and uniqueness, and Ulam--Hyers stabilities of solutions of nonlinear impulsive "],["spin-transfer-torque random access memory is a promising candidate. the","Write error rate of spin-transfer-torque random access memory including micromagnetic effects using rare event enhancement","summarize: Spin-transfer-torque random access memory is a promising candidate for the next-generation of random-access-memory due to improved scalability, read-write speeds and endurance. However, the write pulse duration must be long enough to ensure a low write error rate , the probability that a bit will remain unswitched after the write pulse is turned off, in the presence of stochastic thermal effects. WERs on the scale of 10"],["the numerical range of a Hilbert-space operator is a numerical range.","Remarks on the Crouzeix-Palencia proof that the numerical range is a ","summarize: Crouzeix and Palencia recently showed that the numerical range of a Hilbert-space operator is a "],["this research is a school action research conducted for four months starting in October 2018 to January 2019","Application of the Among Leadership Model to Improve Teacher Work Discipline","summarize: This study aims to improve teacher work discipline related to their duties as classroom teachers. This research is a school action research conducted for four months starting in October 2018 to January 2019 at SDN 11 Simpang Rimba, South Bangka, Indonesia in the 2018\/2019 academic year. The data collection instrument used observation sheets and documentation. Data analysis used quantitative data analysis techniques with descriptive statistics. The results of this study indicate that the application of the Among leadership model can improve teacher work discipline in the aspects of arriving on time, work hours fulfillment, and prepare lesson plans."],["quadrature by expansion solves the problem by locally approximating the potential using","Adaptive quadrature by expansion for layer potential evaluation in two dimensions","summarize: When solving partial differential equations using boundary integral equation methods, accurate evaluation of singular and nearly singular integrals in layer potentials is crucial. A recent scheme for this is quadrature by expansion , which solves the problem by locally approximating the potential using a local expansion centered at some distance from the source boundary. In this paper we introduce an extension of the QBX scheme in 2D denoted AQBX - adaptive quadrature by expansion - which combines QBX with an algorithm for automated selection of parameters, based on a target error tolerance. A key component in this algorithm is the ability to accurately estimate the numerical errors in the coefficients of the expansion. Combining previous results for flat panels with a procedure for taking the panel shape into account, we derive such error estimates for arbitrarily shaped boundaries in 2D that are discretized using panel-based Gauss-Legendre quadrature. Applying our scheme to numerical solutions of Dirichlet problems for the Laplace and Helmholtz equations, and also for solving these equations, we find that the scheme is able to satisfy a given target tolerance to within an order of magnitude, making it useful for practical applications. This represents a significant simplification over the original QBX algorithm, in which choosing a good set of parameters can be hard."],["deep neural networks have been quite successful in solving complex learning problems. but the complex learning parameters","Compressed Learning of Deep Neural Networks for OpenCL-Capable Embedded Systems","summarize: Deep neural networks have been quite successful in solving many complex learning problems. However, DNNs tend to have a large number of learning parameters, leading to a large memory and computation requirement. In this paper, we propose a model compression framework for efficient training and inference of deep neural networks on embedded systems. Our framework provides data structures and kernels for OpenCL-based parallel forward and backward computation in a compressed form. In particular, our method learns sparse representations of parameters using "],["a method for analyzing chemical bonds and their energy distributions in a two-dimensional","Observing the 3D chemical bond and its energy distribution in a projected space","summarize: Our curiosity-driven desire to see chemical bonds dates back at least one-hundred years, perhaps to antiquity. Sweeping improvements in the accuracy of measured and predicted electron charge densities, alongside our largely bondcentric understanding of molecules and materials, heighten this desire with means and significance. Here we present a method for analyzing chemical bonds and their energy distributions in a two-dimensional projected space called the condensed charge density. Bond silhouettes in the condensed charge density can be reverse-projected to reveal precise three-dimensional bonding regions we call bond bundles. We show that delocalized metallic bonds and organic covalent bonds alike can be objectively analyzed, the formation of bonds observed, and that the crystallographic structure of simple metals can be rationalized in terms of bond bundle structure. Our method also reproduces the expected results of organic chemistry, enabling the recontextualization of existing bond models from a charge density perspective."],["distributed speech recognition requires to aggregate outputs of distributed deep neural network -based models.","Submodular Rank Aggregation on Score-based Permutations for Distributed Automatic Speech Recognition","summarize: Distributed automatic speech recognition requires to aggregate outputs of distributed deep neural network -based models. This work studies the use of submodular functions to design a rank aggregation on score-based permutations, which can be used for distributed ASR systems in both supervised and unsupervised modes. Specifically, we compose an aggregation rank function based on the Lovasz Bregman divergence for setting up linear structured convex and nested structured concave functions. The algorithm is based on stochastic gradient descent and can obtain well-trained aggregation models. Our experiments on the distributed ASR system show that the submodular rank aggregation can obtain higher speech recognition accuracy than traditional aggregation methods like Adaboost. Code is available online~\\footnote."],["research output from India is covered in social media platforms. only 28.5% of the total","How much research output from India gets social media attention?","summarize: Scholarly articles are now increasingly being mentioned and discussed in social media platforms, sometimes even as pre- or post-print version uploads. Measures of social media mentions and coverage are now emerging as an alternative indicator of impact of scholarly articles. This article aims to explore how much scholarly research output from India is covered in different social media platforms, and how similar or different it is from the world average. It also analyses the discipline-wise variations in coverage and altmetric attention for Indian research output, including a comparison with the world average. Results obtained show interesting patterns. Only 28.5% of the total research output from India is covered in social media platforms, which is about 18% less than the world average. ResearchGate and Mendeley are the most popular social media platforms in India for scholarly article coverage. In terms of discipline-wise variation, medical sciences and biological sciences have relatively higher coverage across different platforms compared to disciplines like information science and engineering."],["new approach leads to giant gain enhancement in a Fabry-Perot cavity","Giant Gain Enhancement in Photonic Crystals with a Degenerate Band Edge","summarize: We propose a new approach leading to giant gain enhancement. It is based on unconventional slow wave resonance associated to a degenerate band edge in the dispersion diagram for a special class of photonic crystals supporting two modes at each frequency. We show that the gain enhancement in a Fabry-Perot cavity when operating at the DBE is several orders of magnitude stronger when compared to a cavity of the same length made of a standard photonic crystal with a regular band edge . The giant gain condition is explained by a significant increase in the photon lifetime and in the local density of states. We have demonstrated the existence of DBE operated special cavities that provide for superior gain conditions for solid-state lasers, quantum cascade lasers, traveling wave tubes, and distributed solid state amplifiers. We also report the possibility to achieve low-threshold lasing in FPC with DBE compared to RBE-based lasers."],["Glioma heterogeneous nature makes segmentation difficult. a","3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction","summarize: Glioma, the malignant brain tumor, requires immediate treatment to improve the survival of patients. Gliomas heterogeneous nature makes the segmentation difficult, especially for sub-regions like necrosis, enhancing tumor, non-enhancing tumor, and Edema. Deep neural networks like full convolution neural networks and ensemble of fully convolution neural networks are successful for Glioma segmentation. The paper demonstrates the use of a 3D fully convolution neural network with a three layer encoder decoder approach for layer arrangement. The encoder blocks include the dense modules, and decoder blocks include convolution modules. The input to the network is 3D patches. The loss function combines dice loss and focal loss functions. The validation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing tumor, whole tumor, and tumor core, respectively. The Random Forest Regressor uses shape, volumetric, and age features extracted from ground truth for overall survival prediction. The regressor achieves an accuracy of 44.8% on the validation set."],["we experimentally quantify the Raman scattering from individual carbyne chains confined in","Raman Scattering Cross Section of Confined Carbyne","summarize: We experimentally quantify the Raman scattering from individual carbyne chains confined in double-walled carbon nanotubes. We find that the resonant differential Raman cross section of confined carbyne is on the order of "],["the original minimal path model computes the globally minimal geodesic by solving an Eikonal","Global Minimum for a Finsler Elastica Minimal Path Approach","summarize: In this paper, we propose a novel curvature-penalized minimal path model via an orientation-lifted Finsler metric and the Euler elastica curve. The original minimal path model computes the globally minimal geodesic by solving an Eikonal partial differential equation . Essentially, this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models. To solve this problem, we present an Eikonal PDE-based Finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem. We were successful at adding the curvature penalization to the classical geodesic energy. The basic idea of this work is to interpret the Euler elastica bending energy via a novel Finsler elastica metric that embeds a curvature penalty. This metric is non-Riemannian, anisotropic and asymmetric, and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension. Based on this orientation lifting, the proposed minimal path model can benefit from both the curvature and orientation of the paths. Thanks to the fast marching method, the global minimum of the curvature-penalized geodesic energy can be computed efficiently. We introduce two anisotropic image data-driven speed functions that are computed by steerable filters. Based on these orientation-dependent speed functions, we can apply the proposed Finsler elastica minimal path model to the applications of closed contour detection, perceptual grouping and tubular structure extraction. Numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results."],["the same degree of control has not yet been achieved for few-cycle extreme ultraviolet pulses generated","Phase Control of Attosecond Pulses in a Train","summarize: Ultrafast processes in matter can be captured and even controlled by using sequences of few-cycle optical pulses, which need to be well characterized, both in amplitude and phase. The same degree of control has not yet been achieved for few-cycle extreme ultraviolet pulses generated by high-order harmonic generation in gases, with duration in the attosecond range. Here, we show that by varying the spectral phase and carrier-envelope phase of a high-repetition rate laser, using dispersion in glass, we achieve a high degree of control of the relative phase and CEP between consecutive attosecond pulses. The experimental results are supported by a detailed theoretical analysis based upon the semiclassical three-step model for high-order harmonic generation."],["a basic susceptible-infected-susceptible model is based on","Heterogeneous Population Dynamics and Scaling Laws near Epidemic Outbreaks","summarize: In this paper, we focus on the influence of heterogeneity and stochasticity of the population on the dynamical structure of a basic susceptible-infected-susceptible model. First we prove that, upon a suitable mathematical reformulation of the basic reproduction number, the homogeneous system and the heterogeneous system exhibit a completely analogous global behaviour. Then we consider noise terms to incorporate the fluctuation effects and the random import of the disease into the population and analyse the influence of heterogeneity on warning signs for critical transitions . This theory shows that one may be able to anticipate whether a bifurcation point is close before it happens. We use numerical simulations of a stochastic fast-slow heterogeneous population SIS model and show various aspects of heterogeneity have crucial influences on the scaling laws that are used as early-warning signs for the homogeneous system. Thus, although the basic structural qualitative dynamical properties are the same for both systems, the quantitative features for epidemic prediction are expected to change and care has to be taken to interpret potential warning signs for disease outbreaks correctly."],["multiple Schramm-Loewner Evolutions are conformally invariant random","Pure partition functions of multiple SLEs","summarize: Multiple Schramm-Loewner Evolutions are conformally invariant random processes of several curves, whose construction by growth processes relies on partition functions: M\\obius covariant solutions to a system of second order partial differential equations. In this article, we use a quantum group technique to construct a distinguished basis of solutions, which conjecturally correspond to the extremal points of the convex set of probability measures of multiple SLEs."],["the project is to focus on applications of muography. the telescope may have to be operated","A portable muon telescope based on small and gas-tight Resistive Plate Chambers","summarize: We report on the first steps in the development of a small-size muon telescope based on glass Resistive Plate Chambers with small active area . The long-term goal of this project is to focus on applications of muography where the telescope may have to be operated underground and\/or inside small rooms, and in challenging logistic situations. Driving principles in our design are therefore compact size, light weight, gas tightness, and robustness. The first data-taking experiences have been encouraging, and we elaborate on the lessons learnt and future directions for development."],["we discuss software design issues related to the development of parallel computational intelligence algorithms on multi-core CPU","Probabilistic Graphical Models on Multi-Core CPUs using Java 8","summarize: In this paper, we discuss software design issues related to the development of parallel computational intelligence algorithms on multi-core CPUs, using the new Java 8 functional programming features. In particular, we focus on probabilistic graphical models and present the parallelisation of a collection of algorithms that deal with inference and learning of PGMs from data. Namely, maximum likelihood estimation, importance sampling, and greedy search for solving combinatorial optimisation problems. Through these concrete examples, we tackle the problem of defining efficient data structures for PGMs and parallel processing of same-size batches of data sets using Java 8 features. We also provide straightforward techniques to code parallel algorithms that seamlessly exploit multi-core processors. The experimental analysis, carried out using our open source AMIDST Java toolbox, shows the merits of the proposed solutions."],["model update is formulated as an online learning problem where a target model is learned over the","Learning to Update for Object Tracking with Recurrent Meta-learner","summarize: Model update lies at the heart of object tracking. Generally, model update is formulated as an online learning problem where a target model is learned over the online training set. Our key innovation is to \\emph, i.e., \\emph. The learned updater takes as input the online training set and outputs an updated target model. As a first attempt, we design the learned updater based on recurrent neural networks and demonstrate its application in a template-based tracker and a correlation filter-based tracker. Our learned updater consistently improves the base trackers and runs faster than realtime on GPU while requiring small memory footprint during testing. Experiments on standard benchmarks demonstrate that our learned updater outperforms commonly used update baselines including the efficient exponential moving average -based update and the well-designed stochastic gradient descent -based update. Equipped with our learned updater, the template-based tracker achieves state-of-the-art performance among realtime trackers on GPU."],["paper deals with the problem of the classification of the local graded artinian quotient","Hilbert-Samuel sequences of homogeneous finite type","summarize: This paper deals with the problem of the classification of the local graded Artinian quotients "],["cooperation in scale-free networks is a key evolutionary strategy for a non-iter","Invasion of cooperation in scale-free networks: Accumulated vs. average payoffs","summarize: It is well known that cooperation cannot be an evolutionary stable strategy for a non-iterative game in a well-mixed population. In contrast, structured populations favor cooperation since cooperators can benefit each other by forming local clusters. Previous studies have shown that scale-free networks strongly promote cooperation. However, little is known about the invasion mechanism of cooperation in scale-free networks. To study microscopic and macroscopic behaviors of cooperators' invasion, we conducted computational experiments of the evolution of cooperation in scale-free networks where, starting from all defectors, cooperators can spontaneously emerge by mutation. Since the evolutionary dynamics are influenced by the definition of fitness, we tested two commonly adopted fitness functions: accumulated payoff and average payoff. Simulation results show that cooperation is strongly enhanced with the accumulated payoff fitness compared to the average payoff fitness. However, the difference between the two functions decreases as the average degree increases. As the average degree increases, cooperation decreases with the accumulated payoff fitness, while it increases with the average payoff fitness. Moreover, with the average payoff fitness, low-degree nodes play a more important role in spreading cooperative strategies compared to the case of the accumulated payoff fitness."],["high-throughput computational screening has emerged as a critical component of materials discovery.","Predicting Electronic Structure Properties of Transition Metal Complexes with Neural Networks","summarize: High-throughput computational screening has emerged as a critical component of materials discovery. Direct density functional theory simulation of inorganic materials and molecular transition metal complexes is often used to describe subtle trends in inorganic bonding and spin-state ordering, but these calculations are computationally costly and properties are sensitive to the exchange-correlation functional employed. To begin to overcome these challenges, we trained artificial neural networks to predict quantum-mechanically-derived properties, including spin-state ordering, sensitivity to Hartree-Fock exchange, and spin- state specific bond lengths in transition metal complexes. Our ANN is trained on a small set of inorganic-chemistry-appropriate empirical inputs that are both maximally transferable and do not require precise three-dimensional structural information for prediction. Using these descriptors, our ANN predicts spin-state splittings of single-site transition metal complexes at arbitrary amounts of Hartree-Fock exchange to within 3 kcal\/mol accuracy of DFT calculations. Our exchange-sensitivity ANN enables improved predictions on a diverse test set of experimentally-characterized transition metal complexes by extrapolation from semi-local DFT to hybrid DFT. The ANN also outperforms other machine learning models , demonstrating particularly improved performance in transferability, as measured by prediction errors on the diverse test set. We establish the value of new uncertainty quantification tools to estimate ANN prediction uncertainty in computational chemistry, and we provide additional heuristics for identification of when a compound of interest is likely to be poorly predicted by the ANN."],["a stacked denoising autoencoder based BR method is proposed","Segmented and Non-Segmented Stacked Denoising Autoencoder for Hyperspectral Band Reduction","summarize: Hyperspectral image analysis often requires selecting the most informative bands instead of processing the whole data without losing the key information. Existing band reduction methods have the capability to reveal the nonlinear properties exhibited in the data but at the expense of loosing its original representation. To cope with the said issue, an unsupervised non-linear segmented and non-segmented stacked denoising autoencoder based BR method is proposed. Our aim is to find an optimal mapping and construct a lower-dimensional space that has a similar structure to the original data with least reconstruction error. The proposed method first confronts the original hyperspectral data into smaller regions in a spatial domain and then each region is processed by UDAE individually. This results in reduced complexity and improved efficiency of BR for both semi-supervised and unsupervised tasks, i.e. classification and clustering. Our experiments on publicly available hyperspectral datasets with various types of classifiers demonstrate the effectiveness of UDAE method which equates favorably with other state-of-the-art dimensionality reduction and BR methods."],["wholeness is defined as a mathematical structure of physical space in our surroundings. but there","A Recursive Definition of Goodness of Space for Bridging the Concepts of Space and Place for Sustainability","summarize: Conceived and developed by Christopher Alexander through his life's work: The Nature of Order, wholeness is defined as a mathematical structure of physical space in our surroundings. Yet, there was no mathematics, as Alexander admitted then, that was powerful enough to capture his notion of wholeness. Recently, a mathematical model of wholeness, together with its topological representation, has been developed that is capable of addressing not only why a space is good, but also how much goodness the space has. This paper develops a structural perspective on goodness of space - both large- and small-scale - in order to bridge two basic concepts of space and place through the very concept of wholeness. The wholeness provides a de facto recursive definition of goodness of space from a holistic and organic point of view. A space is good, genuinely and objectively, if its adjacent spaces are good, the larger space to which it belongs is good, and what is contained in the space is also good. Eventually, goodness of space - sustainability of space - is considered a matter of fact rather than of opinion under the new view of space: space is neither lifeless nor neutral, but a living structure capable of being more living or less living, or more sustainable or less sustainable. Under the new view of space, geography or architecture will become part of complexity science, not only for understanding complexity, but also for making and remaking complex or living structures. Keywords: Scaling law, head\/tail breaks, living structure, beauty, streets, cities"],["post-Lie Magnus expansion is a post-Lie algebra. the results are then","Post-Lie Algebras, Factorization Theorems and Isospectral-Flows","summarize: In these notes we review and further explore the Lie enveloping algebra of a post-Lie algebra. From a Hopf algebra point of view, one of the central results, which will be recalled in detail, is the existence of a second Hopf algebra structure. By comparing group-like elements in suitable completions of these two Hopf algebras, we derive a particular map which we dub post-Lie Magnus expansion. These results are then considered in the case of Semenov-Tian-Shansky's double Lie algebra, where a post-Lie algebra is defined in terms of solutions of modified classical Yang-Baxter equation. In this context, we prove a factorization theorem for group-like elements. An explicit exponential solution of the corresponding Lie bracket flow is presented, which is based on the aforementioned post-Lie Magnus expansion."],["the x-vector system is the low-rank factorized version of the x","Compact Speaker Embedding: lrx-vector","summarize: Deep neural networks have recently been widely used in speaker recognition systems, achieving state-of-the-art performance on various benchmarks. The x-vector architecture is especially popular in this research community, due to its excellent performance and manageable computational complexity. In this paper, we present the lrx-vector system, which is the low-rank factorized version of the x-vector embedding network. The primary objective of this topology is to further reduce the memory requirement of the speaker recognition system. We discuss the deployment of knowledge distillation for training the lrx-vector system and compare against low-rank factorization with SVD. On the VOiCES 2019 far-field corpus we were able to reduce the weights by 28% compared to the full-rank x-vector system while keeping the recognition rate constant ."],["we use tunable, vacuum ultraviolet laser-based angle-resolved photoe","Fragility of Fermi arcs in Dirac semimetals","summarize: We use tunable, vacuum ultraviolet laser-based angle-resolved photoemission spectroscopy and density functional theory calculations to study the electronic properties of Dirac semimetal candidate cubic PtBi"],["we define non-commutative versions of the vertex packing polytope, thet","Sandwich theorems and capacity bounds for non-commutative graphs","summarize: We define non-commutative versions of the vertex packing polytope, the theta convex body and the fractional vertex packing polytope of a graph, and establish a quantum version of the Sandwich Theorem of Gr\\tschel, Lov\\'sz and Schrijver. We define new non-commutative versions of the Lov\\'sz number of a graph which lead to an upper bound of the zero-error capacity of the corresponding quantum channel that can be genuinely better than the one established previously by Duan, Severini and Winter. We define non-commutative counterparts of widely used classical graph parameters and establish their interrelation."],["a system that participates in the shared task of Hate Speech Detection on","Hate Speech Detection on Vietnamese Social Media Text using the Bidirectional-LSTM Model","summarize: In this paper, we describe our system which participates in the shared task of Hate Speech Detection on Social Networks of VLSP 2019 evaluation campaign. We are provided with the pre-labeled dataset and an unlabeled dataset for social media comments or posts. Our mission is to pre-process and build machine learning models to classify comments\/posts. In this report, we use Bidirectional Long Short-Term Memory to build the model that can predict labels for social media text according to Clean, Offensive, Hate. With this system, we achieve comparative results with 71.43% on the public standard test set of VLSP 2019."],["the space-time fractional wave equation is based on the space-time additive noise","Galerkin Finite Element Approximations for Stochastic Space-Time Fractional Wave Equations","summarize: The traditional wave equation models wave propagation in an ideal conducting medium. For characterizing the wave propagation in inhomogeneous media with frequency dependent power-law attenuation, the space-time fractional wave equation appears; further incorporating the additive white Gaussian noise coming from many natural sources leads to the stochastic space-time fractional wave equation. This paper discusses the Galerkin finite element approximations for the stochastic space-time fractional wave equation forced by an additive space-time white noise. We firstly discretize the space-time additive noise, which introduces a modeling error and results in a regularized stochastic space-time fractional wave equation; then the regularity of the regularized equation is analyzed. For the discretization in space, the finite element approximation is used and the definition of the discrete fractional Laplacian is introduced. We derive the mean-squared "],["Using the machinery of weak fibration categories due to Schlank and the first author, we","Model structure on projective systems of ","summarize: Using the machinery of weak fibration categories due to Schlank and the first author, we construct a convenient model structure on the pro-category of separable "],["the coupled discretization treats the coupled system of deformation and flow directly. the discretization","Stable cell-centered finite volume discretization for Biot equations","summarize: In this paper we discuss a new discretization for the Biot equations. The discretization treats the coupled system of deformation and flow directly, as opposed to combining discretizations for the two separate sub-problems. The coupled discretization has the following key properties, the combination of which is novel: 1) The variables for the pressure and displacement are co-located, and are as sparse as possible . 2) With locally computable restrictions on grid types, the discretization is stable with respect to the limits of incompressible fluid and small time-steps. 3) No artificial stabilization term has been introduced. Furthermore, due to the finite volume structure embedded in the discretization, explicit local expressions for both momentum-balancing forces as well as mass-conservative fluid fluxes are available. We prove stability of the proposed method with respect to all relevant limits. Together with consistency, this proves convergence of the method. Finally, we give numerical examples verifying both the analysis and convergence of the method."],["we study the leading coefficient in the formula asymptotical formula. we study","Generic asymptotics of resonance counting function for Schr\\odinger point interactions","summarize: We study the leading coefficient in the asymptotical formula "],["an equivalent circuit model is proposed to describe current density-voltage characteristics of practical perovs","Loss Mechanism Analyses of Perovskite Solar Cells with an Equivalent Circuit Model","summarize: Understanding and quantifying the main loss factors affecting the power conversion efficiency of perovskite solar cells are urgently needed. In this work, based on semiconductor physics, the expressions of bulk and surface recombination currents are analytically derived. Then taking the optical loss, series and shunt resistance losses, and bulk and surface recombination losses into consideration, an equivalent circuit model is proposed to describe the current density-voltage characteristics of practical perovskite solar cells. Furthermore, by comparing to the drift-diffusion model, the pre-defined physical parameters of the drift-diffusion model well agree with the fitting parameters retrieved by the equivalent circuit model, which verifies the reliability of the proposed model. Moreover, when the circuit model is applied to analyze experimental results, the fitting outcomes show favorable consistency to the physical investigations offered by the experiments. And the relative fitting errors of the above cases are all less than 2%. Through employing the model, the dominant recombination type is clearly identified and split current density-voltage curves characterizing different loss mechanisms are offered, which intuitively reveals the physical principles of efficiency loss. Additionally, through calculating the efficiency loss ratios under the open-circuit voltage condition, quantifying the above-mentioned loss mechanisms becomes simple and compelling. Consequently, this model offers a guideline to approach the efficiency limit from a circuit-level perspective. And the model is a comprehensive simulation and analysis tool for understanding the device physics of perovskite solar cells."],["for any countable homogeneous ordered graph, we show that for any countable homo","Conjugacy for homogeneous ordered graphs","summarize: We show that for any countable homogeneous ordered graph "],["overlapping temporal windows are represented by a Fisher vector. features from each window are","Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors","summarize: We propose a hierarchical approach to multi-action recognition that performs joint classification and segmentation. A given video is processed via a sequence of overlapping temporal windows. Each frame in a temporal window is represented through selective low-level spatio-temporal features which efficiently capture relevant local dynamics. Features from each window are represented as a Fisher vector, which captures first and second order statistics. Instead of directly classifying each Fisher vector, it is converted into a vector of class probabilities. The final classification decision for each frame is then obtained by integrating the class probabilities at the frame level, which exploits the overlapping of the temporal windows. Experiments were performed on two datasets: s-KTH , and the challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an accuracy of 85.0%, significantly outperforming two recent approaches based on GMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, the proposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMM approaches which obtained 33.7% and 38.4%, respectively. Furthermore, the proposed system is on average 40 times faster than the GMM based approach."],["proposed formulation is consistent with reservoir-type models of the MFD literature. proposed multi-","A Continuum Model for Cities Based on the Macroscopic Fundamental Diagram: a Semi-Lagrangian Solution Method","summarize: This paper presents a formulation of the reactive dynamic user equilibrium problem in continuum form using a network-level Macroscopic Fundamental Diagram . Compared to existing continuum models for cities -- all based in Hughes' pedestrian model in 2002 -- the proposed formulation is consistent with reservoir-type models of the MFD literature, shedding some light into the connection between these two modeling approaches, can have destinations continuously distributed on the region, and can incorporate multi-commodity flows without additional numerical error. The proposed multi-reservoir numerical solution method treats the multi-commodity component of the model in Lagrangian coordinates, which is the natural representation to propagate origin-destination information through the traffic stream. Fluxes between reservoir boundaries are computed in the Eulerian representation, and are used to calculate the speed of vehicles crossing the boundary. Simple examples are included that show the convergence of the model and its agreements with the available analytical solutions. We find that when origins and destinations are uniformly distributed in a region, the distribution of the travel times can be approximated analytically, the magnitude of the detours from the optimal free-flow route due to congestion increase linearly with the inflow and decreases with the square of the speed, and the total delay of vehicles in the network converges to the analytical approximation when the size of reservoirs tends to zero."],["solution takes advantage of a nonparametric representation of the stochastic kernel. it","Model-Free Stochastic Reachability Using Kernel Distribution Embeddings","summarize: We present a solution to the terminal-hitting stochastic reach-avoid problem for a Markov control process. This solution takes advantage of a nonparametric representation of the stochastic kernel as a conditional distribution embedding within a reproducing kernel Hilbert space . Because the disturbance is modeled as a data-driven stochastic process, this representation avoids intractable integrals in the dynamic recursion of the reach-avoid problem since the expectations can be calculated as an inner product within the RKHS. We demonstrate this approach on a high-dimensional chain of integrators and on Clohessy-Wiltshire-Hill dynamics."],["a graph of the graph shows the sex of the sex of the","The vertex Folkman numbers ","summarize: For a graph "],["entropy solutions are uniformly bounded with respect to space and time variables","Large time behavior of entropy solutions to one-dimensional unipolar hydrodynamic model for semiconductor devices","summarize: We are concerned with the global existence and large time behavior of entropy solutions to the one dimensional unipolar hydrodynamic model for semiconductors in the form of Euler-Poisson equations in a bounded interval. In this paper, we first prove the global existence of entropy solution by vanishing viscosity and compensated compactness framework. In particular, the solutions are uniformly bounded with respect to space and time variables by introducing modified Riemann invariants and the theory of invariant region. Based on the uniform estimates of density, we further show that the entropy solution converges to the corresponding unique stationary solution exponentially in time. No any smallness condition is assumed on the initial data and doping profile. Moreover, the novelty in this paper is about the unform bound with respect to time for the weak solutions of the isentropic Euler-Possion system."],["a method based on time-sensitive distant supervision exploits emerging entities. the method","Early Discovery of Emerging Entities in Microblogs","summarize: Keeping up to date on emerging entities that appear every day is indispensable for various applications, such as social-trend analysis and marketing research. Previous studies have attempted to detect unseen entities that are not registered in a particular knowledge base as emerging entities and consequently find non-emerging entities since the absence of entities in knowledge bases does not guarantee their emergence. We therefore introduce a novel task of discovering truly emerging entities when they have just been introduced to the public through microblogs and propose an effective method based on time-sensitive distant supervision, which exploits distinctive early-stage contexts of emerging entities. Experimental results with a large-scale Twitter archive show that the proposed method achieves 83.2% precision of the top 500 discovered emerging entities, which outperforms baselines based on unseen entity recognition with burst detection. Besides notable emerging entities, our method can discover massive long-tail and homographic emerging entities. An evaluation of relative recall shows that the method detects 80.4% emerging entities newly registered in Wikipedia; 92.4% of them are discovered earlier than their registration in Wikipedia, and the average lead-time is more than one year ."],["the sign structure of the Jacobi matrix carries the information about which components of the network inhibit","Representing Model Ensembles as Boolean Functions","summarize: Families of ODE models characterized by a common sign structure of their Jacobi matrix are investigated within the formalism of qualitative differential equations. In the context of regulatory networks the sign structure of the Jacobi matrix carries the information about which components of the network inhibit or activate each other. Information about constraints on the behavior of models in this family is stored in a so called qualitative state transition graph. We showed previously that a similar approach can be used to analyze a model pool of Boolean functions characterized by a common interaction graph. Here we show that the opposite approach is fruitful as well. We show that the qualitative state transition graph can be reduced to a skeleton represented by a Boolean function conserving the reachability properties. This reduction has the advantage that approaches such as model checking and network inference methods can be applied to the skeleton within the framework of Boolean networks. Furthermore, our work constitutes an alternative to approach to link Boolean networks and differential equations."],["acyclic graphs are a class of graphs. a t","Identifying causal effects in maximally oriented partially directed acyclic graphs","summarize: We develop a necessary and sufficient causal identification criterion for maximally oriented partially directed acyclic graphs . MPDAGs as a class of graphs include directed acyclic graphs , completed partially directed acyclic graphs , and CPDAGs with added background knowledge. As such, they represent the type of graph that can be learned from observational data and background knowledge under the assumption of no latent variables. Our identification criterion can be seen as a generalization of the g-formula of Robins . We further obtain a generalization of the truncated factorization formula and compare our criterion to the generalized adjustment criterion of Perkovi\\'c et al. which is sufficient, but not necessary for causal identification."],["adaptive optics has been employed to compensate for aberrations. but it only applies to","Distortion matrix concept for deep optical imaging in scattering media","summarize: In optical imaging, light propagation is affected by the inhomogeneities of the medium. Sample-induced aberrations and multiple scattering can strongly degrade the image resolution and contrast. Based on a dynamic correction of the incident and\/or reflected wave-fronts, adaptive optics has been employed to compensate for those aberrations. However, it only applies to spatially-invariant aberrations or to thin aberrating layers. Here, we propose a global and non-invasive approach based on the distortion matrix concept. This matrix basically connects any focusing point of the image with the distorted part of its wave-front in reflection. A singular value decomposition of the distortion matrix allows to correct for high-order aberrations and forward multiple scattering over multiple isoplanatic modes. Proof-of-concept experiments are performed through biological tissues including a turbid cornea. We demonstrate a Strehl ratio enhancement up to 2500 and recover a diffraction-limited resolution until a depth of ten scattering mean free paths."],["a model for analyzing spatial data is shown over a coastline. four models were","Coastline Kriging: A Bayesian Approach","summarize: Statistical interpolation of chemical concentrations at new locations is an important step in assessing a worker's exposure level. When measurements are available from coastlines, as is the case in coastal clean-up operations in oil spills, one may need a mechanism to carry out spatial interpolation at new locations along the coast. In this paper we present a simple model for analyzing spatial data that is observed over a coastline. We demonstrate four different models using two different representations of the coast using curves. The four models were demonstrated on simulated data and one of them was also demonstrated on a dataset from the GuLF STUDY. Our contribution here is to offer practicing hygienists and exposure assessors with a simple and easy method to implement Bayesian hierarchical models for analyzing and interpolating coastal chemical concentrations."],["a structure theorem for invertible skew-symmetric operators is proved","Real Normal Operators and Williamson's Normal Form","summarize: A simple proof is provided to show that any bounded normal operator on a real Hilbert space is orthogonally equivalent to its transpose. A structure theorem for invertible skew-symmetric operators, which is analogous to the finite dimensional situation is also proved using elementary techniques. The second result is used to establish the main theorem of this article, which is a generalization of Williamson's normal form for bounded positive operators on infinite dimensional separable Hilbert spaces. This has applications in the study of infinite mode Gaussian states."],["quantum theory is a widely used and successful theory of chemical reaction networks. it describes interactions","Quantum Techniques for Stochastic Mechanics","summarize: Some ideas from quantum theory are just beginning to percolate back to classical probability theory. For example, there is a widely used and successful theory of chemical reaction networks, which describes the interactions of molecules in a stochastic rather than quantum way. Computer science and population biology use the same ideas under a different name: stochastic Petri nets. But if we look at these theories from the perspective of quantum theory, they turn out to involve creation and annihilation operators, coherent states and other well-known ideas - but in a context where probabilities replace amplitudes. We explain this connection as part of a detailed analogy between quantum mechanics and stochastic mechanics. We use this analogy to present new proofs of two major results in the theory of chemical reaction networks: the deficiency zero theorem and the Anderson-Craciun-Kurtz theorem. We also study the overlap of quantum mechanics and stochastic mechanics, which involves Hamiltonians that can generate either unitary or stochastic time evolution. These Hamiltonians are called Dirichlet forms, and they arise naturally from electrical circuits made only of resistors."],["conversion prediction plays an important role in online advertising. MT-FwFM improve the","Predicting Different Types of Conversions with Multi-Task Learning in Online Advertising","summarize: Conversion prediction plays an important role in online advertising since Cost-Per-Action has become one of the primary campaign performance objectives in the industry. Unlike click prediction, conversions have different types in nature, and each type may be associated with different decisive factors. In this paper, we formulate conversion prediction as a multi-task learning problem, so that the prediction models for different types of conversions can be learned together. These models share feature representations, but have their specific parameters, providing the benefit of information-sharing across all tasks. We then propose Multi-Task Field-weighted Factorization Machine to solve these tasks jointly. Our experiment results show that, compared with two state-of-the-art models, MT-FwFM improve the AUC by 0.74% and 0.84% on two conversion types, and the weighted AUC across all conversion types is also improved by 0.50%."],["quantum mechanics is a geometrical and non-linear theory that is defined on","Geometric formulation of quantum mechanics","summarize: Quantum mechanics is among the most important and successful mathematical model for describing our physical reality. The traditional formulation of quantum mechanics is linear and algebraic. In contrast classical mechanics is a geometrical and non-linear theory that is defined on a symplectic manifold. However, after invention of general relativity, we are convinced that geometry is physical and effect us in all scale. Hence the geometric formulation of quantum mechanics sought to give a unified picture of physical systems based on its underling geometrical structures, e.g., now, the states are represented by points of a symplectic manifold with a compatible Riemannian metric, the observables are real-valued functions on the manifold, and the quantum evolution is governed by a symplectic flow that is generated by a Hamiltonian function. In this work we will give a compact introduction to main ideas of geometric formulation of quantum mechanics. We will provide the reader with the details of geometrical structures of both pure and mixed quantum states. We will also discuss and review some important applications of geometric quantum mechanics."],["the resonant frequency analysis is done numerically through the determination of the eigen","Resonant frequency analysis of dental implants","summarize: Dental implant stability influences the decision on the determination of the duration between implant insertion and loading, This work investigates the resonant frequency analysis by means of a numerical model. The investigation is done numerically through the determination of the eigenfrequencies and performing a steady state response analyses using a commercial finite element package. A peri-implant interface, of simultaneously varying stiffness and layer thickness is introduced in the numerical 3D model in order to probe the sensitivity of the eigenfrequencies and steady state response to an evolving weakened layer, in an attempt to identify the bone reconstruction around the implant. For the first two modes, the resonant frequency is somewhat insensitive to the healing process, unless the weakened layer is rather large and compliant, like in the very early stages of the implantation. A Normalized Healing Factor is devised in the spirit of the Implant Stability Quotient, which can identify the healing process especially at the early stages after implantation. The sensitivity of the RFA to changes of mechanical properties of periprosthetic bone tissue seems relatively weak. Another indicator considering the amplitude as well as the resonance frequency might be more adapted to bone healing estimations. However, these results need to be verified experimentally as well as clinically."],["we focus on the case when rewards are piecewise i.i.d. and","Distribution-dependent and Time-uniform Bounds for Piecewise i.i.d Bandits","summarize: We consider the setup of stochastic multi-armed bandits in the case when reward distributions are piecewise i.i.d. and bounded with unknown changepoints. We focus on the case when changes happen simultaneously on all arms, and in stark contrast with the existing literature, we target gap-dependent regret bounds involving the magnitude of changes "],["quantum theory is a widely used and successful theory of chemical reaction networks. it describes interactions","Quantum Techniques for Stochastic Mechanics","summarize: Some ideas from quantum theory are just beginning to percolate back to classical probability theory. For example, there is a widely used and successful theory of chemical reaction networks, which describes the interactions of molecules in a stochastic rather than quantum way. Computer science and population biology use the same ideas under a different name: stochastic Petri nets. But if we look at these theories from the perspective of quantum theory, they turn out to involve creation and annihilation operators, coherent states and other well-known ideas - but in a context where probabilities replace amplitudes. We explain this connection as part of a detailed analogy between quantum mechanics and stochastic mechanics. We use this analogy to present new proofs of two major results in the theory of chemical reaction networks: the deficiency zero theorem and the Anderson-Craciun-Kurtz theorem. We also study the overlap of quantum mechanics and stochastic mechanics, which involves Hamiltonians that can generate either unitary or stochastic time evolution. These Hamiltonians are called Dirichlet forms, and they arise naturally from electrical circuits made only of resistors."],["the Poincar'e inequality is a generalized measurement function of subsamp","Function Approximation via The Subsampled Poincar\\' e Inequality","summarize: Function approximation and recovery via some sampled data have long been studied in a wide array of applied mathematics and statistics fields. Analytic tools, such as the Poincar\\'e inequality, have been handy for estimating the approximation errors in different scales. The purpose of this paper is to study a generalized Poincar\\' e inequality, where the measurement function is of subsampled type, with a small but non-zero lengthscale that will be made precise. Our analysis identifies this inequality as a basic tool for function recovery problems. We discuss and demonstrate the optimality of the inequality concerning the subsampled lengthscale, connecting it to existing results in the literature. In application to function approximation problems, the approximation accuracy using different basis functions and under different regularity assumptions is established by using the subsampled Poincar\\'e inequality. We observe that the error bound blows up as the subsampled lengthscale approaches zero, due to the fact that the underlying function is not regular enough to have well-defined pointwise values. A weighted version of the Poincar\\' e inequality is proposed to address this problem; its optimality is also discussed."],["automated visual inspection methods have been studied in recent years. but most steel manufacturing industries still use","TLU-Net: A Deep Learning Approach for Automatic Steel Surface Defect Detection","summarize: Visual steel surface defect detection is an essential step in steel sheet manufacturing. Several machine learning-based automated visual inspection methods have been studied in recent years. However, most steel manufacturing industries still use manual visual inspection due to training time and inaccuracies involved with AVI methods. Automatic steel defect detection methods could be useful in less expensive and faster quality control and feedback. But preparing the annotated training data for segmentation and classification could be a costly process. In this work, we propose to use the Transfer Learning-based U-Net framework for steel surface defect detection. We use a U-Net architecture as the base and explore two kinds of encoders: ResNet and DenseNet. We compare these nets' performance using random initialization and the pre-trained networks trained using the ImageNet data set. The experiments are performed using Severstal data. The results demonstrate that the transfer learning performs 5% better than that of the random initialization in defect classification. We found that the transfer learning performs 26% better than that of the random initialization in defect segmentation. We also found the gain of transfer learning increases as the training data decreases, and the convergence rate with transfer learning is better than that of the random initialization."],["the golden rule defines the transition rate between weakly coupled states. it can only be calculated","Instanton formulation of Fermi's golden rule in the Marcus inverted regime","summarize: Fermi's golden rule defines the transition rate between weakly coupled states and can thus be used to describe a multitude of molecular processes including electron-transfer reactions and light-matter interaction. However, it can only be calculated if the wave functions of all internal states are known, which is typically not the case in molecular systems. Marcus theory provides a closed-form expression for the rate constant, which is a classical limit of the golden rule, and indicates the existence of a normal regime and an inverted regime. Semiclassical instanton theory presents a more accurate approximation to the golden-rule rate including nuclear quantum effects such as tunnelling, which has so far been applicable to complex anharmonic systems in the normal regime only. In this paper we extend the instanton method to the inverted regime and study the properties of the periodic orbit, which describes the tunnelling mechanism via two imaginary-time trajectories, one of which now travels in negative imaginary time. It is known that tunnelling is particularly prevalent in the inverted regime, even at room temperature, and thus this method is expected to be useful in studying a wide range of molecular transitions occurring in this regime."],["the degree of the generators of invariant polynomial rings of local unitary","Characterization of multipartite entanglement in terms of local transformations","summarize: The degree of the generators of invariant polynomial rings of is a long standing open problem since the very initial study of the invariant theory in the 19th century. Motivated by its significant role in characterizing multipartite entanglement, we study the invariant polynomial rings of local unitary group---the tensor product of unitary group, and local general linear group---the tensor product of general linear group. For these two groups, we prove polynomial upper bounds on the degree of the generators of invariant polynomial rings. On the other hand, systematic methods are provided to to construct all homogenous polynomials that are invariant under these two groups for any fixed degree. Thus, our results can be regarded as a complete characterization of the invariant polynomial rings. As an interesting application, we show that multipartite entanglement is additive in the sense that two multipartite states are local unitary equivalent if and only if "],["kernels are based on spectral characteristics to facilitate decomposition. kernels","On the Importance of Temporal Context in Proximity Kernels: A Vocal Separation Case Study","summarize: Musical source separation methods exploit source-specific spectral characteristics to facilitate the decomposition process. Kernel Additive Modelling models a source applying robust statistics to time-frequency bins as specified by a source-specific kernel, a function defining similarity between bins. Kernels in existing approaches are typically defined using metrics between single time frames. In the presence of noise and other sound sources information from a single-frame, however, turns out to be unreliable and often incorrect frames are selected as similar. In this paper, we incorporate a temporal context into the kernel to provide additional information stabilizing the similarity search. Evaluated in the context of vocal separation, our simple extension led to a considerable improvement in separation quality compared to previous kernels."],["intuitionistic logic introduces an epistemic operator to intuitionistic logic. the a","An Arithmetical Interpretation of Verification and Intuitionistic Knowledge","summarize: Intuitionistic epistemic logic introduces an epistemic operator, which reflects the intended BHK semantics of intuitionism, to intuitionistic logic. The fundamental assumption concerning intuitionistic knowledge and belief is that it is the product of verification. The BHK interpretation of intuitionistic logic has a precise formulation in the Logic of Proofs and its arithmetical semantics. We show here that this interpretation can be extended to the notion of verification upon which intuitionistic knowledge is based, thereby providing the systems of intuitionistic epistemic logic extended by an epistemic operator based on verification with an arithmetical semantics too."],["sphere packing bound with a prefactor that is polynomial in the block length","The Sphere Packing Bound via Augustin's Method","summarize: A sphere packing bound with a prefactor that is polynomial in the block length "],["trajectory optimization is solved in the time domain. the optimization is solved in the time domain","Frequency-Aware Model Predictive Control","summarize: Transferring solutions found by trajectory optimization to robotic hardware remains a challenging task. When the optimization fully exploits the provided model to perform dynamic tasks, the presence of unmodeled dynamics renders the motion infeasible on the real system. Model errors can be a result of model simplifications, but also naturally arise when deploying the robot in unstructured and nondeterministic environments. Predominantly, compliant contacts and actuator dynamics lead to bandwidth limitations. While classical control methods provide tools to synthesize controllers that are robust to a class of model errors, such a notion is missing in modern trajectory optimization, which is solved in the time domain. We propose frequency-shaped cost functions to achieve robust solutions in the context of optimal control for legged robots. Through simulation and hardware experiments we show that motion plans can be made compatible with bandwidth limits set by actuators and contact dynamics. The smoothness of the model predictive solutions can be continuously tuned without compromising the feasibility of the problem. Experiments with the quadrupedal robot ANYmal, which is driven by highly-compliant series elastic actuators, showed significantly improved tracking performance of the planned motion, torque, and force trajectories and enabled the machine to walk robustly on terrain with unmodeled compliance."],["theoretical work has derived optimal strategies for when to leave a patch.","Foraging as an evidence accumulation process","summarize: A canonical foraging task is the patch-leaving problem, in which a forager must decide to leave a current resource in search for another. Theoretical work has derived optimal strategies for when to leave a patch, and experiments have tested for conditions where animals do or do not follow an optimal strategy. Nevertheless, models of patch-leaving decisions do not consider the imperfect and noisy sampling process through which an animal gathers information, and how this process is constrained by neurobiological mechanisms. In this theoretical study, we formulate an evidence accumulation model of patch-leaving decisions where the animal averages over noisy measurements to estimate the state of the current patch and the overall environment. Evidence accumulation models belong to the class of drift diffusion processes and have been used to model decision making in different contexts. We solve the model for conditions where foraging decisions are optimal and equivalent to the marginal value theorem, and perform simulations to analyze deviations from optimal when these conditions are not met. By adjusting the drift rate and decision threshold, the model can represent different strategies, for example an increment-decrement or counting strategy. These strategies yield identical decisions in the limiting case but differ in how patch residence times adapt when the foraging environment is uncertain. To account for sub-optimal decisions, we introduce an energy-dependent utility function that predicts longer than optimal patch residence times when food is plentiful. Our model provides a quantitative connection between ecological models of foraging behavior and evidence accumulation models of decision making. Moreover, it provides a theoretical framework for potential experiments which seek to identify neural circuits underlying patch leaving decisions."],["re-identification requires models to capture diverse features. but with continuous training, models","ES-Net: Erasing Salient Parts to Learn More in Re-Identification","summarize: As an instance-level recognition problem, re-identification requires models to capture diverse features. However, with continuous training, re-ID models pay more and more attention to the salient areas. As a result, the model may only focus on few small regions with salient representations and ignore other important information. This phenomenon leads to inferior performance, especially when models are evaluated on small inter-identity variation data. In this paper, we propose a novel network, Erasing-Salient Net , to learn comprehensive features by erasing the salient areas in an image. ES-Net proposes a novel method to locate the salient areas by the confidence of objects and erases them efficiently in a training batch. Meanwhile, to mitigate the over-erasing problem, this paper uses a trainable pooling layer P-pooling that generalizes global max and global average pooling. Experiments are conducted on two specific re-identification tasks . Our ES-Net outperforms state-of-the-art methods on three Person re-ID benchmarks and two Vehicle re-ID benchmarks. Specifically, mAP \/ Rank-1 rate: 88.6% \/ 95.7% on Market1501, 78.8% \/ 89.2% on DuckMTMC-reID, 57.3% \/ 80.9% on MSMT17, 81.9% \/ 97.0% on Veri-776, respectively. Rank-1 \/ Rank-5 rate: 83.6% \/ 96.9% on VehicleID , 79.9% \/ 93.5% on VehicleID , 76.9% \/ 90.7% on VehicleID , respectively. Moreover, the visualized salient areas show human-interpretable visual explanations for the ranking results."],["the free surface is seeded with buoyant particles that are advected and","Statistics of single and multiple floaters in experiments of surface wave turbulence","summarize: We present laboratory experiments of surface wave turbulence excited by paddles in the deep water regime. The free surface is seeded with buoyant particles that are advected and dispersed by the flow. Positions and velocities of the floaters are measured using particle tracking velocimetry. We study the statistics of velocity and acceleration of the particles, mean vertical displacements, single-particle horizontal dispersion, and the phenomenon of preferential concentration. Using a simple model together with the experimental data, we show that the time evolution of the particles has three characteristic processes that dominate the dynamics at different times: drag by surface waves at early times, trapping by horizontal eddies at intermediate times, and advection by a large-scale mean circulation at late times."],["Object tracking systems play important roles in tracking objects. problems arise from the difficulties in creating","Tracking Systems as Thinging Machine: A Case Study of a Service Company","summarize: Object tracking systems play important roles in tracking moving objects and overcoming problems such as safety, security and other location-related applications. Problems arise from the difficulties in creating a well-defined and understandable description of tracking systems. Nowadays, describing such processes results in fragmental representation that most of the time leads to difficulties creating documentation. Additionally, once learned by assigned personnel, repeated tasks result in them continuing on autopilot in a way that often degrades their effectiveness. This paper proposes the modeling of tracking systems in terms of a new diagrammatic methodology to produce engineering-like schemata. The resultant diagrams can be used in documentation, explanation, communication, education and control."],["the arcs contain quasi-conformally conjugate parabolic parameters. the","Parabolic arcs of the multicorns: Real-analyticity of Hausdorff dimension, and singularities of ","summarize: The boundaries of the hyperbolic components of odd period of the multicorns contain real-analytic arcs consisting of quasi-conformally conjugate parabolic parameters. One of the main results of this paper asserts that the Hausdorff dimension of the Julia sets is a real-analytic function of the parameter along these parabolic arcs. This is achieved by constructing a complex one-dimensional quasiconformal deformation space of the parabolic arcs which are contained in the dynamically defined algebraic curves "],["proposed approach uses multi-hop reasoning over knowledge graphs. it generates potential paths through","Exploiting Explicit Paths for Multi-hop Reading Comprehension","summarize: We propose a novel, path-based reasoning approach for the multi-hop reading comprehension task where a system needs to combine facts from multiple passages to answer a question. Although inspired by multi-hop reasoning over knowledge graphs, our proposed approach operates directly over unstructured text. It generates potential paths through passages and scores them without any direct path supervision. The proposed model, named PathNet, attempts to extract implicit relations from text through entity pair representations, and compose them to encode each path. To capture additional context, PathNet also composes the passage representations along each path to compute a passage-based representation. Unlike previous approaches, our model is then able to explain its reasoning via these explicit paths through the passages. We show that our approach outperforms prior models on the multi-hop Wikihop dataset, and also can be generalized to apply to the OpenBookQA dataset, matching state-of-the-art performance."],["four circulant constructions for self-dual codes and bordered versions of the construction","New extremal binary self-dual codes from a modified four circulant construction","summarize: In this work, we propose a modified four circulant construction for self-dual codes and a bordered version of the construction using the properties of \\lambda-circulant and \\lambda-reverse circulant matrices. By using the constructions on "],["OATM can significantly save tuning time compared to state-of-the-art methods","Deep Neural Network Hyperparameter Optimization with Orthogonal Array Tuning","summarize: Deep learning algorithms have achieved excellent performance lately in a wide range of fields . However, a severe challenge faced by deep learning is the high dependency on hyper-parameters. The algorithm results may fluctuate dramatically under the different configuration of hyper-parameters. Addressing the above issue, this paper presents an efficient Orthogonal Array Tuning Method for deep learning hyper-parameter tuning. We describe the OATM approach in five detailed steps and elaborate on it using two widely used deep neural network structures . The proposed method is compared to the state-of-the-art hyper-parameter tuning methods including manually and automatically ones. The experiment results state that OATM can significantly save the tuning time compared to the state-of-the-art methods while preserving the satisfying performance. The codes are open in GitHub "],["modular Politics would enable platform operators to build bottom-up governance processes from computational components that","Modular Politics: Toward a Governance Layer for Online Communities","summarize: Governance in online communities is an increasingly high-stakes challenge, and yet many basic features of offline governance legacies--juries, political parties, term limits, and formal debates, to name a few--are not in the feature-sets of the software most community platforms use. Drawing on the paradigm of Institutional Analysis and Development, this paper proposes a strategy for addressing this lapse by specifying basic features of a generalizable paradigm for online governance called Modular Politics. Whereas classical governance typologies tend to present a choice among wholesale ideologies, such as democracy or oligarchy, Modular Politics would enable platform operators and their users to build bottom-up governance processes from computational components that are modular and composable, highly versatile in their expressiveness, portable from one context to another, and interoperable across platforms. This kind of approach could implement pre-digital governance systems as well as accelerate innovation in uniquely digital techniques. As diverse communities share and connect their components and data, governance could occur through a ubiquitous network layer. To that end, this paper proposes the development of an open standard for networked governance."],["a custom template can be semi-automatic according to the preoperative plan. mesh","A semi-automatic computer-aided method for surgical template design","summarize: This paper presents a generalized integrated framework of semi-automatic surgical template design. Several algorithms were implemented including the mesh segmentation, offset surface generation, collision detection, ruled surface generation, etc., and a special software named TemDesigner was developed. With a simple user interface, a customized template can be semi- automatically designed according to the preoperative plan. Firstly, mesh segmentation with signed scalar of vertex is utilized to partition the inner surface from the input surface mesh based on the indicated point loop. Then, the offset surface of the inner surface is obtained through contouring the distance field of the inner surface, and segmented to generate the outer surface. Ruled surface is employed to connect inner and outer surfaces. Finally, drilling tubes are generated according to the preoperative plan through collision detection and merging. It has been applied to the template design for various kinds of surgeries, including oral implantology, cervical pedicle screw insertion, iliosacral screw insertion and osteotomy, demonstrating the efficiency, functionality and generality of our method."],["image classification for the English language is an ongoing research challenge. the method used in this study","Image Classification for Arabic: Assessing the Accuracy of Direct English to Arabic Translations","summarize: Image classification is an ongoing research challenge. Most of the available research focuses on image classification for the English language, however there is very little research on image classification for the Arabic language. Expanding image classification to Arabic has several applications. The present study investigated a method for generating Arabic labels for images of objects. The method used in this study involved a direct English to Arabic translation of the labels that are currently available on ImageNet, a database commonly used in image classification research. The purpose of this study was to test the accuracy of this method. In this study, 2,887 labeled images were randomly selected from ImageNet. All of the labels were translated from English to Arabic using Google Translate. The accuracy of the translations was evaluated. Results indicated that that 65.6% of the Arabic labels were accurate. This study makes three important contributions to the image classification literature: it determined the baseline level of accuracy for algorithms that provide Arabic labels for images, it provided 1,895 images that are tagged with accurate Arabic labels, and provided the accuracy of translations of image labels from English to Arabic."],["entropy solutions are uniformly bounded with respect to space and time variables","Large time behavior of entropy solutions to one-dimensional unipolar hydrodynamic model for semiconductor devices","summarize: We are concerned with the global existence and large time behavior of entropy solutions to the one dimensional unipolar hydrodynamic model for semiconductors in the form of Euler-Poisson equations in a bounded interval. In this paper, we first prove the global existence of entropy solution by vanishing viscosity and compensated compactness framework. In particular, the solutions are uniformly bounded with respect to space and time variables by introducing modified Riemann invariants and the theory of invariant region. Based on the uniform estimates of density, we further show that the entropy solution converges to the corresponding unique stationary solution exponentially in time. No any smallness condition is assumed on the initial data and doping profile. Moreover, the novelty in this paper is about the unform bound with respect to time for the weak solutions of the isentropic Euler-Possion system."],["Using the machinery of weak fibration categories due to Schlank and the first author, we","Model structure on projective systems of ","summarize: Using the machinery of weak fibration categories due to Schlank and the first author, we construct a convenient model structure on the pro-category of separable "],["the sign structure of the Jacobi matrix carries the information about which components of the network inhibit","Representing Model Ensembles as Boolean Functions","summarize: Families of ODE models characterized by a common sign structure of their Jacobi matrix are investigated within the formalism of qualitative differential equations. In the context of regulatory networks the sign structure of the Jacobi matrix carries the information about which components of the network inhibit or activate each other. Information about constraints on the behavior of models in this family is stored in a so called qualitative state transition graph. We showed previously that a similar approach can be used to analyze a model pool of Boolean functions characterized by a common interaction graph. Here we show that the opposite approach is fruitful as well. We show that the qualitative state transition graph can be reduced to a skeleton represented by a Boolean function conserving the reachability properties. This reduction has the advantage that approaches such as model checking and network inference methods can be applied to the skeleton within the framework of Boolean networks. Furthermore, our work constitutes an alternative to approach to link Boolean networks and differential equations."],["each year, member states deliver statements during the General Debate. these speeches provide invaluable information","Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus","summarize: Every year at the United Nations, member states deliver statements during the General Debate discussing major issues in world politics. These speeches provide invaluable information on governments' perspectives and preferences on a wide range of issues, but have largely been overlooked in the study of international politics. This paper introduces a new dataset consisting of over 7,701 English-language country statements from 1970-2016. We demonstrate how the UN General Debate Corpus can be used to derive country positions on different policy dimensions using text analytic methods. The paper provides applications of these estimates, demonstrating the contribution the UNGDC can make to the study of international politics."],["a ring is always nil, which extends a well known result from","Nilpotent, algebraic and quasi-regular elements in rings and algebras","summarize: We prove that an integral Jacobson radical ring is always nil, which extends a well known result from algebras over fields to rings. As a consequence we show that if every element x of a ring R is a zero of some polynomial p_x with integer coefficients, such that p_x=1, then R is a nil ring. With these results we are able to give new characterizations of the upper nilradical of a ring and a new class of rings that satisfy the K\\othe conjecture, namely the integral rings."],["post-Lie Magnus expansion is a post-Lie algebra. the results are then","Post-Lie Algebras, Factorization Theorems and Isospectral-Flows","summarize: In these notes we review and further explore the Lie enveloping algebra of a post-Lie algebra. From a Hopf algebra point of view, one of the central results, which will be recalled in detail, is the existence of a second Hopf algebra structure. By comparing group-like elements in suitable completions of these two Hopf algebras, we derive a particular map which we dub post-Lie Magnus expansion. These results are then considered in the case of Semenov-Tian-Shansky's double Lie algebra, where a post-Lie algebra is defined in terms of solutions of modified classical Yang-Baxter equation. In this context, we prove a factorization theorem for group-like elements. An explicit exponential solution of the corresponding Lie bracket flow is presented, which is based on the aforementioned post-Lie Magnus expansion."],["text analysis techniques can be used to uncover unstructured information from text. a 3-","From Review to Rating: Exploring Dependency Measures for Text Classification","summarize: Various text analysis techniques exist, which attempt to uncover unstructured information from text. In this work, we explore using statistical dependence measures for textual classification, representing text as word vectors. Student satisfaction scores on a 3-point scale and their free text comments written about university subjects are used as the dataset. We have compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors provide a greater accuracy. However, these word vectors have a large number of features which aggravates the burden of computational complexity. Thus, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between the text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset shows that our approach achieves comparable accuracy to the full feature vector, while being an order of magnitude faster in testing. These text analysis and feature reduction techniques can be used for other textual data applications such as sentiment analysis."],["paper's main purpose is to present some recent results on metric characterizations of super","Metric characterizations of some classes of Banach spaces","summarize: The main purpose of the paper is to present some recent results on metric characterizations of superreflexivity and the Radon-Nikod\\'ym property."],["golfarshchi and Khalilzadeh disprove two results.","On the paper: Numerical radius preserving linear maps on Banach algebras","summarize: We give an example of a unital commutative complex Banach algebra having a normalized state which is not a spectral state and admitting an extreme normalized state which is not multiplicative. This disproves two results by Golfarshchi and Khalilzadeh."],["setup is developed with the scope of mapping the field generated by a known magnetic source.","Fast, cheap, and scalable magnetic tracker with an array of magnetoresistors","summarize: We present the hardware of a cheap multi-sensor magnetometric setup where a relatively large set of magnetic field components is measured in several positions by calibrated magnetoresistive detectors. The setup is developed with the scope of mapping the field generated by a known magnetic source, which is measured as superimposed to the geomagnetic field. The final goal is to use the data produced by this hardware to reconstruct position and orientation of the magnetic source with respect to the sensor frame, simultaneously with the orientation of the frame with respect to the environmental field. Possible applications of the setup are shortly discussed, together with a synthetic description of the data elaboration and analysis."],["quasars can serve as precise cosmological probes. a number","Kernel regression estimates of time delays between gravitationally lensed fluxes","summarize: Strongly lensed variable quasars can serve as precise cosmological probes, provided that time delays between the image fluxes can be accurately measured. A number of methods have been proposed to address this problem. In this paper, we explore in detail a new approach based on kernel regression estimates, which is able to estimate a single time delay given several datasets for the same quasar. We develop realistic artificial data sets in order to carry out controlled experiments to test of performance of this new approach. We also test our method on real data from strongly lensed quasar Q0957+561 and compare our estimates against existing results."],["voicing detection is a classification problem and F0 contour estimation as a regression problem","Traditional Machine Learning for Pitch Detection","summarize: Pitch detection is a fundamental problem in speech processing as F0 is used in a large number of applications. Recent articles have proposed deep learning for robust pitch tracking. In this paper, we consider voicing detection as a classification problem and F0 contour estimation as a regression problem. For both tasks, acoustic features from multiple domains and traditional machine learning methods are used. The discrimination power of existing and proposed features is assessed through mutual information. Multiple supervised and unsupervised approaches are compared. A significant relative reduction of voicing errors over the best baseline is obtained: 20% with the best clustering method and 45% with a Multi-Layer Perceptron. For F0 contour estimation, the benefits of regression techniques are limited though. We investigate whether those objective gains translate in a parametric synthesis task. Clear perceptual preferences are observed for the proposed approach over two widely-used baselines ."],["wearables leverage machine learning techniques to profile behavioral routine of their end-users through activity","Resource-Efficient Wearable Computing for Real-Time Reconfigurable Machine Learning: A Cascading Binary Classification","summarize: Advances in embedded systems have enabled integration of many lightweight sensory devices within our daily life. In particular, this trend has given rise to continuous expansion of wearable sensors in a broad range of applications from health and fitness monitoring to social networking and military surveillance. Wearables leverage machine learning techniques to profile behavioral routine of their end-users through activity recognition algorithms. Current research assumes that such machine learning algorithms are trained offline. In reality, however, wearables demand continuous reconfiguration of their computational algorithms due to their highly dynamic operation. Developing a personalized and adaptive machine learning model requires real-time reconfiguration of the model. Due to stringent computation and memory constraints of these embedded sensors, the training\/re-training of the computational algorithms need to be memory- and computation-efficient. In this paper, we propose a framework, based on the notion of online learning, for real-time and on-device machine learning training. We propose to transform the activity recognition problem from a multi-class classification problem to a hierarchical model of binary decisions using cascading online binary classifiers. Our results, based on Pegasos online learning, demonstrate that the proposed approach achieves 97% accuracy in detecting activities of varying intensities using a limited memory while power usages of the system is reduced by more than 40%."],["paper introduces an investigation of the healthcare monitoring systems. the different roles that exist in healthcare","A Context Aware Framework for IoT Based Healthcare Monitoring Systems","summarize: This paper introduces an investigation of the healthcare monitoring systems and their provisioning in the IoT platform. The different roles that exist in healthcare systems are specified and modeled here. This paper also attempts to introduce and propose a generic framework for the design and development of context aware healthcare monitoring systems in the IoT platform. In such a framework, the fundamental components of the healthcare monitoring systems are identified and modelled as well as the relationship between these components. The paper also stresses on the crucial role played by the AI field in addressing resilient context aware healthcare monitoring systems. Architecturally, this framework is based on a distributed layered architecture where the different components are deployed over the physical layer, fog platform and the cloud platform."],["flow instabilities and fluctuating shear stresses are held responsible for a variety of cardiovascular","Nonlinear hydrodynamic instability and turbulence in pulsatile flow","summarize: Pulsating flows through tubular geometries are laminar provided that velocities are moderate. This in particular is also believed to apply to cardiovascular flows where inertial forces are typically too low to sustain turbulence. On the other hand flow instabilities and fluctuating shear stresses are held responsible for a variety of cardiovascular diseases. Here we report a nonlinear instability mechanism for pulsating pipe flow that gives rise to bursts of turbulence at low flow rates. Geometrical distortions of small, yet finite amplitude are found to excite a state consisting of helical vortices during flow deceleration. The resulting flow pattern grows rapidly in magnitude, breaks down into turbulence, and eventually returns to laminar when the flow accelerates. This scenario causes shear stress fluctuations and flow reversal during each pulsation cycle. Such unsteady conditions can adversely affect blood vessels and have been shown to promote inflammation and dysfunction of the shear stress sensitive endothelial cell layer."],["the coupled discretization treats the coupled system of deformation and flow directly. the discretization","Stable cell-centered finite volume discretization for Biot equations","summarize: In this paper we discuss a new discretization for the Biot equations. The discretization treats the coupled system of deformation and flow directly, as opposed to combining discretizations for the two separate sub-problems. The coupled discretization has the following key properties, the combination of which is novel: 1) The variables for the pressure and displacement are co-located, and are as sparse as possible . 2) With locally computable restrictions on grid types, the discretization is stable with respect to the limits of incompressible fluid and small time-steps. 3) No artificial stabilization term has been introduced. Furthermore, due to the finite volume structure embedded in the discretization, explicit local expressions for both momentum-balancing forces as well as mass-conservative fluid fluxes are available. We prove stability of the proposed method with respect to all relevant limits. Together with consistency, this proves convergence of the method. Finally, we give numerical examples verifying both the analysis and convergence of the method."],["new work on adversary-aware classifiers has been focusing on evasion","Adversarial Feature Selection against Evasion Attacks","summarize: Pattern recognition and machine learning techniques have been increasingly adopted in adversarial settings such as spam, intrusion and malware detection, although their security against well-crafted attacks that aim to evade detection by manipulating data at test time has not yet been thoroughly assessed. While previous work has been mainly focused on devising adversary-aware classification algorithms to counter evasion attempts, only few authors have considered the impact of using reduced feature sets on classifier security against the same attacks. An interesting, preliminary result is that classifier security to evasion may be even worsened by the application of feature selection. In this paper, we provide a more detailed investigation of this aspect, shedding some light on the security properties of feature selection against evasion attacks. Inspired by previous work on adversary-aware classifiers, we propose a novel adversary-aware feature selection model that can improve classifier security against evasion attacks, by incorporating specific assumptions on the adversary's data manipulation strategy. We focus on an efficient, wrapper-based implementation of our approach, and experimentally validate its soundness on different application examples, including spam and malware detection."],["Let us know what you think about it!","Existence and Nonexistence results for anisotropic p-Laplace equation with singular nonlinearities","summarize: Let "],["vectorial vortex beam launcher is designed to launch a single transverse electric mode","Microwave Vortex Beam Launcher Design","summarize: A novel design for a vectorial vortex beam launcher in the microwave regime is devised. The beam is formed by launching a single guided transverse electric mode of a metallic circular waveguide into free-space. Excitation is achieved by the mean of an inserted coaxial loop antenna. Modal expansion coefficients are computed, and the resulting electric and magnetic fields are determined. The effect of the antenna location inside the waveguide on its effective input impedance is modelled using transmission-line relations and location for optimal matching is established. The analytical results are confirmed using multi-level fast multipole method full-wave simulations."],["Let us know what you think about it!","Unitary Subgroups of commutative group algebras of characteristic two","summarize: Let "],["this research is a school action research conducted for four months starting in October 2018 to January 2019","Application of the Among Leadership Model to Improve Teacher Work Discipline","summarize: This study aims to improve teacher work discipline related to their duties as classroom teachers. This research is a school action research conducted for four months starting in October 2018 to January 2019 at SDN 11 Simpang Rimba, South Bangka, Indonesia in the 2018\/2019 academic year. The data collection instrument used observation sheets and documentation. Data analysis used quantitative data analysis techniques with descriptive statistics. The results of this study indicate that the application of the Among leadership model can improve teacher work discipline in the aspects of arriving on time, work hours fulfillment, and prepare lesson plans."],["a two-dimensional model is used for solid liquid crystalline plates. the model equation","A plate theory for nematic liquid crystalline solids","summarize: We derive a F\\ppl-von K\\'rm\\'n-type constitutive model for solid liquid crystalline plates where the nematic director may or may not rotate freely relative to the elastic network. To obtain the reduced two-dimensional model, we rely on the deformation decomposition of a nematic solid into an elastic deformation and a natural shape change. The full solution to the resulting equilibrium equations consists of both the deformation displacement and stress fields. The model equations are applicable to a wide range of thin nematic bodies subject to optothermal stimuli and mechanical loads. For illustration, we consider certain reversible natural shape changes in simple systems which are stress free, and their counterparts, where the natural deformations are blocked and internal stresses appear. More general problems can be addressed within the same framework."],["unified cross-modality feature disentagling approach for multi-domain image translation and","Unified cross-modality feature disentangler for unsupervised multi-domain MRI abdomen organs segmentation","summarize: Our contribution is a unified cross-modality feature disentagling approach for multi-domain image translation and multiple organ segmentation. Using CT as the labeled source domain, our approach learns to segment multi-modal MRI having no labeled data. Our approach uses a variational auto-encoder to disentangle the image content from style. The VAE constrains the style feature encoding to match a universal prior that is assumed to span the styles of all the source and target modalities. The extracted image style is converted into a latent style scaling code, which modulates the generator to produce multi-modality images according to the target domain code from the image content features. Finally, we introduce a joint distribution matching discriminator that combines the translated images with task-relevant segmentation probability maps to further constrain and regularize image-to-image translations. We performed extensive comparisons to multiple state-of-the-art I2I translation and segmentation methods. Our approach resulted in the lowest average multi-domain image reconstruction error of 1.34"],["poset structure extends weak order on a finite Coxeter group.","The facial weak order and its lattice quotients","summarize: We investigate a poset structure that extends the weak order on a finite Coxeter group "],["the design of an experiment can be always considered implicitly Bayesian. prior knowledge can","Bayesian design of experiments for generalised linear models and dimensional analysis with industrial and scientific application","summarize: The design of an experiment can be always be considered at least implicitly Bayesian, with prior knowledge used informally to aid decisions such as the variables to be studied and the choice of a plausible relationship between the explanatory variables and measured responses. Bayesian methods allow uncertainty in these decisions to be incorporated into design selection through prior distributions that encapsulate information available from scientific knowledge or previous experimentation. Further, a design may be explicitly tailored to the aim of the experiment through a decision-theoretic approach using an appropriate loss function. We review the area of decision-theoretic Bayesian design, with particular emphasis on recent advances in computational methods. For many problems arising in industry and science, experiments result in a discrete response that is well described by a member of the class of generalised linear models. We describe how Gaussian process emulation, commonly used in computer experiments, can play an important role in facilitating Bayesian design for realistic problems. A main focus is the combination of Gaussian process regression to approximate the expected loss with cyclic descent optimisation algorithms to allow optimal designs to be found for previously infeasible problems. We also present the first optimal design results for statistical models formed from dimensional analysis, a methodology widely employed in the engineering and physical sciences to produce parsimonious and interpretable models. Using the famous paper helicopter experiment, we show the potential for the combination of Bayesian design, generalised linear models and dimensional analysis to produce small but informative experiments."],["network virtualization suffers from the problem of mapping virtual links and nodes to physical network in","Energy-Aware Virtual Network Embedding Approach for Distributed Cloud","summarize: Network virtualization has caught the attention of many researchers in recent years. It facilitates the process of creating several virtual networks over a single physical network. Despite this advantage, however, network virtualization suffers from the problem of mapping virtual links and nodes to physical network in most efficient way. This problem is called virtual network embedding . Many researches have been proposed in an attempt to solve this problem, which have many optimization aspects, such as improving embedding strategies in a way that preserves energy, reducing embedding cost and increasing embedding revenue. Moreover, some researchers have extended their algorithms to be more compatible with the distributed clouds instead of a single infrastructure provider . This paper proposes energy aware particle swarm optimization algorithm for distributed clouds. This algorithm aims to partition each virtual network request to subgraphs, using the Heavy Clique Matching technique to generate a coarsened graph. Each coarsened node in the coarsened graph is assigned to a suitable data center . Inside each DC, a modified particle swarm optimization algorithm is initiated to find the near optimal solution for the VNE problem. The proposed algorithm was tested and evaluated against existing algorithms using extensive simulations, which shows that the proposed algorithm outperforms other algorithms."],["the analysis of Dirac's charge quantization condition in the presence of a magnetic","Comment on Jackson's analysis of electric charge quantization due to interaction with Dirac's magnetic monopole","summarize: In J.D. Jackson's Classical Electrodynamics textbook, the analysis of Dirac's charge quantization condition in the presence of a magnetic monopole has a mathematical omission and an all too brief physical argument that might mislead some students. This paper presents a detailed derivation of Jackson's main result, explains the significance of the missing term, and highlights the close connection between Jackson's findings and Dirac's original argument."],["a signed probability distribution may extend a given traditional probability to all events. we formal","Negative probabilities, II: What they are and what they are for","summarize: A signed probability distribution may extend a given traditional probability from observable events to all events. We formalize and illustrate this approach. We also illustrate its limitation. We argue that the right question is not what negative probabilities are but what they are for."],["we propose two schemes that adaptively utilize networks. we first pose an adaptive network evaluation scheme","Adaptive Neural Networks for Efficient Inference","summarize: We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal loss of top5 accuracy."],["interval pairwise comparison matrices have been widely used to account for uncertain statements concerning","A general unified framework for interval pairwise comparison matrices","summarize: Interval Pairwise Comparison Matrices have been widely used to account for uncertain statements concerning the preferences of decision makers. Several approaches have been proposed in the literature, such as multiplicative and fuzzy interval matrices. In this paper, we propose a general unified approach to Interval Pairwise Comparison Matrices, based on Abelian linearly ordered groups. In this framework, we generalize some consistency conditions provided for multiplicative and\/or fuzzy interval pairwise comparison matrices and provide inclusion relations between them. Then, we provide a concept of distance between intervals that, together with a notion of mean defined over real continuous Abelian linearly ordered groups, allows us to provide a consistency index and an indeterminacy index. In this way, by means of suitable isomorphisms between Abelian linearly ordered groups, we will be able to compare the inconsistency and the indeterminacy of different kinds of Interval Pairwise Comparison Matrices, e.g. multiplicative, additive, and fuzzy, on a unique Cartesian coordinate system."],["random matrices with continuous entries are divided by columns. the number of columns is","Balancing Gaussian vectors in high dimension","summarize: Motivated by problems in controlled experiments, we study the discrepancy of random matrices with continuous entries where the number of columns "],["the action spaces are polyhedral and described by parametric linear programs. this type","Toward breaking the curse of dimensionality: an FPTAS for stochastic dynamic programs with multidimensional actions and scalar states","summarize: We propose a Fully Polynomial-Time Approximation Scheme for stochastic dynamic programs with multidimensional action, scalar state, convex costs and linear state transition function. The action spaces are polyhedral and described by parametric linear programs. This type of problems finds applications in the area of optimal planning under uncertainty, and can be thought of as the problem of optimally managing a single non-discrete resource over a finite time horizon. We show that under a value oracle model for the cost functions this result for one-dimensional state space is best possible, because a similar dynamic programming model with two-dimensional state space does not admit a PTAS. The FPTAS relies on the solution of polynomial-sized linear programs to recursively compute an approximation of the value function at each stage. Our paper enlarges the class of dynamic programs that admit an FPTAS by showing, under suitable conditions, how to deal with multidimensional action spaces and with vectors of continuous random variables with bounded support. These results bring us one step closer to overcoming the curse of dimensionality of dynamic programming."],["the aim of the present investigation is to study the spatio-temporal distribution of precursor flare","Statistical study of spatio-temporal distribution of precursor solar flares associated with major flares","summarize: The aim of the present investigation is to study the spatio-temporal distribution of precursor flares during the 24-hour interval preceding M- and X-class major flares and the evolution of follower flares. Information on associated flares is provided by Reuven Ramaty High Energy Solar Spectroscopic Imager . Flare List, while the major flares are observed by the Geostationary Operational Environmental Satellite system satellites between 2002 and 2014. There are distinct evolutionary differences between the spatio-temporal distributions of associated flares in about one day period depending on the type of the main flare. The spatial distribution was characterised by the normalised frequency distribution of the quantity "],["the new theory is a combination of Eulerian and Lagrangian hydrodynamics","Action Principle for Hydrodynamics and Thermodynamics including general, rotational flows","summarize: The restriction of hydrodynamics to non-viscous, potential flows is a theory both simple and elegant; a favorite topic of introductory textbooks. It is known that this theory can be formulated as an action principle and expanded to include thermodynamicics. This paper presents an action principle for hydrodynamics that includes general, rotational flows. The new theory is a combination of Eulerian and Lagrangian hydrodynamics, with an extension to thermodynamics that includes all the elements of the Gibbsean variational principle. In the first place it is an action principle for adiabatic systems, including the usual conservation laws. Viscosity can be introduced in the usual way, by adding a dissipative term to the momentum equation. The equation for energy dissipation then follows. It is an ideal framework for the description of quasi-static processes, including dissipation. It is a major development of the Navier-Stokes-Fourier approach, the principal advantage being a hamiltonian structure with a natural concept of energy in the form of a first integral of the motion, conserved by virtue of the Euler-Lagrange equations."],["we compute two families of determinants whose entries are Bessel moments. we also","Wro\\'nskian factorizations and Broadhurst-Mellit determinant formulae","summarize: Drawing on Vanhove's contributions to mixed Hodge structures for Feynman integrals in two-di\\-men\\-sion\\-al quantum field theory, we compute two families of determinants whose entries are Bessel moments. Via explicit factorizations of certain Wro\\'nskian determinants, we verify two recent conjectures proposed by Broadhurst and Mellit, concerning determinants of arbitrary sizes. With some extensions to our methods, we also relate two more determinants of Broadhurst--Mellit to the logarithmic Mahler measures of certain polynomials."],["quantum superposition permits two-way communication between two distant parties. this is achieved by","Experimental two-way communication with one photon","summarize: Superposition of two or more states is one of the fundamental concepts of quantum mechanics and provides the basis for several advantages quantum information processing offers. In this work, we experimentally demonstrate that quantum superposition permits two-way communication between two distant parties that can exchange only one particle once, an impossible task in classical physics. This is achieved by preparing a single photon in a coherent superposition of the two parties' locations. Furthermore, we show that this concept allows the parties to perform secure quantum communication, where the transmitted bits and even the direction of communication remain private. These important features can lead to the development of new quantum communication schemes, which are simultaneously secure and resource-efficient."],["in this paper, we give sufficient and necessary conditions for cosine operator functions on solid Ban","Linear dynamics of cosine operator functions on solid Banach function spaces","summarize: In this paper, we give some sufficient and necessary conditions for cosine operator functions on solid Banach function spaces to be chaotic or topologically transitive."],["polarization memory effects in single-crystal CuFeO2. magnetic","Polarization memory in the nonpolar magnetic ground state of multiferroic CuFeO2","summarize: We investigate polarization memory effects in single-crystal CuFeO2, which has a magnetically-induced ferroelectric phase at low temperatures and applied B fields between 7.5 and 13 T. Following electrical poling of the ferroelectric phase, we find that the nonpolar collinear antiferromagnetic ground state at B = 0 T retains a strong memory of the polarization magnitude and direction, such that upon re-entering the ferroelectric phase a net polarization of comparable magnitude to the initial polarization is recovered in the absence of external bias. This memory effect is very robust: in pulsed-magnetic-field measurements, several pulses into the ferroelectric phase with reverse bias are required to switch the polarization direction, with significant switching only seen after the system is driven out of the ferroelectric phase and ground state either magnetically or thermally. The memory effect is also largely insensitive to the magnetoelastic domain composition, since no change in the memory effect is observed for a sample driven into a single-domain state by application of stress in the direction. On the basis of Monte Carlo simulations of the ground state spin configurations, we propose that the memory effect is due to the existence of helical domain walls within the nonpolar collinear antiferromagnetic ground state, which would retain the helicity of the polar phase for certain magnetothermal histories."],["X-ray spectral study of quasar PG 1211+143","The Ultra-Fast Outflow of the Quasar PG 1211+143 as Viewed by Time-Averaged Chandra Grating Spectroscopy","summarize: We present a detailed X-ray spectral study of the quasar PG 1211+143 based on Chandra High Energy Transmission Grating Spectrometer observations collected in a multi-wavelength campaign with UV data using the Hubble Space Telescope Cosmic Origins Spectrograph and radio bands using the Jansky Very Large Array . We constructed a multi-wavelength ionizing spectral energy distribution using these observations and archival infrared data to create XSTAR photoionization models specific to the PG 1211+143 flux behavior during the epoch of our observations. Our analysis of the Chandra-HETGS spectra yields complex absorption lines from H-like and He-like ions of Ne, Mg and Si which confirm the presence of an ultra-fast outflow with a velocity ~ "],["exemplar-based image translation synthesizes a photo-realistic image from the","Cross-domain Correspondence Learning for Exemplar-based Image Translation","summarize: We present a general framework for exemplar-based image translation, which synthesizes a photo-realistic image from the input in a distinct domain , given an exemplar image. The output has the style in consistency with the semantically corresponding objects in the exemplar. We propose to jointly learn the crossdomain correspondence and the image translation, where both tasks facilitate each other and thus can be learned with weak supervision. The images from distinct domains are first aligned to an intermediate domain where dense correspondence is established. Then, the network synthesizes images based on the appearance of semantically corresponding patches in the exemplar. We demonstrate the effectiveness of our approach in several image translation tasks. Our method is superior to state-of-the-art methods in terms of image quality significantly, with the image style faithful to the exemplar with semantic consistency. Moreover, we show the utility of our method for several applications"],["polynomials defined via fillings of diagrams satisfy linear recurrence","Polynomials defined by tableaux and linear recurrences","summarize: We show that several families of polynomials defined via fillings of diagrams satisfy linear recurrences under a natural operation on the shape of the diagram. We focus on key polynomials, , and Demazure atoms. The same technique can be applied to Hall-Littlewood polynomials and dual Grothendieck polynomials. The motivation behind this is that such recurrences are strongly connected with other nice properties, such as interpretations in terms of lattice points in polytopes and divided difference operators."],["the zeroth Landau level is filled by electrons and holes with opposite chirality","Robust helical edge transport at ","summarize: Among the most interesting predictions in two-dimensional materials with a Dirac cone is the existence of the zeroth Landau level , equally filled by electrons and holes with opposite chirality. The gapless edge states with helical spin structure emerge from Zeeman splitting at the LL filling factor "],["Graph-based methods are known to be successful in many machine learning tasks. a","Stochastic Graphlet Embedding","summarize: Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit\/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts\/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases."],["silicon wafer manufacturers are experimenting with new technology. the demand for longer mono-sili","Nonlinear dynamics in the flexible shaft rotating-lifting system of silicon crystal puller using Czochralski method","summarize: Silicon crystal puller is a key equipment in silicon wafer manufacture, which is, in turn, the base material for the most currently used integrated circuit chips. With the development of the techniques, the demand for longer mono-silicon crystal rod with larger diameter is continuously increasing in order to reduce the manufacture time and the price of the wafer. This demand calls for larger SCP with increasing height, however, it causes serious swing phenomenon of the crystal seed. The strong swing of the seed causes difficulty in the solidification and increases the risk of mono-silicon growth failure.The main aim of this paper is to analyze the nonlinear dynamics in the FSRL system of the SCP. A mathematical model for the swing motion of the FSRL system is derived. The influence of relevant parameters, such as system damping, excitation amplitude and rotation speed, on the stability and the responses of the system are analyzed. The stability of the equilibrium, bifurcation and chaotic motion are demonstrated, which are often observed in practical situations. Melnikov method is used to derive the possible parameter region that leads to chaotic motion. Three routes to chaos are identified in the FSRL system, including period doubling, symmetry-breaking bifurcation and interior crisis. The work in this paper explains the complex dynamics in the FSRL system of the SCP, which will be helpful for the SCP designers in order to avoid the swing phenomenon in the SCP."],["the Hessian discretisation method provides a unified convergence analysis framework. some examples","Improved ","summarize: The Hessian discretisation method for fourth order linear elliptic equations provides a unified convergence analysis framework based on three properties namely coercivity, consistency, and limit-conformity. Some examples that fit in this approach include conforming and nonconforming finite element methods, finite volume methods and methods based on gradient recovery operators. A generic error estimate has been established in "],["Let us know what you think about it!","Unitary Subgroups of commutative group algebras of characteristic two","summarize: Let "],["MC is mostly studied in microscale but new practical applications emerge in macroscale. it","Fluid Dynamics-Based Distance Estimation Algorithm for Macroscale Molecular Communication","summarize: Many species, from single-cell bacteria to advanced animals, use molecular communication to share information with each other via chemical signals. Although MC is mostly studied in microscale, new practical applications emerge in macroscale. It is essential to derive an estimation method for channel parameters such as distance for practical macroscale MC systems which include a sprayer emitting molecules as a transmitter and a sensor as the receiver . In this paper, a novel approach based on fluid dynamics is proposed for the derivation of the distance estimation in practical MC systems. According to this approach, transmitted molecules are considered as moving droplets in the MC channel. With this approach, the Fluid Dynamics-Based Distance Estimation algorithm which predicts the propagation distance of the transmitted droplets by updating the diameter of evaporating droplets at each time step is proposed. FDDE algorithm is validated by experimental data. The results reveal that the distance can be estimated by the fluid dynamics approach which introduces novel parameters such as the volume fraction of droplets in a mixture of air and liquid droplets and the beamwidth of the TX. Furthermore, the effect of the evaporation is shown with the numerical results."],["the CG algorithm was discovered in the early 1900s. it could solve a","One-Minute Derivation of The Conjugate Gradient Algorithm","summarize: One of the great triumphs in the history of numerical methods was the discovery of the Conjugate Gradient algorithm. It could solve a symmetric positive-definite system of linear equations of dimension N in exactly N steps. As many practical problems at that time belonged to this category, CG algorithm became rapidly popular. It remains popular even today due to its immense computational power. But despite its amazing computational ability, mathematics of this algorithm is not easy to learn. Lengthy derivations, redundant notations, and over-emphasis on formal presentation make it much difficult for a beginner to master this algorithm. This paper aims to serve as a starting point for such readers. It provides a curt, easy-to-follow but minimalist derivation of the algorithm by keeping the sufficient steps only, maintaining a uniform notation, and focusing entirely on the ease of reader."],["hippocampus segmentation is an active research field. most current state-of-","Hippocampus Segmentation on Epilepsy and Alzheimer's Disease Studies with Multiple Convolutional Neural Networks","summarize: Hippocampus segmentation on magnetic resonance imaging is of key importance for the diagnosis, treatment decision and investigation of neuropsychiatric disorders. Automatic segmentation is an active research field, with many recent models using deep learning. Most current state-of-the art hippocampus segmentation methods train their methods on healthy or Alzheimer's disease patients from public datasets. This raises the question whether these methods are capable of recognizing the hippocampus on a different domain, that of epilepsy patients with hippocampus resection. In this paper we present a state-of-the-art, open source, ready-to-use, deep learning based hippocampus segmentation method. It uses an extended 2D multi-orientation approach, with automatic pre-processing and orientation alignment. The methodology was developed and validated using HarP, a public Alzheimer's disease hippocampus segmentation dataset. We test this methodology alongside other recent deep learning methods, in two domains: The HarP test set and an in-house epilepsy dataset, containing hippocampus resections, named HCUnicamp. We show that our method, while trained only in HarP, surpasses others from the literature in both the HarP test set and HCUnicamp in Dice. Additionally, Results from training and testing in HCUnicamp volumes are also reported separately, alongside comparisons between training and testing in epilepsy and Alzheimer's data and vice versa. Although current state-of-the-art methods, including our own, achieve upwards of 0.9 Dice in HarP, all tested methods, including our own, produced false positives in HCUnicamp resection regions, showing that there is still room for improvement for hippocampus segmentation methods when resection is involved."],["accretion via disks can make neutron stars fast spinning. some of these","Application of a new method to study the spin equilibrium of Aql X-1: the possibility of gravitational radiation","summarize: Accretion via disks can make neutron stars in low-mass X-ray binaries fast spinning, and some of these stars are detected as millisecond pulsars. Here we report a practical way to find out if a neutron star in a transient LMXB has reached the spin equilibrium by disk--magnetosphere interaction alone, and if not, to estimate this spin equilibrium frequency. These can be done using specific measurable source luminosities, such as the luminosity corresponding to the transition between the accretion and propeller phases, and the known stellar spin rate. Such a finding can be useful to test if the spin distribution of millisecond pulsars, as well as an observed upper cutoff of their spin rates, can be explained using disk--magnetosphere interaction alone, or additional spin-down mechanisms, such as gravitational radiation, are required. Applying our method, we find that the neutron star in the transient LMXB Aql X--1 has not yet reached the spin equilibrium by disk--magnetosphere interaction alone. We also perform numerical computations, with and without gravitational radiation, to study the spin evolution of Aql X--1 through a series of outbursts and to constrain its properties. While we find that the gravitational wave emission from Aql X--1 cannot be established with certainty, our numerical results show that the gravitational radiation from Aql X--1 is possible, with a "],["pulsating members define the subgroup of rapidly oscillating Ap stars. Alpha","New BRITE-Constellation observations of the roAp star Alpha Circini","summarize: Chemically peculiar stars with a measurable magnetic field comprise the group of mCP stars. The pulsating members define the subgroup of rapidly oscillating Ap stars, of which Alpha Circini is the brightest member. Hence, Alpha Circini allows the application of challenging techniques, such as interferometry, very high temporal and spectral resolution photometry, and spectroscopy in a wide wavelength range, that have the potential to provide unique information about the structure and evolution of a star. Based on new photometry from BRITE-Constellation, obtained with blue and red filters, and on photometry from WIRE, SMEI, and TESS we attempt to determine the surface spot structure of Alpha Circini and investigate pulsation frequencies. We used photometric surface imaging and frequency analyses and Bayesian techniques in order to quantitatively compare the probability of different models. BRITE-Constellation photometry obtained from 2014 to 2016 is put in the context of space photometry obtained by WIRE, SMEI, and TESS. This provides improvements in the determination of the rotation period and surface features . The main pulsation frequencies indicate two consecutive radial modes and one intermediate dipolar mode. Advantages and problems of the applied Bayesian technique are discussed."],["in the past several years, we have taken advantage of a number of opportunities to advance the","Precision Medicine as an Accelerator for Next Generation Cognitive Supercomputing","summarize: In the past several years, we have taken advantage of a number of opportunities to advance the intersection of next generation high-performance computing AI and big data technologies through partnerships in precision medicine. Today we are in the throes of piecing together what is likely the most unique convergence of medical data and computer technologies. But more deeply, we observe that the traditional paradigm of computer simulation and prediction needs fundamental revision. This is the time for a number of reasons. We will review what the drivers are, why now, how this has been approached over the past several years, and where we are heading."],["pseudodifferential operators with low regular symbols appear naturally in a Cauchy Problem.","Semiclassical estimates for pseudodifferential operators and the Muskat problem in the unstable regime","summarize: We obtain new semiclassical estimates for pseudodifferential operators with low regular symbols. Such symbols appear naturally in a Cauchy Problem related to recent weak solutions to the unstable Muskat problem constructed via convex integration in . In particular, our new estimates reveal the tight relation between the speed of opening of the mixing zone and the regularity of the interphase."],["probability of stochastic processes is higher than probability. probability of stochastic processes is","Survival probability of stochastic processes beyond persistence exponents","summarize: For many stochastic processes, the probability "],["ergodic control allows us to specify distributions as objectives for area coverage problems for non","Decentralized Ergodic Control: Distribution-Driven Sensing and Exploration for Multi-Agent Systems","summarize: We present a decentralized ergodic control policy for time-varying area coverage problems for multiple agents with nonlinear dynamics. Ergodic control allows us to specify distributions as objectives for area coverage problems for nonlinear robotic systems as a closed-form controller. We derive a variation to the ergodic control policy that can be used with consensus to enable a fully decentralized multi-agent control policy. Examples are presented to illustrate the applicability of our method for multi-agent terrain mapping as well as target localization. An analysis on ergodic policies as a Nash equilibrium is provided for game theoretic applications."],["a linear decoder is trained with 7 GB worth of data. a","UNIQUE: Unsupervised Image Quality Estimation","summarize: In this paper, we estimate perceived image quality using sparse representations obtained from generic image databases through an unsupervised learning approach. A color space transformation, a mean subtraction, and a whitening operation are used to enhance descriptiveness of images by reducing spatial redundancy; a linear decoder is used to obtain sparse representations; and a thresholding stage is used to formulate suppression mechanisms in a visual system. A linear decoder is trained with 7 GB worth of data, which corresponds to 100,000 8x8 image patches randomly obtained from nearly 1,000 images in the ImageNet 2013 database. A patch-wise training approach is preferred to maintain local information. The proposed quality estimator UNIQUE is tested on the LIVE, the Multiply Distorted LIVE, and the TID 2013 databases and compared with thirteen quality estimators. Experimental results show that UNIQUE is generally a top performing quality estimator in terms of accuracy, consistency, linearity, and monotonic behavior."],["phase diagram of two-dimensional systems with continuous symmetry of the vector order parameter containing defects","Two-dimensional O Models with Defects of Random Local Anisotropy Type","summarize: The phase diagram of two-dimensional systems with continuous symmetry of the vector order parameter containing defects of the random local anisotropy type is investigated. In the case of a weakly anisotropic distribution of the easy anisotropy axes in the space of the order parameter, with decreasing temperature, a smooth transition takes place from the paramagnetic phase with dynamic fluctuations of the order parameter to the Imri-Ma phase with its static fluctuations. In the case when the anisotropic distribution of the easy axes induces a global anisotropy of the easy axis type that exceeds a critical value, the system goes into the Ising class of universality, and a phase transition to the ordered state occurs in it at a finite temperature."],["we study the notion of algebraic tangent cones at singularities of reflexive she","Algebraic tangent cones of reflexive sheaves","summarize: We study the notion of algebraic tangent cones at singularities of reflexive sheaves. These correspond to extensions of reflexive sheaves across a negative divisor. We show the existence of optimal extensions in a constructive manner, and we prove the uniqueness in a suitable sense. The results here are an algebro-geometric counterpart of our previous study on singularities of Hermitian-Yang-Mills connections."],["digital systems engineering is coming globally. aims at developing theory, methods, models, and","Towards Digital Engineering -- The Advent of Digital Systems Engineering","summarize: Digital Engineering, the digital transformation of engineering to leverage digital technologies, is coming globally. This paper explores digital systems engineering, which aims at developing theory, methods, models, and tools to support the emerging digital engineering. A critical task is to digitalize engineering artifacts, thus enabling information sharing across platform, across life cycle, and across domains. We identify significant challenges and enabling digital technologies; analyze the transition from traditional engineering to digital engineering; define core concepts, including digitalization, unique identification, digitalized artifacts, digital augmentation, and others; present a big picture of digital systems engineering in four levels: vision, strategy, action, and foundation; briefly discuss each of main areas of research issues. Digitalization enables fast infusing and leveraging novel digital technologies; unique identification enables information traceability and accountability in engineering lifecycle; provenance enables tracing dependency relations among engineering artifacts; supporting model reproducibility and replicability; helping with trustworthiness evaluation of digital engineering artifacts."],["in this paper, we develop an upper bound for the SPARSEVA estimation error.","An analysis of the SPARSEVA estimate for the finite sample data case","summarize: In this paper, we develop an upper bound for the SPARSEVA estimation error in a general scheme, i.e., when the cost function is strongly convex and the regularized norm is decomposable for a pair of subspaces. We show how this general bound can be applied to a sparse regression problem to obtain an upper bound for the traditional SPARSEVA problem. Numerical results are used to illustrate the effectiveness of the suggested bound."],["the informal learning in the workplace is realized during daily collaborators' activities. the informal learning","Adaptation des rseaux sociaux d'entreprise pour favoriser l'apprentissage informel sur le lieu de travail","summarize: The informal learning in the workplace is realized during daily collaborators' activities and represent more than 75 % of the learning occurring in a company. Enterprise social networks are currently massively used to promote this type of learning. From a pilot study, we show that they are actually adapted concerning the social aspects, but that the design must be rethought to consider user contextual needs linked to the content and access to informational corpus, the information quality indicators and the forms of moderation and control."],["glycinium oxalate is the simplest amino acid- carboxy","Hydrogen Bond Symmetrization in Glycinium Oxalate under Pressure","summarize: We report here the evidences of hydrogen bond symmetrization in the simplest amino acid- carboxylic acid complex, glycinium oxalate, at moderate pressures of 8 GPa using in-situ infrared and Raman spectroscopic investigations combined with first-principles simulations. The protonation of the semioxalate units through dynamic proton movement results in infinite oxalate chains. At pressures above 12 GPa, the glycine units systematically reorient with pressure to form hydrogen bonded supramolecular assemblies held together by these chains."],["the temple of Jupiter at Baalbek, Lebanon, is one of the most complex architectural","Archaeoastronomy and the chronology of the Temple of Jupiter at Baalbek","summarize: One of the most complex architectural feats ever conceived is the magnificent temple of Jupiter at Baalbek, Lebanon. Several issues remain unsolved about this site, and in particular the chronology and dating of the two podia and the true nature of the cult. We present here some hints coming from orientation and from other features of the temple, which seem to point to a unified project of both podia, originally conceived under Herod the Great."],["the current practice in neural network optimization is to rely on the stochastic gradient descent algorithm","Deep Frank-Wolfe For Neural Network Optimization","summarize: Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while converging faster. The code is publicly available at https:\/\/github.com\/oval-group\/dfw."],["linear discrete ill-posed problem for large-scale linear discrete ill-posed","Approximation Accuracy of the Krylov Subspaces for Linear Discrete Ill-Posed Problems","summarize: For the large-scale linear discrete ill-posed problem "],["centrality measures have been proposed to address this problem. the proposed method considers the local","Identification of influencers in complex networks by local information dimensionality","summarize: The identification of influential spreaders in complex networks is a popular topic in studies of network characteristics. Many centrality measures have been proposed to address this problem, but most have limitations. In this paper, a method for identifying influencers in complex networks via the local information dimensionality. The proposed method considers the local structural properties around the central node; therefore, the scale of locality only increases to half of the maximum value of the shortest distance from the central node. Thus, the proposed method considers the quasilocal information and reduces the computational complexity. The information in boxes is described via the Shannon entropy, which is more reasonable. A node is more influential when its local information dimensionality is higher. In order to show the effectiveness of the proposed method, five existing centrality measures are used as comparison methods to rank influential nodes in six real world complex networks. In addition, a susceptible infected model and Kendall's tau coefficient are applied to show the correlation between different methods. Experiment results show the superiority of the proposed method."],["thin bismuth films on Bi. could be considered for realization of topological supercon","Origin of Suppression of Proximity Induced Superconductivity in Bi\/Bi","summarize: Mixing of topological states with superconductivity could result in topological superconductivity with the elusive Majorana fermions potentially applicable in fault-tolerant quantum computing. One possible candidate considered for realization of topological superconductivity is thin bismuth films on Bi"],["residualized factor adaptation is a novel approach to community prediction tasks. we use eleven demographic","Residualized Factor Adaptation for Community Social Media Prediction Tasks","summarize: Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both effectively integrates community attributes, as well as adapts linguistic features to community attributes . We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health , psychology , and economics . Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts."],["research conducted in educational institutions during the week. most students in the experiment and graduate students took","Method of Counteraction in Social Engineering on Information Activity Objectives","summarize: The article presents a study using attacks such as a fake access point and a phishing page. The previous publications on social engineering have been reviewed, statistics of break-ups are analyzed and directions and mechanism of realization of attacks having elements of social engineering are analyzed. The data from the research in three different places were collected and analyzed and the content statistics were provided. For comparison, three categories of higher education institutions were chosen: technical, humanitarian and mixed profiles. Since the research was conducted in educational institutions during the week, most students in the experiment and graduate students took part in the experiment. For each educational institution, a registration form template was created that mimicked the design of the main pages. Examples of hardware and software implementation of a typical stand for attack, data collection and analysis are given. In order to construct a test stand, widely available components were chosen to show how easy it is to carry out attacks of this kind without significant initial costs and special skills. The article provides statistics on the number of connections, permission to use the address of the e-mail and password, as well as permission to automatically transfer service data to the browser . The statistics are processed using specially written algorithms. The proposed approaches to solving the problem of socio-technical attacks can be used and implemented for operation on any objects of information activity. As a result of the experiments, it is clear that the awareness of users of even technical specialties is not enough, so one needs to pay particular attention to the development of methods for raising awareness of users and reducing the number of potential attacks on objects of information activity."],["probability of stochastic processes is higher than probability. probability of stochastic processes is","Survival probability of stochastic processes beyond persistence exponents","summarize: For many stochastic processes, the probability "],["colon cancer is one of the most common types of cancer. treatment is planned to depend on","Machine learning approach for segmenting glands in colon histology images using local intensity and texture features","summarize: Colon Cancer is one of the most common types of cancer. The treatment is planned to depend on the grade or stage of cancer. One of the preconditions for grading of colon cancer is to segment the glandular structures of tissues. Manual segmentation method is very time-consuming, and it leads to life risk for the patients. The principal objective of this project is to assist the pathologist to accurate detection of colon cancer. In this paper, the authors have proposed an algorithm for an automatic segmentation of glands in colon histology using local intensity and texture features. Here the dataset images are cropped into patches with different window sizes and taken the intensity of those patches, and also calculated texture-based features. Random forest classifier has been used to classify this patch into different labels. A multilevel random forest technique in a hierarchical way is proposed. This solution is fast, accurate and it is very much applicable in a clinical setup."],["a spin casting the dispersions at room temperature promotes the formation of high quality pe","Room Temperature Nanoparticulate Interfacial Layers for Perovskite Solar Cells via solvothermal synthesis","summarize: We present a solvothermal synthetic route to produce monodispersed CuO nanoparticles in the range of 5-10 nm that can be used as hole selective interfacial layer between indium tin oxide and perovskite active layer for p-i-n perovskite solar cells by a spin casting the dispersions at room temperature. The bottom electrode interface modification provided by spherical CuO-NPs at room temperature promotes the formation of high quality perovskite photoactive layers with large crystal size and strong optical absorption. Furthermore, it is shown that the nanoparticulate nature of the CuO hole transporting interfacial layer can be used to improve light manipulation within perovskite solar cell device structure. The corresponding p-i-n CH3NH3PbI3-based solar cells show high Voc values of 1.09 V, which is significantly higher compared to the Voc values obtained with conventional PEDOT:PSS hole selective contact based perovskite solar cells."],["traditional techniques for indexing these collections fail to properly exploit regularities in order to reduce space","Universal Indexes for Highly Repetitive Document Collections","summarize: Indexing highly repetitive collections has become a relevant problem with the emergence of large repositories of versioned documents, among other applications. These collections may reach huge sizes, but are formed mostly of documents that are near-copies of others. Traditional techniques for indexing these collections fail to properly exploit their regularities in order to reduce space. We introduce new techniques for compressing inverted indexes that exploit this near-copy regularity. They are based on run-length, Lempel-Ziv, or grammar compression of the differential inverted lists, instead of the usual practice of gap-encoding them. We show that, in this highly repetitive setting, our compression methods significantly reduce the space obtained with classical techniques, at the price of moderate slowdowns. Moreover, our best methods are universal, that is, they do not need to know the versioning structure of the collection, nor that a clear versioning structure even exists. We also introduce compressed self-indexes in the comparison. These are designed for general strings and represent the text collection plus the index structure in integrated form. We show that these techniques can compress much further, using a small fraction of the space required by our new inverted indexes. Yet, they are orders of magnitude slower."],["recent works have shown that recommendations of popular content apps are responsible for a significant percentage of users","The Order of Things: Position-Aware Network-friendly Recommendations in Long Viewing Sessions","summarize: Caching has recently attracted a lot of attention in the wireless communications community, as a means to cope with the increasing number of users consuming web content from mobile devices. Caching offers an opportunity for a win-win scenario: nearby content can improve the video streaming experience for the user, and free up valuable network resources for the operator. At the same time, recent works have shown that recommendations of popular content apps are responsible for a significant percentage of users requests. As a result, some very recent works have considered how to nudge recommendations to facilitate the network . In this paper, we follow up on this line of work, and consider the problem of designing cache friendly recommendations for long viewing sessions; specifically, we attempt to answer two open questions in this context: given that recommendation position affects user click rates, what is the impact on the performance of such network-friendly recommender solutions? can the resulting optimization problems be solved efficiently, when considering both sequences of dependent accesses and position preference? To this end, we propose a stochastic model that incorporates position-aware recommendations into a Markovian traversal model of the content catalog, and derive the average cost of a user session using absorbing Markov chain theory. We then formulate the optimization problem, and after a careful sequence of equivalent transformations show that it has a linear program equivalent and thus can be solved efficiently. Finally, we use a range of real datasets we collected to investigate the impact of position preference in recommendations on the proposed optimal algorithm. Our results suggest more than 30\\% improvement with respect to state-of-the-art methods."],["event cameras are novel sensors that output pixel-level brightness changes. they offer significant advantages","Asynchronous, Photometric Feature Tracking using Events and Frames","summarize: We present a method that leverages the complementarity of event cameras and standard cameras to track visual features with low-latency. Event cameras are novel sensors that output pixel-level brightness changes, called events. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, because the same scene pattern can produce different events depending on the motion direction, establishing event correspondences across time is challenging. By contrast, standard cameras provide intensity measurements that do not depend on motion direction. Our method extracts features on frames and subsequently tracks them asynchronously using events, thereby exploiting the best of both types of data: the frames provide a photometric representation that does not depend on motion direction and the events provide low-latency updates. In contrast to previous works, which are based on heuristics, this is the first principled method that uses raw intensity measurements directly, based on a generative event model within a maximum-likelihood framework. As a result, our method produces feature tracks that are both more accurate and longer than the state of the art, across a wide variety of scenes."],["complex networks are pervasive in our society, and impact human behavior via social networks, search","The impossibility of low rank representations for triangle-rich complex networks","summarize: The study of complex networks is a significant development in modern science, and has enriched the social sciences, biology, physics, and computer science. Models and algorithms for such networks are pervasive in our society, and impact human behavior via social networks, search engines, and recommender systems to name a few. A widely used algorithmic technique for modeling such complex networks is to construct a low-dimensional Euclidean embedding of the vertices of the network, where proximity of vertices is interpreted as the likelihood of an edge. Contrary to the common view, we argue that such graph embeddings do not}capture salient properties of complex networks. The two properties we focus on are low degree and large clustering coefficients, which have been widely established to be empirically true for real-world networks. We mathematically prove that any embedding that can successfully create these two properties must have rank nearly linear in the number of vertices. Among other implications, this establishes that popular embedding techniques such as Singular Value Decomposition and node2vec fail to capture significant structural aspects of real-world complex networks. Furthermore, we empirically study a number of different embedding techniques based on dot product, and show that they all fail to capture the triangle structure."],["benzonitrile is a derivative of the simplest aromatic hydrocarbon benzen","Possible detection of interstellar benzonitrile","summarize: The simplest cyanobenzene, benzonitrile have been possibly detected toward the cyanopolyyne peak in TMC-1. We used the results of the 8.8 -- 50 GHz spectral survey of TMC-1 by Kaifu et al. and stacked the lines of benzonitrile that fall within the range of this survey. The obtained spectrum strongly suggests the presence of this molecule. Benzonitrile is a derivative of the simplest aromatic hydrocarbon benzene. Aromatic hydrocarbons are thought to be ubiquitous in the ISM, but it is difficult to study them in molecular cloud interiors, since they are nonpolar and have no allowed transitions at radio frequencies. Therefore it is important to search for their derivatives, such as cyanobenzenes. Thus, the detection of benzonitrile might be important for astrochemistry, but additional sensitive observations are necessary in order to confirm it."],["we leverage longitudinal data from 56 conspiracy communities on Reddit. we first identify 30K","What Makes People Join Conspiracy Communities?: Role of Social Factors in Conspiracy Engagement","summarize: Widespread conspiracy theories, like those motivating anti-vaccination attitudes or climate change denial, propel collective action and bear society-wide consequences. Yet, empirical research has largely studied conspiracy theory adoption as an individual pursuit, rather than as a socially mediated process. What makes users join communities endorsing and spreading conspiracy theories? We leverage longitudinal data from 56 conspiracy communities on Reddit to compare individual and social factors determining which users join the communities. Using a quasi-experimental approach, we first identify 30K future conspiracists- and 30K matched non-conspiracists-. We then provide empirical evidence of importance of social factors across six dimensions relative to the individual factors by analyzing 6 million Reddit comments and posts. Specifically in social factors, we find that dyadic interactions with members of the conspiracy communities and marginalization outside of the conspiracy communities, are the most important social precursors to conspiracy joining-even outperforming individual factor baselines. Our results offer quantitative backing to understand social processes and echo chamber effects in conspiratorial engagement, with important implications for democratic institutions and online communities."],["tempered power-law PDF appears to be a more physical choice. tempered fraction","Third order quasi-compact schemes for space tempered fractional diffusion equations","summarize: Power-law probability density function plays a key role in both subdiffusion and L\\'vy flights. However, sometimes because of the finite of the lifespan of the particles or the boundedness of the physical space, tempered power-law PDF seems to be a more physical choice and then the tempered fractional operators appear; in fact, the tempered fractional operators can also characterize the transitions among subdiffusion, normal diffusion, and L\\'vy flights. This paper focuses on the quasi-compact schemes for space tempered fractional diffusion equations, being much different from the ones for pure fractional derivatives. By using the generation function of the matrix and Weyl's theorem, the stability and convergence of the derived schemes are strictly proved. Some numerical simulations are performed to testify the effectiveness and numerical accuracy of the obtained schemes."],["a unified method for extracting geometric shape features from binary image data is presented in this","Geometric Shape Features Extraction Using a Steady State Partial Differential Equation System","summarize: A unified method for extracting geometric shape features from binary image data using a steady state partial differential equation system as a boundary value problem is presented in this paper. The PDE and functions are formulated to extract the thickness, orientation, and skeleton simultaneously. The main advantages of the proposed method is that the orientation is defined without derivatives and thickness computation is not imposed a topological constraint on the target shape. A one-dimensional analytical solution is provided to validate the proposed method. In addition, two-dimensional numerical examples are presented to confirm the usefulness of the proposed method."],["the numerical range of a Hilbert-space operator is a numerical range.","Remarks on the Crouzeix-Palencia proof that the numerical range is a ","summarize: Crouzeix and Palencia recently showed that the numerical range of a Hilbert-space operator is a "],["generic object recognition is a challenging problem because all possible appearances of objects cannot be registered","Construction of Latent Descriptor Space and Inference Model of Hand-Object Interactions","summarize: Appearance-based generic object recognition is a challenging problem because all possible appearances of objects cannot be registered, especially as new objects are produced every day. Function of objects, however, has a comparatively small number of prototypes. Therefore, function-based classification of new objects could be a valuable tool for generic object recognition. Object functions are closely related to hand-object interactions during handling of a functional object; i.e., how the hand approaches the object, which parts of the object and contact the hand, and the shape of the hand during interaction. Hand-object interactions are helpful for modeling object functions. However, it is difficult to assign discrete labels to interactions because an object shape and grasping hand-postures intrinsically have continuous variations. To describe these interactions, we propose the interaction descriptor space which is acquired from unlabeled appearances of human hand-object interactions. By using interaction descriptors, we can numerically describe the relation between an object's appearance and its possible interaction with the hand. The model infers the quantitative state of the interaction from the object image alone. It also identifies the parts of objects designed for hand interactions such as grips and handles. We demonstrate that the proposed method can unsupervisedly generate interaction descriptors that make clusters corresponding to interaction types. And also we demonstrate that the model can infer possible hand-object interactions."],["a qualitative analysis is given to the data on the full magnetic and velocity vector fields in","Development of active regions: flows, magnetic-field patterns and bordering effect","summarize: A qualitative analysis is given to the data on the full magnetic and velocity vector fields in a growing sunspot group, recorded nearly simultaneously with the Solar Optical Telescope on the Hinode satellite. Observations of a young bipolar subregion developing within AR 11313 were carried out on 9-10 October 2011. Our aim was to form am idea about the consistency of the observed pattern with the well-known rising-tube model of the formation of bipolar acrive regions and sunspot groups. We find from our magnetograms that the distributions of the vertical and the horizontal component of the magnetic field over the area of the magnetic subregion are spatially well correlated; in contrast, the rise of a flux-tube loop would result in a qualitatively different pattern, with the maxima of the two magnetic-field components spatially separated: the vertical field would be the strongest where either spot emerges, while the maximum horizontal-field strengths would be reached in between them. A specific feature, which we call the bordering effect, is revealed: some local extrema of B_v are bordered with areas of locally enhanced B_h. This effect suggests a fountainlike spatial structure of the magnetic field near the B_v extrema, which is also hardly compatible with the emergence of a flux-tube loop. The vertical-velocity field in the area of the developing active subregion does not exhibit any upflow on the scale of the whole subregion, which should be related to the rising-tube process. Thus, our observational data can hardly be interpreted in the framework of the rising-tube model."],["design and architecture are becoming increasingly increasingly popular in the digital world. the technology is being taught","Hybrid design tools - making of a digitally augmented blackboard","summarize: The way that design is being taught is continuously changing under the pressure of the transition from analogical to digital environments. This becomes even more important as the novelty and the alleged superiority of the digital world is used as a marketing tool by competing universities. Even though in some fields of application this approach is desirable, some particular aspects of teaching design and architecture make this transition debatable. The advantages of drawing on blackboards over drawing on whiteboard surfaces in regards of line aesthetic and expression possibilities were previously identified, along with the complementary necessary features for improvement. This study showcases a proof of concept in digitally augmenting a blackboard surface. The system allows the capturing, processing and making real time projections of images over the blackboard surface as trace references. Such a hybrid system, along with providing support for design and architecture related presentations and discussions could also mediate the contradictory relation towards technology that students and teachers have."],["numerical methods for fractional calculus attract increasing interest. we focus on constructing","High-order Numerical Methods for Riesz Space Fractional Turbulent Diffusion Equation","summarize: Numerical methods for fractional calculus attract increasing interests due to its wide applications in various fields such as physics, mechanics, etc. In this paper, we focus on constructing high-order algorithms for Riesz derivatives, where the convergence orders cover from the second order to the sixth order. Then we apply the established schemes to the Riesz space fractional turbulent diffusion equation. Numerical experiments are displayed which support the theoretical analysis."],["Steinhaus theorem is a result that deals with a property of the difference","An Alternative Proof of Steinhaus Theorem","summarize: In measure theory, Steinhaus theorem is a result that deals with a property of the difference between two sets of positive measure. We give a simple elementary proof of the result."],["we tackle the problem of recovering a complex signal.","Solving Complex Quadratic Systems with Full-Rank Random Matrices","summarize: We tackle the problem of recovering a complex signal "],["the first scheme is a balanced Euler scheme and is of order half in the mean-","Order-preserving strong schemes for SDEs with locally Lipschitz coefficients","summarize: We introduce a class of explicit balanced schemes for stochastic differential equations with coefficients of superlinearly growth satisfying a global monotone condition. The first scheme is a balanced Euler scheme and is of order half in the mean-square sense whereas it is of order one under additive noise. The second scheme is a balanced Milstein scheme, which is of order one in the mean-square sense. Some numerical results are presented."],["the paper presents standard and handcrafted features. the influence of various objective factors on the subjective","Study on the Assessment of the Quality of Experience of Streaming Video","summarize: Dynamic adaptive streaming over HTTP provides the work of most multimedia services, however, the nature of this technology further complicates the assessment of the QoE . In this paper, the influence of various objective factors on the subjective estimation of the QoE of streaming video is studied. The paper presents standard and handcrafted features, shows their correlation and p-Value of significance. VQA models based on regression and gradient boosting with SRCC reaching up to 0.9647 on the validation subsample are proposed. The proposed regression models are adapted for applied applications ; the Gradient Boosting Regressor model is perspective for further improvement of the quality estimation model. We take SQoE-III database, so far the largest and most realistic of its kind. The VQA models are available at https:\/\/github.com\/AleksandrIvchenko\/QoE-assesment"],["we prove new pinching estimate for the inverse curvature flow of strictly convex","New pinching estimates for Inverse curvature flows in space forms","summarize: We prove new pinching estimate for the inverse curvature flow of strictly convex hypersurfaces in the space form "],["proposed objective function better estimates sparse low-rank matrices than a con","Improved Sparse Low-Rank Matrix Estimation","summarize: We address the problem of estimating a sparse low-rank matrix from its noisy observation. We propose an objective function consisting of a data-fidelity term and two parameterized non-convex penalty functions. Further, we show how to set the parameters of the non-convex penalty functions, in order to ensure that the objective function is strictly convex. The proposed objective function better estimates sparse low-rank matrices than a convex method which utilizes the sum of the nuclear norm and the "],["the same degree of control has not yet been achieved for few-cycle extreme ultraviolet pulses generated","Phase Control of Attosecond Pulses in a Train","summarize: Ultrafast processes in matter can be captured and even controlled by using sequences of few-cycle optical pulses, which need to be well characterized, both in amplitude and phase. The same degree of control has not yet been achieved for few-cycle extreme ultraviolet pulses generated by high-order harmonic generation in gases, with duration in the attosecond range. Here, we show that by varying the spectral phase and carrier-envelope phase of a high-repetition rate laser, using dispersion in glass, we achieve a high degree of control of the relative phase and CEP between consecutive attosecond pulses. The experimental results are supported by a detailed theoretical analysis based upon the semiclassical three-step model for high-order harmonic generation."],["new materials are necessary for critical advances in technologies. a small electrical current engages with","Towards Electrical-Current Control of Quantum States in Spin-Orbit-Coupled Matter","summarize: Novel materials, which often exhibit surprising or even revolutionary physical properties, are necessary for critical advances in technologies. Simultaneous control of structural and physical properties via a small electrical current is of great significance both fundamentally and technologically. Recent studies demonstrate that a combination of strong spin-orbit interactions and a distorted crystal structure in magnetic Mott insulators is sufficient to attain this long-desired goal. In this Topical Review, we highlight underlying properties of this class of materials and present two representative antiferromagnetic Mott insulators, namely, 4d-electron based Ca2RuO4 and 5d-electron based Sr2IrO4, as model systems. In essence, a small, applied electrical current engages with the lattice, critically reducing structural distortions, which in turn readily suppresses the antiferromagnetic and insulating state and subsequently results in emergent new states. While details may vary in different materials, at the heart of these phenomena are current-reduced lattice distortions, which, via spin-orbit interactions, dictate physical properties. Electrical current, which joins magnetic field, electric field, pressure, light, etc. as a new external stimulus, provides a new, key dimension for materials research, and also pose a series of intriguing questions that may provide the impetus for advancing our understanding of spin-orbit-coupled matter. This Topical Review provides a brief introduction, a few hopefully informative examples and some general remarks. It is by no means an exhaustive report of the current state of studies on this topic."],["neural network is a novel and convenient human-computer interaction. the technology is applied in","Classification and Recognition of Encrypted EEG Data Neural Network","summarize: With the rapid development of Machine Learning technology applied in electroencephalography signals, Brain-Computer Interface has emerged as a novel and convenient human-computer interaction for smart home, intelligent medical and other Internet of Things scenarios. However, security issues such as sensitive information disclosure and unauthorized operations have not received sufficient concerns. There are still some defects with the existing solutions to encrypted EEG data such as low accuracy, high time complexity or slow processing speed. For this reason, a classification and recognition method of encrypted EEG data based on neural network is proposed, which adopts Paillier encryption algorithm to encrypt EEG data and meanwhile resolves the problem of floating point operations. In addition, it improves traditional feed-forward neural network by using the approximate function instead of activation function and realizes multi-classification of encrypted EEG data. Extensive experiments are conducted to explore the effect of several metrics on the recognition results. Followed by security and time cost analysis, the proposed model and approach are validated and evaluated on public EEG datasets provided by PhysioNet, BCI Competition IV and EPILEPSIAE. The experimental results show that our proposal has the satisfactory accuracy, efficiency and feasibility compared with other solutions."],["wireless sensor networks are being deployed for different applications. each protocol has its own structure, goals","A General Model for MAC Protocol Selection in Wireless Sensor Networks","summarize: Wireless Sensor Networks are being deployed for different applications, each having its own structure, goals and requirements. Medium access control protocols play a significant role in WSNs and hence should be tuned to the applications. However, there is no for selecting MAC protocols for different situations. Therefore, it is hard to decide which MAC protocol is good for a given situation. Having a precise model for each MAC protocol, on the other hand, is almost impossible. Using the intuition that the protocols in the same behavioral category perform similarly, our goal in this paper is to introduce a general model that selects the protocol that satisfy the given requirements from the category that performs better for a given context. We define the Combined Performance Function to demonstrate the performance of different categories protocols for different contexts. Having the general model, we then discuss the model scalability for adding new protocols, categories, requirements, and performance criteria. Considering energy consumption and delay as the initial performance criteria of the model, we focus on deriving mathematical models for them. The results extracted from CPF are the same as the well-known rule of thumb for the MAC protocols that verifies our model. We validate our models with the help of simulation study. We also implemented the current CPF model in a web page to make the model online and useful."],["the mechanical properties of Mg-4wt.% Zn alloy single crystals along the","Precipitate strengthening of pyramidal slip in Mg-Zn alloys","summarize: The mechanical properties of Mg-4wt.% Zn alloy single crystals along the orientation were measured through micropillar compression at 23C and 100C. Basal slip was dominant in the solution treated alloy, while pyramidal slip occurred in the precipitation hardened alloy. Pyramidal dislocations pass the precipitates by forming Orowan loops, leading to homogeneous deformation and to a strong hardening. The predictions of the yield stress based on the Orowan model were in reasonable agreement with the experimental data. The presence of rod-shape precipitates perpendicular to the basal plane leads to a strong reduction in the plastic anisotropy of Mg."],["the sustainable development strategy in the management of information and communication technology is an advanced research sector.","Eco-Strategy: Towards a New Generation Managerial Model Based on Green IT and CSR","summarize: The sustainable development strategy in the management of information and communication technology is an advanced research sector which provides a theoretical framework for integrating social and environmental responsibilities of business in the development and implementation of the management strategy. This article offers an original management model that integrates the Corporate Social Responsibility approach and Green IT, which enables decision makers, governance and strategic alignment of ICT, business and sustainability. The model offers a new vision of decision making through economic opportunities and increasing pressure from stakeholders. This paper reveals the strategic relevance of the model, on the basis of a literature review, and provides guidelines for sustainable business development of effective management systems, and improvement of the economic, social and environmental performance of companies. The proposed framework provides a new generation managerial approach to ICT management strategy that we call Eco-Strategy."],["we study the existence\/nonexistence of positive solution of positive solution.","Caffarelli-Kohn-Nirenberg type equations of fourth order with the critical exponent and Rellich potential","summarize: We study the existence\/nonexistence of positive solution of "],["the fourth international workshop on Verification and program transformation is held in Eindhoven, the","Proceedings of the Fourth International Workshop on Verification and Program Transformation","summarize: This volume contains the revised versions of papers presented at the Fourth International Workshop on Verification and Program Transformation on April 2, 2016 in Eindhoven, The Netherlands. The workshop is an event of the European Joint Conferences on Theory and Practice of Software . The aim of the VPT workshops is to provide a forum where people from the area of program transformation and the area of program verification can fruitfully exchange ideas and gain a deeper understanding of the interactions between those two fields. The research papers which have been recently published in those fields, show that the interactions are very beneficial and, indeed, go both ways. In one direction, methods and tools developed in the field of program transformation, such as partial deduction, partial evaluation, fold\/unfold transformations, and supercompilation, have all been applied with success for the verification of systems, and in particular, the verification of infinite state and parameterized systems. In the other direction, methods developed in program verification, such as model checking, abstract interpretation, SAT and SMT solving, and automated theorem proving, have been used to enhance program transformation techniques, thereby making these techniques more powerful and useful in practice."],["2D deep learning methods are favored for their computational efficiency. but existing 2D deep","ACEnet: Anatomical Context-Encoding Network for Neuroanatomy Segmentation","summarize: Segmentation of brain structures from magnetic resonance scans plays an important role in the quantification of brain morphology. Since 3D deep learning models suffer from high computational cost, 2D deep learning methods are favored for their computational efficiency. However, existing 2D deep learning methods are not equipped to effectively capture 3D spatial contextual information that is needed to achieve accurate brain structure segmentation. In order to overcome this limitation, we develop an Anatomical Context-Encoding Network to incorporate 3D spatial and anatomical contexts in 2D convolutional neural networks for efficient and accurate segmentation of brain structures from MR scans, consisting of 1) an anatomical context encoding module to incorporate anatomical information in 2D CNNs and 2) a spatial context encoding module to integrate 3D image information in 2D CNNs. In addition, a skull stripping module is adopted to guide the 2D CNNs to attend to the brain. Extensive experiments on three benchmark datasets have demonstrated that our method achieves promising performance compared with state-of-the-art alternative methods for brain structure segmentation in terms of both computational efficiency and segmentation accuracy."],["field equations are derived in an extended theory of gravity. the model behaves as","Magnetized cosmological model with variable deceleration parameter","summarize: In this paper, we have derived the field equations in an extended theory of gravity in an anisotropic space time background and in the presence of magnetic field. The physical and geometrical parameters of the models are determined with respect to the Hubble parameter using some algebraic approaches. A time varying scale factor has been introduced to analyze the behavior of the model. From some diagnostic approach, we found that the model behaves as "],["we covariantize the results and add all possible terms implying curvature","Generalized Proca action for an Abelian vector field","summarize: We revisit the most general theory for a massive vector field with derivative self-interactions, extending previous works on the subject to account for terms having trivial total derivative interactions for the longitudinal mode. In the flat spacetime case, we obtain all the possible terms containing products of up to five first-order derivatives of the vector field, and provide a conjecture about higher-order terms. Rendering the metric dynamical, we covariantize the results and add all possible terms implying curvature."],["the Madala hypothesis postulates a heavy scalar, H, which","The Madala hypothesis with Run 1 and 2 data at the LHC","summarize: The Madala hypothesis postulates a new heavy scalar, H, which explains several independent anomalous features seen in ATLAS and CMS data simultaneously. It has already been discussed and constrained in the literature by Run 1 results, and its underlying theory has been explored under the interpretation of a two Higgs doublet model coupled with a scalar singlet, "],["a graph is based on a graph.","Long Cycles and Spanning Subgraphs of Locally Maximal 1-planar Graphs","summarize: A graph is "],["a graph of the graph shows the sex of the sex of the","The vertex Folkman numbers ","summarize: For a graph "],["the animation industry outsources large animation workloads to foreign countries where labor is inexpensive and long","Artist-Guided Semiautomatic Animation Colorization","summarize: There is a delicate balance between automating repetitive work in creative domains while staying true to an artist's vision. The animation industry regularly outsources large animation workloads to foreign countries where labor is inexpensive and long hours are common. Automating part of this process can be incredibly useful for reducing costs and creating manageable workloads for major animation studios and outsourced artists. We present a method for automating line art colorization by keeping artists in the loop to successfully reduce this workload while staying true to an artist's vision. By incorporating color hints and temporal information to an adversarial image-to-image framework, we show that it is possible to meet the balance between automation and authenticity through artist's input to generate colored frames with temporal consistency."],["we present the latest ATLAS and CMS measurements of several properties of the Higgs boson","New results on Higgs boson properties","summarize: We present the latest ATLAS and CMS measurements of several properties of the Higgs boson, such as signal-strength modifiers for the main production modes, fiducial and differential cross sections, and the Higgs mass. We have analyzed the 13 TeV proton-proton LHC collision data recorded in 2016, corresponding to integrated luminosities up to "],["the atDELFI system was created to investigate procedural generation of instructions that teach players how","AtDelfi: Automatically Designing Legible, Full Instructions For Games","summarize: This paper introduces a fully automatic method for generating video game tutorials. The AtDELFI system was created to investigate procedural generation of instructions that teach players how to play video games. We present a representation of game rules and mechanics using a graph system as well as a tutorial generation method that uses said graph representation. We demonstrate the concept by testing it on games within the General Video Game Artificial Intelligence framework; the paper discusses tutorials generated for eight different games. Our findings suggest that a graph representation scheme works well for simple arcade style games such as Space Invaders and Pacman, but it appears that tutorials for more complex games might require higher-level understanding of the game than just single mechanics."],["the feasible region can be employed for the selection of feasible footholds and CoM tra","Feasible Region: an Actuation-Aware Extension of the Support Region","summarize: In legged locomotion the projection of the robot Center of Mass being inside the convex hull of the contact points is a commonly accepted sufficient condition to achieve static balancing. However, some of these configurations cannot be realized because the joint torques required to sustain them would be above their limits . In this manuscript we rule out such configurations and define the Feasible Region, a revisited support region that guarantees both global static stability in the sense of tipover and slippage avoidance and of existence of a set of joint-torques that are able to sustain the robot body weight. We show that the feasible region can be employed for the selection of feasible footholds and CoM trajectories to achieve static locomotion on rough terrains, also in presence of load intensive tasks. Key results of our approach include the efficiency in the computation of the feasible region thanks to an Iterative Projection algorithm. This allowed us to carry out successful experiments on the HyQ robot, that was able to negotiate obstacles of moderate dimensions while carrying an extra 10 kg payload."],["Gavruta introduces the new gavruta.","Weaving K-frames in Hilbert Spaces","summarize: Gavruta introduced "],["NNs are increasingly being deployed in safety critical domains. we propose a novel","Deep Evidential Regression","summarize: Deterministic neural networks are increasingly being deployed in safety critical domains, where calibrated, robust, and efficient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution examples for training, thus enabling efficient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and OOD test samples."],["the nonzero level sets of a homogeneous polynomial are proper","Functions dividing their Hessian determinants and affine spheres","summarize: The nonzero level sets of a homogeneous, logarithmically homogeneous, or translationally homogeneous function are affine spheres if and only if the Hessian determinant of the function is a multiple of a power or an exponential of the function. In particular, the nonzero level sets of a homogeneous polynomial are proper affine spheres if some power of it equals a nonzero multiple of its Hessian determinant. The relative invariants of real forms of regular irreducible prehomogeneous vector spaces yield many such polynomials which are moreover irreducible. For example, the nonzero level sets of the Cayley hyperdeterminant are affine spheres."],["We study the Ricci flow on the Ricci flow on the Ricci flow.","Rotationally symmetric Ricci flow on ","summarize: We study the Ricci flow on "],["CPCL is a distributed MIMO radar service. it can be offered by mobile","Cooperative Passive Coherent Location: A Promising 5G Service to Support Road Safety","summarize: 5G promises many new vertical service areas beyond simple communication and data transfer. We propose CPCL , a distributed MIMO radar service, which can be offered by mobile radio network operators as a service for public user groups. CPCL comes as an inherent part of the radio network and takes advantage of the most important key features proposed for 5G. It extends the well-known idea of passive radar by introducing cooperative principles. These range from cooperative, synchronous radio signaling, and MAC up to radar data fusion on sensor and scenario levels. By using software-defined radio and network paradigms, as well as real-time mobile edge computing facilities intended for 5G, CPCL promises to become a ubiquitous radar service which may be adaptive, reconfigurable, and perhaps cognitive. As CPCL makes double use of radio resources , it can be considered a green technology. Although we introduce the CPCL idea from the viewpoint of vehicle-to-vehicle\/infrastructure communication, it can definitely also be applied to many other applications in industry, transport, logistics, and for safety and security applications."],["49 new times of minimum were obtained between 2005 and 2020. this allows the derivation","CK Aqr time keeping. Evidence for a third body","summarize: Photometric measurements of the contact binary system CK Aqr are presented. 49 new times of minimum were obtained between 2005 and 2020. Along with already published observations, this allows the derivation of an improved ephemeris. The resulting O-C diagram shows oscillations which are interpreted as the light time travel effect due to a third component, with a period of 8.2 years."],["the exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data","When Simpler Data Does Not Imply Less Information: A Study of User Profiling Scenarios with Constrained View of Mobile HTTP Traffic","summarize: The exponential growth in smartphone adoption is contributing to the availability of vast amounts of human behavioral data. This data enables the development of increasingly accurate data-driven user models that facilitate the delivery of personalized services which are often free in exchange for the use of its customers' data. Although such usage conventions have raised many privacy concerns, the increasing value of personal data is motivating diverse entities to aggressively collect and exploit the data. In this paper, we unfold profiling scenarios around mobile HTTP traffic, focusing on those that have limited but meaningful segments of the data. The capability of the scenarios to profile personal information is examined with real user data, collected in-the-wild from 61 mobile phone users for a minimum of 30 days. Our study attempts to model heterogeneous user traits and interests, including personality, boredom proneness, demographics, and shopping interests. Based on our modeling results, we discuss various implications to personalization, privacy, and personal data rights."],["an IRS is deployed to adjust its surface reflecting elements to ensure secure communication of multiple legitimate users in","Deep Reinforcement Learning Based Intelligent Reflecting Surface for Secure Wireless Communications","summarize: In this paper, we study an intelligent reflecting surface -aided wireless secure communication system for physical layer security, where an IRS is deployed to adjust its surface reflecting elements to guarantee secure communication of multiple legitimate users in the presence of multiple eavesdroppers. Aiming to improve the system secrecy rate, a design problem for jointly optimizing the base station 's beamforming and the IRS's reflecting beamforming is formulated given the different quality of service requirements and time-varying channel condition. As the system is highly dynamic and complex, and it is challenging to address the non-convex optimization problem, a novel deep reinforcement learning -based secure beamforming approach is firstly proposed to achieve the optimal beamforming policy against eavesdroppers in dynamic environments. Furthermore, post-decision state and prioritized experience replay schemes are utilized to enhance the learning efficiency and secrecy performance. Specifically, PDS is capable of tracing the environment dynamic characteristics and adjust the beamforming policy accordingly. Simulation results demonstrate that the proposed deep PDS-PER learning-based secure beamforming approach can significantly improve the system secrecy rate and QoS satisfaction probability in IRS-aided secure communication systems."],["a specific FSPVM, denoted as FSPVM-E, was developed","Optimization of Executable Formal Interpreters developed in Higher-order Theorem Proving Systems","summarize: In recent publications, we presented a novel formal symbolic process virtual machine framework that combined higher-order theorem proving and symbolic execution for verifying the reliability and security of smart contracts developed in the Ethereum blockchain system without suffering the standard issues surrounding reusability, consistency, and automation. A specific FSPVM, denoted as FSPVM-E, was developed in Coq based on a general, extensible, and reusable formal memory framework, an extensible and universal formal intermediate programming language, denoted as Lolisa, which is a large subset of the Solidity programming language that uses generalized algebraic datatypes, and a corresponding formally verified interpreter for Lolisa, denoted as FEther, which serves as a crucial component of FSPVM-E. However, our past work has demonstrated that the execution efficiency of the standard development of FEther is extremely low. As a result, FSPVM-E fails to achieve its expected verification effect. The present work addresses this issue by first identifying three root causes of the low execution efficiency of formal interpreters. We then build abstract models of these causes, and present respective optimization schemes for rectifying the identified conditions. Finally, we apply these optimization schemes to FEther, and demonstrate that its execution efficiency has been improved significantly."],["quartz-crystal microbalance is used to measure mass of gas uptake.","Using a high-stability quartz-crystal microbalance to measure and model the chemical kinetics for gases in and on metals: oxygen in gold","summarize: This paper describes the use of a high-stability quartz-crystal microbalance to measure the mass of a gas absorbed on and in the metal electrode on the quartz oscillator, when the gas pressure is low and the gas can be considered as rigidly attached to the metal, so viscosity effects are negligible. This provides an absolute measure of the total mass of gas uptake as a function of time, which can be used to model the kinetic processes involved. The technique can measure diffusion parameters of gases in metals close to room temperature at gas pressures much below one atmosphere, as relevant to surface processes such as atomic layer deposition and model studies of heterogeneous catalysis, whereas traditional diffusion measurements require temperatures over 400oC at gas pressures of at least a few Torr. A strong aspect of the method is the ability to combine the bulk measurement of absorbed mass by a QCM with a surface-sensitive technique such as Auger electron spectroscopy in the same vacuum chamber. The method is illustrated using atomic oxygen, formed under O2 gas at 6x10-5 Torr in the presence of a hot tungsten filament, interacting with the gold electrode on a QCM crystal held at 52 to 120oC. Some of the incident oxygen forms a surface oxide which eventually blocks more uptake, and the rest, about 80%, indiffuses. Surprisingly, the rate of oxygen uptake initially increases with the amount of oxygen previously absorbed; therefore, the measured oxygen uptake with time is reproducible only if pre-adsorption of oxygen conditions the sample. Temperatures above 130oC are necessary for measurable thermal desorption, but all the oxygen can be removed by CO scavenging at all temperatures of these experiments. Simple kinetic models are developed for fitting the experimental data to extract relevant parameters."],["a linear decoder is trained with 7 GB worth of data. a","UNIQUE: Unsupervised Image Quality Estimation","summarize: In this paper, we estimate perceived image quality using sparse representations obtained from generic image databases through an unsupervised learning approach. A color space transformation, a mean subtraction, and a whitening operation are used to enhance descriptiveness of images by reducing spatial redundancy; a linear decoder is used to obtain sparse representations; and a thresholding stage is used to formulate suppression mechanisms in a visual system. A linear decoder is trained with 7 GB worth of data, which corresponds to 100,000 8x8 image patches randomly obtained from nearly 1,000 images in the ImageNet 2013 database. A patch-wise training approach is preferred to maintain local information. The proposed quality estimator UNIQUE is tested on the LIVE, the Multiply Distorted LIVE, and the TID 2013 databases and compared with thirteen quality estimators. Experimental results show that UNIQUE is generally a top performing quality estimator in terms of accuracy, consistency, linearity, and monotonic behavior."],["difference-in-difference estimator for a comparative interrupted time series design will","Bracketing in the Comparative Interrupted Time-Series Design to Address Concerns about History Interacting with Group: Evaluating Missouri Handgun Purchaser Law","summarize: In the comparative interrupted time series design , the change in outcome in a group exposed to treatment in the periods before and after the exposure is compared to the change in outcome in a control group not exposed to treatment in either period. The standard difference-in-difference estimator for a comparative interrupted time series design will be biased for estimating the causal effect of the treatment if there is an interaction between history in the after period and the groups; for example, there is a historical event besides the start of the treatment in the after period that benefits the treated group more than the control group. We present a bracketing method for bounding the effect of an interaction between history and the groups that arises from a time-invariant unmeasured confounder having a different effect in the after period than the before period. The method is applied to a study of the effect of the repeal of Missouri's permit-to-purchase handgun law on its firearm homicide rate. We estimate that the effect of the permit-to-purchase repeal on Missouri's firearm homicide rate is bracketed between 0.9 and 1.3 homicides per 100,000 people, corresponding to a percentage increase of 17% to 27% . A placebo study provides additional support for the hypothesis that the repeal has a causal effect of increasing the rate of state-wide firearm homicides."],["a paper focuses on the world of CAPs, based exclusively on the analysis","Results of a Collective Awareness Platforms Investigation","summarize: In this paper we provide two introductory analyses of CAPs, based exclusively on the analysis of documents found on the Internet. The first analysis allowed us to investigate the world of CAPs, in particular for what concerned their status , the scope of those platforms and the typology of users. In order to develop a more accurate model of CAPs, and to understand more deeply the motivation of the users and the type of expected payoff, we analysed those CAPs from the above list that are still alive and we used two models developed for what concerned the virtual community and the collective intelligence."],["top-down saliency models produce a probability map that peaks at target locations specified","Backtracking Spatial Pyramid Pooling -based Image Classifier for Weakly Supervised Top-down Salient Object Detection","summarize: Top-down saliency models produce a probability map that peaks at target locations specified by a task\/goal such as object detection. They are usually trained in a fully supervised setting involving pixel-level annotations of objects. We propose a weakly supervised top-down saliency framework using only binary labels that indicate the presence\/absence of an object in an image. First, the probabilistic contribution of each image region to the confidence of a CNN-based image classifier is computed through a backtracking strategy to produce top-down saliency. From a set of saliency maps of an image produced by fast bottom-up saliency approaches, we select the best saliency map suitable for the top-down task. The selected bottom-up saliency map is combined with the top-down saliency map. Features having high combined saliency are used to train a linear SVM classifier to estimate feature saliency. This is integrated with combined saliency and further refined through a multi-scale superpixel-averaging of saliency map. We evaluate the performance of the proposed weakly supervised topdown saliency and achieve comparable performance with fully supervised approaches. Experiments are carried out on seven challenging datasets and quantitative results are compared with 40 closely related approaches across 4 different applications."],["energy storage units can use excess electricity from RES efficiently and to prevent curtailment.","Cost-Optimal Operation of Energy Storage Units: Benefits of a Problem-Specific Approach","summarize: The integration of large shares of electricity produced by non-dispatchable Renewable Energy Sources leads to an increasingly volatile energy generation side, with temporary local overproduction. The application of energy storage units has the potential to use this excess electricity from RES efficiently and to prevent curtailment. The objective of this work is to calculate cost-optimal charging strategies for energy storage units used as buffers. For this purpose, a new mathematical optimization method is presented that is applicable to general storage-related problems. Due to a tremendous gain in efficiency of this method compared with standard solvers and proven optimality, calculations of complex problems as well as a high-resolution sensitivity analysis of multiple system combinations are feasible within a very short time. As an example technology, Power-to-Heat converters used in combination with thermal storage units are investigated in detail and optimal system configurations, including storage units with and without energy losses, are calculated and evaluated. The benefits of a problem-specific approach are demonstrated by the mathematical simplicity of our approach as well as the general applicability of the proposed method."],["the weak order on the lattice is a weak order. the weak order","Quotientopes","summarize: For any lattice congruence of the weak order on "],["zinc and oxygen in the bulge are key elements for understanding of the bulge chemical evolution","Oxygen and zinc abundances in 417 Galactic bulge red giants","summarize: Oxygen and zinc in the Galactic bulge are key elements for the understanding of the bulge chemical evolution. Oxygen-to-iron abundance ratios provide a most robust indicator of the star formation rate and chemical evolution of the bulge. Zinc is enhanced in metal-poor stars, behaving as an "],["event logs capture execution of business processes in terms of executed activities. existing techniques for such","PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual Information","summarize: Event logs capture the execution of business processes in terms of executed activities and their execution context. Since logs contain potentially sensitive information about the individuals involved in the process, they should be pre-processed before being published to preserve the individuals' privacy. However, existing techniques for such pre-processing are limited to a process' control-flow and neglect contextual information, such as attribute values and durations. This thus precludes any form of process analysis that involves contextual factors. To bridge this gap, we introduce PRIPEL, a framework for privacy-aware event log publishing. Compared to existing work, PRIPEL takes a fundamentally different angle and ensures privacy on the level of individual cases instead of the complete log. This way, contextual information as well as the long tail process behaviour are preserved, which enables the application of a rich set of process analysis techniques. We demonstrate the feasibility of our framework in a case study with a real-world event log."],["the thermally treated and shaped materials prepared in the shape of cylindrical samples were modified to obtain","Porosity properties of porous ceramic substrates added with zinc and magnesium material","summarize: Ceramic based materials covered with a mixture of mullite and zircon on the top, strengthened by the addition of zinc and magnesium compounds at different rates and made by a co-precipitation method are studied. The thermally treated and shaped materials prepared in the shape of cylindrical samples were modified to obtain different porosities. The addition of zinc to the ceramic based material leads to a significant increase and control of the density of porosity compared to the addition of magnesium. The porosity of all the materials was characterized. The permeability characteristics and the ability for all the pellets to absorb water until the saturation were also studied as a function of the porosity."],["thin film optical elements generate an annular beam by filtering the fundamental Gaussian mode of","Translationally Invariant Generation of Annular Beams using Thin Films","summarize: Thin film optical elements exhibiting translational invariance, and thus robustness to optical misalignment, are crucial for rapid development of compact and integrated optical devices. In this letter, we experimentally demonstrate a beam-shaping element that generates an annular beam by spatially filtering the fundamental Gaussian mode of a laser beam. The element comprises of a one-dimensional photonic crystal cavity fabricated using sputtered thin films. The planar architecture and in-plane symmetry of the element render our beam-shaping technique translationally invariant. The generated annular beam is sensitive to the polarization direction and the wavelength of the incident laser beam. Using this property of the annular beam, we show simultaneous generation of concentric annular beams of different wavelengths. Our experimental observations show an excellent agreement with simulation results performed using finite-difference time-domain method. Such a beam-shaping element has applications in areas ranging from microscopy and medicine to semiconductor lithography and manufacturing in microelectronics industry."],["the Noble Element Simulation Technique. a model reproduces the scintillation","Improved Modeling of ","summarize: We report here methods and techniques for creating and improving a model that reproduces the scintillation and ionization response of a dual-phase liquid and gaseous xenon time-projection chamber. Starting with the recent release of the Noble Element Simulation Technique , electronic recoil data from the "],["unsupervised machine learning techniques are designed and implemented in this project to analyze a dataset from YouTube","Novel Machine Learning Algorithms for Centrality and Cliques Detection in Youtube Social Networks","summarize: The goal of this research project is to analyze the dynamics of social networks using machine learning techniques to locate maximal cliques and to find clusters for the purpose of identifying a target demographic. Unsupervised machine learning techniques are designed and implemented in this project to analyze a dataset from YouTube to discover communities in the social network and find central nodes. Different clustering algorithms are implemented and applied to the YouTube dataset. The well-known Bron-Kerbosch algorithm is used effectively in this research to find maximal cliques. The results obtained from this research could be used for advertising purposes and for building smart recommendation systems. All algorithms were implemented using Python programming language. The experimental results show that we were able to successfully find central nodes through clique-centrality and degree centrality. By utilizing clique detection algorithms, the research shown how machine learning algorithms can detect close knit groups within a larger network."],["phase diagram of two-dimensional systems with continuous symmetry of the vector order parameter containing defects","Two-dimensional O Models with Defects of Random Local Anisotropy Type","summarize: The phase diagram of two-dimensional systems with continuous symmetry of the vector order parameter containing defects of the random local anisotropy type is investigated. In the case of a weakly anisotropic distribution of the easy anisotropy axes in the space of the order parameter, with decreasing temperature, a smooth transition takes place from the paramagnetic phase with dynamic fluctuations of the order parameter to the Imri-Ma phase with its static fluctuations. In the case when the anisotropic distribution of the easy axes induces a global anisotropy of the easy axis type that exceeds a critical value, the system goes into the Ising class of universality, and a phase transition to the ordered state occurs in it at a finite temperature."],["a theoretical prediction of 2D siC compounds with different stoichiometries","Novel bonding patterns and optoelectronic properties of the two-dimensional Si","summarize: The search of new two-dimensional materials with novel optical and electronic properties is always desirable for material development. Here, we report a comprehensive theoretical prediction of 2D SiC compounds with different stoichiometries from C-rich to Si-rich. Besides the previously known hexagonal SiC sheet, we identified two types of hitherto-unknown structural motifs with distinctive bonding features. The first type of 2D SiC monolayer, including t-SiC and t-Si"],["quadrature by expansion solves the problem by locally approximating the potential using","Adaptive quadrature by expansion for layer potential evaluation in two dimensions","summarize: When solving partial differential equations using boundary integral equation methods, accurate evaluation of singular and nearly singular integrals in layer potentials is crucial. A recent scheme for this is quadrature by expansion , which solves the problem by locally approximating the potential using a local expansion centered at some distance from the source boundary. In this paper we introduce an extension of the QBX scheme in 2D denoted AQBX - adaptive quadrature by expansion - which combines QBX with an algorithm for automated selection of parameters, based on a target error tolerance. A key component in this algorithm is the ability to accurately estimate the numerical errors in the coefficients of the expansion. Combining previous results for flat panels with a procedure for taking the panel shape into account, we derive such error estimates for arbitrarily shaped boundaries in 2D that are discretized using panel-based Gauss-Legendre quadrature. Applying our scheme to numerical solutions of Dirichlet problems for the Laplace and Helmholtz equations, and also for solving these equations, we find that the scheme is able to satisfy a given target tolerance to within an order of magnitude, making it useful for practical applications. This represents a significant simplification over the original QBX algorithm, in which choosing a good set of parameters can be hard."],["variance-reduced incremental methods are based on SAGA and SVRG.","Proximal Splitting Meets Variance Reduction","summarize: Despite the rise to fame of incremental variance-reduced methods in recent years, their use in nonsmooth optimization is still limited to few simple cases. This is due to the fact that existing methods require to evaluate the proximity operator for the nonsmooth terms, which can be a costly operation for complex penalties. In this work we introduce two variance-reduced incremental methods based on SAGA and SVRG that can efficiently take into account complex penalties which can be expressed as a sum of proximal terms. This includes penalties such as total variation, group lasso with overlap and trend filtering, to name a few. Furthermore, we also develop sparse variants of the proposed algorithms which can take advantage of sparsity in the input data. Like other incremental methods, it only requires to evaluate the gradient of a single sample per iteration, and so is ideally suited for large scale applications. We provide a convergence rate analysis for the proposed methods and show that they converge with a fixed step-size, achieving in some cases the same asymptotic rate as their full gradient variants. Empirical benchmarks on 3 different datasets illustrate the practical advantages of the proposed methods."],["the gamma-ray burst monitor and the Large Area Telescope on board","Multiple Components in the Broadband ","summarize: GRB 160709A is one of the few bright short gamma-ray bursts detected by both the Gamma-ray Burst Monitor and the Large Area Telescope on board the "],["the affinity matrix is constructed by shallow linear embedding methods. we pretrain a","Generative approach to unsupervised deep local learning","summarize: Most existing feature learning methods optimize inflexible handcrafted features and the affinity matrix is constructed by shallow linear embedding methods. Different from these conventional methods, we pretrain a generative neural network by stacking convolutional autoencoders to learn the latent data representation and then construct an affinity graph with them as a prior. Based on the pretrained model and the constructed graph, we add a self-expressive layer to complete the generative model and then fine-tune it with a new loss function, including the reconstruction loss and a deliberately defined locality-preserving loss. The locality-preserving loss designed by the constructed affinity graph serves as prior to preserve the local structure during the fine-tuning stage, which in turn improves the quality of feature representation effectively. Furthermore, the self-expressive layer between the encoder and decoder is based on the assumption that each latent feature is a linear combination of other latent features, so the weighted combination coefficients of the self-expressive layer are used to construct a new refined affinity graph for representing the data structure. We conduct experiments on four datasets to demonstrate the superiority of the representation ability of our proposed model over the state-of-the-art methods."],["moving active particle is observed to be surrounded by localized topological defects. such","Dressed Active Particles in Spherical Crystals","summarize: We investigate the dynamics of an active particle in two-dimensional spherical crystals, which provide an ideal environment to illustrate the interplay of active particle and crystallographic defects. A moving active particle is observed to be surrounded by localized topological defects, becoming a dressed active particle. Such a physical picture characterizes both the lattice distortion around the moving particle and the healing of the distorted lattice in its trajectory. We find that the dynamical behaviors of an active particle in both random and ballistic motions uniformly conform to this featured scenario, whether the particle is initially a defect or not. We further observe that the defect pattern around a dressed ballistic active particle randomly oscillates between two well-defined wing-like defect motifs regardless of its speed. The established physical picture of dressed active particles in this work partially deciphers the complexity of the intriguing nonequilibrium behaviors in active crystals, and opens the promising possibility of introducing the activity to engineer defects, which has strong connections with the design of materials."],["randomized methods for numerical linear algebra have received growing interest as a general approach to large-","A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication","summarize: In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. In this way, the dimension reduction step encodes a tradeoff between cost and accuracy. However, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know how accurate a given solution is, or how much computation is needed to achieve a given level of accuracy. In the current paper, we study randomized matrix multiplication as a prototype setting for addressing these general problems. As a solution, we develop a bootstrap method for \\emph the accuracy as a function of the reduced dimension . From a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an extrapolation technique. In addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method."],["experiments show a massive acceleration of the annealing of a monolayer of passive","Activity-controlled Annealing of Colloidal Monolayers","summarize: Molecular motors are essential to the living, they generate additional fluctuations that boost transport and assist assembly. Self-propelled colloids, that consume energy to move, hold similar potential for the man-made assembly of microparticles. Yet, experiments showing their use as a powerhouse in materials science lack. Our work explores the design of man-made materials controlled by fluctuations, arising from the internal forces generated by active colloids. Here we show a massive acceleration of the annealing of a monolayer of passive beads by moderate addition of self-propelled microparticles. We rationalize our observations with a model of collisions that drive active fluctuations to overcome kinetic barriers and activate the annealing. The experiment is quantitatively compared with Brownian dynamic simulations that further unveil a dynamical transition in the mechanism of annealing. Active dopants travel uniformly in the system or co-localize at the grain boundaries as a result of the persistence of their motion. Our findings uncover the potential of man-made materials controlled by internal activity and lay the groundwork for the rise of materials science beyond equilibrium."],["a signal experiences multipath propagation in the wireless communication system. the value of Bit","Bit Error Rate Analysis of M-ARY PSK and M-ARY QAM Over Rician Fading Channel","summarize: This paper mainly illustrates the Bit error rate performance of M-ary QAM and M-ary PSK for different values of SNR over Rician Fading channel. A signal experiences multipath propagation in the wireless communication system which causes expeditious signal amplitude fluctuations in time, is defined as fading. Rician Fading is a small signal fading. Rician fading is a hypothetical model for radio propagation inconsistency produced by fractional cancellation of a radio signal by itself and as a result the signal reaches in the receiver by several different paths. In this case, at least one of the destination paths is being lengthened or shortened. From this paper , it can be observed that the value of Bit error rate decreases when signal to noise ratio increases in decibel for Mary QAM and M-ary PSK such as 256 QAM, 64 PSK etc. Constellation diagrams of M-QAM and M-PSK have also been showed in this paper using MATLAB Simulation. The falling of Bit error rate with the increase of diversity order for a fixed value of SNR has also been included in this paper. Diversity is a influential receiver system which offers improvement over received signal strength."],["we study the problem of testing identity against a given distribution. given samples from an unknown","Optimal Identity Testing with High Probability","summarize: We study the problem of testing identity against a given distribution with a focus on the high confidence regime. More precisely, given samples from an unknown distribution "],["a movie description of a singular link cobordism in 4-space is a","Movie moves for singular link cobordisms in 4-dimensional space","summarize: Two singular links are cobordant if one can be obtained from the other by singular link isotopy together with a combination of births or deaths of simple unknotted curves, and saddle point transformations. A movie description of a singular link cobordism in 4-space is a sequence of singular link diagrams obtained from a projection of the cobordism into 3-space by taking 2-dimensional cross sections perpendicular to a fixed direction. We present a set of movie moves that are sufficient to connect any two movies of isotopic singular link cobordisms."],["evolution of variability properties of Galactic transient sources is a diagnostic tool to understand various regime","Time-domain variability properties of XTE J1650-500 during its 2001 outburst: Evidence of disc-jet connection","summarize: Evolution of variability properties of Galactic transient sources is a diagnostic tool to understand various regimes of the accretion flow and its dynamics close to the central black hole. In this paper, we concentrate on the variability properties of the X-ray transient XTE J1650-500 and investigate the evolution of viscous delay, time lag, QPO frequency, and their energy dependence throughout the rising phase as observed by RXTE during its outburst in 2001. Our analysis reveals a delay of 12 \\pm 1 days between one day averaged hard and soft photon light-curves as observed by RXTE\/ASM; QPOs with high rms values are observed in lower energy range; the QPO frequencies and associated time lags were anti-correlated during the initial days of the rising phase, and later on, they were found to be correlated; the time lags of iron line photons with respect to hard and soft photons remained almost constant during the initial days of hard state and the lag magnitude increased during the state transition. We perform comparative studies with outbursts of GX 339-4 and XTE J1550-564. We find the evolution of time lags associated with the QPO characteristics during the outburst - stronger QPOs at low energy, and constant lags of broad Fe-line photons present a unique nature of outburst profile in XTE J1650-500. The possible cause of such variabilities is explained by considering disk-jet geometry closer to the central black hole."],["linearized implicit schemes are proposed to approximate the nonlinear equation with continuous\/discontinu","A preconditioning technique for all-at-once system from the nonlinear tempered fractional diffusion equation","summarize: An all-at-once linear system arising from the nonlinear tempered fractional diffusion equation with variable coefficients is studied. Firstly, the nonlinear and linearized implicit schemes are proposed to approximate such the nonlinear equation with continuous\/discontinuous coefficients. The stabilities and convergences of the two schemes are proved under several suitable assumptions, and numerical examples show that the convergence orders of these two schemes are "],["AFMT is a packet scheduling algorithm to achieve adaptive traffic splitting. it implements","An Adaptive Flow-Aware Packet Scheduling Algorithm for Multipath Tunnelling","summarize: This paper proposes AFMT, a packet scheduling algorithm to achieve adaptive flow-aware multipath tunnelling. AFMT has two unique properties. Firstly, it implements robust adaptive traffic splitting for the subtunnels. Secondly, it detects and schedules bursts of packets cohesively, a scheme that already enabled traffic splitting for load balancing with little to no packet reordering. Several NS-3 experiments over different network topologies show that AFMT successfully deals with changing path characteristics due to background traffic while increasing throughput and reliability."],["digital signatures are cryptographic techniques for validating authenticity and integrity of messages, software, or","Measurement-Device-Independent Quantum Digital Signatures","summarize: Digital signatures play an important role in software distribution, modern communication and financial transactions, where it is important to detect forgery and tampering. Signatures are a cryptographic technique for validating the authenticity and integrity of messages, software, or digital documents. The security of currently used classical schemes relies on computational assumptions. Quantum digital signatures , on the other hand, provide information-theoretic security based on the laws of quantum physics. Recent work on QDS shows that such schemes do not require trusted quantum channels and are unconditionally secure against general coherent attacks. However, in practical QDS, just as in quantum key distribution , the detectors can be subjected to side-channel attacks, which can make the actual implementations insecure. Motivated by the idea of measurement-device-independent quantum key distribution , we present a measurement-device-independent QDS scheme, which is secure against all detector side-channel attacks. Based on the rapid development of practical MDI-QKD, our MDI-QDS protocol could also be experimentally implemented, since it requires a similar experimental setup."],["phishing attacks have become increasingly necessary. we propose a deep learning based data","HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep Learning Techniques on HTML Analysis","summarize: Recently, the development and implementation of phishing attacks require little technical skills and costs. This uprising has led to an ever-growing number of phishing attacks on the World Wide Web. Consequently, proactive techniques to fight phishing attacks have become extremely necessary. In this paper, we propose HTMLPhish, a deep learning based data-driven end-to-end automatic phishing web page classification approach. Specifically, HTMLPhish receives the content of the HTML document of a web page and employs Convolutional Neural Networks to learn the semantic dependencies in the textual contents of the HTML. The CNNs learn appropriate feature representations from the HTML document embeddings without extensive manual feature engineering. Furthermore, our proposed approach of the concatenation of the word and character embeddings allows our model to manage new features and ensure easy extrapolation to test data. We conduct comprehensive experiments on a dataset of more than 50,000 HTML documents that provides a distribution of phishing to benign web pages obtainable in the real-world that yields over 93 percent Accuracy and True Positive Rate. Also, HTMLPhish is a completely language-independent and client-side strategy which can, therefore, conduct web page phishing detection regardless of the textual language."],["a family of operators on the Sturm-Liouville operator family is on the","Trace estimation of a family of periodic Sturm-Liouville operators with application to Robe's restricted three-body problem","summarize: In this paper, we consider a family of Sturm-Liouville operators on the "],["the equation is a stochastic forcing term which is a fractional Wiener noise","2D Navier-Stokes equation with cylindrical fractional Brownian noise","summarize: We consider the Navier-Stokes equation on the 2D torus, with a stochastic forcing term which is a cylindrical fractional Wiener noise of Hurst parameter "],["2D decomposition techniques have shown their advantages over traditional one-dimensional techniques. but","An Optimal Strategy for Accurate Bulge-to-disk Decomposition of Disk Galaxies","summarize: The development of two-dimensional bulge-to-disk decomposition techniques has shown their advantages over traditional one-dimensional techniques, especially for galaxies with non-axisymmetric features. However, the full potential of 2D techniques has yet to be fully exploited. Secondary morphological features in nearby disk galaxies, such as bars, lenses, rings, disk breaks, and spiral arms, are seldom accounted for in 2D image decompositions, even though some image-fitting codes, such as GALFIT, are capable of handling them. We present detailed, 2D multi-model and multi-component decomposition of high-quality "],["we propose a method for extracting 3D models of deformable objects from 2D","C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion","summarize: We propose C3DPO, a method for extracting 3D models of deformable objects from 2D keypoint annotations in unconstrained images. We do so by learning a deep network that reconstructs a 3D object from a single view at a time, accounting for partial occlusions, and explicitly factoring the effects of viewpoint changes and object deformations. In order to achieve this factorization, we introduce a novel regularization technique. We first show that the factorization is successful if, and only if, there exists a certain canonicalization function of the reconstructed shapes. Then, we learn the canonicalization function together with the reconstruction one, which constrains the result to be consistent. We demonstrate state-of-the-art reconstruction results for methods that do not use ground-truth 3D supervision for a number of benchmarks, including Up3D and PASCAL3D+. Source code has been made available at https:\/\/github.com\/facebookresearch\/c3dpo_nrsfm."],["RSA can be used to compare representations in models, model components, and human brains","Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains","summarize: In this paper, we define and apply representational stability analysis , an intuitive way of analyzing neural language models. ReStA is a variant of the popular representational similarity analysis in cognitive neuroscience. While RSA can be used to compare representations in models, model components, and human brains, ReStA compares instances of the same model, while systematically varying single model parameter. Using ReStA, we study four recent and successful neural language models, and evaluate how sensitive their internal representations are to the amount of prior context. Using RSA, we perform a systematic study of how similar the representational spaces in the first and second layers of these models are to each other and to patterns of activation in the human brain. Our results reveal surprisingly strong differences between language models, and give insights into where the deep linguistic processing, that integrates information over multiple sentences, is happening in these models. The combination of ReStA and RSA on models and brains allows us to start addressing the important question of what kind of linguistic processes we can hope to observe in fMRI brain imaging data. In particular, our results suggest that the data on story reading from Wehbe et al. contains a signal of shallow linguistic processing, but show no evidence on the more interesting deep linguistic processing."],["adaptive optics has been employed to compensate for aberrations. but it only applies to","Distortion matrix concept for deep optical imaging in scattering media","summarize: In optical imaging, light propagation is affected by the inhomogeneities of the medium. Sample-induced aberrations and multiple scattering can strongly degrade the image resolution and contrast. Based on a dynamic correction of the incident and\/or reflected wave-fronts, adaptive optics has been employed to compensate for those aberrations. However, it only applies to spatially-invariant aberrations or to thin aberrating layers. Here, we propose a global and non-invasive approach based on the distortion matrix concept. This matrix basically connects any focusing point of the image with the distorted part of its wave-front in reflection. A singular value decomposition of the distortion matrix allows to correct for high-order aberrations and forward multiple scattering over multiple isoplanatic modes. Proof-of-concept experiments are performed through biological tissues including a turbid cornea. We demonstrate a Strehl ratio enhancement up to 2500 and recover a diffraction-limited resolution until a depth of ten scattering mean free paths."],["it is shown that if a non-invertible area preserving local homeo","Transitivity of conservative toral endomorphisms","summarize: It is shown that if a non-invertible area preserving local homeomorphism on "],["a sample of 1286 -selected AGN pairs systematically selected from the Slo","Active Galactic Nucleus Pairs from the Sloan Digital Sky Survey. III. Chandra X-ray Observations Unveil Obscured Double Nuclei","summarize: We present Chandra ACIS-S X-ray imaging spectroscopy for five dual active galactic nucleus candidates. Our targets were drawn from a sample of 1286 -selected AGN pairs systematically selected from the Sloan Digital Sky Survey Seventh Data Release. Each of the targets contains two nuclei separated by "],["a translation unit of a 'chunk' named Frame which comes from frame semantic","Semi-automatic Simultaneous Interpreting Quality Evaluation","summarize: Increasing interpreting needs a more objective and automatic measurement. We hold a basic idea that 'translating means translating meaning' in that we can assessment interpretation quality by comparing the meaning of the interpreting output with the source input. That is, a translation unit of a 'chunk' named Frame which comes from frame semantics and its components named Frame Elements which comes from Frame Net are proposed to explore their matching rate between target and source texts. A case study in this paper verifies the usability of semi-automatic graded semantic-scoring measurement for human simultaneous interpreting and shows how to use frame and FE matches to score. Experiments results show that the semantic-scoring metrics have a significantly correlation coefficient with human judgment."],["we look at the performance and cost implication of running analytics on IoT data.","JANUS: Benchmarking Commercial and Open-Source Cloud and Edge Platforms for Object and Anomaly Detection Workloads","summarize: With diverse IoT workloads, placing compute and analytics close to where data is collected is becoming increasingly important. We seek to understand what is the performance and the cost implication of running analytics on IoT data at the various available platforms. These workloads can be compute-light, such as outlier detection on sensor data, or compute-intensive, such as object detection from video feeds obtained from drones. In our paper, JANUS, we profile the performance\/"],["this paper presents a structured survey of mechanism and protocols to update computer networks in a fast","Survey of Consistent Software-Defined Network Updates","summarize: Computer networks have become a critical infrastructure. In fact, networks should not only meet strict requirements in terms of correctness, availability, and performance, but they should also be very flexible and support fast updates, e.g., due to policy changes, increasing traffic, or failures. This paper presents a structured survey of mechanism and protocols to update computer networks in a fast and consistent manner. In particular, we identify and discuss the different desirable consistency properties that should be provided throughout a network update, the algorithmic techniques which are needed to meet these consistency properties, and the implications on the speed and costs at which updates can be performed. We also explain the relationship between consistent network update problems and classic algorithmic optimization ones. While our survey is mainly motivated by the advent of Software-Defined Networks and their primary need for correct and efficient update techniques, the fundamental underlying problems are not new, and we provide a historical perspective of the subject as well."],["MCTS is one of the most widely used methods for planning. it is used to","Static and Dynamic Values of Computation in MCTS","summarize: Monte-Carlo Tree Search is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the myopic limitations of existing computation-value-based methods in two senses: we are able to account for the impact of non-immediate computations on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art."],["COLTRANE, ConvolutiOnaL TRAjectory NEt","COLTRANE: ConvolutiOnaL TRAjectory NEtwork for Deep Map Inference","summarize: The process of automatic generation of a road map from GPS trajectories, called map inference, remains a challenging task to perform on a geospatial data from a variety of domains as the majority of existing studies focus on road maps in cities. Inherently, existing algorithms are not guaranteed to work on unusual geospatial sites, such as an airport tarmac, pedestrianized paths and shortcuts, or animal migration routes, etc. Moreover, deep learning has not been explored well enough for such tasks. This paper introduces COLTRANE, ConvolutiOnaL TRAjectory NEtwork, a novel deep map inference framework which operates on GPS trajectories collected in various environments. This framework includes an Iterated Trajectory Mean Shift module to localize road centerlines, which copes with noisy GPS data points. Convolutional Neural Network trained on our novel trajectory descriptor is then introduced into our framework to detect and accurately classify junctions for refinement of the road maps. COLTRANE yields up to 37% improvement in F1 scores over existing methods on two distinct real-world datasets: city roads and airport tarmac."],["a procedure to recover explicit self-adjoint matrix Dirac systems on semi","Inverse problems for self-adjoint Dirac systems: explicit solutions and stability of the procedure","summarize: A procedure to recover explicitly self-adjoint matrix Dirac systems on semi-axis from rational Weyl functions is considered. Its stability is proved. GBDT version of Baecklund-Darboux transformation and various important results on Riccati equations are used for this purpose."],["we use hundreds of annotated real-world gaits to classify perceived human emotion from","STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits","summarize: We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the emotional state of the human into one of four emotions: happy, sad, angry, or neutral. We use hundreds of annotated real-world gait videos and augment them with thousands of annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder . We incorporate a novel push-pull regularization loss in the CVAE formulation of STEP-Gen to generate realistic gaits and improve the classification accuracy of STEP. We also release a novel dataset , which consists of "],["superIso Relic v4 includes the superIso routines for the","SuperIso Relic v4: A program for calculating dark matter and flavour physics observables in Supersymmetry","summarize: We describe SuperIso Relic, a public program for the calculation of dark matter relic density and direct and indirect detection rates, which includes in addition the SuperIso routines for the calculation of flavour physics observables. SuperIso Relic v4 incorporates many new features, namely the possibility of multiprocessor calculation of the relic density, new cosmological models, and the implementations of the calculation of the observables related to direct and indirect detection experiments. Furthermore, the new version includes an implementation of the nuclear and astrophysical uncertainties, from namely nuclear form factors, dark matter density and velocity, as well as cosmic ray propagation through the galactic medium."],["PET electronics with a low power consumption digitizing signals close to the SiPMs are preferred","Investigation of the Power Consumption of the PETsys TOFPET2 ASIC","summarize: In state-of-the-art positron emission tomography systems, application-specific integrated circuits are commonly used to precisely digitize the signals of analog silicon photo-multipliers . However, when operating PET electronics in a magnetic resonance system, one faces the challenge of mutual interference of these imaging techniques. To prevent signal deterioration along long analog signal lines, PET electronics with a low power consumption digitizing the signals close to the SiPMs are preferred. In this study, we evaluate the power consumption of the TOFPET2 ASIC. Its power consumption ranges from 3.6 to 7.2 mW\/channel as a function of the input stage impedance and discriminator noise settings. We present an analytical model allowing to compute the power consumption of a given ASIC configuration. The configured input stage impedance and discriminator noise have an impact on the coincidence resolution time, energy resolution, and photon trigger level. Since the TOFPET2 ASIC delivers state-of-the-art performance with a power consumption similar or even lower than other ASICs typically used for PET applications, it is a favorable candidate to digitize the signals of SiPMs in future simultaneous PET\/MR systems."],["multiplier Hopf monoid is a morphism of multiplier bimon","Multiplier Hopf monoids","summarize: The notion of multiplier Hopf monoid in any braided monoidal category is introduced as a multiplier bimonoid whose constituent fusion morphisms are isomorphisms. In the category of vector spaces over the complex numbers, Van Daele's definition of multiplier Hopf algebra is re-obtained. It is shown that the key features of multiplier Hopf algebras remain valid in this more general context. Namely, for a multiplier Hopf monoid A, the existence of a unique antipode is proved --- in an appropriate, multiplier-valued sense --- which is shown to be a morphism of multiplier bimonoids from a twisted version of A to A. For a regular multiplier Hopf monoid the antipode is proved to factorize through a proper automorphism of the object A. Under mild further assumptions, duals in the base category are shown to lift to the monoidal categories of modules and of comodules over a regular multiplier Hopf monoid. Finally, the so-called Fundamental Theorem of Hopf modules is proved --- which states an equivalence between the base category and the category of Hopf modules over a multiplier Hopf monoid."],["negative user preference is an important context that is not sufficiently used by many existing recommender systems","Loss Aversion in Recommender Systems: Utilizing Negative User Preference to Improve Recommendation Quality","summarize: Negative user preference is an important context that is not sufficiently utilized by many existing recommender systems. This context is especially useful in scenarios where the cost of negative items is high for the users. In this work, we describe a new recommender algorithm that explicitly models negative user preferences in order to recommend more positive items at the top of recommendation-lists. We build upon existing machine-learning model to incorporate the contextual information provided by negative user preference. With experimental evaluations on two openly available datasets, we show that our method is able to improve recommendation quality: by improving accuracy and at the same time reducing the number of negative items at the top of recommendation-lists. Our work demonstrates the value of the contextual information provided by negative feedback, and can also be extended to signed social networks and link prediction in other networks."],["randomized methods for low-rank approximation outperform or at least match classical","An implementation of a randomized algorithm for principal component analysis","summarize: Recent years have witnessed intense development of randomized methods for low-rank approximation. These methods target principal component analysis and the calculation of truncated singular value decompositions . The present paper presents an essentially black-box, fool-proof implementation for Mathworks' MATLAB, a popular software platform for numerical computation. As illustrated via several tests, the randomized algorithms for low-rank approximation outperform or at least match the classical techniques in basically all respects: accuracy, computational efficiency , ease-of-use, parallelizability, and reliability. However, the classical procedures remain the methods of choice for estimating spectral norms, and are far superior for calculating the least singular values and corresponding singular vectors ."],["unsupervised object discovery involves uncovering patterns that define objects. this is more challenging than image","StampNet: unsupervised multi-class object discovery","summarize: Unsupervised object discovery in images involves uncovering recurring patterns that define objects and discriminates them against the background. This is more challenging than image clustering as the size and the location of the objects are not known: this adds additional degrees of freedom and increases the problem complexity. In this work, we propose StampNet, a novel autoencoding neural network that localizes shapes over a simple background in images and categorizes them simultaneously. StampNet consists of a discrete latent space that is used to categorize objects and to determine the location of the objects. The object categories are formed during the training, resulting in the discovery of a fixed set of objects. We present a set of experiments that demonstrate that StampNet is able to localize and cluster multiple overlapping shapes with varying complexity including the digits from the MNIST dataset. We also present an application of StampNet in the localization of pedestrians in overhead depth-maps."],["research in computational linguistics is dependent on text corpora. the corp","Provenance for Linguistic Corpora Through Nanopublications","summarize: Research in Computational Linguistics is dependent on text corpora for training and testing new tools and methodologies. While there exists a plethora of annotated linguistic information, these corpora are often not interoperable without significant manual work. Moreover, these annotations might have evolved into different versions, making it challenging for researchers to know the data's provenance. This paper addresses this issue with a case study on event annotated corpora and by creating a new, more interoperable representation of this data in the form of nanopublications. We demonstrate how linguistic annotations from separate corpora can be reliably linked from the start, and thereby be accessed and queried as if they were a single dataset. We describe how such nanopublications can be created and demonstrate how SPARQL queries can be performed to extract interesting content from the new representations. The queries show that information of multiple corpora can be retrieved more easily and effectively because the information of different corpora is represented in a uniform data format."],["the observed sample of such supernovae is still low. the supernovae are similar","A low-luminosity core-collapse supernova very similar to SN 2005cs","summarize: We present observations and analysis of PSN J17292918+7542390, a low-luminosity Type II-P supernova . The observed sample of such events is still low, and their nature is still under debate. Such supernovae are similar to SN 2005cs, a well-observed low-luminosity Type II-P event, having low expansion velocities, and small ejected "],["the involvement of external stakeholders in capstone projects and project courses is desirable due to its potential positive","Involving External Stakeholders in Project Courses","summarize: Problem: The involvement of external stakeholders in capstone projects and project courses is desirable due to its potential positive effects on the students. Capstone projects particularly profit from the inclusion of an industrial partner to make the project relevant and help students acquire professional skills. In addition, an increasing push towards education that is aligned with industry and incorporates industrial partners can be observed. However, the involvement of external stakeholders in teaching moments can create friction and could, in the worst case, lead to frustration of all involved parties. Contribution: We developed a model that allows analysing the involvement of external stakeholders in university courses both in a retrospective fashion, to gain insights from past course instances, and in a constructive fashion, to plan the involvement of external stakeholders. Key Concepts: The conceptual model and the accompanying guideline guide the teachers in their analysis of stakeholder involvement. The model is comprised of several activities . The guideline provides questions that the teachers should answer for each of these activities. In the constructive use, the model allows teachers to define an action plan based on an analysis of potential stakeholders and the pedagogical objectives. In the retrospective use, the model allows teachers to identify issues that appeared during the project and their underlying causes. Drawing from ideas of the reflective practitioner, the model contains an emphasis on reflection and interpretation of the observations made by the teacher and other groups involved in the courses. Key Lessons: Applying the model retrospectively to a total of eight courses shows that it is possible to reveal hitherto implicit risks and assumptions and to gain a better insight into the interaction..."],["the CG algorithm was discovered in the early 1900s. it could solve a","One-Minute Derivation of The Conjugate Gradient Algorithm","summarize: One of the great triumphs in the history of numerical methods was the discovery of the Conjugate Gradient algorithm. It could solve a symmetric positive-definite system of linear equations of dimension N in exactly N steps. As many practical problems at that time belonged to this category, CG algorithm became rapidly popular. It remains popular even today due to its immense computational power. But despite its amazing computational ability, mathematics of this algorithm is not easy to learn. Lengthy derivations, redundant notations, and over-emphasis on formal presentation make it much difficult for a beginner to master this algorithm. This paper aims to serve as a starting point for such readers. It provides a curt, easy-to-follow but minimalist derivation of the algorithm by keeping the sufficient steps only, maintaining a uniform notation, and focusing entirely on the ease of reader."],["social media is a cyber-security risk for every business. it could become a","Protect Against Unintentional Insider Threats: The risk of an employee's cyber misconduct on a Social Media Site","summarize: Social Media is a cyber-security risk for every business. What do people share on the Internet? Almost everything about oneself is shared: friendship, demographics, family, activities, and work-related information. This could become a potential risk in every business if the organization's policies, training and technology fail to properly address these issues. In many cases, it is the employees' behaviour that can put key company information at danger. Social media has turned into a reconnaissance tool for malicious actors and users accounts are now seen as a goldmine for cyber criminals. Investigation of Social Media is in the embryonic stage and thus, is not yet well understood. This research project aims to collect and analyse open-source data from LinkedIn, discover data leakage and analyse personality types through software as a service . The final aim of the study is to understand if there are behavioral factors that can predicting one's attitude toward disclosing sensitive data."],["a framework for the detection of the drivers' intention based on in-cabin and","Driver Intention Anticipation Based on In-Cabin and Driving Scene Monitoring","summarize: Numerous car accidents are caused by improper driving maneuvers. Serious injuries are however avoidable if such driving maneuvers are detected beforehand and the driver is assisted accordingly. In fact, various recent research has focused on the automated prediction of driving maneuver based on hand-crafted features extracted mainly from in-cabin driver videos. Since the outside view from the traffic scene may also contain informative features for driving maneuver prediction, we present a framework for the detection of the drivers' intention based on both in-cabin and traffic scene videos. More specifically, we propose a Convolutional-LSTM -based auto-encoder to extract motion features from the out-cabin traffic, train a classifier which considers motions from both in- and outside of the cabin jointly for maneuver intention anticipation, experimentally prove that the in- and outside image features have complementary information. Our evaluation based on the publicly available dataset Brain4cars shows that our framework achieves a prediction with the accuracy of 83.98% and F1-score of 84.3%."],["the sign structure of the Jacobi matrix carries the information about which components of the network inhibit","Representing Model Ensembles as Boolean Functions","summarize: Families of ODE models characterized by a common sign structure of their Jacobi matrix are investigated within the formalism of qualitative differential equations. In the context of regulatory networks the sign structure of the Jacobi matrix carries the information about which components of the network inhibit or activate each other. Information about constraints on the behavior of models in this family is stored in a so called qualitative state transition graph. We showed previously that a similar approach can be used to analyze a model pool of Boolean functions characterized by a common interaction graph. Here we show that the opposite approach is fruitful as well. We show that the qualitative state transition graph can be reduced to a skeleton represented by a Boolean function conserving the reachability properties. This reduction has the advantage that approaches such as model checking and network inference methods can be applied to the skeleton within the framework of Boolean networks. Furthermore, our work constitutes an alternative to approach to link Boolean networks and differential equations."],["the model accounts for all the particle-level and many-body physics of the system","Nonaffine deformation and tunable yielding of colloidal assemblies at the air-water interface","summarize: Silica nanoparticles trapped at air-water interface form a 2D solid state with amorphous order. We propose a theoretical model to describe how this solid-like state deforms under a shear strain ramp up to and beyond a yielding point which leads to plastic flow. The model accounts for all the particle-level and many-body physics of the system: nonaffine displacements, local connectivity and its evolution in terms of cage-breaking, and interparticle interactions mediated by the particle chemistry and colloidal forces. The model is able to reproduce experimental data with only two non-trivial fitting parameters: the relaxation time of the cage and the viscous relaxation time. The interparticle spring constant contains information about the strength of interparticle bonding which is tuned by the amount of surfactant that renders the particles hydrophobic and mutually attractive. This framework opens up the possibility of quantitatively tuning and rationally designing the mechanical response of colloidal assemblies at the air-water interface. Also, it provides a mechanistic explanation to the observed non-monotonic dependence of yield strain on surfactant concentration."],["gerrymandering has long legal history. it has been a long legal","On partisan bias in redistricting: computational complexity meets the science of gerrymandering","summarize: The topic of this paper is gerrymandering, namely the curse of deliberate creations of district maps with highly asymmetric electoral outcomes to disenfranchise voters, and it has a long legal history. Measuring and eliminating gerrymandering has enormous implications to sustain the backbone of democratic principles of a society. Although there is no dearth of legal briefs involving gerrymandering over many years, it is only more recently that mathematicians and applied computational researchers have started to investigate this topic. However, it has received relatively little attention so far from the computational complexity researchers dealing with theoretical analysis of computational complexity issues, such as computational hardness, approximability issues, etc. There could be many reasons for this, such as descriptions of these problem non-CS non-math journals that theoretical CS people usually do not follow, or the lack of coverage of these topics in TCS publication venues. One of our modest goals in writing this article is to improve upon this situation by stimulating further interactions between the gerrymandering and TCS researchers. To this effect, our main contributions are twofold: we provide formalization of several models, related concepts, and corresponding problem statements using TCS frameworks from the descriptions of these problems as available in existing non-TCS venues, and we also provide computational complexity analysis of some versions of these problems, leaving other versions for future research. The goal of writing this article is not to have the final word on gerrymandering, but to introduce a series of concepts, models and problems to the TCS community and to show that science of gerrymandering involves an intriguing set of partitioning problems involving geometric and combinatorial optimization."],["necklace solitary waves on annular domains are stable at low powers. they become","Positive and necklace solitary waves on bounded domains","summarize: We present new solitarywave solutions of the two-dimensional nonlinear Schrodinger equation on bounded domains . These multipeak necklace solitary waves consist of several identical positive profiles , such that adjacent pearls have opposite signs. They are stable at low powers, but become unstable at powers well below the critical power for collapse Pcr. This is in contrast with the ground-state solitary waves on bounded domains, which are stable at any power below Pcr. On annular domains, the ground state solitary waves are radial at low powers, but undergo a symmetry breaking at a threshold power well below Pcr. As in the case of convex bounded domains, necklace solitary waves on the annulus are stable at low powers and become unstable at powers well below Pcr. Unlike on convex bounded domains, however, necklace solitarywaves on the annulus have a second stability regime at powers well above Pcr. For example, when the ratio of the inner to outer radii is 1:2, four-pearl necklaces are stable when their power is between 3.1Pcr and 3.7Pcr. This finding opens the possibility to propagate localized laser beams with substantiallymore power than was possible until now. The instability of necklace solitary waves is excited by perturbations that break the antisymmetry between adjacent pearls, and is manifested by power transfer between pearls. In particular, necklace instability is unrelated to collapse. In order to compute numerically the profile of necklace solitary waves on bounded domains, we introduce a non-spectral variant of Petviashvilis renormalization method."],["paper proves existence of a positive solution of the nonlinear and nonlocal elli","A fractional elliptic problem in ","summarize: In this paper we prove the existence of a positive solution of the nonlinear and nonlocal elliptic equation in "],["afghanistan's eliota sao","Real-Time Dense Stereo Matching With ELAS on FPGA Accelerated Embedded Devices","summarize: For many applications in low-power real-time robotics, stereo cameras are the sensors of choice for depth perception as they are typically cheaper and more versatile than their active counterparts. Their biggest drawback, however, is that they do not directly sense depth maps; instead, these must be estimated through data-intensive processes. Therefore, appropriate algorithm selection plays an important role in achieving the desired performance characteristics. Motivated by applications in space and mobile robotics, we implement and evaluate a FPGA-accelerated adaptation of the ELAS algorithm. Despite offering one of the best trade-offs between efficiency and accuracy, ELAS has only been shown to run at 1.5-3 fps on a high-end CPU. Our system preserves all intriguing properties of the original algorithm, such as the slanted plane priors, but can achieve a frame rate of 47fps whilst consuming under 4W of power. Unlike previous FPGA based designs, we take advantage of both components on the CPU\/FPGA System-on-Chip to showcase the strategy necessary to accelerate more complex and computationally diverse algorithms for such low power, real-time systems."],["the most expensive integration required 4months wall-clock time with a maximum relative energy","Numerical Solutions for the orbital motion of the Solar System over the Past 100 Myr: Limits and new results","summarize: I report results from accurate numerical integrations of Solar System orbits over the past 100Myr with the integrator package HNBody. The simulations used different integrator algorithms, step sizes, initial conditions, and included effects from general relativity, different models of the Moon, the Sun's quadrupole moment, and up to sixteen asteroids. I also probed the potential effect of a hypothetical Planet 9, using one set of possible orbital elements. The most expensive integration required 4~months wall-clock time with a maximum relative energy error <~3e. The difference in Earth's eccentricity was used to track the difference between two solutions, considered to diverge at time tau when max|DeE| irreversibly crossed ~10\\% of mean eE . The results indicate that finding a unique orbital solution is limited by initial conditions from current ephemerides and asteroid perturbations to ~54Myr. Bizarrely, the 4-month Bulirsch-Stoer integration and a symplectic integration that required only 5~hours wall-clock time , agree to ~63Myr. Internally, such symplectic integrations are remarkably consistent even for large time steps, suggesting that the relationship between time step and tau is not a robust indicator for the absolute accuracy of symplectic integrations. The effect of a hypothetical Planet~9 on DeE becomes discernible at ~65Myr. Using tau as a criterion, the current state-of-the-art solutions all differ from previously published results beyond ~50Myr. I also conducted an eigenmode analysis, which provides some insight into the chaotic nature of the inner Solar System. The current study provides new orbital solutions for applications in geological studies."],["a new algorithm aimed at prioritzing measurements can be applied to a wide class","An adaptive scheduling tool to optimize measurements to reach a scientific objective: methodology and application to the measurements of stellar orbits in the Galactic Center","summarize: In various fields of physics and astronomy, access to experimental facilities or to telescopes is becoming more and more competitive and limited. It becomes therefore important to optimize the type of measurements and their scheduling to reach a given scientific objective and to increase the chances of success of a scientific project. In this communication, extending the work of Ford and of Loredo et al. , we present an efficient adaptive scheduling tool aimed at prioritzing measurements in order to reach a scientific goal. The algorithm, based on the Fisher matrix, can be applied to a wide class of measurements. We present this algorithm in detail and discuss some practicalities such as systematic errors or measurements losses due to contigencies . As an illustration, we consider measurements of the short-period star S0-2 in our Galactic Center. We show that the radial velocity measurements at the two turning points of the radial velocity curve are more powerful for detecting the gravitational redshift than measurements at the maximal relativistic signal. We also explicitly present the methodology that was used to plan measurements in order to detect the relativistic redshift considering systematics and possible measurements losses. For the future, we identify the astrometric turning points to be highly sensitive to the relativistic advance of the periastron. Finally, we also identify measurements particularly sensitive to the distance to our Galactic Center: the radial velocities around periastron and the astrometric measurements just before closest approach and at the maximal right ascension astrometric turning point."],["knowledge graph embedding methods have been proposed to address this issue. embedding vector","Analyzing Knowledge Graph Embedding Methods from a Multi-Embedding Interaction Perspective","summarize: Knowledge graph is a popular format for representing knowledge, with many applications to semantic search engines, question-answering systems, and recommender systems. Real-world knowledge graphs are usually incomplete, so knowledge graph embedding methods, such as Canonical decomposition\/Parallel factorization , DistMult, and ComplEx, have been proposed to address this issue. These methods represent entities and relations as embedding vectors in semantic space and predict the links between them. The embedding vectors themselves contain rich semantic information and can be used in other applications such as data analysis. However, mechanisms in these models and the embedding vectors themselves vary greatly, making it difficult to understand and compare them. Given this lack of understanding, we risk using them ineffectively or incorrectly, particularly for complicated models, such as CP, with two role-based embedding vectors, or the state-of-the-art ComplEx model, with complex-valued embedding vectors. In this paper, we propose a multi-embedding interaction mechanism as a new approach to uniting and generalizing these models. We derive them theoretically via this mechanism and provide empirical analyses and comparisons between them. We also propose a new multi-embedding model based on quaternion algebra and show that it achieves promising results using popular benchmarks. Source code is available on github at https:\/\/github.com\/tranhungnghiep\/AnalyzingKGEmbeddings"],["we consider Hilbert spaces of analytic functions in the disk with a normalized reproduc","Hilbert spaces of analytic functions with a contractive backward shift","summarize: We consider Hilbert spaces of analytic functions in the disk with a normalized reproducing kernel and such that the backward shift "],["spectral method and semigroup theory are based on verified computations. the method","Rigorous numerical computations for 1D advection equations with variable coefficients","summarize: This paper provides a methodology of verified computing for solutions to 1-dimensional advection equations with variable coefficients. The advection equation is typical partial differential equations of hyperbolic type. There are few results of verified numerical computations to initial-boundary value problems of hyperbolic PDEs. Our methodology is based on the spectral method and semigroup theory. The provided method in this paper is regarded as an efficient application of semigroup theory in a sequence space associated with the Fourier series of unknown functions. This is a foundational approach of verified numerical computations for hyperbolic PDEs. Numerical examples show that the rigorous error estimate showing the well-posedness of the exact solution is given with high accuracy and high speed."],["generative adversarial networks have been successfully applied to zero-shot learning. generative","Alleviating Feature Confusion for Generative Zero-shot Learning","summarize: Lately, generative adversarial networks have been successfully applied to zero-shot learning and achieved state-of-the-art performance. By synthesizing virtual unseen visual features, GAN-based methods convert the challenging ZSL task into a supervised learning problem. However, GAN-based ZSL methods have to train the generator on the seen categories and further apply it to unseen instances. An inevitable issue of such a paradigm is that the synthesized unseen features are prone to seen references and incapable to reflect the novelty and diversity of real unseen instances. In a nutshell, the synthesized features are confusing. One cannot tell unseen categories from seen ones using the synthesized features. As a result, the synthesized features are too subtle to be classified in generalized zero-shot learning which involves both seen and unseen categories at the test stage. In this paper, we first introduce the feature confusion issue. Then, we propose a new feature generating network, named alleviating feature confusion GAN , to challenge the issue. Specifically, we present a boundary loss which maximizes the decision boundary of seen categories and unseen ones. Furthermore, a novel metric named feature confusion score is proposed to quantify the feature confusion. Extensive experiments on five widely used datasets verify that our method is able to outperform previous state-of-the-arts under both ZSL and GZSL protocols."],["COLTRANE, ConvolutiOnaL TRAjectory NEt","COLTRANE: ConvolutiOnaL TRAjectory NEtwork for Deep Map Inference","summarize: The process of automatic generation of a road map from GPS trajectories, called map inference, remains a challenging task to perform on a geospatial data from a variety of domains as the majority of existing studies focus on road maps in cities. Inherently, existing algorithms are not guaranteed to work on unusual geospatial sites, such as an airport tarmac, pedestrianized paths and shortcuts, or animal migration routes, etc. Moreover, deep learning has not been explored well enough for such tasks. This paper introduces COLTRANE, ConvolutiOnaL TRAjectory NEtwork, a novel deep map inference framework which operates on GPS trajectories collected in various environments. This framework includes an Iterated Trajectory Mean Shift module to localize road centerlines, which copes with noisy GPS data points. Convolutional Neural Network trained on our novel trajectory descriptor is then introduced into our framework to detect and accurately classify junctions for refinement of the road maps. COLTRANE yields up to 37% improvement in F1 scores over existing methods on two distinct real-world datasets: city roads and airport tarmac."],["handwritten annotations can appear in form of underlines and text. the best model achieve","Recognizing Challenging Handwritten Annotations with Fully Convolutional Networks","summarize: This paper introduces a very challenging dataset of historic German documents and evaluates Fully Convolutional Neural Network based methods to locate handwritten annotations of any kind in these documents. The handwritten annotations can appear in form of underlines and text by using various writing instruments, e.g., the use of pencils makes the data more challenging. We train and evaluate various end-to-end semantic segmentation approaches and report the results. The task is to classify the pixels of documents into two classes: background and handwritten annotation. The best model achieves a mean Intersection over Union score of 95.6% on the test documents of the presented dataset. We also present a comparison of different strategies used for data augmentation and training on our presented dataset. For evaluation, we use the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts."],["the largest body in the Main Belt is characterized by a large abundance of water ice","Thermal convection in the crust of the dwarf planet Ceres","summarize: Ceres is the largest body in the Main Belt, and it is characterized by a large abundance of water ice in its interior. This feature is suggested by its relatively low bulk density , while its partial differentiation into a rocky core and icy crust is suggested by several geological and geochemical features: minerals and salts produced by aqueous alteration, icy patches on the surface, lobate morphology interpreted as surface flows. In this work we explore how the composition can influence the characteristics of thermal convection in the crust of Ceres. Our results suggest that the onset of thermal convection is difficult and when it occurs it is short lived and this could imply that Ceres preserved deep liquid until present, as recent suggested by the work of Castillo-Rogez et al.. Moreover, cryovolcanism could be driven by diapirism rather than thermal convection."],["event logs capture execution of business processes in terms of executed activities. existing techniques for such","PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual Information","summarize: Event logs capture the execution of business processes in terms of executed activities and their execution context. Since logs contain potentially sensitive information about the individuals involved in the process, they should be pre-processed before being published to preserve the individuals' privacy. However, existing techniques for such pre-processing are limited to a process' control-flow and neglect contextual information, such as attribute values and durations. This thus precludes any form of process analysis that involves contextual factors. To bridge this gap, we introduce PRIPEL, a framework for privacy-aware event log publishing. Compared to existing work, PRIPEL takes a fundamentally different angle and ensures privacy on the level of individual cases instead of the complete log. This way, contextual information as well as the long tail process behaviour are preserved, which enables the application of a rich set of process analysis techniques. We demonstrate the feasibility of our framework in a case study with a real-world event log."],["new model-based reinforcement learning algorithm uses supervision to constrain exploration and learn efficiently while handling complex constraints","Safety Augmented Value Estimation from Demonstrations : Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","summarize: Reinforcement learning for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations , which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations. Code and supplementary material is available at https:\/\/tinyurl.com\/saved-rl."],["we created an augmented reality platform that recognizes buildings facades. fast recognition and stable","Recognizing and tracking outdoor objects by using ARToolKit markers","summarize: We created an augmented reality platform for spatial exploration that recognizes buildings facades and displays various multimedia for different time points. In order to provide the user with the best user experience fast recognition and stable tracking are the key elements of any augmented reality app. In an outdoor environment, lighting, reflective surfaces and occlusion can drastically affect the user experience. In a setup where these conditions are similar, marker creation methodology and the app parameters are key. In this paper we focus on resizing the photo prior marker creating and the importance of camera calibration and resolution and their effect on the recognition speed and quality of tracking outdoor objects."],["this paper focuses on a class of important two-hop relay mobile ad","End-to-end delay modeling in buffer-limited MANETs: a general theoretical framework","summarize: This paper focuses on a class of important two-hop relay mobile ad hoc networks with limited-buffer constraint and any mobility model that leads to the uniform distribution of the locations of nodes in steady state, and develops a general theoretical framework for the end-to-end delay modeling there. We first combine the theories of Fixed-Point, Quasi-Birth-and-Death process and embedded Markov chain to model the limiting distribution of the occupancy states of a relay buffer, and then apply the absorbing Markov chain theory to characterize the packet delivery process, such that a complete theoretical framework is developed for the E2E delay analysis. With the help of this framework, we derive a general and exact expression for the E2E delay based on the modeling of both packet queuing delay and delivery delay. To demonstrate the application of our framework, case studies are further provided under two network scenarios with different MAC protocols to show how the E2E delay can be analytically determined for a given network scenario. Finally, we present extensive simulation and numerical results to illustrate the efficiency of our delay analysis as well as the impacts of network parameters on delay performance."],["low-cost and ready-to-connect devices are designed to provide new services and applications","MSNM-Sensor: An Applied Network Monitoring Tool for Anomaly Detection in Complex Networks and Systems","summarize: Technology evolves quickly. Low-cost and ready-to-connect devices are designed to provide new services and applications. Smart grids or smart healthcare systems are some examples of these applications, all of which are in the context of smart cities. In this total-connectivity scenario, some security issues arise since the larger the number of connected devices is, the greater the surface attack dimension. In this way, new solutions for monitoring and detecting security events are needed to address new challenges brought about by this scenario, among others, the large number of devices to monitor, the large amount of data to manage and the real-time requirement to provide quick security event detection and, consequently, quick response to attacks. In this work, a practical and ready-to-use tool for monitoring and detecting security events in these environments is developed and introduced. The tool is based on the Multivariate Statistical Network Monitoring methodology for monitoring and anomaly detection and we call it MSNM-Sensor. Although it is in its early development stages, experimental results based on the detection of well-known attacks in hierarchical network systems prove the suitability of this tool for more complex scenarios, such as those found in smart cities or IoT ecosystems."],["glycinium oxalate is the simplest amino acid- carboxy","Hydrogen Bond Symmetrization in Glycinium Oxalate under Pressure","summarize: We report here the evidences of hydrogen bond symmetrization in the simplest amino acid- carboxylic acid complex, glycinium oxalate, at moderate pressures of 8 GPa using in-situ infrared and Raman spectroscopic investigations combined with first-principles simulations. The protonation of the semioxalate units through dynamic proton movement results in infinite oxalate chains. At pressures above 12 GPa, the glycine units systematically reorient with pressure to form hydrogen bonded supramolecular assemblies held together by these chains."],["a dimension witness is a criterion that sets a lower bound on the","A new device-independent dimension witness and its experimental implementation","summarize: A dimension witness is a criterion that sets a lower bound on the dimension needed to reproduce the observed data. Three types of dimension witnesses can be found in the literature: device-dependent ones, in which the bound is obtained assuming some knowledge on the state and the measurements; device-independent prepare-and-measure ones, that can be applied to any system including classical ones; and device-independent Bell-based ones, that certify the minimal dimension of some entangled systems. Here we consider the Collins-Gisin-Linden-Massar-Popescu Bell-type inequality for four outcomes. We show that a sufficiently high violation of this inequality witnesses "],["the first order Sobolev spaces are contained in the generally nonlinear class of general","Sobolev functions on varifolds","summarize: This paper introduces first order Sobolev spaces on certain rectifiable varifolds. These complete locally convex spaces are contained in the generally nonlinear class of generalised weakly differentiable functions and share key functional analytic properties with their Euclidean counterparts. Assuming the varifold to satisfy a uniform lower density bound and a dimensionally critical summability condition on its mean curvature, the following statements hold. Firstly, continuous and compact embeddings of Sobolev spaces into Lebesgue spaces and spaces of continuous functions are available. Secondly, the geodesic distance associated to the varifold is a continuous, not necessarily H\\older continuous Sobolev function with bounded derivative. Thirdly, if the varifold additionally has bounded mean curvature and finite measure, the present Sobolev spaces are isomorphic to those previously available for finite Radon measures yielding many new results for those classes as well. Suitable versions of the embedding results obtained for Sobolev functions hold in the larger class of generalised weakly differentiable functions."],["Bloom Filter is the predominant data structure in approximate membership filtering. the response time is","Shed More Light on Bloom Filter's Variants","summarize: Bloom Filter is a probabilistic membership data structure and it is excessively used data structure for membership query. Bloom Filter becomes the predominant data structure in approximate membership filtering. Bloom Filter extremely enhances the query response time, and the response time is very fast. Bloom filter is used to detect whether an element belongs to a given set or not. The Bloom Filter returns True Positive , False Positive , or True Negative . The Bloom Filter is widely adapted in numerous areas to enhance the performance of a system. In this paper, we present a) in-depth insight on the Bloom Filter,and b) the prominent variants of the Bloom Filters."],["a joint frame and carrier frequency synchronization algorithm is proposed. the algorithm uses the","Discrete FRFT-Based Frame and Frequency Synchronization for Coherent Optical Systems","summarize: A joint frame and carrier frequency synchronization algorithm for coherent optical systems, based on the digital computation of the fractional Fourier transform , is proposed. The algorithm utilizes the characteristics of energy centralization of chirp signals in the FRFT domain, together with the time and phase shift properties of the FRFT. Chirp signals are used to construct a training sequence , and fractional cross-correlation is employed to define a detection metric for the TS, from which a set of equations can be obtained. Estimates of both the timing offset and carrier frequency offset are obtained by solving these equations. This TS is later employed in a phase-dependent decision-directed least-mean square algorithm for adaptive equalization. Simulation results of a 32-Gbaud coherent polarization division multiplexed Nyquist system show that the proposed scheme has a wide CFO estimation range and accurate synchronization performance even in poor optical signal-to-noise ratio conditions."],["paper deals with the problem of the classification of the local graded artinian quotient","Hilbert-Samuel sequences of homogeneous finite type","summarize: This paper deals with the problem of the classification of the local graded Artinian quotients "],["the Madala hypothesis postulates a heavy scalar, H, which","The Madala hypothesis with Run 1 and 2 data at the LHC","summarize: The Madala hypothesis postulates a new heavy scalar, H, which explains several independent anomalous features seen in ATLAS and CMS data simultaneously. It has already been discussed and constrained in the literature by Run 1 results, and its underlying theory has been explored under the interpretation of a two Higgs doublet model coupled with a scalar singlet, "],["previous work has relied heavily on handcrafted features. we investigate different representations of mouse","Learning Efficient Representations of Mouse Movements to Predict User Attention","summarize: Tracking mouse cursor movements can be used to predict user attention on heterogeneous page layouts like SERPs. So far, previous work has relied heavily on handcrafted features, which is a time-consuming approach that often requires domain expertise. We investigate different representations of mouse cursor movements, including time series, heatmaps, and trajectory-based images, to build and contrast both recurrent and convolutional neural networks that can predict user attention to direct displays, such as SERP advertisements. Our models are trained over raw mouse cursor data and achieve competitive performance. We conclude that neural network models should be adopted for downstream tasks involving mouse cursor movements, since they can provide an invaluable implicit feedback signal for re-ranking and evaluation."],["automated segmentation of kidneys and kidney tumors is important step in quantifying the tumor'","3D Kidneys and Kidney Tumor Semantic Segmentation using Boundary-Aware Networks","summarize: Automated segmentation of kidneys and kidney tumors is an important step in quantifying the tumor's morphometrical details to monitor the progression of the disease and accurately compare decisions regarding the kidney tumor treatment. Manual delineation techniques are often tedious, error-prone and require expert knowledge for creating unambiguous representation of kidneys and kidney tumors segmentation. In this work, we propose an end-to-end boundary aware fully Convolutional Neural Networks for reliable kidney and kidney tumor semantic segmentation from arterial phase abdominal 3D CT scans. We propose a segmentation network consisting of an encoder-decoder architecture that specifically accounts for organ and tumor edge information by devising a dedicated boundary branch supervised by edge-aware loss terms. We have evaluated our model on 2019 MICCAI KiTS Kidney Tumor Segmentation Challenge dataset and our method has achieved dice scores of 0.9742 and 0.8103 for kidney and tumor repetitively and an overall composite dice score of 0.8923."],["Cumulative Prospect Theory is a modeling tool widely used in behavioral economics and cognitive psychology","Cumulative Prospect Theory Based Dynamic Pricing for Shared Mobility on Demand Services","summarize: Cumulative Prospect Theory is a modeling tool widely used in behavioral economics and cognitive psychology that captures subjective decision making of individuals under risk or uncertainty. In this paper, we propose a dynamic pricing strategy for Shared Mobility on Demand Services using a passenger behavioral model based on CPT. This dynamic pricing strategy together with dynamic routing via a constrained optimization algorithm that we have developed earlier, provide a complete solution customized for SMoDS of multi-passenger transportation. The basic principles of CPT and the derivation of the passenger behavioral model in the SMoDS context are described in detail. The implications of CPT on dynamic pricing of the SMoDS are delineated using computational experiments involving passenger preferences. These implications include interpretation of the classic fourfold pattern of risk attitudes, strong risk aversion over mixed prospects, and behavioral preferences of self reference. Overall, it is argued that the use of the CPT framework corresponds to a crucial building block in designing socio-technical systems by allowing quantification of subjective decision making under risk or uncertainty that is perceived to be otherwise qualitative."],["we propose a power and data transfer network on a conductive fabric material based on","Inter-IC for Wearables : Power and Data Transfer over Double-sided Conductive Textile","summarize: We propose a power and data transfer network on a conductive fabric material based on an existing serial communication protocol, Inter-Integrated Circuit . We call the proposed network Inter-IC for Wearables . Continuous dc power and I2C-formatted data are simultaneously transferred to tiny sensor nodes distributed on a double-sided conductive textile. The textile has two conductive sides isolated from each other and is used as a single planar transmission line. I2C data are transferred along with dc power supply based on frequency division multiplexing . Two carriers are modulated with the clock and the data signals of I2C. A modulation and demodulation circuit is designed to enable using off-the-shelf I2C-interfaced sensor ICs. One significant originality of this work is that a special filter to enable passive modulation is designed by locating its impedance poles and zeros at appropriate frequencies. The proposed scheme enables flexible implementation of wearable sensor systems in which multiple off-the-shelf tiny sensors are distributed all over a wear."],["RARD II complements datasets from other domains such as books, movies, and music","RARD II: The 94 Million Related-Article Recommendation Dataset","summarize: The main contribution of this paper is to introduce and describe a new recommender-systems dataset . It is based on data from Mr. DLib, a recommender-system as-a-service in the digital library and reference-management-software domain. As such, RARD II complements datasets from other domains such as books, movies, and music. The dataset encompasses 94m recommendations, delivered in the two years from September 2016 to September 2018. The dataset covers an item-space of 24m unique items. RARD II provides a range of rich recommendation data, beyond conventional ratings. For example, in addition to the usual ratings matrices, RARD II includes the original recommendation logs, which provide a unique insight into many aspects of the algorithms that generated the recommendations. The logs enable researchers to conduct various analyses about a real-world recommender system. This includes the evaluation of meta-learning approaches for predicting algorithm performance. In this paper, we summarise the key features of this dataset release, describe how it was generated and discuss some of its unique features. Compared to its predecessor RARD, RARD II contains 64% more recommendations, 187% more features , 50% more clicks, 140% more documents, and one additional service partner ."],["the problem of direction of arrival estimation has been studied for decades. the problem of direction of","A New Atomic Norm for DOA Estimation With Gain-Phase Errors","summarize: The problem of direction of arrival estimation has been studied for decades as an essential technology in enabling radar, wireless communications, and array signal processing related applications. In this paper, the DOA estimation problem in the scenario with gain-phase errors is considered, and a sparse model is formulated by exploiting the signal sparsity in the spatial domain. By proposing a new atomic norm, named as GP-ANM, an optimization method is formulated via deriving a dual norm of GP-ANM. Then, the corresponding semidefinite program is given to estimate the DOA efficiently, where the SDP is obtained based on the Schur complement. Moreover, a regularization parameter is obtained theoretically in the convex optimization problem. Simulation results show that the proposed method outperforms the existing methods, including the subspace-based and sparse-based methods in the scenario with gain-phase errors."],["we review a method to construct a method.","Current progress on ","summarize: We review a method to construct "],["resetting is a potential particle diffusing in a potential.","Diffusion with resetting in a logarithmic potential","summarize: We study the effect of resetting on diffusion in a logarithmic potential. In this model, a particle diffusing in a potential "],["epitaxial growth of single crystalline noble metals on dielectric substrates has received tremendous","Single Crystalline Silver Films for Plasmonics: From Monolayer to Optically Thick Film","summarize: Epitaxial growth of single crystalline noble metals on dielectric substrates has received tremendous attention recently due to their technological potentials as low loss plasmonic materials. Currently there are two different growth approaches, each with its strengths and weaknesses. One adopts a sophisticated molecular beam epitaxial procedure to grow atomically smooth epitaxial Ag films. However, the procedure is rather slow and becomes impractical to grow films with thickness > 50 nm. Another approach adopts a growth process using rapid e-beam deposition which is capable of growing single crystalline Ag films in the thick regime . However, the rapid growth procedure makes it difficult to control film thickness precisely, i.e., the method is not applicable to growing thin epitaxial films. Here we report a universal approach to grow atomically smooth epitaxial Ag films with precise thickness control from a few monolayers to the optically thick regime, overcoming the limitations of the two aforementioned methods. In addition, we develop an in-situ growth of aluminum oxide as the capping layer which exhibits excellent properties protecting the epitaxial Ag films. The performance of the epitaxial Ag films as a function of the film thickness is investigated by directly measuring the propagation length of the surface plasmon polaritons as well as their device performance to support a waveguide plasmonic nanolaser in infrared incorporating an InGaAsP quantum well as the gain media."],["software requirements are to find an optimal set of requirements that gives the highest value for a release","Modeling and Selection of Interdependent Software Requirements using Fuzzy Graphs","summarize: Software requirement selection is to find an optimal set of requirements that gives the highest value for a release of software while keeping the cost within the budget. However, value-related dependencies among software requirements may impact the value of an optimal set. Moreover, value-related dependencies can be of varying strengths. Hence, it is important to consider both the existence and the strengths of value-related dependencies during a requirement selection. The existing selection models however, either assume that software requirements are independent or they ignore strengths of requirement dependencies. This paper presents a cost-value optimization model that considers the impacts of value-related requirement dependencies on the value of selected requirements . We have exploited algebraic structure of fuzzy graphs for modeling value-related requirement dependencies and their strengths. Validity and practicality of the work are verified through carrying out several simulations and studying a real world software project."],["a seq2seq model is widely adopted in Neural Machine Translation.","Guiding attention in Sequence-to-sequence models for Dialogue Act prediction","summarize: The task of predicting dialog acts based on conversational dialog is a key component in the development of conversational agents. Accurately predicting DAs requires a precise modeling of both the conversation and the global tag dependencies. We leverage seq2seq approaches widely adopted in Neural Machine Translation to improve the modelling of tag sequentiality. Seq2seq models are known to learn complex global dependencies while currently proposed approaches using linear conditional random fields only model local tag dependencies. In this work, we introduce a seq2seq model tailored for DA classification using: a hierarchical encoder, a novel guided attention mechanism and beam search applied to both training and inference. Compared to the state of the art our model does not require handcrafted features and is trained end-to-end. Furthermore, the proposed approach achieves an unmatched accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on MRDA."],["integration by parts is an important tool when deriving energy or entropy estimates for","A New Class of ","summarize: Since integration by parts is an important tool when deriving energy or entropy estimates for differential equations, one may conjecture that some form of summation by parts property is involved in provably stable numerical methods. This article contributes to this topic by proposing a novel class of "],["high resolution infrared spectrographs now available are reaching the high precision of","High precision radial velocities with GIANO spectra","summarize: Radial velocities measured from near-infrared spectra are a potentially excellent tool to search for extrasolar planets around cool or active stars. High resolution infrared spectrographs now available are reaching the high precision of visible instruments, with a constant improvement over time. GIANO is an infrared echelle spectrograph at the Telescopio Nazionale Galileo and it is a powerful tool to provide high resolution spectra for accurate RV measurements of exoplanets and for chemical and dynamical studies of stellar or extragalactic objects. No other high spectral resolution IR instrument has GIANO's capability to cover the entire NIR wavelength range in a single exposure. In this paper we describe the ensemble of procedures that we have developed to measure high precision RVs on GIANO spectra acquired during the Science Verification run, using the telluric lines as wavelength reference. We used the Cross Correlation Function method to determine the velocity for both the star and the telluric lines. For this purpose, we constructed two suitable digital masks that include about 2000 stellar lines, and a similar number of telluric lines. The method is applied to various targets with different spectral type, from K2V to M8 stars. We reached different precisions mainly depending on the H -magnitudes: for H ~ 5 we obtain an rms scatter of ~ 10 m s-1, while for H ~ 9 the standard deviation increases to ~ 50 - 80 m s-1. The corresponding theoretical error expectations are ~4 m s-1 and 30 m s-1, respectively. Finally we provide the RVs measured with our procedure for the targets observed during GIANO Science Verification."],["software needs to evolve over the time to meet the new user's requirements. software companies","The Impact of the Object-Oriented Software Evolution on Software Metrics: The Iris Approach","summarize: The Object-Oriented software system evolves over the time to meet the new requirements. Based on the initial release of software, the continuous modification of software code leads to software evolution. Software needs to evolve over the time to meet the new user's requirements. Software companies often develop variant software of the original one depends on customers' needs. The main hypothesis of this paper states that the software when it evolves over the time, its code continues to grow, change and become more complex. This paper proposes an automatic approach to examine the proposed hypothesis. Originality of this approach is the exploiting of the software variants to study the impact of software evolution on the software metrics. This paper presents the results of experiments conducted on three releases of drawing shapes software, sixteen releases of rhino software, eight releases of mobile media software and ten releases of ArgoUML software. Based on the extracted software metrics, It has been found that Iris hypothesis is supported by the computed metrics."],["synchrotron external shock model predicts evolution of spectral and temporal indices","Closure relations of Gamma Ray Bursts in high energy emission","summarize: The synchrotron external shock model predicts the evolution of the spectral and temporal indices during the gamma-ray burst afterglow for different environmental density profiles, electron spectral indices, electron cooling regimes, and regions of the spectrum. We study the relationship between "],["current state-of-the-art adversarial attack methods require hyper-parameter tuning","Trust Region Based Adversarial Attack on Neural Networks","summarize: Deep Neural Networks are quite vulnerable to adversarial perturbations. Current state-of-the-art adversarial attack methods typically require very time consuming hyper-parameter tuning, or require many iterations to solve an optimization based adversarial attack. To address this problem, we present a new family of trust region based adversarial attacks, with the goal of computing adversarial perturbations efficiently. We propose several attacks based on variants of the trust region optimization method. We test the proposed methods on Cifar-10 and ImageNet datasets using several different models including AlexNet, ResNet-50, VGG-16, and DenseNet-121 models. Our methods achieve comparable results with the Carlini-Wagner attack, but with significant speed up of up to "],["an investigation of the monitoring activity of MAPE-K control loop is performed. this paper","An Investigation of the Monitoring Activity in Self Adaptive Systems","summarize: Runtime monitoring is essential for the violation detection during the underlying software system execution. In this paper, an investigation of the monitoring activity of MAPE-K control loop is performed which aims at exploring: the architecture of the monitoring activity in terms of the involved components and control and data flow between them; the standard interface of the monitoring component with other MAPE-K components; the adaptive monitoring and its importance to the monitoring overhead issue; and the monitoring mode and its relevance to some specific situations and systems. This paper also presented a Java framework for the monitoring process for self adaptive systems."],["golfarshchi and Khalilzadeh disprove two results.","On the paper: Numerical radius preserving linear maps on Banach algebras","summarize: We give an example of a unital commutative complex Banach algebra having a normalized state which is not a spectral state and admitting an extreme normalized state which is not multiplicative. This disproves two results by Golfarshchi and Khalilzadeh."],["we consider convergence of alternating projections between non-convex sets. we obtain","Alternating projections with applications to Gerchberg-Saxton error reduction","summarize: We consider convergence of alternating projections between non-convex sets and obtain applications to convergence of the Gerchberg-Saxton error reduction method, of the Gaussian expectation-maximization algorithm, and of Cadzow's algorithm."],["a three-dimensional numerical simulation is conducted for a complex process in a laser-","Multicomponent Gas-Particle Flow and Heat\/Mass Transfer Induced by a Localized Laser Irradiation on a Urethane-Coated Stainless Steel Substrate","summarize: A three-dimensional numerical simulation is conducted for a complex process in a laser-material system, which involves heat and mass transfer in a compressible gaseous phase and chemical reaction during laser irradiation on a urethane paint coated on a stainless steel substrate. A finite volume method with a co-located grid mesh that discretizes the entire computational domain is employed to simulate the heating process. The results show that when the top surface of the paint reaches a threshold temperature of 560 K, the polyurethane starts to decompose through chemical reaction. As a result, combustion products CO2, H2O and NO2 are produced and chromium oxide, which serves as pigment in the paint, is ejected as solid parcels from the paint into the gaseous domain. Variations of temperature, density and velocity at the center of the laser irradiation spot, and the concentrations of reaction reactant\/products in the gaseous phase are presented and discussed, by comparing six scenarios with different laser powers ranging from 2.5 kW to 15 kW with an increment of 2.5 kW."],["MTs regulate intracellular trafficking, participate in cytoskeleton reorganization","Tracking of plus-ends reveals microtubule functional diversity in different cell types","summarize: Many cellular processes are tightly connected to the dynamics of microtubules . While in neuronal axons MTs mainly regulate intracellular trafficking, they participate in cytoskeleton reorganization in many other eukaryotic cells, enabling the cell to efficiently adapt to changes in the environment. We show that the functional differences of MTs in different cell types and regions is reflected in the dynamic properties of MT tips. Using plus-end tracking proteins EB1 to monitor growing MT plus-ends, we show that MT dynamics and life cycle in axons of human neurons significantly differ from that of fibroblast cells. The density of plus-ends, as well as the rescue and catastrophe frequencies increase while the growth rate decreases toward the fibroblast cell margin. This results in a rather stable filamentous network structure and maintains the connection between nucleus and membrane. In contrast, plus-ends are uniformly distributed along the axons and exhibit diverse polymerization run times and spatially homogeneous rescue and catastrophe frequencies, leading to MT segments of various lengths. The probability distributions of the excursion length of polymerization and the MT length both follow nearly exponential tails, in agreement with the analytical predictions of a two-state model of MT dynamics."],["we derive point-wise and integral rigidity\/gap results for a closed","On Closed Manifolds with Harmonic Weyl Curvature","summarize: We derive point-wise and integral rigidity\/gap results for a closed manifold with harmonic Weyl curvature in any dimension. In particular, there is a generalization of Tachibana's theorem for non-negative curvature operator. The key ingredients are new Bochner-Weitzenb\\ock-Lichnerowicz type formulas for the Weyl tensor, which are generalizations of identities in dimension four."],["effective applications of vehicular ad hoc networks in traffic signal control require new methods for","Detection of malicious data in vehicular ad-hoc networks for traffic signal control applications","summarize: Effective applications of vehicular ad hoc networks in traffic signal control require new methods for detection of malicious data. Injection of malicious data can result in significantly decreased performance of such applications, increased vehicle delays, fuel consumption, congestion, or even safety threats. This paper introduces a method, which combines a model of expected driver behaviour with position verification in order to detect the malicious data injected by vehicle nodes that perform Sybil attacks. Effectiveness of this approach was demonstrated in simulation experiments for a decentralized self-organizing system that controls the traffic signals at multiple intersections in an urban road network. Experimental results show that the proposed method is useful for mitigating the negative impact of malicious data on the performance of traffic signal control."],["elastic properties of a material with spherical voids of equal volume are","Beam model for the elastic properties of material with spherical voids","summarize: The elastic properties of a material with spherical voids of equal volume are analysed using a new model, with particular attention paid to the hexagonal close-packed and the face-centred cubic arrangement of voids. Void fractions well above 74 \\% are considered, yielding overlapping voids as in an open-cell foam and hence a connected pore structure. The material is represented by a network of beams. The elastic behaviour of each beam is derived analytically from the material structure. By computing the linear elastic properties of the beam network, the Young's moduli and Poisson ratios for different directions are evaluated. In the limit of rigidity loss a power law is obtained, describing the relation between Young's modulus and void fraction with an exponent of 5\/2 for bending-dominated and 3\/2 for stretching-dominated directions. The corresponding Poisson ratios vary between 0 and 1. With decreasing void fraction, these exponents become 2.3 and 1.3, respectively. The data obtained analytically and from the new beam model are compared to Finite Element simulations carried out in a companion study, and good agreement is found. The hexagonal close-packed void arrangement features very anisotropic behaviour, comparable to that of fibre-reinforced materials, This might allow for new applications of open-cell materials."],["a quasi-order is a binary, reflexive and transitive relation. in","Quasi-ordered Rings","summarize: A quasi-order is a binary, reflexive and transitive relation. In the Journal of Pure and Applied Algebra 45 , S.M. Fakhruddin introduced the notion of quasi-ordered fields and showed that each such field is either an ordered field or else a valued field. Hence, quasi-ordered fields are very well suited to treat ordered and valued fields simultaneously. In this note, we will prove that the same dichotomy holds for commutative rings with 1 as well. For that purpose we first develop an appropriate notion of quasi-ordered rings. Our proof of the dichotomy then exploits Fakhruddin's result that was mentioned above."],["we observed a sample of eight extremely red AGB stars with the APEX telescope","On the detection of CO and mass loss of Bulge OH\/IR stars","summarize: We report on the succesful search for CO and emission associated with OH\/IR stars in the Galactic Bulge. We observed a sample of eight extremely red AGB stars with the APEX telescope and detected seven. The sources were selected at sufficient high galactic latitude to avoid interference by interstellar CO, which hampered previous studies of inner galaxy stars. To study the nature of our sample and the mass loss we constructed the SEDs from photometric data and Spitzer IRS spectroscopy. In a first step we apply radiative transfer modelling to fit the SEDs and obtain luminosities and dust mass loss rates . Through dynamical modelling we then retrieve the total MLR and the gas-to-dust ratios. We derived variability periods of our stars. The luminosities range between approximately 4000 and 5500 Lsun and periods are below 700 days. The total MLR ranges between 1E-5 and 1E-4 Msun\/yr. Comparison with evolutionary models shows that the progenitor mass is approximately 1.5 Msun, similar to the Bulge Miras and are of intermediate age . The gas-to-dust ratios are between 100 and 400 and are similar to what is found for OH\/IR stars in the galactic Disk. One star, IRAS 17347-2319, has a very short period of approximately 300 days which may be decreasing further. It may belong to a class of Mira variables with a sudden change in period as observed in some Galactic objects. It would be the first example of an OH\/IR star in this class and deserves further follow-up observations."],["new method of multi-cell tracking is based on a study of brain-wide 4","SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter","summarize: Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: cells are imaged as globular-like objects, it is difficult to distinguish cells based only on shape and size, the number of imaged cells ranges in several hundreds, moves of nearly-located cells are strongly correlated and cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. ."],["a DCNN trained to recognize user actions can classify five actions in a collection of","Building Usage Profiles Using Deep Neural Nets","summarize: To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we consider publicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos . In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product."],["the proposed Res2Net block can be plugged into the state-of-the-art","Res2Net: A New Multi-scale Backbone Architecture","summarize: Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https:\/\/mmcheng.net\/res2net\/."],["the widespread use of wireless technologies leads to an ever-increasing number of users","Results and Tools for Evaluating the Effectiveness of Focusing Systems to Improve Accessibility in Wireless Networks","summarize: The widespread use of wireless technologies leads to an ever-increasing number of users and permanently functioning devices. However, the growth of the number of wireless users in a limited space and a limited frequency range leads to an increase in their mutual influence, which ultimately affects the throughput of wireless channels and even the performance of the system as a whole. The article presents the statistics and tendencies of the distribution of wireless networks of the IEEE 802.11 standard systems, as well as analyzes the main problems that arise during the expansion of their use. Substantiation and choice of ways to overcome these difficulties largely depends on the objective control of radiation parameters of access points and subscriber funds in a particular environment. The review of the state control facilities provided by the developers of the equipment is presented, and author's variants of experimental measuring complexes are offered, allowing to control signal and information parameters of Wi-Fi systems. The experimental results obtained with the use of the indicated means, obtained using the accelerating metal-plate lens as an additional autonomous element for focusing the field, including for MIMO systems, the effect of the accelerating metal-plate lens on the spatial distribution of the field, on the spectral structure of the signal are presented. In addition, polarization effects were investigated. Possible ways to further increase the availability, integrity of information and energy efficiency of wireless access systems are discussed. The authors propose simpler and less costly options for increasing the direction of radiation on the basis of an accelerating metal-plate lens, experimentally tested, as well as the use of zone zoning on the path of the computer."],["the first projection method is a generalization of the classical primal-dual method","Projection based model order reduction methods for the estimation of vector-valued variables of interest","summarize: We propose and compare goal-oriented projection based model order reduction methods for the estimation of vector-valued functionals of the solution of parameter-dependent equations. The first projection method is a generalization of the classical primal-dual method to the case of vector-valued variables of interest. We highlight the role played by three reduced spaces: the approximation space and the test space associated to the primal variable, and the approximation space associated to the dual variable. Then we propose a Petrov-Galerkin projection method based on a saddle point problem involving an approximation space for the primal variable and an approximation space for an auxiliary variable. A goal-oriented choice of the latter space, defined as the sum of two spaces, allows us to improve the approximation of the variable of interest compared to a primal-dual method using the same reduced spaces. Then, for both approaches, we derive computable error estimates for the approximations of the variable of interest and we propose greedy algorithms for the goal-oriented construction of reduced spaces. The performance of the algorithms are illustrated on numerical examples and compared to standard algorithms."],["the proportion of false null hypotheses is a very important quantity in statistical modelling and","Uniformly consistently estimating the proportion of false null hypotheses via Lebesgue-Stieltjes integral equations","summarize: The proportion of false null hypotheses is a very important quantity in statistical modelling and inference based on the two-component mixture model and its extensions, and in control and estimation of the false discovery rate and false non-discovery rate. Most existing estimators of this proportion threshold p-values, deconvolve the mixture model under constraints on its components, or depend heavily on the location-shift property of distributions. Hence, they usually are not consistent, applicable to non-location-shift distributions, or applicable to discrete statistics or p-values. To eliminate these shortcomings, we construct uniformly consistent estimators of the proportion as solutions to Lebesgue-Stieltjes integral equations. In particular, we provide such estimators respectively for random variables whose distributions have Riemann-Lebesgue type characteristic functions, form discrete natural exponential families with infinite supports, and form natural exponential families with separable moment sequences. We provide the speed of convergence and uniform consistency class for each such estimator under independence. In addition, we provide example distribution families for which a consistent estimator of the proportion cannot be constructed using our techniques."],["we propose a new design principle for incrementally growing robust networks. the networks are self","A new design principle of robust onion-like networks self-organized in growth","summarize: Today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading efficiency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures."],["random matrices with continuous entries are divided by columns. the number of columns is","Balancing Gaussian vectors in high dimension","summarize: Motivated by problems in controlled experiments, we study the discrepancy of random matrices with continuous entries where the number of columns "],["a learnable transformation is placed at the end of such models returning a value for each","Regular Polytope Networks","summarize: Neural networks are widely used as a model for classification in a large variety of tasks. Typically, a learnable transformation is placed at the end of such models returning a value for each class used for classification. This transformation plays an important role in determining how the generated features change during the learning process. In this work, we argue that this transformation not only can be fixed with no loss of accuracy and with a reduction in memory usage, but it can also be used to learn stationary and maximally separated embeddings. We show that the stationarity of the embedding and its maximal separated representation can be theoretically justified by setting the weights of the fixed classifier to values taken from the coordinate vertices of the three regular polytopes available in "],["time-dependent spin current mediated spin transfer torque behaviour investigated. a model semicon","Electronic structure and Magneto-transport in MoS","summarize: The time-dependent spin current mediated spin transfer torque behaviour has been investigated via scattering formalism within density functional theory framework supported by Green's function. Quantum magnetotransport characteristics have been revealed in a model semiconducting MoS"],["black hole candidate 1E 1740.7-2942 is one of the strongest hard X","Tandem Swift and INTEGRAL Data to Revisit the Orbital and Superorbital Periods of 1E 1740.7-2942","summarize: The black hole candidate 1E 1740.7-2942 is one of the strongest hard X-ray sources in the Galactic Center region. No counterparts in longer wavelengths have been identified for this object yet. The presence of characteristic timing signatures in the flux history of X-ray sources has been shown to be an important diagnostic tool for the properties of these systems. Using simultaneous data from NASA's Swift and ESA's INTEGRAL missions, we have found two periodic signatures at 12.61 "],["deep neural networks have been quite successful in solving complex learning problems. but the complex learning parameters","Compressed Learning of Deep Neural Networks for OpenCL-Capable Embedded Systems","summarize: Deep neural networks have been quite successful in solving many complex learning problems. However, DNNs tend to have a large number of learning parameters, leading to a large memory and computation requirement. In this paper, we propose a model compression framework for efficient training and inference of deep neural networks on embedded systems. Our framework provides data structures and kernels for OpenCL-based parallel forward and backward computation in a compressed form. In particular, our method learns sparse representations of parameters using "],["building management systems are crucial in the drive towards smart sustainable cities. a typical BMS","Towards Smart Sustainable Cities: Addressing semantic heterogeneity in building management systems using discriminative models","summarize: Building Management Systems are crucial in the drive towards smart sustainable cities. This is due to the fact that they have been effective in significantly reducing the energy consumption of buildings. A typical BMS is composed of smart devices that communicate with one another in order to achieve their purpose. However, the heterogeneity of these devices and their associated meta-data impede the deployment of solutions that depend on the interactions among these devices. Nonetheless, automatically inferring the semantics of these devices using data-driven methods provides an ideal solution to the problems brought about by this heterogeneity. In this paper, we undertake a multi-dimensional study to address the problem of inferring the semantics of IoT devices using machine learning models. Using two datasets with over 67 million data points collected from IoT devices, we developed discriminative models that produced competitive results. Particularly, our study highlights the potential of Image Encoded Time Series as a robust alternative to statistical feature-based inference methods. Leveraging just a fraction of the data required by feature-based methods, our evaluations show that this encoding competes with and even outperforms traditional methods in many cases."],["each node in the network has a local buffer of bounded size. the","A Constant Approximation Algorithm for Scheduling Packets on Line Networks","summarize: In this paper we improve the approximation ratio for the problem of scheduling packets on line networks with bounded buffers, where the aim is that of maximizing the throughput. Each node in the network has a local buffer of bounded size "],["a procedure to recover explicit self-adjoint matrix Dirac systems on semi","Inverse problems for self-adjoint Dirac systems: explicit solutions and stability of the procedure","summarize: A procedure to recover explicitly self-adjoint matrix Dirac systems on semi-axis from rational Weyl functions is considered. Its stability is proved. GBDT version of Baecklund-Darboux transformation and various important results on Riccati equations are used for this purpose."],["a stochastic control problem is a stochastic control problem. the control","A Dual Method For Backward Stochastic Differential Equations with Application to Risk Valuation","summarize: We propose a numerical recipe for risk evaluation defined by a backward stochastic differential equation. Using dual representation of the risk measure, we convert the risk valuation to a stochastic control problem where the control is a certain Radon-Nikodym derivative process. By exploring the maximum principle, we show that a piecewise-constant dual control provides a good approximation on a short interval. A dynamic programming algorithm extends the approximation to a finite time horizon. Finally, we illustrate the application of the procedure to financial risk management in conjunction with nested simulation and on an multidimensional portfolio valuation problem."],["a video description can be described in natural language. it can convey the same abstract idea","Mining for meaning: from vision to language through multiple networks consensus","summarize: Describing visual data into natural language is a very challenging task, at the intersection of computer vision, natural language processing and machine learning. Language goes well beyond the description of physical objects and their interactions and can convey the same abstract idea in many ways. It is both about content at the highest semantic level as well as about fluent form. Here we propose an approach to describe videos in natural language by reaching a consensus among multiple encoder-decoder networks. Finding such a consensual linguistic description, which shares common properties with a larger group, has a better chance to convey the correct meaning. We propose and train several network architectures and use different types of image, audio and video features. Each model produces its own description of the input video and the best one is chosen through an efficient, two-phase consensus process. We demonstrate the strength of our approach by obtaining state of the art results on the challenging MSR-VTT dataset."],["paper introduces non-stationary adversarial cost with a variation constraint.","Bayesian adversarial multi-node bandit for optimal smart grid protection against cyber attacks","summarize: The cybersecurity of smart grids has become one of key problems in developing reliable modern power and energy systems. This paper introduces a non-stationary adversarial cost with a variation constraint for smart grids and enables us to investigate the problem of optimal smart grid protection against cyber attacks in a relatively practical scenario. In particular, a Bayesian multi-node bandit model with adversarial costs is constructed and a new regret function is defined for this model. An algorithm called Thompson-Hedge algorithm is presented to solve the problem and the superior performance of the proposed algorithm is proven in terms of the convergence rate of the regret function. The applicability of the algorithm to real smart grid scenarios is verified and the performance of the algorithm is also demonstrated by numerical examples."],["a term has come up in the effective Friedmann equation for a homogeneous","Thermodynamic Prescription of Cosmological Constant in Randall Sundrum-II Brane","summarize: In this work, we apply quantum corrected entropy function derived from the Generalized Uncertainty Principle to the Holographic Equipartition Law to study the cosmological scenario in Randall-Sundrum II brane. An extra driving term has come up in the effective Friedmann equation for a homogeneous, isotropic and spatially flat universe. Further, thermodynamic prescription of the universe constraints this term eventually with order equivalent to that of the cosmological constant."],["a class of novel algorithms aim at solving bilinear and quadratic inverse problems","Tensor-Free Proximal Methods for Lifted Bilinear\/Quadratic Inverse Problems with Applications to Phase Retrieval","summarize: We propose and study a class of novel algorithms that aim at solving bilinear and quadratic inverse problems. Using a convex relaxation based on tensorial lifting, and applying first-order proximal algorithms, these problems could be solved numerically by singular value thresholding methods. However, a direct realization of these algorithms for, e.g., image recovery problems is often impracticable, since computations have to be performed on the tensor-product space, whose dimension is usually tremendous. To overcome this limitation, we derive tensor-free versions of common singular value thresholding methods by exploiting low-rank representations and incorporating an augmented Lanczos process. Using a novel reweighting technique, we further improve the convergence behavior and rank evolution of the iterative algorithms. Applying the method to the two-dimensional masked Fourier phase retrieval problem, we obtain an efficient recovery method. Moreover, the tensor-free algorithms are flexible enough to incorporate a-priori smoothness constraints that greatly improve the recovery results."],["the original lemma states that concentration of the Laplace spectrum implies combinatorial expansion in","Mixing in high-dimensional expanders","summarize: We prove a generalization of the Expander Mixing Lemma for arbitrary simplicial complexes. The original lemma states that concentration of the Laplace spectrum of a graph implies combinatorial expansion . Recently, an analogue of this Lemma was proved for simplicial complexes of arbitrary dimension, provided that the skeleton of the complex is complete. More precisely, it was shown that a concentrated spectrum of the simplicial Hodge Laplacian implies a similar type of expansion as in graphs. In this paper we remove the assumption of a complete skeleton, showing that concentration of the Laplace spectra in all dimensions implies combinatorial expansion in any complex. As applications we show that spectral concentration implies Gromov's geometric overlap property, and can be used to bound the chromatic number of a complex."],["dynamic regime literature aims to map characteristics of a unit to a action tailored to maximize","Estimation of Personalized Effects Associated With Causal Pathways","summarize: The goal of personalized decision making is to map a unit's characteristics to an action tailored to maximize the expected outcome for that unit. Obtaining high-quality mappings of this type is the goal of the dynamic regime literature. In healthcare settings, optimizing policies with respect to a particular causal pathway may be of interest as well. For example, we may wish to maximize the chemical effect of a drug given data from an observational study where the chemical effect of the drug on the outcome is entangled with the indirect effect mediated by differential adherence. In such cases, we may wish to optimize the direct effect of a drug, while keeping the indirect effect to that of some reference treatment. shows how to combine mediation analysis and dynamic treatment regime ideas to defines policies associated with causal pathways and counterfactual responses to these policies. In this paper, we derive a variety of methods for learning high quality policies of this type from data, in a causal model corresponding to a longitudinal setting of practical importance. We illustrate our methods via a dataset of HIV patients undergoing therapy, gathered in the Nigerian PEPFAR program."],["a custom template can be semi-automatic according to the preoperative plan. mesh","A semi-automatic computer-aided method for surgical template design","summarize: This paper presents a generalized integrated framework of semi-automatic surgical template design. Several algorithms were implemented including the mesh segmentation, offset surface generation, collision detection, ruled surface generation, etc., and a special software named TemDesigner was developed. With a simple user interface, a customized template can be semi- automatically designed according to the preoperative plan. Firstly, mesh segmentation with signed scalar of vertex is utilized to partition the inner surface from the input surface mesh based on the indicated point loop. Then, the offset surface of the inner surface is obtained through contouring the distance field of the inner surface, and segmented to generate the outer surface. Ruled surface is employed to connect inner and outer surfaces. Finally, drilling tubes are generated according to the preoperative plan through collision detection and merging. It has been applied to the template design for various kinds of surgeries, including oral implantology, cervical pedicle screw insertion, iliosacral screw insertion and osteotomy, demonstrating the efficiency, functionality and generality of our method."],["a characterizing non-local Stein operator boils down to classical Stein operators. we","On Stein's Method for Infinitely Divisible Laws With Finite First Moment","summarize: We present, in a unified way, a Stein methodology for infinitely divisible laws having finite first moment. Based on a correlation representation, we obtain a characterizing non-local Stein operator which boils down to classical Stein operators in specific examples. Thanks to this characterizing operator, we introduce various extensions of size bias and zero bias distributions and prove that these notions are closely linked to infinite divisibility. Combined with standard Fourier techniques, these extensions also allow obtaining explicit rates of convergence for compound Poisson approximation in particular towards the symmetric "],["a new technique is proposed to control the coronavirus outbreak. a positive","A Privacy Preserved and Cost Efficient Control Scheme for Coronavirus Outbreak Using Call Data Record and Contact Tracing","summarize: Coronavirus or COVID-19, which has been declared pandemic by the World Health Organization, has incurred huge losses to the lives of people throughout the world. Although, the scientists, researchers and doctors are working round the clock to develop a vaccine for COVID-19, it may take a year or two to make a safe and effective vaccine available for the world. In current circumstances, a solution must be developed to control or stop the spread of the virus. For this purpose, a novel technique based on call data record analysis and contact tracing is proposed that can effectively control the coronavirus outbreak. A positive coronavirus patient can be traced through CDRA and contact tracing. The technique can track the path traversed by the patient and collect the cell numbers of all those people who have met with the patient. Keeping in tact the privacy of this group of people, who are contacted through their cell numbers so that they can isolate themselves till the result of their coronavirus test arrives. If a test result of a person comes positive among the group, then he\/she must be isolated and same CDRA and contact tracing procedures are adopted for that person. A COVID-19 patient is geo tagged and alerts are sent if any violation of isolation is done by the patient. Moreover, the general public is informed in advance to avoid the path followed by the patients. This cost effective mechanism is not only capable to control the coronavirus outbreak but also helps in isolating the patient in his\/her house."],["this work presents a detailed experimental investigation of the interaction between molecular hydrogen and monolayer","Probing the Electronic Properties of Monolayer MoS","summarize: This work presents a detailed experimental investigation of the interaction between molecular hydrogen and monolayer MoS"],["semantic segmentation algorithms could easily surpass 80%. this is due to the domain shift impact","Unsupervised Domain Adaptation using Generative Adversarial Networks for Semantic Segmentation of Aerial Images","summarize: Segmenting aerial images is being of great potential in surveillance and scene understanding of urban areas. It provides a mean for automatic reporting of the different events that happen in inhabited areas. This remarkably promotes public safety and traffic management applications. After the wide adoption of convolutional neural networks methods, the accuracy of semantic segmentation algorithms could easily surpass 80% if a robust dataset is provided. Despite this success, the deployment of a pre-trained segmentation model to survey a new city that is not included in the training set significantly decreases the accuracy. This is due to the domain shift between the source dataset on which the model is trained and the new target domain of the new city images. In this paper, we address this issue and consider the challenge of domain adaptation in semantic segmentation of aerial images. We design an algorithm that reduces the domain shift impact using Generative Adversarial Networks . In the experiments, we test the proposed methodology on the International Society for Photogrammetry and Remote Sensing semantic segmentation dataset and found that our method improves the overall accuracy from 35% to 52% when passing from Potsdam domain to Vaihingen domain . In addition, the method allows recovering efficiently the inverted classes due to sensor variation. In particular, it improves the average segmentation accuracy of the inverted classes due to sensor variation from 14% to 61%."],["image forensic plays a crucial role in both criminal investigations and civil litigation. there are","A Survey of Machine Learning Techniques in Adversarial Image Forensics","summarize: Image forensic plays a crucial role in both criminal investigations and civil litigation . Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial examples, with real-world consequences . Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios."],["network virtualization suffers from the problem of mapping virtual links and nodes to physical network in","Energy-Aware Virtual Network Embedding Approach for Distributed Cloud","summarize: Network virtualization has caught the attention of many researchers in recent years. It facilitates the process of creating several virtual networks over a single physical network. Despite this advantage, however, network virtualization suffers from the problem of mapping virtual links and nodes to physical network in most efficient way. This problem is called virtual network embedding . Many researches have been proposed in an attempt to solve this problem, which have many optimization aspects, such as improving embedding strategies in a way that preserves energy, reducing embedding cost and increasing embedding revenue. Moreover, some researchers have extended their algorithms to be more compatible with the distributed clouds instead of a single infrastructure provider . This paper proposes energy aware particle swarm optimization algorithm for distributed clouds. This algorithm aims to partition each virtual network request to subgraphs, using the Heavy Clique Matching technique to generate a coarsened graph. Each coarsened node in the coarsened graph is assigned to a suitable data center . Inside each DC, a modified particle swarm optimization algorithm is initiated to find the near optimal solution for the VNE problem. The proposed algorithm was tested and evaluated against existing algorithms using extensive simulations, which shows that the proposed algorithm outperforms other algorithms."],["cite has proposed the use of the maximum loss over random structured outputs. this","Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms","summarize: Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \\cite. In natural language processing, recent work \\cite has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \\cite. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \\cite, our theoretical results show that more general techniques are possible."],["we propose an augmented gradient-play dynamics with correction. players communicate locally only with their","A Passivity-Based Approach to Nash Equilibrium Seeking over Networks","summarize: In this paper we consider the problem of distributed Nash equilibrium seeking over networks, a setting in which players have limited local information. We start from a continuous-time gradient-play dynamics that converges to an NE under strict monotonicity of the pseudo-gradient and assumes perfect information, i.e., instantaneous all-to-all player communication. We consider how to modify this gradient-play dynamics in the case of partial, or networked information between players. We propose an augmented gradient-play dynamics with correction in which players communicate locally only with their neighbours to compute an estimate of the other players' actions. We derive the new dynamics based on the reformulation as a multi-agent coordination problem over an undirected graph. We exploit incremental passivity properties and show that a synchronizing, distributed Laplacian feedback can be designed using relative estimates of the neighbours. Under a strict monotonicity property of the pseudo-gradient, we show that the augmented gradient-play dynamics converges to consensus on the NE of the game. We further discuss two cases that highlight the tradeoff between properties of the game and the communication graph."],["deep neural networks have been quite successful in solving complex learning problems. but the complex learning parameters","Compressed Learning of Deep Neural Networks for OpenCL-Capable Embedded Systems","summarize: Deep neural networks have been quite successful in solving many complex learning problems. However, DNNs tend to have a large number of learning parameters, leading to a large memory and computation requirement. In this paper, we propose a model compression framework for efficient training and inference of deep neural networks on embedded systems. Our framework provides data structures and kernels for OpenCL-based parallel forward and backward computation in a compressed form. In particular, our method learns sparse representations of parameters using "],["a robust transmit\/receive adaptive beamforming for multiple-input multipleoutput","Joint Robust Transmit\/Receive Adaptive Beamforming for MIMO Radar Using Probability-Constrained Optimization","summarize: A joint robust transmit\/receive adaptive beamforming for multiple-input multipleoutput radar based on probability-constrained optimization approach is developed in the case of Gaussian and arbitrary distributed mismatch present in both the transmit and receive signal steering vectors. A tight lower bound of the probability constraint is also derived by using duality theory. The formulated probability-constrained robust beamforming problem is nonconvex and NP-hard. However, we reformulate its cost function into a bi-quadratic function while the probability constraint splits into transmit and receive parts. Then, a block coordinate descent method based on second-order cone programming is developed to address the biconvex problem. Simulation results show an improved robustness of the proposed beamforming method as compared to the worst-case and other existing state-of-the-art joint transmit\/receive robust adaptive beamforming methods for MIMO radar."],["the remainders of two locally compact Hausdorff spaces in their StoneCech compactifications","Hausdorffness of General Compactifications","summarize: Magill proved that the remainders of two locally compact Hausdorff spaces in their StoneCech compactifications are homeomorphic if and only if the lattices of their Hausdorff compactifications are lattice isomorphic. His construction for compactifications are explicitely discussed through the partitions of their StoneCech compactifications. Partitions in a StoneCech compactification which lead to Hausdorff compactifications are characterized in this article. Embeddings of certain upper semi-lattices of compactifications into lattices of compactifications are constructed."],["the main purpose of present paper is to determine some lower bounds for the quotient of","Partial sums of Hyper-Bessel function with applications","summarize: The main purpose of present paper is to determine some lower bounds for the quotient of the normalized hyper-Bessel function and its partial sum, as well as for the quotient of the derivative of normalized hyper-Bessel function and its partial sum. In addition, some applications related to obtained results are also given."],["electrocatalysts can catalyze ORR and OER.","Doping of Self-Standing CNT Fibers: Promising Flexible Air-Cathodes for High Energy Density Structural Zn-air batteries","summarize: Finding proper electrocatalysts capable of efficient catalyzing both ORR and OER is of great importance for metal-air batteries. With increasing inclination towards structural and flexible devices, developing a high-performance self-standing air-cathode is highly demanded and challenging, as most of oxygen catalysts are powder and need to be further processed. Here, we construct highly bifunctional air catalyst from macroscopic CNT fibers through direct CVD spinning followed by hydrothermal method. The electrocatalytic properties of the samples were tuned by altering nitrogen-doping and defect densities readily adjusted at different hydrothermal reaction temperatures. The treated CNTfs showed excellent bifunctional activity and demonstrated exceptional performance as carbon-based self-standing air-cathodes in liquid and solid-state rechargeable Zn-air batteries, with high capacity of 698 mAh g-1 and ultrahigh energy density of 838 Wh kg-1. The rechargeable Zn-air batteries exhibit a low discharge-charge overpotential and excellent stability. This work provides novel simply achieved self-standing air electrodes with exceptional performance for structural Zn-air batteries."],["a novel equation of state is applied to the study of neutron star properties. the","The induced surface tension contribution for the equation of state of neutron stars","summarize: We apply a novel equation of state that includes the surface tension contribution induced by interparticle interaction and asymmetry between neutrons and protons, to the study of neutron star properties. This elaborated EoS is obtained from the virial expansion applied to multicomponent particle mixtures with hard core repulsion. The considered model is in full concordance with all the known properties of normal nuclear matter, provides a high-quality description of the proton flow constraints, hadron multiplicities created during the nuclear-nuclear collision experiments, and equally is consistent with astrophysical data coming from NS observations. The analysis suggests that the best model parameterization gives the incompressibility factor "],["building management systems are crucial in the drive towards smart sustainable cities. a typical BMS","Towards Smart Sustainable Cities: Addressing semantic heterogeneity in building management systems using discriminative models","summarize: Building Management Systems are crucial in the drive towards smart sustainable cities. This is due to the fact that they have been effective in significantly reducing the energy consumption of buildings. A typical BMS is composed of smart devices that communicate with one another in order to achieve their purpose. However, the heterogeneity of these devices and their associated meta-data impede the deployment of solutions that depend on the interactions among these devices. Nonetheless, automatically inferring the semantics of these devices using data-driven methods provides an ideal solution to the problems brought about by this heterogeneity. In this paper, we undertake a multi-dimensional study to address the problem of inferring the semantics of IoT devices using machine learning models. Using two datasets with over 67 million data points collected from IoT devices, we developed discriminative models that produced competitive results. Particularly, our study highlights the potential of Image Encoded Time Series as a robust alternative to statistical feature-based inference methods. Leveraging just a fraction of the data required by feature-based methods, our evaluations show that this encoding competes with and even outperforms traditional methods in many cases."],["a multi-class single-server queueing model with finite buffers is studied","Asymptotically optimal control for a multiclass queueing model in the moderate deviation heavy traffic regime","summarize: A multi-class single-server queueing model with finite buffers, in which scheduling and admission of customers are subject to control, is studied in the moderate deviation heavy traffic regime. A risk-sensitive cost set over a finite time horizon "],["black hole candidate 1E 1740.7-2942 is one of the strongest hard X","Tandem Swift and INTEGRAL Data to Revisit the Orbital and Superorbital Periods of 1E 1740.7-2942","summarize: The black hole candidate 1E 1740.7-2942 is one of the strongest hard X-ray sources in the Galactic Center region. No counterparts in longer wavelengths have been identified for this object yet. The presence of characteristic timing signatures in the flux history of X-ray sources has been shown to be an important diagnostic tool for the properties of these systems. Using simultaneous data from NASA's Swift and ESA's INTEGRAL missions, we have found two periodic signatures at 12.61 "],["a graph can assign directions to each edge of the graph, thus orienting the graph","Egalitarian Graph Orientations","summarize: Given an undirected graph, one can assign directions to each of the edges of the graph, thus orienting the graph. To be as egalitarian as possible, one may wish to find an orientation such that no vertex is unfairly hit with too many arcs directed into it. We discuss how this objective arises in problems resulting from telecommunications. We give optimal, polynomial-time algorithms for: finding an orientation that minimizes the lexicographic order of the indegrees and finding a strongly-connected orientation that minimizes the maximum indegree. We show that minimizing the lexicographic order of the indegrees is NP-hard when the resulting orientation is required to be acyclic."],["we propose projected gradient descent algorithm to estimate the population minimizer in the finite sample regime","Quickly Finding the Best Linear Model in High Dimensions","summarize: We study the problem of finding the best linear model that can minimize least-squares loss given a data-set. While this problem is trivial in the low dimensional regime, it becomes more interesting in high dimensions where the population minimizer is assumed to lie on a manifold such as sparse vectors. We propose projected gradient descent algorithm to estimate the population minimizer in the finite sample regime. We establish linear convergence rate and data dependent estimation error bounds for PGD. Our contributions include: 1) The results are established for heavier tailed sub-exponential distributions besides sub-gaussian. 2) We directly analyze the empirical risk minimization and do not require a realizable model that connects input data and labels. 3) Our PGD algorithm is augmented to learn the bias terms which boosts the performance. The numerical experiments validate our theoretical results."],["the hybrid policy gradient estimator is shown to be biased, but has variance reduced property.","A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning","summarize: We propose a novel hybrid stochastic policy gradient estimator by combining an unbiased policy gradient estimator, the REINFORCE estimator, with another biased one, an adapted SARAH estimator for policy optimization. The hybrid policy gradient estimator is shown to be biased, but has variance reduced property. Using this estimator, we develop a new Proximal Hybrid Stochastic Policy Gradient Algorithm to solve a composite policy optimization problem that allows us to handle constraints or regularizers on the policy parameters. We first propose a single-looped algorithm then introduce a more practical restarting variant. We prove that both algorithms can achieve the best-known trajectory complexity "],["Travis CI handles thousands of builds every day to provide valuable feedback to thousands of open-source","An Analysis of 35+ Million Jobs of Travis CI","summarize: Travis CI handles automatically thousands of builds every day to, amongst other things, provide valuable feedback to thousands of open-source developers. In this paper, we investigate Travis CI to firstly understand who is using it, and when they start to use it. Secondly, we investigate how the developers use Travis CI and finally, how frequently the developers change the Travis CI configurations. We observed during our analysis that the main users of Travis CI are corporate users such as Microsoft. And the programming languages used in Travis CI by those users do not follow the same popularity trend than on GitHub, for example, Python is the most popular language on Travis CI, but it is only the third one on GitHub. We also observe that Travis CI is set up on average seven days after the creation of the repository and the jobs are still mainly used to run tests. And finally, we observe that 7.34% of the commits modify the Travis CI configuration. We share the biggest benchmark of Travis CI jobs : it contains 35,793,144 jobs from 272,917 different GitHub projects."],["semiprime ideals of an arbitrary Leavitt path algebra L are described in","Chains of Semiprime and Prime Ideals in Leavitt Path Algebras","summarize: Semiprime ideals of an arbitrary Leavitt path algebra L are described in terms of their generators. This description is then used to show that the semiprime ideals form a complete sublattice of the lattice of ideals of L, and they enjoy a certain gap property identified by Kaplansky in prime spectra of commutative rings. It is also shown that the totally ordered sets that can be realized as prime spectra of Leavitt path algebras are precisely those that have greatest lower bounds on subchains and enjoy the aforementioned gap property. Finally, it is shown that a conjecture of Kaplansky regarding identifying von Neumann regular rings via their prime factors holds for Leavitt path algebras."],["nonylphenol ethoxylate is a non ionic surfactant","Degradation of Nonylphenol Ethoxylate-10 by Mediated Electrochemical Oxidation Technology","summarize: Nonylphenol ethoxylate is a non ionic surfactant which is synthesized from alkylphenol ethoxylate. The accumulation of NPE-10 in wastewater will endanger the ecosystem as well as human being. At present, by an advancement of technology NPE 10 can be degraded indirectly by using an electrochemically treatment. Thus, this study aimed to evaluate the potential electrodegradation of NPE 10 by mediated electrochemical oxidation using Ce ionic mediator. In addition, the influence of Ag ionic catalyst in the performance of MEO for degradation of NPE 10 was also observed. The potency of MEO technology in degradation NPE 10 was evaluated by voltammetry technique and confirmed by titrimetry and LC-MS analyses. The results showed that in the absence of Ag ionic catalyst, the degradation of NPE 10 by MEO was 85.93 %. Furthermore, when the Ag ionic catalyst was applied, the performance of MEO in degradation of NPE 10 was improved to 95.12 %. The back titration using Ba2 confirmed the formation of CO2 by 46.79 %. Whereas the redox titration shows the total of degradation organic compounds by 42.50 %, which was emphasized by formation of two new peaks in LC-MS chromatogram. In summary, our results confirm the potential of MEO technology for NPE-10 degradation."],["we define an assembly map in relative geometric geometric. we define an assembly map in relative geometric","Relative geometric assembly and mapping cones, Part I: The geometric model and applications","summarize: Inspired by an analytic construction of Chang, Weinberger and Yu, we define an assembly map in relative geometric "],["the phase-space structure is provided by a family of resonant 2-DOF","Structure of the center manifold of the L1 and L2 collinear libration points in the restricted three-body problem","summarize: We present a global analysis of the center manifold of the collinear points in the circular restricted three-body problem. The phase-space structure is provided by a family of resonant 2-DOF Hamiltonian normal forms. The near 1:1 commensurability leads to the construction of a detuned Birkhoff-Gustavson normal form. The bifurcation sequences of the main orbit families are investigated by a geometric theory based on the reduction of the symmetries of the normal form, invariant under spatial mirror symmetries and time reversion. This global picture applies to any values of the mass parameter."],["compression is used to reduce communication costs in distributed data-parallel training of deep neural networks","On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning","summarize: Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layer-wise and entire-model compression."],["the original minimal path model computes the globally minimal geodesic by solving an Eikonal","Global Minimum for a Finsler Elastica Minimal Path Approach","summarize: In this paper, we propose a novel curvature-penalized minimal path model via an orientation-lifted Finsler metric and the Euler elastica curve. The original minimal path model computes the globally minimal geodesic by solving an Eikonal partial differential equation . Essentially, this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models. To solve this problem, we present an Eikonal PDE-based Finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem. We were successful at adding the curvature penalization to the classical geodesic energy. The basic idea of this work is to interpret the Euler elastica bending energy via a novel Finsler elastica metric that embeds a curvature penalty. This metric is non-Riemannian, anisotropic and asymmetric, and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension. Based on this orientation lifting, the proposed minimal path model can benefit from both the curvature and orientation of the paths. Thanks to the fast marching method, the global minimum of the curvature-penalized geodesic energy can be computed efficiently. We introduce two anisotropic image data-driven speed functions that are computed by steerable filters. Based on these orientation-dependent speed functions, we can apply the proposed Finsler elastica minimal path model to the applications of closed contour detection, perceptual grouping and tubular structure extraction. Numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results."],["the field has been receiving prominent consideration by researchers, developers and the industry. the literature is","Audiovisual Analytics Vocabulary and Ontology : initial core and example expansion","summarize: Visual Analytics might be defined as data mining assisted by interactive visual interfaces. The field has been receiving prominent consideration by researchers, developers and the industry. The literature, however, is complex because it involves multiple fields of knowledge and is considerably recent. In this article we describe an initial tentative organization of the knowledge in the field as an OWL ontology and a SKOS vocabulary. This effort might be useful in many ways that include conceptual considerations and software implementations. Within the results and discussions, we expose a core and an example expansion of the conceptualization, and incorporate design issues that enhance the expressive power of the abstraction."],["the network combines multiple recording setups. it tries to simultaneously compress the input E","Universal Joint Feature Extraction for P300 EEG Classification using Multi-task Autoencoder","summarize: The process of recording Electroencephalography signals is onerous and requires massive storage to store signals at an applicable frequency rate. In this work, we propose the EventRelated Potential Encoder Network ; a multi-task autoencoder-based model, that can be applied to any ERP-related tasks. The strength of ERPENet lies in its capability to handle various kinds of ERP datasets and its robustness across multiple recording setups, enabling joint training across datasets. ERPENet incorporates Convolutional Neural Networks and Long Short-Term Memory , in an autoencoder setup, which tries to simultaneously compress the input EEG signal and extract related P300 features into a latent vector. Here, we can infer the process for generating the latent vector as universal joint feature extraction. The network also includes a classification part for attended and unattended events classification as an auxiliary task. We experimented on six different P300 datasets. The results show that the latent vector exhibits better compression capability than the previous state-of-the-art semi-supervised autoencoder model. For attended and unattended events classification, pre-trained weights are adopted as initial weights and tested on unseen P300 datasets to evaluate the adaptability of the model, which shortens the training process as compared to using random Xavier weight initialization. At the compression rate of 6.84, the classification accuracy outperforms conventional P300 classification models: XdawnLDA, DeepConvNet, and EEGNet achieving 79.37% - 88.52% classification accuracy depending on the dataset."],["the much-used trace distance of coherence was shown to be not a proper measure","The modified trace distance of coherence is constant on most pure states","summarize: Recently, the much-used trace distance of coherence was shown to not be a proper measure of coherence, so a modification of it was proposed. We derive an explicit formula for this modified trace distance of coherence on pure states. Our formula shows that, despite satisfying the axioms of proper coherence measures, it is likely not a good measure to use, since it is maximal on all except for an exponentially-small fraction of pure states."],["Let us know what you think about it!","","summarize: Let "],["in this paper, we give sufficient and necessary conditions for cosine operator functions on solid Ban","Linear dynamics of cosine operator functions on solid Banach function spaces","summarize: In this paper, we give some sufficient and necessary conditions for cosine operator functions on solid Banach function spaces to be chaotic or topologically transitive."],["we present an approach to tackle the speaker recognition problem using Triplet Neural Networks.","Latent space representation for multi-target speaker detection and identification with a sparse dataset using Triplet neural networks","summarize: We present an approach to tackle the speaker recognition problem using Triplet Neural Networks. Currently, the "],["a parallel effort is to design adversaries that profit from misclassifications. not","On the Similarity of Deep Learning Representations Across Didactic and Adversarial Examples","summarize: The increasing use of deep neural networks has motivated a parallel endeavor: the design of adversaries that profit from successful misclassifications. However, not all adversarial examples are crafted for malicious purposes. For example, real world systems often contain physical, temporal, and sampling variability across instrumentation. Adversarial examples in the wild may inadvertently prove deleterious for accurate predictive modeling. Conversely, naturally occurring covariance of image features may serve didactic purposes. Here, we studied the stability of deep learning representations for neuroimaging classification across didactic and adversarial conditions characteristic of MRI acquisition variability. We show that representational similarity and performance vary according to the frequency of adversarial examples in the input space."],["the involvement of external stakeholders in capstone projects and project courses is desirable due to its potential positive","Involving External Stakeholders in Project Courses","summarize: Problem: The involvement of external stakeholders in capstone projects and project courses is desirable due to its potential positive effects on the students. Capstone projects particularly profit from the inclusion of an industrial partner to make the project relevant and help students acquire professional skills. In addition, an increasing push towards education that is aligned with industry and incorporates industrial partners can be observed. However, the involvement of external stakeholders in teaching moments can create friction and could, in the worst case, lead to frustration of all involved parties. Contribution: We developed a model that allows analysing the involvement of external stakeholders in university courses both in a retrospective fashion, to gain insights from past course instances, and in a constructive fashion, to plan the involvement of external stakeholders. Key Concepts: The conceptual model and the accompanying guideline guide the teachers in their analysis of stakeholder involvement. The model is comprised of several activities . The guideline provides questions that the teachers should answer for each of these activities. In the constructive use, the model allows teachers to define an action plan based on an analysis of potential stakeholders and the pedagogical objectives. In the retrospective use, the model allows teachers to identify issues that appeared during the project and their underlying causes. Drawing from ideas of the reflective practitioner, the model contains an emphasis on reflection and interpretation of the observations made by the teacher and other groups involved in the courses. Key Lessons: Applying the model retrospectively to a total of eight courses shows that it is possible to reveal hitherto implicit risks and assumptions and to gain a better insight into the interaction..."],["learned models can achieve significant performance gains over traditional methods. this special issue covers the state of","Editorial: Introduction to the Issue on Deep Learning for Image\/Video Restoration and Compression","summarize: Recent works have shown that learned models can achieve significant performance gains, especially in terms of perceptual quality measures, over traditional methods. Hence, the state of the art in image restoration and compression is getting redefined. This special issue covers the state of the art in learned image\/video restoration and compression to promote further progress in innovative architectures and training methods for effective and efficient networks for image\/video restoration and compression."],["auxiliary reward function provides more refined feedback for learning. auxiliary reward function provides more refined","Learning to Generalize from Sparse and Underspecified Rewards","summarize: We consider the problem of learning from sparse and underspecified rewards, where an agent receives a complex input, such as a natural language instruction, and needs to generate a complex response, such as an action sequence, while only receiving binary success-failure feedback. Such success-failure rewards are often underspecified: they do not distinguish between purposeful and accidental success. Generalization from underspecified rewards hinges on discounting spurious trajectories that attain accidental success, while learning from sparse feedback requires effective exploration. We address exploration by using a mode covering direction of KL divergence to collect a diverse set of successful trajectories, followed by a mode seeking KL divergence to train a robust policy. We propose Meta Reward Learning to construct an auxiliary reward function that provides more refined feedback for learning. The parameters of the auxiliary reward function are optimized with respect to the validation performance of a trained policy. The MeRL approach outperforms our alternative reward learning technique based on Bayesian Optimization, and achieves the state-of-the-art on weakly-supervised semantic parsing. It improves previous work by 1.2% and 2.4% on WikiTableQuestions and WikiSQL datasets respectively."],["a multi-head attention Molecular Transformer model outperforms all algorithms in the literature","Molecular Transformer - A Model for Uncertainty-Calibrated Chemical Reaction Prediction","summarize: Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between SMILES strings of reactants-reagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable."],["the discretisation is based on several adaptations of the Hybrid-High-Or","An arbitrary order scheme on generic meshes for miscible displacements in porous media","summarize: We design, analyse and implement an arbitrary order scheme applicable to generic meshes for a coupled elliptic-parabolic PDE system describing miscible displacement in porous media. The discretisation is based on several adaptations of the Hybrid-High-Order method due to Di Pietro et al. . The equation governing the pressure is discretised using an adaptation of the HHO method for variable diffusion, while the discrete concentration equation is based on the HHO method for advection-diffusion-reaction problems combined with numerically stable flux reconstructions for the advective velocity that we have derived using the results of Cockburn et al. . We perform some rigorous analysis of the method to demonstrate its "],["the el'fand-alinin-uks class in the e","An affirmative answer to a conjecture for Metoki class","summarize: In The el'fand-alinin-uks class and characteristic classes of transversely symplectic foliations arXiv:0910.3414, Kotschick and Morita showed that the Gel'fand-Kalinin-Fuks class in "],["each node in the network has a local buffer of bounded size. the","A Constant Approximation Algorithm for Scheduling Packets on Line Networks","summarize: In this paper we improve the approximation ratio for the problem of scheduling packets on line networks with bounded buffers, where the aim is that of maximizing the throughput. Each node in the network has a local buffer of bounded size "],["we study a biased random walk on the interlacement set of interlacement sets of","Biased random walk on the interlacement set","summarize: We study a biased random walk on the interlacement set of "],["crowd models can be used to assess occupant exposure in confined spaces. the proposed model","EXPOSED: An occupant exposure model for confined spaces to retrofit crowd models during a pandemic","summarize: Crowd models can be used for the simulation of people movement in the built environment. Crowd model outputs have been used for evaluating safety and comfort of pedestrians, inform crowd management and perform forensic investigations. Microscopic crowd models allow the representation of each person and the obtainment of information concerning their location over time and interactions with the physical space\/other people. Pandemics such as COVID-19 have posed several questions on safe building usage, given the risk of disease transmission among building occupants. Here we show how crowd modelling can be used to assess occupant exposure in confined spaces. The policies adopted concerning building usage and social distancing during a pandemic can vary greatly, and they are mostly based on the macroscopic analysis of the spread of disease rather than a safety assessment performed at a building level. The proposed model allows the investigation of occupant exposure in buildings based on the analysis of microscopic people movement. Risk assessment is performed by retrofitting crowd models with a universal model for exposure assessment which can account for different types of disease transmissions. This work allows policy makers to perform informed decisions concerning building usage during a pandemic."],["thin film optical elements generate an annular beam by filtering the fundamental Gaussian mode of","Translationally Invariant Generation of Annular Beams using Thin Films","summarize: Thin film optical elements exhibiting translational invariance, and thus robustness to optical misalignment, are crucial for rapid development of compact and integrated optical devices. In this letter, we experimentally demonstrate a beam-shaping element that generates an annular beam by spatially filtering the fundamental Gaussian mode of a laser beam. The element comprises of a one-dimensional photonic crystal cavity fabricated using sputtered thin films. The planar architecture and in-plane symmetry of the element render our beam-shaping technique translationally invariant. The generated annular beam is sensitive to the polarization direction and the wavelength of the incident laser beam. Using this property of the annular beam, we show simultaneous generation of concentric annular beams of different wavelengths. Our experimental observations show an excellent agreement with simulation results performed using finite-difference time-domain method. Such a beam-shaping element has applications in areas ranging from microscopy and medicine to semiconductor lithography and manufacturing in microelectronics industry."],["Habibullin emph proposed an approach to construct Lax pairs of a","An upper order bound of the invariant manifold in Lax pairs of a nonlinear evolution partial differential equation","summarize: In \\cite, Habibullin \\emph proposed an approach to construct Lax pairs of a nonlinear integrable partial differential equation , where one is the linearized equation of the studied PDE and the other is the invariant manifold of the linearized equation. In this paper, we show that the invariant manifold is the characteristic of a generalized conditional symmetry of the system composed of the studied PDE and its linearized PDE. Then we give an upper order bound of the invariant manifold which provides a theoretical basis for a complete classification of such type of invariant manifold. Moreover, we suggest a modified method to construct Lax pair of the KdV equation which can not be obtained by the original method in \\cite."],["spatial collection efficiency portrays the driving forces and loss mechanisms in photovoltaic and photoe","The spatial collection efficiency of photogenerated charge carriers in photovoltaic and photoelectrochemical devices","summarize: The spatial collection efficiency portrays the driving forces and loss mechanisms in photovoltaic and photoelectrochemical devices. It is defined as the fraction of photogenerated charge carriers created at a specific point within the device that contribute to the photocurrent. In stratified planar structures, the spatial collection efficiency can be extracted out of photocurrent action spectra measurements empirically, with few a priori assumptions. Although this method was applied to photovoltaic cells made of well-understood materials, it has never been used to study unconventional materials such as metal-oxide semiconductors that are often employed in photoelectrochemical cells. This perspective shows the opportunities that this method has to offer for investigating new materials and devices with unknown properties. The relative simplicity of the method, and its applicability to operando performance characterization, makes it an important tool for analysis and design of new photovoltaic and photoelectrochemical materials and devices."],["VSSS-RL is a traditional league in the Latin American robotics competition.","Learning to Play Soccer by Reinforcement and Applying Sim-to-Real to Compete in the Real World","summarize: This work presents an application of Reinforcement Learning for the complete control of real soccer robots of the IEEE Very Small Size Soccer , a traditional league in the Latin American Robotics Competition . In the VSSS league, two teams of three small robots play against each other. We propose a simulated environment in which continuous or discrete control policies can be trained, and a Sim-to-Real method to allow using the obtained policies to control a robot in the real world. The results show that the learned policies display a broad repertoire of behaviors that are difficult to specify by hand. This approach, called VSSS-RL, was able to beat the human-designed policy for the striker of the team ranked 3rd place in the 2018 LARC, in 1-vs-1 matches."],["two-dimensional semimetals beyond graphene have been relatively unexplored in the","High Current Density and Low Thermal Conductivity of Atomically Thin Semimetallic WTe2","summarize: Two-dimensional semimetals beyond graphene have been relatively unexplored in the atomically-thin limit. Here we introduce a facile growth mechanism for semimetallic WTe2 crystals, then fabricate few-layer test structures while carefully avoiding degradation from exposure to air. Low-field electrical measurements of 80 nm to 2 um long devices allow us to separate intrinsic and contact resistance, revealing metallic response in the thinnest encapsulated and stable WTe2 devices studied to date . High-field electrical measurements and electro-thermal modeling demonstrate that ultra-thin WTe2 can carry remarkably high current density despite a very low thermal conductivity . These results suggest several pathways for air-stable technological viability of this layered semimetal."],["multiplier Hopf monoid is a morphism of multiplier bimon","Multiplier Hopf monoids","summarize: The notion of multiplier Hopf monoid in any braided monoidal category is introduced as a multiplier bimonoid whose constituent fusion morphisms are isomorphisms. In the category of vector spaces over the complex numbers, Van Daele's definition of multiplier Hopf algebra is re-obtained. It is shown that the key features of multiplier Hopf algebras remain valid in this more general context. Namely, for a multiplier Hopf monoid A, the existence of a unique antipode is proved --- in an appropriate, multiplier-valued sense --- which is shown to be a morphism of multiplier bimonoids from a twisted version of A to A. For a regular multiplier Hopf monoid the antipode is proved to factorize through a proper automorphism of the object A. Under mild further assumptions, duals in the base category are shown to lift to the monoidal categories of modules and of comodules over a regular multiplier Hopf monoid. Finally, the so-called Fundamental Theorem of Hopf modules is proved --- which states an equivalence between the base category and the category of Hopf modules over a multiplier Hopf monoid."],["signed graphs are homomorphism problems. signed graphs are a computational problem","The complexity of signed graph and edge-coloured graph homomorphisms","summarize: We study homomorphism problems of signed graphs from a computational point of view. A signed graph "],["three publicly available Hungarian corpora are evaluated. perplexity values comparable to","emLam -- a Hungarian Language Modeling baseline","summarize: This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungar- ian benchmark corpus is introduced."],["Bayeslands used the Bayesian paradigm to make inference for unknown parameters in the","Multi-core parallel tempering Bayeslands for basin and landscape evolution","summarize: The Bayesian paradigm is becoming an increasingly popular framework for estimation and uncertainty quantification of unknown parameters in geo-physical inversion problems. Badlands is a basin and landscape evolution forward model for simulating topography evolution at a large range of spatial and time scales. Our previous work presented Bayeslands that used the Bayesian paradigm to make inference for unknown parameters in the Badlands model using Markov chain Monte Carlo sampling. Bayeslands faced challenges in convergence due to multi-modal posterior distributions in the selected parameters of Badlands. Parallel tempering is an advanced MCMC method suited for irregular and multi-modal posterior distributions. In this paper, we extend Bayeslands using parallel tempering with high performance computing to address previous limitations in parameter space exploration in the context of the computationally expensive Badlands model. Our results show that PT-Bayeslands not only reduces the computation time, but also provides an improvement of the sampling for multi-modal posterior distributions. This provides an improvement over Bayeslands which used single chain MCMC that face difficulties in convergence and can lead to misleading inference. This motivates its usage in large-scale basin and landscape evolution models."],["long Lorentz tube with absorbing boundaries is injected to the tube. we compute","Non equilibrium density profiles in Lorentz tubes with thermostated boundaries","summarize: We consider a long Lorentz tube with absorbing boundaries. Particles are injected to the tube from the left end. We compute the equilibrium density profiles in two cases: the semi-infinite tube and a long finite tube . In the latter case, we also show that convergence to equilibrium is well described by the heat equation. In order to prove these results, we obtain new results for the Lorentz particle which are of independent interest. First, we show that a particle conditioned not to hit the boundary for a long time converges to the Brownian meander. Second, we prove several local limit theorems for particles having a prescribed behavior in the past."],["the Hausdorff distance is a measure of similarity between two sets. it is","Computing the Hausdorff Distance of Two Sets from Their Signed Distance Functions","summarize: The Hausdorff distance is a measure of similarity between two sets which is widely used in various applications. Most of the applied literature is devoted to the computation for sets consisting of a finite number of points. This has applications, for instance, in image processing. However, we would like to apply the Hausdorff distance to control and evaluate optimisation methods in level-set based shape optimisation. In this context, the involved sets are not finite point sets but characterised by level-set or signed distance functions. This paper discusses the computation of the Hausdorff distance between two such sets. We recall fundamental properties of the Hausdorff distance, including a characterisation in terms of distance functions. In numerical applications, this result gives at least an exact lower bound on the Hausdorff distance. We also derive an upper bound, and consequently a precise error estimate. By giving an example, we show that our error estimate cannot be further improved for a general situation. On the other hand, we also show that much better accuracy can be expected for non-pathological situations that are more likely to occur in practice. The resulting error estimate can be improved even further if one assumes that the grid is rotated randomly with respect to the involved sets."],["deterministic incremental aggregated gradient method is based on a deterministic order","On the Convergence Rate of Incremental Aggregated Gradient Algorithms","summarize: Motivated by applications to distributed optimization over networks and large-scale data processing in machine learning, we analyze the deterministic incremental aggregated gradient method for minimizing a finite sum of smooth functions where the sum is strongly convex. This method processes the functions one at a time in a deterministic order and incorporates a memory of previous gradient values to accelerate convergence. Empirically it performs well in practice; however, no theoretical analysis with explicit rate results was previously given in the literature to our knowledge, in particular most of the recent efforts concentrated on the randomized versions. In this paper, we show that this deterministic algorithm has global linear convergence and characterize the convergence rate. We also consider an aggregated method with momentum and demonstrate its linear convergence. Our proofs rely on a careful choice of a Lyapunov function that offers insight into the algorithm's behavior and simplifies the proofs considerably."],["polaritons can propagate along anisotropic metasurfaces with either hyperbol","Collective near-field coupling in infrared-phononic metasurfaces for nano-light canalization","summarize: Polaritons, coupled excitations of photons and dipolar matter excitations, can propagate along anisotropic metasurfaces with either hyperbolic or elliptical dispersion. At the transition from hyperbolic to elliptical dispersion , various intriguing phenomena are found, such as an enhancement of the photonic density of states, polariton canalization and hyperlensing. Here we investigate theoretically and experimentally the topological transition and the polaritonic coupling of deeply subwavelength elements in a uniaxial infrared-phononic metasurface, a grating of hexagonal boron nitride nanoribbons. By hyperspectral infrared nanoimaging, we observe, for the first time, a synthetic transverse optical phonon resonance in the middle of the hBN Reststrahlen band, yielding a topological transition from hyperbolic to elliptical dispersion. We further visualize and characterize the spatial evolution of a deeply subwavelength canalization mode near the transition frequency, which is a collimated polariton that is the basis for hyperlensing and diffraction-less propagation. Our results provide fundamental insights into the role of polaritonic near-field coupling in metasurfaces for creating topological transitions and polariton canalization."],["the widely used saliency methods are proposed for explaining CNN-based classifications. they","Interpretable Graph Capsule Networks for Object Recognition","summarize: Capsule Networks, as alternatives to Convolutional Neural Networks, have been proposed to recognize objects from images. The current literature demonstrates many advantages of CapsNets over CNNs. However, how to create explanations for individual classifications of CapsNets has not been well explored. The widely used saliency methods are mainly proposed for explaining CNN-based classifications; they create saliency map explanations by combining activation values and the corresponding gradients, e.g., Grad-CAM. These saliency methods require a specific architecture of the underlying classifiers and cannot be trivially applied to CapsNets due to the iterative routing mechanism therein. To overcome the lack of interpretability, we can either propose new post-hoc interpretation methods for CapsNets or modifying the model to have build-in explanations. In this work, we explore the latter. Specifically, we propose interpretable Graph Capsule Networks , where we replace the routing part with a multi-head attention-based Graph Pooling approach. In the proposed model, individual classification explanations can be created effectively and efficiently. Our model also demonstrates some unexpected benefits, even though it replaces the fundamental part of CapsNets. Our GraCapsNets achieve better classification performance with fewer parameters and better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets also keep other advantages of CapsNets, namely, disentangled representations and affine transformation robustness."],["the study was conducted to develop an appropriate model that could predict the weekly reported Malaria incidence in","Malaria Incidence in the Philippines: Prediction using the Autoregressive Moving Average Models","summarize: The study was conducted to develop an appropriate model that could predict the weekly reported Malaria incidence in the Philippines using the Box-Jenkins method.The data were retrieved from the Department of Health website in the Philippines. It contains 70 data points of which 60 data points were used in model building and the remaining 10 data points were used for forecast evaluation. The R Statistical Software was used to do all the necessary computations in the study. Box-Cox Transformation and Differencing was done to make the series stationary. Based on the results of the analysis, ARIMA is the appropriate model for the weekly Malaria incidence in the Philippines."],["biclustering enables us to identify subsets of genes that are co-ex","Convex Biclustering","summarize: In the biclustering problem, we seek to simultaneously group observations and features. While biclustering has applications in a wide array of domains, ranging from text mining to collaborative filtering, the problem of identifying structure in high dimensional genomic data motivates this work. In this context, biclustering enables us to identify subsets of genes that are co-expressed only within a subset of experimental conditions. We present a convex formulation of the biclustering problem that possesses a unique global minimizer and an iterative algorithm, COBRA, that is guaranteed to identify it. Our approach generates an entire solution path of possible biclusters as a single tuning parameter is varied. We also show how to reduce the problem of selecting this tuning parameter to solving a trivial modification of the convex biclustering problem. The key contributions of our work are its simplicity, interpretability, and algorithmic guarantees - features that arguably are lacking in the current alternative algorithms. We demonstrate the advantages of our approach, which includes stably and reproducibly identifying biclusterings, on simulated and real microarray data."],["the simulation software ParFlow has been demonstrated to meet this requirement. the code requires further enhancement","Enhancing speed and scalability of the ParFlow simulation code","summarize: Regional hydrology studies are often supported by high resolution simulations of subsurface flow that require expensive and extensive computations. Efficient usage of the latest high performance parallel computing systems becomes a necessity. The simulation software ParFlow has been demonstrated to meet this requirement and shown to have excellent solver scalability for up to 16,384 processes. In the present work we show that the code requires further enhancements in order to fully take advantage of current petascale machines. We identify ParFlow's way of parallelization of the computational mesh as a central bottleneck. We propose to reorganize this subsystem using fast mesh partition algorithms provided by the parallel adaptive mesh refinement library p4est. We realize this in a minimally invasive manner by modifying selected parts of the code to reinterpret the existing mesh data structures. We evaluate the scaling performance of the modified version of ParFlow, demonstrating good weak and strong scaling up to 458k cores of the Juqueen supercomputer, and test an example application at large scale."],["affine connections on manifolds are determined by the 1-connection form and the fundamental","Flat Affine Manifolds And Their Transformations","summarize: We give a characterization of flat affine connections on manifolds by means of a natural affine representation of the universal covering of the Lie group of diffeomorphisms preserving the connection. From the infinitesimal point of view, this representation is determined by the 1-connection form and the fundamental form of the bundle of linear frames of the manifold. We show that the group of affine transformations of a real flat affine "],["system consists of a Willow Garage PR2 and specialized autonomous behaviors for scooping and","Towards Assistive Feeding with a General-Purpose Mobile Manipulator","summarize: General-purpose mobile manipulators have the potential to serve as a versatile form of assistive technology. However, their complexity creates challenges, including the risk of being too difficult to use. We present a proof-of-concept robotic system for assistive feeding that consists of a Willow Garage PR2, a high-level web-based interface, and specialized autonomous behaviors for scooping and feeding yogurt. As a step towards use by people with disabilities, we evaluated our system with 5 able-bodied participants. All 5 successfully ate yogurt using the system and reported high rates of success for the system's autonomous behaviors. Also, Henry Evans, a person with severe quadriplegia, operated the system remotely to feed an able-bodied person. In general, people who operated the system reported that it was easy to use, including Henry. The feeding system also incorporates corrective actions designed to be triggered either autonomously or by the user. In an offline evaluation using data collected with the feeding system, a new version of our multimodal anomaly detection system outperformed prior versions."],["5G networks can support multiple vertical services. such services may include several common virtual functions","Getting the Most Out of Your VNFs: Flexible Assignment of Service Priorities in 5G","summarize: Through their computational and forwarding capabilities, 5G networks can support multiple vertical services. Such services may include several common virtual functions , which could be shared to increase resource efficiency. In this paper, we focus on the seldom studied VNF-sharing problem, and decide whether sharing a VNF instance is possible\/beneficial or not, how to scale virtual machines hosting the VNFs to share, and the priorities of the different services sharing the same VNF. These decisions are made with the aim to minimize the mobile operator's costs while meeting the verticals' performance requirements. Importantly, we show that the aforementioned priorities should not be determined a priori on a per-service basis, rather they should change across VNFs since such additional flexibility allows for more efficient solutions. We then present an effective methodology called FlexShare, enabling near-optimal VNF-sharing decisions in polynomial time. Our performance evaluation, using real-world VNF graphs, confirms the effectiveness of our approach, which consistently outperforms baseline solutions using per-service priorities."],["the vacant set is a.s. infinite, thus solving an open problem.","The vacant set of two-dimensional critical random interlacement is infinite","summarize: For the model of two-dimensional random interlacements in the critical regime , we prove that the vacant set is a.s.\\ infinite, thus solving an open problem from arXiv:1502.03470. Also, we prove that the entrance measure of simple random walk on annular domains has certain regularity properties; this result is useful when dealing with soft local times for excursion processes."],["in the last six years, indium selenide has appeared as a van der Wa","Magnetotransport and lateral confinement in an InSe van der Waals Heterostructure","summarize: In the last six years, Indium selenide has appeared as a new van der Waals heterostructure platform which has been extensively studied due to its unique electronic and optical properties. Such as transition metal dichalcogenides , the considerable bandgap and high electron mobility can provide a potential optoelectronic application. Here we present low-temperature transport measurements on a few-layer InSe van der Waals heterostructure with graphene-gated contacts. For high magnetic fields, we observe magnetoresistance minima at even filling factors related to two-fold spin degeneracy. By electrostatic gating with negatively biased split gates, a one-dimensional channel is realized. Close to pinch-off, transport through the constriction is dominated by localized states with charging energies ranging from 2 to 5 meV. This work opens new possibility to explore the low-dimensional physics including quantum point contact and quantum dot."],["a single self-play architecture has learned three different games at super human level. the","Warm-Start AlphaZero Self-Play Search Enhancements","summarize: Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms . Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different board games, with especially RAVE based variants playing strongly."],["constraints on scaling relations of galaxy cluster X-ray luminosity, temperature and gas mass","Weighing the Giants V: Galaxy Cluster Scaling Relations","summarize: We present constraints on the scaling relations of galaxy cluster X-ray luminosity, temperature and gas mass with mass and redshift, employing masses from robust weak gravitational lensing measurements. These are the first such results obtained from an analysis that simultaneously accounts for selection effects and the underlying mass function, and directly incorporates lensing data to constrain total masses. Our constraints on the scaling relations and their intrinsic scatters are in good agreement with previous studies, and reinforce a picture in which departures from self-similar scaling laws are primarily limited to cluster cores. However, the data are beginning to reveal new features that have implications for cluster astrophysics and provide new tests for hydrodynamical simulations. We find a positive correlation in the intrinsic scatters of luminosity and temperature at fixed mass, which is related to the dynamical state of the clusters. While the evolution of the nominal scaling relations over the redshift range "],["the newman-watts model is given by taking a cycle graph of","First passage percolation on the Newman-Watts small world model","summarize: The Newman-Watts model is given by taking a cycle graph of n vertices and then adding each possible edge "],["paper introduces an investigation of the healthcare monitoring systems. the different roles that exist in healthcare","A Context Aware Framework for IoT Based Healthcare Monitoring Systems","summarize: This paper introduces an investigation of the healthcare monitoring systems and their provisioning in the IoT platform. The different roles that exist in healthcare systems are specified and modeled here. This paper also attempts to introduce and propose a generic framework for the design and development of context aware healthcare monitoring systems in the IoT platform. In such a framework, the fundamental components of the healthcare monitoring systems are identified and modelled as well as the relationship between these components. The paper also stresses on the crucial role played by the AI field in addressing resilient context aware healthcare monitoring systems. Architecturally, this framework is based on a distributed layered architecture where the different components are deployed over the physical layer, fog platform and the cloud platform."],["we construct canonical rack bialgebras for any Leibniz algebra","Algebraic deformation quantization of Leibniz algebras","summarize: In this paper we focus on a certain self-distributive multiplication on coalgebras, which leads to so-called rack bialgebra. We construct canon-ical rack bialgebras for any Leibniz algebra. Our motivation is deformation quantization of Leibniz algebras in the sense of . Namely, the canonical rack bialgebras we have constructed for any Leibniz algebra lead to a simple explicit formula of the rack-star-product on the dual of a Leibniz algebra recently constructed by Dherin and Wagemann in . We clarify this framework setting up a general deformation theory for rack bialgebras and show that the rack-star-product turns out to be a deformation of the trivial rack bialgebra product."],["new approach leads to giant gain enhancement in a Fabry-Perot cavity","Giant Gain Enhancement in Photonic Crystals with a Degenerate Band Edge","summarize: We propose a new approach leading to giant gain enhancement. It is based on unconventional slow wave resonance associated to a degenerate band edge in the dispersion diagram for a special class of photonic crystals supporting two modes at each frequency. We show that the gain enhancement in a Fabry-Perot cavity when operating at the DBE is several orders of magnitude stronger when compared to a cavity of the same length made of a standard photonic crystal with a regular band edge . The giant gain condition is explained by a significant increase in the photon lifetime and in the local density of states. We have demonstrated the existence of DBE operated special cavities that provide for superior gain conditions for solid-state lasers, quantum cascade lasers, traveling wave tubes, and distributed solid state amplifiers. We also report the possibility to achieve low-threshold lasing in FPC with DBE compared to RBE-based lasers."],["kernels are based on spectral characteristics to facilitate decomposition. kernels","On the Importance of Temporal Context in Proximity Kernels: A Vocal Separation Case Study","summarize: Musical source separation methods exploit source-specific spectral characteristics to facilitate the decomposition process. Kernel Additive Modelling models a source applying robust statistics to time-frequency bins as specified by a source-specific kernel, a function defining similarity between bins. Kernels in existing approaches are typically defined using metrics between single time frames. In the presence of noise and other sound sources information from a single-frame, however, turns out to be unreliable and often incorrect frames are selected as similar. In this paper, we incorporate a temporal context into the kernel to provide additional information stabilizing the similarity search. Evaluated in the context of vocal separation, our simple extension led to a considerable improvement in separation quality compared to previous kernels."],["a large-scale aerialterrestrial heterogenous cellular network can be either ground","Coverage Performance of Aerial-Terrestrial HetNets","summarize: Providing seamless coverage under current cellular network technologies is surmountable only through gross overengineering. Alternatively, as an economically effective solution, the use of unmanned aerial vehicles , augmented with the functionalities of terrestrial base stations , is recently advocated. In this paper we investigate the effect that the incorporation of UAV-mounted BSs poses on the coverage probability of cellular networks. To this end, we focus on the evaluation of the coverage probability of a large-scale aerialterrestrial heterogenous cellular network , in which BSs of each technology\/tier can be either ground or UBS. Our analysis incorporates the impact of Line-of-Sight and non-LOS path-loss attenuations of both ground-toground and Air-to-Ground links. Adopting tools of stochastic geometry we provide an expression for the coverage probability based on main system parameters and percentage of BSs in each tier that are aerial. We confirm the accuracy of our analysis. Using our analysis, we observe that for several common communication environments, e.g., high-rise and dense urban environments, the inclusion of U-BSs can be detrimental to the coverage probability. Nevertheless, it is still possible to minimize the coverage cost by turning off a percentage of G-BSs. Interestingly, for urban and sub-urban areas one can adjust the altitude of U-BSs in order to increase the coverage probability."],["specificity prediction systems predict very coarse labels. the aim of this work is to generalize","Domain Agnostic Real-Valued Specificity Prediction","summarize: Sentence specificity quantifies the level of detail in a sentence, characterizing the organization of information in discourse. While this information is useful for many downstream applications, specificity prediction systems predict very coarse labels and are trained on and tailored toward specific domains . The goal of this work is to generalize specificity prediction to domains where no labeled data is available and output more nuanced real-valued specificity ratings. We present an unsupervised domain adaptation system for sentence specificity prediction, specifically designed to output real-valued estimates from binary training labels. To calibrate the values of these predictions appropriately, we regularize the posterior distribution of the labels towards a reference distribution. We show that our framework generalizes well to three different domains with 50%~68% mean absolute error reduction than the current state-of-the-art system trained for news sentence specificity. We also demonstrate the potential of our work in improving the quality and informativeness of dialogue generation systems."],["a distributed computational productive laziness approach is proposed in this paper.","Ethically Aligned Opportunistic Scheduling for Productive Laziness","summarize: In artificial intelligence mediated workforce management systems , long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches."],["we study generalised linear regression and classification for a synthetically generated dataset. we consider","Generalisation error in learning with random features and the hidden manifold model","summarize: We study generalised linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning with random features, neural networks in the lazy training regime, and the hidden manifold model. We consider the high-dimensional regime and using the replica method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalised linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks."],["human-subject study aims to explore two facets of human mental models of robots","Robot Capability and Intention in Trust-based Decisions across Tasks","summarize: In this paper, we present results from a human-subject study designed to explore two facets of human mental models of robots---inferred capability and intention---and their relationship to overall trust and eventual decisions. In particular, we examine delegation situations characterized by uncertainty, and explore how inferred capability and intention are applied across different tasks. We develop an online survey where human participants decide whether to delegate control to a simulated UAV agent. Our study shows that human estimations of robot capability and intent correlate strongly with overall self-reported trust. However, overall trust is not independently sufficient to determine whether a human will decide to trust a given task to a robot. Instead, our study reveals that estimations of robot intention, capability, and overall trust are integrated when deciding to delegate. From a broader perspective, these results suggest that calibrating overall trust alone is insufficient; to make correct decisions, humans need multi-faceted mental models when collaborating with robots across multiple contexts."],["nonconvex procedures often require proper regularization in order to guarantee fast convergence. gradient","Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind Deconvolution","summarize: Recent years have seen a flurry of activities in designing provably efficient nonconvex procedures for solving statistical estimation problems. Due to the highly nonconvex nature of the empirical loss, state-of-the-art procedures often require proper regularization in order to guarantee fast convergence. For vanilla procedures such as gradient descent, however, prior theory either recommends highly conservative learning rates to avoid overshooting, or completely lacks performance guarantees. This paper uncovers a striking phenomenon in nonconvex optimization: even in the absence of explicit regularization, gradient descent enforces proper regularization implicitly under various statistical models. In fact, gradient descent follows a trajectory staying within a basin that enjoys nice geometry, consisting of points incoherent with the sampling mechanism. This implicit regularization feature allows gradient descent to proceed in a far more aggressive fashion without overshooting, which in turn results in substantial computational savings. Focusing on three fundamental statistical estimation problems, i.e. phase retrieval, low-rank matrix completion, and blind deconvolution, we establish that gradient descent achieves near-optimal statistical and computational guarantees without explicit regularization. In particular, by marrying statistical modeling with generic optimization theory, we develop a general recipe for analyzing the trajectories of iterative algorithms via a leave-one-out perturbation argument. As a byproduct, for noisy matrix completion, we demonstrate that gradient descent achieves near-optimal error control --- measured entrywise and by the spectral norm --- which might be of independent interest."],["egocentric videos can be useful to assist the user during his visit. the site manager","Egocentric Visitors Localization in Cultural Sites","summarize: We consider the problem of localizing visitors in a cultural site from egocentric images. Localization information can be useful both to assist the user during his visit and to provide behavioral information to the manager of the cultural site . To tackle the problem, we collected a large dataset of egocentric videos using two cameras: a head-mounted HoloLens device and a chest-mounted GoPro. Each frame has been labeled according to the location of the visitor and to what he was looking at. The dataset is freely available in order to encourage research in this domain. The dataset is complemented with baseline experiments performed considering a state-of-the-art method for location-based temporal segmentation of egocentric videos. Experiments show that compelling results can be achieved to extract useful information for both the visitor and the site-manager."],["ASAP automatically smooths streaming time series visualizations by optimizing the trade-off between noise reduction","ASAP: Prioritizing Attention via Time Series Smoothing","summarize: Time series visualization of streaming telemetry is increasingly prevalent in modern data platforms and applications. However, many existing systems simply plot the raw data streams as they arrive, often obscuring large-scale trends due to small-scale noise. We propose an alternative: to better prioritize end users' attention, smooth time series visualizations as much as possible to remove noise, while retaining large-scale structure to highlight significant deviations. We develop a new analytics operator called ASAP that automatically smooths streaming time series by adaptively optimizing the trade-off between noise reduction and trend retention . We introduce metrics to quantitatively assess the quality of smoothed plots and provide an efficient search strategy for optimizing these metrics that combines techniques from stream processing, user interface design, and signal processing via autocorrelation-based pruning, pixel-aware preaggregation, and on-demand refresh. We demonstrate that ASAP can improve users' accuracy in identifying long-term deviations in time series by up to 38.4% while reducing response times by up to 44.3%. Moreover, ASAP delivers these results several orders of magnitude faster than alternative search strategies."],["paper introduces non-stationary adversarial cost with a variation constraint.","Bayesian adversarial multi-node bandit for optimal smart grid protection against cyber attacks","summarize: The cybersecurity of smart grids has become one of key problems in developing reliable modern power and energy systems. This paper introduces a non-stationary adversarial cost with a variation constraint for smart grids and enables us to investigate the problem of optimal smart grid protection against cyber attacks in a relatively practical scenario. In particular, a Bayesian multi-node bandit model with adversarial costs is constructed and a new regret function is defined for this model. An algorithm called Thompson-Hedge algorithm is presented to solve the problem and the superior performance of the proposed algorithm is proven in terms of the convergence rate of the regret function. The applicability of the algorithm to real smart grid scenarios is verified and the performance of the algorithm is also demonstrated by numerical examples."],["the arcs contain quasi-conformally conjugate parabolic parameters. the","Parabolic arcs of the multicorns: Real-analyticity of Hausdorff dimension, and singularities of ","summarize: The boundaries of the hyperbolic components of odd period of the multicorns contain real-analytic arcs consisting of quasi-conformally conjugate parabolic parameters. One of the main results of this paper asserts that the Hausdorff dimension of the Julia sets is a real-analytic function of the parameter along these parabolic arcs. This is achieved by constructing a complex one-dimensional quasiconformal deformation space of the parabolic arcs which are contained in the dynamically defined algebraic curves "],["the new concept of a Symmetric rf- SQUID allows to fully","Symmetric Traveling Wave Parametric Amplifier","summarize: We developed and experimentally tested a Symmetric Traveling Wave Parametric Amplifier based on Three-Wave Mixing, using the new concept of a Symmetric rf- SQUID. This allows to fully control the second and third order nonlinearities of the STWPA by applying external currents. In this way, the optimal bias point can be reached, taking into account both phase mismatch and pump depletion minimization. The structure was tested at 4.2K, showing a 4GHz bandwidth and a maximum estimated gain of 17dB. STWPA showed also great flexibility, allowing up-down conversion mixer operations and rf-controlled switch."],["elastic properties of a material with spherical voids of equal volume are","Beam model for the elastic properties of material with spherical voids","summarize: The elastic properties of a material with spherical voids of equal volume are analysed using a new model, with particular attention paid to the hexagonal close-packed and the face-centred cubic arrangement of voids. Void fractions well above 74 \\% are considered, yielding overlapping voids as in an open-cell foam and hence a connected pore structure. The material is represented by a network of beams. The elastic behaviour of each beam is derived analytically from the material structure. By computing the linear elastic properties of the beam network, the Young's moduli and Poisson ratios for different directions are evaluated. In the limit of rigidity loss a power law is obtained, describing the relation between Young's modulus and void fraction with an exponent of 5\/2 for bending-dominated and 3\/2 for stretching-dominated directions. The corresponding Poisson ratios vary between 0 and 1. With decreasing void fraction, these exponents become 2.3 and 1.3, respectively. The data obtained analytically and from the new beam model are compared to Finite Element simulations carried out in a companion study, and good agreement is found. The hexagonal close-packed void arrangement features very anisotropic behaviour, comparable to that of fibre-reinforced materials, This might allow for new applications of open-cell materials."],["we try to search for the neuron configuration of a fixed network architecture. using it","NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks","summarize: Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parameterize the change of the neuron number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parameterized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting of default configuration with scaling factor of 0.25)."],["spectral method and semigroup theory are based on verified computations. the method","Rigorous numerical computations for 1D advection equations with variable coefficients","summarize: This paper provides a methodology of verified computing for solutions to 1-dimensional advection equations with variable coefficients. The advection equation is typical partial differential equations of hyperbolic type. There are few results of verified numerical computations to initial-boundary value problems of hyperbolic PDEs. Our methodology is based on the spectral method and semigroup theory. The provided method in this paper is regarded as an efficient application of semigroup theory in a sequence space associated with the Fourier series of unknown functions. This is a foundational approach of verified numerical computations for hyperbolic PDEs. Numerical examples show that the rigorous error estimate showing the well-posedness of the exact solution is given with high accuracy and high speed."],["the explicit connection to Bayesian experimental design suggests several distinct regimes in which optimal probabilistic","Optimality Criteria for Probabilistic Numerical Methods","summarize: It is well understood that Bayesian decision theory and average case analysis are essentially identical. However, if one is interested in performing uncertainty quantification for a numerical task, it can be argued that standard approaches from the decision-theoretic framework are neither appropriate nor sufficient. Instead, we consider a particular optimality criterion from Bayesian experimental design and study its implied optimal information in the numerical context. This information is demonstrated to differ, in general, from the information that would be used in an average-case-optimal numerical method. The explicit connection to Bayesian experimental design suggests several distinct regimes in which optimal probabilistic numerical methods can be developed."],["we present a general approach to deriving tight state-independent uncertainty relations.","Tight state-independent uncertainty relations for qubits","summarize: The well-known Robertson-Schr\\odinger uncertainty relations have state-dependent lower bounds which are trivial for certain states. We present a general approach to deriving tight state-independent uncertainty relations for qubit measurements that completely characterise the obtainable uncertainty values. This approach can give such relations for any number of observables, and we do so explicitly for arbitrary pairs and triples of qubit measurements. We show how these relations can be transformed into equivalent tight entropic uncertainty relations. More generally, they can be expressed in terms of any measure of uncertainty that can be written as a function of the expectation value of the observable for a given state."],["present a visualization framework for annotating and comparing colonoscopy videos.","Visualization Framework for Colonoscopy Videos","summarize: We present a visualization framework for annotating and comparing colonoscopy videos, where these annotations can then be used for semi-automatic report generation at the end of the procedure. Currently, there are approximately 14 million colonoscopies performed every year in the US. In this work, we create a visualization tool to deal with the deluge of colonoscopy videos in a more effective way. We present an interactive visualization framework for the annotation and tagging of colonoscopy videos in an easy and intuitive way. These annotations and tags can later be used for report generation for electronic medical records and for comparison at an individual as well as group level. We also present important use cases and medical expert feedback for our visualization framework."],["the web contains vast repositories of unstructured text. we generate a","Parser Extraction of Triples in Unstructured Text","summarize: The web contains vast repositories of unstructured text. We investigate the opportunity for building a knowledge graph from these text sources. We generate a set of triples which can be used in knowledge gathering and integration. We define the architecture of a language compiler for processing subject-predicate-object triples using the OpenNLP parser. We implement a depth-first search traversal on the POS tagged syntactic tree appending predicate and object information. A parser enables higher precision and higher recall extractions of syntactic relationships across conjunction boundaries. We are able to extract 2-2.5 times the correct extractions of ReVerb. The extractions are used in a variety of semantic web applications and question answering. We verify extraction of 50,000 triples on the ClueWeb dataset."],["graph jobs are a complex structure represented by graphs. the nodes denote the","Power-Aware Allocation of Graph Jobs in Geo-Distributed Cloud Networks","summarize: In the era of big-data, the jobs submitted to the clouds exhibit complicated structures represented by graphs, where the nodes denote the sub-tasks each of which can be accommodated at a slot in a server, while the edges indicate the communication constraints among the sub-tasks. We develop a framework for efficient allocation of graph jobs in geo-distributed cloud networks , explicitly considering the power consumption of the datacenters . We address the following two challenges arising in graph job allocation: i) the allocation problem belongs to NP-hard nonlinear integer programming; ii) the allocation requires solving the NP-complete sub-graph isomorphism problem, which is particularly cumbersome in large-scale GDCNs. We develop a suite of efficient solutions for GDCNs of various scales. For small-scale GDCNs, we propose an analytical approach based on convex programming. For medium-scale GDCNs, we develop a distributed allocation algorithm exploiting the processing power of DCs in parallel. Afterward, we provide a novel low-complexity sub-graph extraction method, based on which we introduce cloud crawlers aiming to extract allocations of good potentials for large-scale GDCNs. Given these suggested strategies, we further investigate strategy selection under both fixed and adaptive DC pricing schemes, and propose an online learning algorithm for each."],["a novel approach is presented to globally model an OF combined with locally adaptive methods. this","Perfect Fingerprint Orientation Fields by Locally Adaptive Global Models","summarize: Fingerprint recognition is widely used for verification and identification in many commercial, governmental and forensic applications. The orientation field plays an important role at various processing stages in fingerprint recognition systems. OFs are used for image enhancement, fingerprint alignment, for fingerprint liveness detection, fingerprint alteration detection and fingerprint matching. In this paper, a novel approach is presented to globally model an OF combined with locally adaptive methods. We show that this model adapts perfectly to the 'true OF' in the limit. This perfect OF is described by a small number of parameters with straightforward geometric interpretation. Applications are manifold: Quick expert marking of very poor quality OFs, high fidelity low parameter OF compression and a direct road to ground truth OFs markings for large databases, say. In this contribution we describe an algorithm to perfectly estimate OF parameters automatically or semi-automatically, depending on image quality, and we establish the main underlying claim of high fidelity low parameter OF compression."],["implicit-explicit Peer methods are well-balanced and asympt","Well-Balanced and Asymptotic Preserving IMEX-Peer Methods","summarize: Peer methods are a comprehensive class of time integrators offering numerous degrees of freedom in their coefficient matrices that can be used to ensure advantageous properties, e.g. A-stability or super-convergence. In this paper, we show that implicit-explicit Peer methods are well-balanced and asymptotic preserving by construction without additional constraints on the coefficients. These properties are relevant when solving hyperbolic systems of balance laws, for example. Numerical examples confirm the theoretical results and illustrate the potential of IMEX-Peer methods."],["turbulence is unlikely to occur in isothermal constant density quasi-Keple","Boundary-layer turbulence in experiments of quasi-Keplerian flows","summarize: Most flows in nature and engineering are turbulent because of their large velocities and spatial scales. Laboratory experiments of rotating quasi-Keplerian flows, for which the angular velocity decreases radially but the angular momentum increases, are however laminar at Reynolds numbers exceeding one million. This is in apparent contradiction to direct numerical simulations showing that in these experiments turbulence transition is triggered by the axial boundaries. We here show numerically that as the Reynolds number increases turbulence becomes progressively confined to the boundary layers and the flow in the bulk fully relaminarizes. Our findings support that turbulence is unlikely to occur in isothermal constant density quasi-Keplerian flows."],["we explicitly construct modular forms on a modular basis.","Inverse period mappings of ","summarize: We explicitly construct modular forms on a "],["financial intelligence is the core technology of the AI 2.0 era. it has elicit","FinBrain: When Finance Meets AI 2.0","summarize: Artificial intelligence is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a financial brain. In this work, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field."],["neural networks have led to state-of-the-art results in many medical imaging tasks including","Layer-Wise Relevance Propagation for Explaining Deep Neural Network Decisions in MRI-Based Alzheimer's Disease Classification","summarize: Deep neural networks have led to state-of-the-art results in many medical imaging tasks including Alzheimer's disease detection based on structural magnetic resonance imaging data. However, the network decisions are often perceived as being highly non-transparent, making it difficult to apply these algorithms in clinical routine. In this study, we propose using layer-wise relevance propagation to visualize convolutional neural network decisions for AD based on MRI data. Similarly to other visualization methods, LRP produces a heatmap in the input space indicating the importance\/relevance of each voxel contributing to the final classification outcome. In contrast to susceptibility maps produced by guided backpropagation , the LRP method is able to directly highlight positive contributions to the network classification in the input space. In particular, we show that the LRP method is very specific for individuals with high inter-patient variability, there is very little relevance for AD in healthy controls and areas that exhibit a lot of relevance correlate well with what is known from literature. To quantify the latter, we compute size-corrected metrics of the summed relevance per brain area, e.g., relevance density or relevance gain. Although these metrics produce very individual fingerprints of relevance patterns for AD patients, a lot of importance is put on areas in the temporal lobe including the hippocampus. After discussing several limitations such as sensitivity toward the underlying model and computation parameters, we conclude that LRP might have a high potential to assist clinicians in explaining neural network decisions for diagnosing AD based on structural MRI data."],["Snorkel DryBell builds on the Snorkel framework. it includes flexible,","Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale","summarize: Labeling training data is one of the most costly bottlenecks in developing machine learning-based applications. We present a first-of-its-kind study showing how existing knowledge resources from across an organization can be used as weak supervision in order to bring development time and cost down by an order of magnitude, and introduce Snorkel DryBell, a new weak supervision management system for this setting. Snorkel DryBell builds on the Snorkel framework, extending it in three critical aspects: flexible, template-based ingestion of diverse organizational knowledge, cross-feature production serving, and scalable, sampling-free execution. On three classification tasks at Google, we find that Snorkel DryBell creates classifiers of comparable quality to ones trained with tens of thousands of hand-labeled examples, converts non-servable organizational resources to servable models for an average 52% performance improvement, and executes over millions of data points in tens of minutes."],["wireless sensor networks are designed to monitor environmental parameters. they then transfer the required information to the","Lightweight Security Protocol for WiSense based Wireless Sensor Network","summarize: Wireless Sensor Networks have emerged as one of the leading technologies. These networks are designed to monitor crucial environmental parameters of humidity, temperature, wind speed, soil moisture content, UV index, sound, etc. and then transfer the required information to the base station. However, security remains the key challenge of such networks as critical data is being transferred. Most sensor nodes currently deployed have constraints on memory and processing power and hence operate without an efficient security protocol. Hereby a protocol which is lightweight and is secure for wireless sensor applications is proposed."],["a human-human interaction is a complex and complex process. a human-","A Review on Learning Planning Action Models for Socio-Communicative HRI","summarize: For social robots to be brought more into widespread use in the fields of companionship, care taking and domestic help, they must be capable of demonstrating social intelligence. In order to be acceptable, they must exhibit socio-communicative skills. Classic approaches to program HRI from observed human-human interactions fails to capture the subtlety of multimodal interactions as well as the key structural differences between robots and humans. The former arises due to a difficulty in quantifying and coding multimodal behaviours, while the latter due to a difference of the degrees of liberty between a robot and a human. However, the notion of reverse engineering from multimodal HRI traces to learn the underlying behavioral blueprint of the robot given multimodal traces seems an option worth exploring. With this spirit, the entire HRI can be seen as a sequence of exchanges of speech acts between the robot and human, each act treated as an action, bearing in mind that the entire sequence is goal-driven. Thus, this entire interaction can be treated as a sequence of actions propelling the interaction from its initial to goal state, also known as a plan in the domain of AI planning. In the same domain, this action sequence that stems from plan execution can be represented as a trace. AI techniques, such as machine learning, can be used to learn behavioral models , intended to be reusable for AI planning, from the aforementioned multimodal traces. This article reviews recent machine learning techniques for learning planning action models which can be applied to the field of HRI with the intent of rendering robots as socio-communicative."],["we define a simplicial complex called the non-kissing complex. we","Non-kissing and non-crossing complexes for locally gentle algebras","summarize: Starting from a locally gentle bound quiver, we define on the one hand a simplicial complex, called the non-kissing complex. On the other hand, we construct a punctured, marked, oriented surface with boundary, endowed with a pair of dual dissections. From those geometric data, we define two simplicial complexes: the accordion complex, and the slalom complex, generalizing work of A. Garver and T. McConville in the case of a disk. We show that all three simplicial complexes are isomorphic, and that they are pure and thin. In particular, there is a notion of mutation on their facets, akin to "],["the model is based on the idea of extending the fluid velocity inside the rigid body and","Rigid body motion in viscous flows using the Finite Element Method","summarize: A new model for the numerical simulation of a rigid body moving in a viscous fluid flow using FEM is presented. One of the most interesting features of this approach is the small computational effort required to solve the motion of the rigid body, comparable to a pure fluid solver. The model is based on the idea of extending the fluid velocity inside the rigid body and solving the flow equations with a penalization term to enforce rigid motion inside the solid. In order to get the velocity field in the fluid domain the Navier-Stokes equations for an incompressible viscous flow are solved using a fractional-step procedure combined with the two-step Taylor-Galerkin for the fractional linear momentum. Once the velocity field in the fluid domain is computed, calculation of the rigid motion is obtained by averaging translation and angular velocities over the solid. One of the main challenges when dealing with the fluid-solid interaction is the proper modelling of the interface which separates the solid moving mass from the viscous fluid. In this work the combination of the level set technique and the two-step Taylor-Galerkin algorithm for tracking the fluid-solid interface is proposed. The good properties exhibited by the two-step Taylor-Galerkin, minimizing oscillations and numerical diffusion, make this method suitable to accurately advect the solid domain avoiding distortions at its boundaries, and thus preserving the initial size and shape of the rigid body. The proposed model has been validated against empirical solutions, experimental data and numerical simulations found in the literature. In all tested cases, the numerical results have shown to be accurate, proving the potential of the proposed model as a valuable tool for the numerical analysis of the fluid-solid interaction."],["GW170104 was measured by the twin advanced detectors of the laser interferometer","GW170104: Observation of a 50-Solar-Mass Binary Black Hole Coalescence at Redshift 0.2","summarize: We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are "],["we establish sharp exponential deviation estimates of the information content. we also establish a sharp bound","Concentration of information content for convex measures","summarize: We establish sharp exponential deviation estimates of the information content as well as a sharp bound on the varentropy for the class of convex measures on Euclidean spaces. This generalizes a similar development for log-concave measures in the recent work of Fradelizi, Madiman and Wang . In particular, our results imply that convex measures in high dimensions are concentrated in an annulus between two convex sets despite their possibly having much heavier tails. Various tools and consequences are developed, including a sharp comparison result for R\\'enyi entropies, inequalities of Kahane-Khinchine type for convex measures that extend those of Koldobsky, Pajor and Yaskin for log-concave measures, and an extension of Berwald's inequality ."],["random walk on the alternating group is a random walk.","Total variation cutoff for the transpose top-","summarize: In this paper, we investigate the properties of a random walk on the alternating group "],["nonlinear optics is an increasingly important field for scientific and technological applications. there is","Grating-graphene metamaterial as a platform for terahertz nonlinear photonics","summarize: Nonlinear optics is an increasingly important field for scientific and technological applications, owing to its relevance and potential for optical and optoelectronic technologies. Currently, there is an active search for suitable nonlinear material systems with efficient conversion and small material footprint. Ideally, the material system should allow for chip-integration and room-temperature operation. Two-dimensional materials are highly interesting in this regard. Particularly promising is graphene, which has demonstrated an exceptionally large nonlinearity in the terahertz regime. Yet, the light-matter interaction length in two-dimensional materials is inherently minimal, thus limiting the overall nonlinear-optical conversion efficiency. Here we overcome this challenge using a metamaterial platform that combines graphene with a photonic grating structure providing field enhancement. We measure terahertz third-harmonic generation in this metamaterial and obtain an effective third-order nonlinear susceptibility with a magnitude as large as 3"],["Let us know what you think about it!","Unitary Subgroups of commutative group algebras of characteristic two","summarize: Let "],["inverse problem is usually solved through a regularized least-squares approach.","Automatic alignment for three-dimensional tomographic reconstruction","summarize: In tomographic reconstruction, the goal is to reconstruct an unknown object from a collection of line integrals. Given a complete sampling of such line integrals for various angles and directions, explicit inverse formulas exist to reconstruct the object. Given noisy and incomplete measurements, the inverse problem is typically solved through a regularized least-squares approach. A challenge for both approaches is that in practice the exact directions and offsets of the x-rays are only known approximately due to, e.g. calibration errors. Such errors lead to artifacts in the reconstructed image. In the case of sufficient sampling and geometrically simple misalignment, the measurements can be corrected by exploiting so-called consistency conditions. In other cases, such conditions may not apply and we have to solve an additional inverse problem to retrieve the angles and shifts. In this paper we propose a general algorithmic framework for retrieving these parameters in conjunction with an algebraic reconstruction technique. The proposed approach is illustrated by numerical examples for both simulated data and an electron tomography dataset."],["unsupervised anomaly detection approaches have been made in the medical domain. previously, deep spatial","Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images","summarize: Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning."],["cross-site scripting flaws are a class of security flaws that permit","DjangoChecker: Applying Extended Taint Tracking and Server Side Parsing for Detection of Context-Sensitive XSS Flaws","summarize: Cross-site scripting flaws are a class of security flaws that permit the injection of malicious code into a web application. In simple situations, these flaws can be caused by missing input sanitizations. Sometimes, however, all application inputs are sanitized, but the sanitizations are not appropriate for the browser contexts of the sanitized values. Using an incorrect sanitizer can make the application look protected, when it is in fact vulnerable as if no sanitization was used, creating a context-sensitive XSS flaw. To discover context-sensitive XSS flaws, we introduce DjangoChecker. DjangoChecker combines extended dynamic taint tracking with a model browser for context analysis. We demonstrate the practical application of DjangoChecker on eight mature web applications based on Django, discovering previously unknown flaws in seven of the eight applications, including highly severe flaws that allow arbitrary JavaScript execution in the seven flawed applications."],["a city park has been built from the organic settlement in the Cikapundung River,","Agent-Based Model for River-Side Land-living: Portrait of Bandung Indonesian Cikapundung Park Case Study","summarize: A city park has been built from the organic urban settlement in the Cikapundung River, Bandung, Indonesia. While the aim for the development is the revitalization of the river for being unhealthy from the waste coming from the settlement. A study on how Indonesian people, in general, treating water source, like river, lake, and ocean is revisited. Throwing waste into the river has actually become paradox with the collective mental understanding about water among Indonesians. Two scenarios of agent-based simulation is presented, to see the dynamics of organic settlement and life of the city park after being opened for public. The simulation is delivered upon the imagery of landscape taken from the satellite and drone. While experience for presented problems gives insights, the computational social laboratory also awaits for further theoretical explorations and endeavors to sharpen good policymaking."],["asymptotic rate code scheme is a zero-error coding scheme of as","A Note on Parallel Asynchronous Channels with Arbitrary Skews","summarize: A zero-error coding scheme of asymptotic rate "],["the course will examine the basics of magnetism and quantum mechanics. we will go","Quantum Magnetism, Spin Waves, and Light","summarize: Both magnetic materials and light have always played a predominant role in information technologies, and continue to do so as we move into the realm of quantum technologies. In this course we review the basics of magnetism and quantum mechanics, before going into more advanced subjects. Magnetism is intrinsically quantum mechanical in nature, and magnetic ordering can only be explained by use of quantum theory. We will go over the interactions and the resulting Hamiltonian that governs magnetic phenomena, and discuss its elementary excitations, denominated magnons. After that we will study magneto-optical effects and derive the classical Faraday effect. We will then move on to the quantization of the electric field and the basics of optical cavities. This will allow us to understand a topic of current research denominated Cavity Optomagnonics. These notes were written as the accompanying material to the course I taught in the Summer Semester 2018 at the Friedrich-Alexander University in Erlangen. The course is intended for Master or advanced Bachelor students. Basic knowledge of quantum mechanics, electromagnetism, and solid state at the Bachelor level is assumed. Each section is followed by a couple of simple exercises which should serve as to fill in the blanks of what has been derived, plus specific references to bibliography, and a couple of check-points for the main concepts developed. The figures are pictures of the blackboard taken during the lecture."],["PHANTOM extracts five measures from Git logs. each measure is transformed","PHANTOM: Curating GitHub for engineered software projects using time-series clustering","summarize: Context: Within the field of Mining Software Repositories, there are numerous methods employed to filter datasets in order to avoid analysing low-quality projects. Unfortunately, the existing filtering methods have not kept up with the growth of existing data sources, such as GitHub, and researchers often rely on quick and dirty techniques to curate datasets. Objective: The objective of this study is to develop a method capable of filtering large quantities of software projects in a resource-efficient way. Method: This study follows the Design Science Research methodology. The proposed method, PHANTOM, extracts five measures from Git logs. Each measure is transformed into a time-series, which is represented as a feature vector for clustering using the k-means algorithm. Results: Using the ground truth from a previous study, PHANTOM was shown to be able to rediscover the ground truth on the training dataset, and was able to identify engineered projects with up to 0.87 Precision and 0.94 Recall on the validation dataset. PHANTOM downloaded and processed the metadata of 1,786,601 GitHub repositories in 21.5 days using a single personal computer, which is over 33% faster than the previous study which used a computer cluster of 200 nodes. The possibility of applying the method outside of the open-source community was investigated by curating 100 repositories owned by two companies. Conclusions: It is possible to use an unsupervised approach to identify engineered projects. PHANTOM was shown to be competitive compared to the existing supervised approaches while reducing the hardware requirements by two orders of magnitude."],["recent studies have emphasized the important role that a shape deformability of scal","Kink-antikink scattering-induced breathing bound states and oscillons in a parametrized ","summarize: Recent studies have emphasized the important role that a shape deformability of scalar-field models pertaining to the same class with the standard "],["the study confirms that the approximate results established by Kabanov and Safarian are still valid","Approximate hedging with proportional transaction costs in stochastic volatility models with jumps","summarize: We study the problem of option replication under constant proportional transaction costs in models where stochastic volatility and jumps are combined to capture the market's important features. Assuming some mild condition on the jump size distribution we show that transaction costs can be approximately compensated by applying the Leland adjusting volatility principle and the asymptotic property of the hedging error due to discrete readjustments is characterized. In particular, the jump risk can be approximately eliminated and the results established in continuous diffusion models are recovered. The study also confirms that for the case of constant trading cost rate, the approximate results established by Kabanov and Safarian and by Pergamenschikov are still valid in jump-diffusion models with deterministic volatility using the classical Leland parameter in Leland ."],["a scalable method can automatically generate diverse audio for image captioning datasets. this","Large-scale representation learning from visually grounded untranscribed speech","summarize: Systems that can associate images with their spoken audio captions are an important step towards visually grounded language learning. We describe a scalable method to automatically generate diverse audio for image captioning datasets. This supports pretraining deep networks for encoding both audio and images, which we do via a dual encoder that learns to align latent representations from both modalities. We show that a masked margin softmax loss for such models is superior to the standard triplet loss. We fine-tune these models on the Flickr8k Audio Captions Corpus and obtain state-of-the-art results---improving recall in the top 10 from 29.6% to 49.5%. We also obtain human ratings on retrieval outputs to better assess the impact of incidentally matching image-caption pairs that were not associated in the data, finding that automatic evaluation substantially underestimates the quality of the retrieved results."],["we quantify the redistribution of energy in the drop and the surrounding fluid during the impact","Local velocity variations for a drop moving through an orifice: effects of edge geometry and surface wettability","summarize: We investigate velocity variations inside of and surrounding a gravity driven drop impacting on and moving through a confining orifice, wherein the effects of edge geometry and surface wettability of the orifice are considered. Using refractive index matching and time-resolved PIV, we quantify the redistribution of energy in the drop and the surrounding fluid during the drop's impact and motion through a round-edged orifice. The measurements show the importance of a) drop kinetic energy transferred to and dissipated within the surrounding liquid, and b) the drop kinetic energy due to internal deformation and rotation during impact and passage through the orifice. While a rounded orifice edge prevents contact between the drop and orifice surface, a sharp edge promotes contact immediately upon impact, changing the near surface flow field as well as the drop passage dynamics. For a sharp-edged hydrophobic orifice, the contact lines remain localized near the orifice edge, but slipping and pinning strongly affect the drop propagation and outcome. For a sharp-edged hydrophilic orifice, on the other hand, the contact lines propagate away from the orifice edge, and their motion is coupled with the global velocity fields in the drop and the surrounding fluid. By examining the contact line propagation over a hydrophilic orifice surface with minimal drop penetration, we characterize two stages of drop spreading that exhibit power-law dependence with variable exponent. In the first stage, the contact line propagates under the influence of impact inertia and gravity. In the second stage, inertial influence subsides, and the contact line propagates mainly due to wettability."],["deep neural networks have been quite successful in solving complex learning problems. but the complex learning parameters","Compressed Learning of Deep Neural Networks for OpenCL-Capable Embedded Systems","summarize: Deep neural networks have been quite successful in solving many complex learning problems. However, DNNs tend to have a large number of learning parameters, leading to a large memory and computation requirement. In this paper, we propose a model compression framework for efficient training and inference of deep neural networks on embedded systems. Our framework provides data structures and kernels for OpenCL-based parallel forward and backward computation in a compressed form. In particular, our method learns sparse representations of parameters using "],["the BC score of a vertex is proportional to the number of all-pair","Algorithms and Heuristics for Scalable Betweenness Centrality Computation on Multi-GPU Systems","summarize: Betweenness Centrality is steadily growing in popularity as a metrics of the influence of a vertex in a graph. The BC score of a vertex is proportional to the number of all-pairs-shortest-paths passing through it. However, complete and exact BC computation for a large-scale graph is an extraordinary challenge that requires high performance computing techniques to provide results in a reasonable amount of time. Our approach combines bi-dimensional decomposition of the graph and multi-level parallelism together with a suitable data-thread mapping that overcomes most of the difficulties caused by the irregularity of the computation on GPUs. Furthermore, we propose novel heuristics which exploit the topology information of the graph in order to reduce time and space requirements of BC computation. Experimental results on synthetic and real-world graphs show that the proposed techniques allow the BC computation of graphs which are too large to fit in the memory of a single computational node along with a significant reduction of the computing time."],["we consider the problem of designing a stabilizing and optimal static controller. this problem is","On Separable Quadratic Lyapunov Functions for Convex Design of Distributed Controllers","summarize: We consider the problem of designing a stabilizing and optimal static controller with a pre-specified sparsity pattern. Since this problem is NP-hard in general, it is necessary to resort to approximation approaches. In this paper, we characterize a class of convex restrictions of this problem that are based on designing a separable quadratic Lyapunov function for the closed-loop system. This approach generalizes previous results based on optimizing over diagonal Lyapunov functions, thus allowing for improved feasibility and performance. Moreover, we suggest a simple procedure to compute favourable structures for the Lyapunov function yielding high-performance distributed controllers. Numerical examples validate our results."],["a meta-algorithm is guaranteed to recover unknown matrices with near","Learning Mixtures of Low-Rank Models","summarize: We study the problem of learning mixtures of low-rank models, i.e. reconstructing multiple low-rank matrices from unlabelled linear measurements of each. This problem enriches two widely studied settings -- low-rank matrix sensing and mixed linear regression -- by bringing latent variables and structural priors into consideration. To cope with the non-convexity issues arising from unlabelled heterogeneous data and low-complexity structure, we develop a three-stage meta-algorithm that is guaranteed to recover the unknown matrices with near-optimal sample and computational complexities under Gaussian designs. In addition, the proposed algorithm is provably stable against random noise. We complement the theoretical studies with empirical evidence that confirms the efficacy of our algorithm."],["circumbinary accretion discs are a problem in restricted three-body problem","A transition in circumbinary accretion discs at a binary mass ratio of 1:25","summarize: We study circumbinary accretion discs in the framework of the restricted three-body problem and via numerically solving the height-integrated equations of viscous hydrodynamics. Varying the mass ratio of the binary, we find a pronounced change in the behaviour of the disc near mass ratio "],["scalar field and cosmological constant behaviour analysed in this article.","Scalar field and time varying Cosmological constant in ","summarize: In this article, we have analysed the behaviour of scalar field and cosmological constant in "],["the dark asteroid Bennu studied by NASAtextquoteright s OS","Modeling optical roughness and first-order scattering processes from OSIRIS-REx color images of the rough surface of asteroid Bennu","summarize: The dark asteroid Bennu studied by NASA\\textquoteright s OSIRIS-REx mission has a boulder-rich and apparently dust-poor surface, providing a natural laboratory to investigate the role of single-scattering processes in rough particulate media. Our goal is to define optical roughness and other scattering parameters that may be useful for the laboratory preparation of sample analogs, interpretation of imaging data, and analysis of the sample that will be returned to Earth. We rely on a semi-numerical statistical model aided by digital terrain model shadow ray-tracing to obtain scattering parameters at the smallest surface element allowed by the DTM . Using a Markov Chain Monte Carlo technique, we solved the inversion problem on all four-band images of the OSIRIS-REx mission\\textquoteright s top four candidate sample sites, for which high-precision laser altimetry DTMs are available. We reconstructed the \\emph probability distribution for each parameter and distinguished primary and secondary solutions. Through the photometric image correction, we found that a mixing of low and average roughness slope best describes Bennu's surface for up to "],["authors review several refinements of Young's integral inequality. authors review several refinements of","Refinements of Young's integral inequality via fundamental inequalities and mean value theorems for derivatives","summarize: In the paper, the authors review several refinements of Young's integral inequality via several mean value theorems, such as Lagrange's and Taylor's mean value theorems of Lagrange's and Cauchy's type remainders, and via several fundamental inequalities, such as \\veby\\vev's integral inequality, Hermite--Hadamard's type integral inequalities, H\\older's integral inequality, and Jensen's discrete and integral inequalities, in terms of higher order derivatives and their norms, survey several applications of several refinements of Young's integral inequality, and further refine Young's integral inequality via P\\'olya's type integral inequalities."],["ML-based solutions address the efficient computing requirements of big data. but they introduce security","Security for Machine Learning-based Systems: Attacks and Challenges during Training and Inference","summarize: The exponential increase in dependencies between the cyber and physical world leads to an enormous amount of data which must be efficiently processed and stored. Therefore, computing paradigms are evolving towards machine learning -based systems because of their ability to efficiently and accurately process the enormous amount of data. Although ML-based solutions address the efficient computing requirements of big data, they introduce security vulnerabilities into the systems, which cannot be addressed by traditional monitoring-based security measures. Therefore, this paper first presents a brief overview of various security threats in machine learning, their respective threat models and associated research challenges to develop robust security measures. To illustrate the security vulnerabilities of ML during training, inferencing and hardware implementation, we demonstrate some key security threats on ML using LeNet and VGGNet for MNIST and German Traffic Sign Recognition Benchmarks , respectively. Moreover, based on the security analysis of ML-training, we also propose an attack that has a very less impact on the inference accuracy. Towards the end, we highlight the associated research challenges in developing security measures and provide a brief overview of the techniques used to mitigate such security threats."],["the socket is critical in a prosthetic system for a person with limb a","Multi-Material 3-D Viscoelastic Model of a Transtibial Residuum from In-vivo Indentation and MRI Data","summarize: Although the socket is critical in a prosthetic system for a person with limb amputation, the methods of its design are largely artisanal. A roadblock for a repeatable and quantitative socket design process is the lack of predictive and patient specific biomechanical models of the residuum. This study presents the evaluation of such a model using a combined experimental-numerical approach. The model geometry and tissue boundaries are derived from MRI. The soft tissue non-linear elastic and viscoelastic mechanical behavior was evaluated using inverse finite element analysis of in-vivo indentation experiments. A custom designed robotic in-vivo indentation system was used to provide a rich experimental data set of force versus time at 18 sites across a limb. During FEA, the tissues were represented by two layers, namely the skin-adipose layer and an underlying muscle-soft tissue complex. The non-linear elastic behavior was modeled using 2nd order Ogden hyperelastic formulations, and viscoelasticity was modeled using the quasi-linear theory of viscoelasticity. To determine the material parameters for each tissue, an inverse FEA based optimization routine was used that minimizes the combined mean of the squared force differences between the numerical and experimental force-time curves for indentations at 4 distinct anatomical regions on the residuum. The optimization provided the following material parameters for the skin-adipose layer: c=5.22 kPa, m=4.79, gamma=3.57 MPa, tau=0.32 s and for the muscle-soft tissue complex: c=5.20 kPa, m=4.78, gamma=3.47 MPa, tau=0.34 s. These parameters were evaluated to predict the force-time curves for the remaining 14 anatomical locations. The mean percentage error for these predictions was 7 +\/- 3%. The mean percentage error at the 4 sites used for the optimization was 4%."],["Graph-based methods are known to be successful in many machine learning tasks. a","Stochastic Graphlet Embedding","summarize: Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit\/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts\/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases."],["the paper considers the effect of least squares procedures for nearly unstable linear time series with strongly","Asymptotic theory of least squares estimators for nearly unstable processes under strong dependence","summarize: This paper considers the effect of least squares procedures for nearly unstable linear time series with strongly dependent innovations. Under a general framework and appropriate scaling, it is shown that ordinary least squares procedures converge to functionals of fractional Ornstein--Uhlenbeck processes. We use fractional integrated noise as an example to illustrate the important ideas. In this case, the functionals bear only formal analogy to those in the classical framework with uncorrelated innovations, with Wiener processes being replaced by fractional Brownian motions. It is also shown that limit theorems for the functionals involve nonstandard scaling and nonstandard limiting distributions. Results of this paper shed light on the asymptotic behavior of nearly unstable long-memory processes."],["modal transmission-line properties such as per-unit-length impedance, admitt","On the Unique Determination of Modal Multiconductor Transmission-Line Properties","summarize: Some modal transmission-line properties such as per-unit-length impedance, admittance, or characteristic impedance have long been held to be, in general, non-unique. This ambiguity arises from the nature of the similarity transformations used to relate the terminal and modal domains, for which the voltage transformation matrix has been shown to be only loosely related to the corresponding current transformation matrix. Modern methods have attempted to relate the two, but these relations typically rely on arbitrary normalizations, leading to strictly incorrect and\/or non-unique results. This work introduces relations between the two transformations, derived from the physical equivalence of total power and currents between the two domains, by which the transformation matrices can be unambiguously related to each other, and the modal properties uniquely solved. This technique allows for the correct extraction of the modal transmission-line properties for any arbitrary system of conductors. Multiple examples are studied to validate the proposed solution process."],["a scheme for wiretap channel with generalized feedback is presented. the scheme uses feedback","The Wiretap Channel with Generalized Feedback: Secure Communication and Key Generation","summarize: It is a well-known fact that feedback does not increase the capacity of point-to-point memoryless channels, however, its effect in secure communications is not fully understood yet. In this work, an achievable scheme for the wiretap channel with generalized feedback is presented. This scheme, which uses the feedback signal to generate a shared secret key between the legitimate users, encrypts the message to be sent at the bit level. New capacity results for a class of channels are provided, as well as some new insights into the secret key agreement problem. Moreover, this scheme recovers previously reported rate regions from the literature, and thus it can be seen as a generalization that unifies several results in the field."],["the weak order on the lattice is a weak order. the weak order","Quotientopes","summarize: For any lattice congruence of the weak order on "],["in the present contribution, we introduce a wireless optical communication-based system architecture which is shown","Optical Wireless Cochlear Implants","summarize: In the present contribution, we introduce a wireless optical communication-based system architecture which is shown to significantly improve the reliability and the spectral and power efficiency of the transcutaneous link in cochlear implants . We refer to the proposed system as optical wireless cochlear implant .In order to provide a quantified understanding of its design parameters, we establish a theoretical framework that takes into account the channel particularities, the integration area of the internal unit, the transceivers misalignment, and the characteristics of the optical units. To this end, we derive explicit expressions for the corresponding average signal-to-noise-ratio, outage probability, ergodic spectral efficiency and capacity of the transcutaneous optical link . These expressions are subsequently used to assess the dependence of the TOL's communication quality on the transceivers design parameters and the corresponding channels characteristics. The offered analytic results are corroborated with respective results from Monte Carlo simulations. Our findings reveal that OWCI is a particularly promising architecture that drastically increases the reliability and effectiveness of the CI TOL, whilst it requires considerably lower transmit power compared to the corresponding widely-used radio frequency solution."],["this paper presents a game based on storytelling. players' choices have consequences on how","A Serious Game for Introducing Software Engineering Ethics to University Students","summarize: This paper presents a game based on storytelling, in which the players are faced with ethical dilemmas related to software engineering specific issues. The players' choices have consequences on how the story unfolds and could lead to various alternative endings. This Ethics Game was used as a tool to mediate the learning activity and it was evaluated by 144 students during a Software Engineering Course on the 2017-2018 academic year. This evaluation was based on a within-subject pre-post design methodology and provided insights on the students learning gain , as well as on the students' perceived educational experience. In addition, it provided the results of the students' usability evaluation of the Ethics Game. The results indicated that the students did improve their knowledge about software engineering ethics by playing this game. Also, they considered this game to be a useful educational tool and of high usability. Female students had statistically significant higher knowledge gain and higher evaluation scores than male students, while no statistically significant differences were measured in groups based on the year of study."],["a PAS stores traffic history in order to detect a source and destination pair.","An Effective Payload Attribution Scheme for Cybercriminal Detection Using Compressed Bitmap Index Tables and Traffic Downsampling","summarize: Payload attribution systems are one of the most important tools of network forensics for detecting an offender after the occurrence of a cybercrime. A PAS stores the network traffic history in order to detect the source and destination pair of a certain data stream in case a malicious activity occurs on the network. The huge volume of information that is daily transferred in the network means that the data stored by a PAS must be as compact and concise as possible. Moreover, the investigation of this large volume of data for a malicious data stream must be handled within a reasonable time. For this purpose, several techniques based on storing a digest of traffic using Bloom filters have been proposed in the literature. The false positive rate of existing techniques for detecting cybercriminals is unacceptably high, i.e., many source and destination pairs are falsely determined as malicious, making it difficult to detect the true criminal. In order to ameliorate this problem, we have proposed a solution based on compressed bitmap index tables and traffic downsampling. Our analytical evaluation and experimental results show that the proposed method significantly reduces the false positive rate."],["the data was provided by the state-of the art digital ionosonde of","Midlatitude ionospheric F2-layer response to eruptive solar events-caused geomagnetic disturbances over Hungary during the maximum of the solar cycle 24: a case study","summarize: In our study we analyze and compare the response and behavior of the ionospheric F2 and of the sporadic E-layer during three strong individual geomagnetic storms from years 2012, 2013 and 2015, winter time period. The data was provided by the state-of the art digital ionosonde of the Sz\\'echenyi Istv\\'an Geophysical Observatory located at midlatitude, Nagycenk, Hungary . The local time of the sudden commencement was used to characterize the type of the ionospheric storm . This way two regular positive phase ionospheric storms and one no-positive phase storm have been analyzed. In all three cases a significant increase in electron density of the foF2 layer can be observed at dawn\/early morning . Also we can observe the fade-out of the ionospheric layers at night during the geomagnetically disturbed time periods. Our results suggest that the fade-out effect is not connected to the occurrence of the sporadic E-layers."],["IETF recently developed Object Security for Constrained RESTful Environments. protocol","IoT Content Object Security with OSCORE and NDN: A First Experimental Comparison","summarize: The emerging Internet of Things challenges the end-to-end transport of the Internet by low power lossy links and gateways that perform protocol translations. Protocols such as CoAP or MQTT-SN are degraded by the overhead of DTLS sessions, which in common deployment protect content transfer only up to the gateway. To preserve content security end-to-end via gateways and proxies, the IETF recently developed Object Security for Constrained RESTful Environments , which extends CoAP with content object security features commonly known from Information Centric Networks . This paper presents a comparative analysis of protocol stacks that protect request-response transactions. We measure protocol performances of CoAP over DTLS, OSCORE, and the information-centric Named Data Networking protocol on a large-scale IoT testbed in single- and multi-hop scenarios. Our findings indicate that OSCORE improves on CoAP over DTLS in error-prone wireless regimes due to omitting the overhead of maintaining security sessions at endpoints, and NDN attains superior robustness and reliability due to its intrinsic network caches and hop-wise retransmissions."],["a mixture model with online knowledge distillation can achieve better evaluation performance. the proposed distill","MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization","summarize: In this paper, we present and discuss a deep mixture model with online knowledge distillation for large-scale video temporal concept localization, which is ranked 3rd in the 3rd YouTube-8M Video Understanding Challenge. Specifically, we find that by enabling knowledge sharing with online distillation, fintuning a mixture model on a smaller dataset can achieve better evaluation performance. Based on this observation, in our final solution, we trained and fintuned 12 NeXtVLAD models in parallel with a 2-layer online distillation structure. The experimental results show that the proposed distillation structure can effectively avoid overfitting and shows superior generalization performance. The code is publicly available at: https:\/\/github.com\/linrongc\/solution_youtube8m_v3"],["this paper we continue the investigation of the regularity of the so-called weak.","On regularity theory for n\/p-harmonic maps into manifolds","summarize: In this paper we continue the investigation of the regularity of the so-called weak "],["a robust transmit\/receive adaptive beamforming for multiple-input multipleoutput","Joint Robust Transmit\/Receive Adaptive Beamforming for MIMO Radar Using Probability-Constrained Optimization","summarize: A joint robust transmit\/receive adaptive beamforming for multiple-input multipleoutput radar based on probability-constrained optimization approach is developed in the case of Gaussian and arbitrary distributed mismatch present in both the transmit and receive signal steering vectors. A tight lower bound of the probability constraint is also derived by using duality theory. The formulated probability-constrained robust beamforming problem is nonconvex and NP-hard. However, we reformulate its cost function into a bi-quadratic function while the probability constraint splits into transmit and receive parts. Then, a block coordinate descent method based on second-order cone programming is developed to address the biconvex problem. Simulation results show an improved robustness of the proposed beamforming method as compared to the worst-case and other existing state-of-the-art joint transmit\/receive robust adaptive beamforming methods for MIMO radar."],["study aims to understand current state of the art of existing platforms. analyzed 16 platforms","Low-code Engineering for Internet of things: A state of research","summarize: Developing Internet of Things systems has to cope with several challenges mainly because of the heterogeneity of the involved sub-systems and components. With the aim of conceiving languages and tools supporting the development of IoT systems, this paper presents the results of the study, which has been conducted to understand the current state of the art of existing platforms, and in particular low-code ones, for developing IoT systems. By analyzing sixteen platforms, a corresponding set of features has been identified to represent the functionalities and the services that each analyzed platform can support. We also identify the limitations of already existing approaches and discuss possible ways to improve and address them in the future."],["a histogram based reversible data hiding scheme is used to sort images","Sorting Methods and Adaptive Thresholding for Histogram Based Reversible Data Hiding","summarize: This paper presents a histogram based reversible data hiding scheme, which divides image pixels into different cell frequency bands to sort them for data embedding. Data hiding is more efficient in lower cell frequency bands because it provides more accurate prediction. Using pixel existence probability in some pixels of ultra-low cell frequency band, another sorting is performed. Employing these two novel sorting methods in combination with the hiding intensity analysis that determines optimum prediction error, we improve the quality of the marked image especially for low embedding capacities. In effect, comparing to existent RDH algorithms, the hiding capacity is increased for a specific level of the distortion for the marked image. Experimental results confirm that the proposed algorithm outperforms state of the art ones."],["causal Bayesian networks provide us with a powerful tool to measure unfairness in a","A Causal Bayesian Networks Viewpoint on Fairness","summarize: We offer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios."],["we characterize both short-term and long-term changes of the solar spectral","Solar Spectral Irradiance Variability in Cycle 24: Observations and Models","summarize: Utilizing the excellent stability of the Ozone Monitoring Instrument , we characterize both short-term and long-term changes of the solar spectral irradiance between 265-500 nm during the on-going Cycle 24. We supplement the OMI data with concurrent observations from the GOME-2 and SORCE instruments and find fair-to-excellent, depending on wavelength, agreement among the observations and predictions of the NRLSSI2 and SATIRE-S models."],["dynamic statistical model learns and adapts processing and parsing rules. we use this","Flexible Log File Parsing using Hidden Markov Models","summarize: We aim to model unknown file processing. As the content of log files often evolves over time, we established a dynamic statistical model which learns and adapts processing and parsing rules. First, we limit the amount of unstructured text by focusing only on those frequent patterns which lead to the desired output table similar to Vaarandi . Second, we transform the found frequent patterns and the output stating the parsed table into a Hidden Markov Model . We use this HMM as a specific, however, flexible representation of a pattern for log file processing. With changes in the raw log file distorting learned patterns, we aim the model to adapt automatically in order to maintain high quality output. After training our model on one system type, applying the model and the resulting parsing rule to a different system with slightly different log file patterns, we achieve an accuracy over 99%."],["studies have demonstrated cross-lingual alignment ability of multilingual pretrained language models. a","Cross-lingual Retrieval for Iterative Self-Supervised Training","summarize: Recent studies have demonstrated the cross-lingual alignment ability of multilingual pretrained language models. In this work, we found that the cross-lingual alignment can be further improved by training seq2seq models on sentence pairs mined using their own encoder outputs. We utilized these findings to develop a new approach -- cross-lingual retrieval for iterative self-supervised training , where mining and training processes are applied iteratively, improving cross-lingual alignment and translation ability at the same time. Using this method, we achieved state-of-the-art unsupervised machine translation results on 9 language directions with an average improvement of 2.4 BLEU, and on the Tatoeba sentence retrieval task in the XTREME benchmark on 16 languages with an average improvement of 21.5% in absolute accuracy. Furthermore, CRISS also brings an additional 1.8 BLEU improvement on average compared to mBART, when finetuned on supervised machine translation downstream tasks."],["we study the generalization properties of minimum-norm solutions for three over-parametri","The Generalization Error of the Minimum-norm Solutions for Over-parameterized Neural Networks","summarize: We study the generalization properties of minimum-norm solutions for three over-parametrized machine learning models including the random feature model, the two-layer neural network model and the residual network model. We proved that for all three models, the generalization error for the minimum-norm solution is comparable to the Monte Carlo rate, up to some logarithmic terms, as long as the models are sufficiently over-parametrized."],["the target network is for smoothly generating the reference signals for a main network in DRL","t-Soft Update of Target Network for Deep Reinforcement Learning","summarize: This paper proposes a new robust update rule of target network for deep reinforcement learning , to replace the conventional update rule, given as an exponential moving average. The target network is for smoothly generating the reference signals for a main network in DRL, thereby reducing learning variance. The problem with its conventional update rule is the fact that all the parameters are smoothly copied with the same speed from the main network, even when some of them are trying to update toward the wrong directions. This behavior increases the risk of generating the wrong reference signals. Although slowing down the overall update speed is a naive way to mitigate wrong updates, it would decrease learning speed. To robustly update the parameters while keeping learning speed, a t-soft update method, which is inspired by student-t distribution, is derived with reference to the analogy between the exponential moving average and the normal distribution. Through the analysis of the derived t-soft update, we show that it takes over the properties of the student-t distribution. Specifically, with a heavy-tailed property of the student-t distribution, the t-soft update automatically excludes extreme updates that differ from past experiences. In addition, when the updates are similar to the past experiences, it can mitigate the learning delay by increasing the amount of updates. In PyBullet robotics simulations for DRL, an online actor-critic algorithm with the t-soft update outperformed the conventional methods in terms of the obtained return and\/or its variance. From the training process by the t-soft update, we found that the t-soft update is globally consistent with the standard soft update, and the update rates are locally adjusted for acceleration or suppression."],["the recent outbreak of COVID-19 pandemic has exposed and highlighted the limitations of current","Future Smart Connected Communities to Fight COVID-19 Outbreak","summarize: Internet of Things has grown rapidly in the last decade and continue to develop in terms of dimension and complexity offering wide range of devices to support diverse set of applications. With ubiquitous Internet, connected sensors and actuators, networking and communication technology, and artificial intelligence , smart cyber-physical systems provide services rendering assistance to humans in their daily lives. However, the recent outbreak of COVID-19 pandemic has exposed and highlighted the limitations of current technological deployments to curtail this disease. IoT and smart connected technologies together with data-driven applications can play a crucial role not only in prevention, continuous monitoring, and mitigation of the disease, but also enable prompt enforcement of guidelines, rules and government orders to contain such future outbreaks. In this paper, we envision an IoT-enabled ecosystem for intelligent monitoring, pro-active prevention and control, and mitigation of COVID-19. We propose different architectures, applications and technology systems for various smart infrastructures including E-health, smart home, smart supply chain management, smart locality, and smart city, to develop future connected communities to manage and mitigate similar outbreaks. Furthermore, we present research challenges together with future directions to enable and develop these smart communities and infrastructures to fight and prepare against such outbreaks."],["hyperbolic tangent and Sigmoid functions are used as non-linear","A Novel Method for Scalable VLSI Implementation of Hyperbolic Tangent Function","summarize: Hyperbolic tangent and Sigmoid functions are used as non-linear activation units in the artificial and deep neural networks. Since, these networks are computationally expensive, customized accelerators are designed for achieving the required performance at lower cost and power. The activation function and MAC units are the key building blocks of these neural networks. A low complexity and accurate hardware implementation of the activation function is required to meet the performance and area targets of such neural network accelerators. Moreover, a scalable implementation is required as the recent studies show that the DNNs may use different precision in different layers. This paper presents a novel method based on trigonometric expansion properties of the hyperbolic function for hardware implementation which can be easily tuned for different accuracy and precision requirements."],["initialization of weights in backpropagation neural net is heavily affected by initialization of","A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition","summarize: Convergence rate of training algorithms for neural networks is heavily affected by initialization of weights. In this paper, an original algorithm for initialization of weights in backpropagation neural net is presented with application to character recognition. The initialization method is mainly based on a customization of the Kalman filter, translating it into Bayesian statistics terms. A metrological approach is used in this context considering weights as measurements modeled by mutually dependent normal random variables. The algorithm performance is demonstrated by reporting and discussing results of simulation trials. Results are compared with random weights initialization and other methods. The proposed method shows an improved convergence rate for the backpropagation training algorithm."],["we construct a classification of dispersion relations for the electromagnetic waves non-minimally","Non-minimal Einstein-Maxwell theory: the Fresnel equation and the Petrov classification of a trace-free susceptibility tensor","summarize: We construct a classification of dispersion relations for the electromagnetic waves non-minimally coupled to the space-time curvature, based on the analysis of the susceptibility tensor, which appears in the non-minimal Einstein-Maxwell theory. We classify solutions to the Fresnel equation for the model with a trace-free non-minimal susceptibility tensor according to the Petrov scheme. For all Petrov types we discuss specific features of the dispersion relations and plot the corresponding wave surfaces."],["convolutional sparse representations are a form of sparse representation with","Convolutional Dictionary Learning: A Comparative Review and New Algorithms","summarize: Convolutional sparse representations are a form of sparse representation with a dictionary that has a structure that is equivalent to convolution with a set of linear filters. While effective algorithms have recently been developed for the convolutional sparse coding problem, the corresponding dictionary learning problem is substantially more challenging. Furthermore, although a number of different approaches have been proposed, the absence of thorough comparisons between them makes it difficult to determine which of them represents the current state of the art. The present work both addresses this deficiency and proposes some new approaches that outperform existing ones in certain contexts. A thorough set of performance comparisons indicates a very wide range of performance differences among the existing and proposed methods, and clearly identifies those that are the most effective."],["abstract error analysis framework for approximation of linear partial differential equation problems in weak formulation","A third Strang lemma and an Aubin-Nitsche trick for schemes in fully discrete formulation","summarize: In this work, we present an abstract error analysis framework for the approximation of linear partial differential equation problems in weak formulation. We consider approximation methods in fully discrete formulation, where the discrete and continuous spaces are possibly not embedded in a common space. A proper notion of consistency is designed, and, under a classical inf-sup condition, it is shown to bound the approximation error. This error estimate result is in the spirit of Strang's first and second lemmas, but applicable in situations not covered by these lemmas . An improved estimate is also established in a weaker norm, using the Aubin--Nitsche trick. We then apply these abstract estimates to an anisotropic heterogeneous diffusion model and two classical families of schemes for this model: Virtual Element and Finite Volume methods. For each of these methods, we show that the abstract results yield new error estimates with a precise and mild dependency on the local anisotropy ratio. A key intermediate step to derive such estimates for Virtual Element Methods is proving optimal approximation properties of the oblique elliptic projector in weighted Sobolev seminorms. This is a result whose interest goes beyond the specific model and methods considered here. We also obtain, to our knowledge, the first clear notion of consistency for Finite Volume methods, which leads to a generic error estimate involving the fluxes and valid for a wide range of Finite Volume schemes. An important application is the first error estimate for Multi-Point Flux Approximation L and G methods. In the appendix, not included in the published version of this work, we show that classical estimates for discontinuous Galerkin methods can be obtained with simplified arguments using the abstract framework."],["the analysis is based on the recent results on the characterization of the maximum coding rate","Performance of Non-orthogonal Multiple Access under Finite Blocklength","summarize: In this paper, we present a finite-block-length comparison between the orthogonal multiple access scheme and the non-orthogonal multiple access for the uplink channel. First, we consider the Gaussian channel, and derive the closed form expressions for the rate and outage probability. Then, we extend our results to the quasi-static Rayleigh fading channel. Our analysis is based on the recent results on the characterization of the maximum coding rate at finite block-length and finite block-error probability. The overall system throughput is evaluated as a function of the number of information bits, channel uses and power. We find what would be the respective values of these different parameters that would enable throughput maximization. Furthermore, we analyze the system performance in terms of reliability and throughput when applying the type-I ARQ protocol with limited number of retransmissions. The throughput and outage probability are evaluated for different blocklengths and number of information bits. Our analysis reveals that there is a trade-off between reliability and throughput in the ARQ. While increasing the number of retransmissions boosts reliability by minimizing the probability of reception error, it results in more delay which decreases the throughput. Nevertheless, the results show that NOMA always outperforms OMA in terms of throughput, reliability and latency regardless of the users priority or the number of retransmissions in both Gaussian and fading channels."],["a new model-independent measurement strategy can be done by comparing arrival times of","Speed of Gravitational Waves from Strongly Lensed Gravitational Waves and Electromagnetic Signals","summarize: We propose a new model-independent measurement strategy for the propagation speed of gravitational waves based on strongly lensed GWs and their electromagnetic counterparts. This can be done in two ways: by comparing arrival times of GWs and their EM counterparts and by comparing the time delays between images seen in GWs and their EM counterparts. The lensed GW-EM event is perhaps the best way to identify an EM counterpart. Conceptually, this method does not rely on any specific theory of massive gravitons or modified gravity. Its differential setting makes it robust against lens modeling details and against internal time delays between GW and EM emission acts. It requires, however, that the theory of gravity is metric and predicts gravitational lensing similar to general relativity. We expect that such a test will become possible in the era of third-generation gravitational-wave detectors, when about 10 lensed GW events would be observed each year. The power of this method is mainly limited by the timing accuracy of the EM counterpart, which for kilonovae is around "],["quantum theory is a widely used and successful theory of chemical reaction networks. it describes interactions","Quantum Techniques for Stochastic Mechanics","summarize: Some ideas from quantum theory are just beginning to percolate back to classical probability theory. For example, there is a widely used and successful theory of chemical reaction networks, which describes the interactions of molecules in a stochastic rather than quantum way. Computer science and population biology use the same ideas under a different name: stochastic Petri nets. But if we look at these theories from the perspective of quantum theory, they turn out to involve creation and annihilation operators, coherent states and other well-known ideas - but in a context where probabilities replace amplitudes. We explain this connection as part of a detailed analogy between quantum mechanics and stochastic mechanics. We use this analogy to present new proofs of two major results in the theory of chemical reaction networks: the deficiency zero theorem and the Anderson-Craciun-Kurtz theorem. We also study the overlap of quantum mechanics and stochastic mechanics, which involves Hamiltonians that can generate either unitary or stochastic time evolution. These Hamiltonians are called Dirichlet forms, and they arise naturally from electrical circuits made only of resistors."],["SAT model identifies span of interest in a tagging step.","Learning to Infer Entities, Properties and their Relations from Clinical Conversations","summarize: Recently we proposed the Span Attribute Tagging Model to infer clinical entities and their properties . It tackles the challenge of large label space and limited training data using a hierarchical two-stage approach that identifies the span of interest in a tagging step and assigns labels to the span in a classification step. We extend the SAT model to jointly infer not only entities and their properties but also relations between them. Most relation extraction models restrict inferring relations between tokens within a few neighboring sentences, mainly to avoid high computational complexity. In contrast, our proposed Relation-SAT model is computationally efficient and can infer relations over the entire conversation, spanning an average duration of 10 minutes. We evaluate our model on a corpus of clinical conversations. When the entities are given, the R-SAT outperforms baselines in identifying relations between symptoms and their properties by about 32% and by about 50% on medications and their properties. On the more difficult task of jointly inferring entities and relations, the R-SAT model achieves a performance of 0.34 and 0.45 for symptoms and medications respectively, which is significantly better than 0.18 and 0.35 for the baseline model. The contributions of different components of the model are quantified using ablation analysis."],["social media presence of peerJ articles is high. about 68.18% of the papers","Social Media Attention Increases Article Visits: An Investigation on Article-Level Referral Data of PeerJ","summarize: In order to better understand the effect of social media in the dissemination of scholarly articles, employing the daily updated referral data of 110 PeerJ articles collected over a period of 345 days, we analyze the relationship between social media attention and article visitors directed by social media. Our results show that social media presence of PeerJ articles is high. About 68.18% of the papers receive at least one tweet from Twitter accounts other than @PeerJ, the official account of the journal. Social media attention increases the dissemination of scholarly articles. Altmetrics could not only act as the complement of traditional citation measures but also play an important role in increasing the article downloads and promoting the impacts of scholarly articles. There also exists a significant correlation among the online attention from different social media platforms. Articles with more Facebook shares tend to get more tweets. The temporal trends show that social attention comes immediately following publication but does not last long, so do the social media directed article views."],["the first projection method is a generalization of the classical primal-dual method","Projection based model order reduction methods for the estimation of vector-valued variables of interest","summarize: We propose and compare goal-oriented projection based model order reduction methods for the estimation of vector-valued functionals of the solution of parameter-dependent equations. The first projection method is a generalization of the classical primal-dual method to the case of vector-valued variables of interest. We highlight the role played by three reduced spaces: the approximation space and the test space associated to the primal variable, and the approximation space associated to the dual variable. Then we propose a Petrov-Galerkin projection method based on a saddle point problem involving an approximation space for the primal variable and an approximation space for an auxiliary variable. A goal-oriented choice of the latter space, defined as the sum of two spaces, allows us to improve the approximation of the variable of interest compared to a primal-dual method using the same reduced spaces. Then, for both approaches, we derive computable error estimates for the approximations of the variable of interest and we propose greedy algorithms for the goal-oriented construction of reduced spaces. The performance of the algorithms are illustrated on numerical examples and compared to standard algorithms."],["a method is presented to evaluate the particle-phonon coupling corrections. in","Phonon-particle coupling effects in single-particle energies of semi-magic nuclei","summarize: A method is presented to evaluate the particle-phonon coupling corrections to the single-particle energies in semi-magic nuclei. In such nuclei always there is a collective low-lying "],["the proposed accelerated method is compared to several thermal conductivities. the method is","Accelerating first-principles estimation of thermal conductivity by machine-learning interatomic potentials: A MTP\/ShengBTE solution","summarize: Accurate evaluation of the thermal conductivity of a material can be a challenging task from both experimental and theoretical points of view. In particular for the nanostructured materials, the experimental measurement of thermal conductivity is associated with diverse sources of uncertainty. As a viable alternative to experiment, the combination of density functional theory simulations and the solution of Boltzmann transport equation is currently considered as the most trusted approach to examine thermal conductivity. The main bottleneck of the aforementioned method is to acquire the anharmonic interatomic force constants using the computationally demanding DFT calculations. In this work we propose a substantially accelerated approach for the evaluation of anharmonic interatomic force constants via employing machine-learning interatomic potentials trained over short ab-initio molecular dynamics trajectories. The remarkable accuracy of the proposed accelerated method is confirmed by comparing the estimated thermal conductivities of several bulk and two-dimensional materials with those computed by the full-DFT approach. The MLIP-based method proposed in this study can be employed as a standard tool, which would substantially accelerate and facilitate the estimation of lattice thermal conductivity in comparison with the commonly used full-DFT solution."],["we consider the algorithmic problem of generating each subset of subsets of subset","Trimming and gluing Gray codes","summarize: We consider the algorithmic problem of generating each subset of "],["incorporating ImageNet did not help much. a similar approach was proposed to combine scene","Scene recognition with CNNs: objects, scales and dataset bias","summarize: Since scenes are composed in part of objects, accurate recognition of scenes requires knowledge about both scenes and objects. In this paper we address two related problems: 1) scale induced dataset bias in multi-scale convolutional neural network architectures, and 2) how to combine effectively scene-centric and object-centric knowledge in CNNs. An earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help much. Here we propose an alternative method taking the scale into account, resulting in significant recognition gains. By analyzing the response of ImageNet-CNNs and Places-CNNs at different scales we find that both operate in different scale ranges, so using the same network for all the scales induces dataset bias resulting in limited performance. Thus, adapting the feature extractor to each particular scale is crucial to improve recognition, since the objects in the scenes have their specific range of scales. Experimental results show that the recognition accuracy highly depends on the scale, and that simple yet carefully chosen multi-scale combinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art recognition accuracy in SUN397 up to 66.26% ."],["a method for predicting error states can be used to predict failures. the error","Error Identification and Recovery in Robotic Snap Assembly","summarize: Existing methods for predicting robotic snap joint assembly cannot predict failures before their occurrence. To address this limitation, this paper proposes a method for predicting error states before the occurence of error, thereby enabling timely recovery. Robotic snap joint assembly requires precise positioning; therefore, even a slight offset between parts can lead to assembly failure. To correctly predict error states, we apply functional principal component analysis to 6D force\/torque profiles that are terminated before the occurence of an error. The error state is identified by applying a feature vector to a decision tree, wherein the support vector machine is employed at each node. If the estimation accuracy is low, we perform additional probing to more correctly identify the error state. Finally, after identifying the error state, a robot performs the error recovery motion based on the identified error state. Through the experimental results of assembling plastic parts with four snap joints, we show that the error states can be correctly estimated and a robot can recover from the identified error state."],["results of searches for supersymmetry by the CMS experiment are interpreted in the framework of simplified models","Interpretation of searches for supersymmetry with simplified models","summarize: The results of searches for supersymmetry by the CMS experiment are interpreted in the framework of simplified models. The results are based on data corresponding to an integrated luminosity of 4.73 to 4.98 inverse femtobarns. The data were collected at the LHC in proton-proton collisions at a center-of-mass energy of 7 TeV. This paper describes the method of interpretation and provides upper limits on the product of the production cross section and branching fraction as a function of new particle masses for a number of simplified models. These limits and the corresponding experimental acceptance calculations can be used to constrain other theoretical models and to compare different supersymmetry-inspired analyses."],["Halo model is a physically intuitive method for modelling the non-linear power spectrum","Non-linear matter power spectrum without screening dynamics modelling in ","summarize: Halo model is a physically intuitive method for modelling the non-linear power spectrum, especially for the alternatives to the standard "],["the fully polarizable QM\/MM approach based on fluctuating charges and","On the Calculation of IR Spectra with a Fully Polarizable QM\/MM Approach Based on Fluctuating Charges and Fluctuating Dipoles","summarize: The fully polarizable QM\/MM approach based on fluctuating charges and fluctuating dipoles, named QM\/FQF , is extended to the evaluation of nuclear gradients and the calculation of IR spectra of molecular systems in condensed phase. To this end, analytical equations defining first and second energy derivatives with respect to nuclear coordinates are derived and discussed. The potentialities of the approach are shown by applying the model to the calculation of IR spectra of Methlyoxirane, Glycidol and Gallic Acid in aqueous solution. The results are compared with the continuum QM\/PCM and the polarizable QM\/FQ, which is based on Fluctuating Charges only."],["a 3-parameter family of integral identities to be used on a class of theories possess","Boundary charges and integral identities for solitons in ","summarize: We establish a 3-parameter family of integral identities to be used on a class of theories possessing solitons with spherical symmetry in "],["the current UAV navigation schemes are unable to capture the UAV motion and select the best","Deep Reinforcement Learning for UAV Navigation Through Massive MIMO Technique","summarize: Unmanned aerial vehicles technique has been recognized as a promising solution in future wireless connectivity from the sky, and UAV navigation is one of the most significant open research problems, which has attracted wide interest in the research community. However, the current UAV navigation schemes are unable to capture the UAV motion and select the best UAV-ground links in real time, and these weaknesses overwhelm the UAV navigation performance. To tackle these fundamental limitations, in this paper, we merge the state-of-theart deep reinforcement learning with the UAV navigation through massive multiple-input-multiple-output technique. To be specific, we carefully design a deep Q-network for optimizing the UAV navigation by selecting the optimal policy, and then we propose a learning mechanism for processing the DQN. The DQN is trained so that the agent is capable of making decisions based on the received signal strengths for navigating theUAVs with the aid of the powerful Q-learning. Simulation results are provided to corroborate the superiority of the proposed schemes in terms of the coverage and convergence compared with those of the other schemes."],["a new approach can be used to remove irrelevant samples from real-world web images.","Exploiting Web Images for Fine-Grained Visual Recognition by Eliminating Noisy Samples and Utilizing Hard Ones","summarize: Labeling objects at a subordinate level typically requires expert knowledge, which is not always available when using random annotators. As such, learning directly from web images for fine-grained recognition has attracted broad attention. However, the presence of label noise and hard examples in web images are two obstacles for training robust fine-grained recognition models. Therefore, in this paper, we propose a novel approach for removing irrelevant samples from real-world web images during training, while employing useful hard examples to update the network. Thus, our approach can alleviate the harmful effects of irrelevant noisy web images and hard examples to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods."],["the SEROA signal is proportional to the magnetic polarizability of the substrate","Strongly Enhanced Raman Optical Activity in Molecules by Magnetic Response of Nanoparticles","summarize: An analytical theory for the surface-enhanced Raman optical activity with the magnetic response of the substrate particle has been presented. We have demonstrated that the SEROA signal is proportional to the magnetic polarizability of the substrate particle, which can be significantly enhanced due to the existence of the magnetic response. At the same time, a large circular intensity difference for the SEROA can also be achieved in the presence of the magnetic response. Taking Si nanoparticles as examples, we have found that the CID enhanced by a Si nanoparticle is 10 times larger than that of Au. Furthermore, when the molecule is located in the hotspot of a Si dimer, CID can be 60 times larger. The phenomena originate from large magnetic fields concentrated near the nanoparticle and boosted magnetic dipole emission of the molecule. The symmetric breaking of the electric fields caused by the magnetic dipole response of the nanoparticle also plays an important role. Our findings provide a new way to tailor theRaman optical activity by designing metamaterials with the strong magnetic response."],["automated visual inspection methods have been studied in recent years. but most steel manufacturing industries still use","TLU-Net: A Deep Learning Approach for Automatic Steel Surface Defect Detection","summarize: Visual steel surface defect detection is an essential step in steel sheet manufacturing. Several machine learning-based automated visual inspection methods have been studied in recent years. However, most steel manufacturing industries still use manual visual inspection due to training time and inaccuracies involved with AVI methods. Automatic steel defect detection methods could be useful in less expensive and faster quality control and feedback. But preparing the annotated training data for segmentation and classification could be a costly process. In this work, we propose to use the Transfer Learning-based U-Net framework for steel surface defect detection. We use a U-Net architecture as the base and explore two kinds of encoders: ResNet and DenseNet. We compare these nets' performance using random initialization and the pre-trained networks trained using the ImageNet data set. The experiments are performed using Severstal data. The results demonstrate that the transfer learning performs 5% better than that of the random initialization in defect classification. We found that the transfer learning performs 26% better than that of the random initialization in defect segmentation. We also found the gain of transfer learning increases as the training data decreases, and the convergence rate with transfer learning is better than that of the random initialization."],["modular wiretap schemes are based on an error-correcting code. the second component","Semantic Security via Seeded Modular Coding Schemes and Ramanujan Graphs","summarize: A novel type of functions called biregular irreducible functions is introduced and applied as security components in seeded modular wiretap coding schemes, whose second component is an error-correcting code. These schemes are called modular BRI schemes. An upper bound on the semantic security information leakage of modular BRI schemes in a one-shot setting is derived which separates the effects of the biregular irreducible function on the one hand and the error-correcting code plus the channel on the other hand. The effect of the biregular irreducible function is described by the second-largest eigenvalue of an associated stochastic matrix. A characterization of biregular irreducible functions is given in terms of connected edge-disjoint biregular graphs. It allows for the construction of new biregular irreducible functions from families of edge-disjoint Ramanujan graphs, which are shown to exist. A frequently used arithmetic universal hash function can be converted into a biregular irreducible function for certain parameters. Sequences of Ramanujan biregular irreducible functions are constructed which exhibit an optimal trade-off between the size of the regularity set and the rate of decrease of the associated second-largest eigenvalue. Together with the one-shot bound on the information leakage, the existence of these sequences implies an asymptotic coding result for modular BRI schemes applied to discrete and Gaussian wiretap channels. It shows that the separation of error correction and security as done in a modular BRI scheme is secrecy capacity-achieving for discrete and Gaussian wiretap channels. The same holds for a derived construction where the seed is generated locally by the sender and reused several times. Finally, optimal sequences of biregular irreducible functions used in the above constructions must be nearly Ramanujan."],["the gamma-ray burst monitor and the Large Area Telescope on board","Multiple Components in the Broadband ","summarize: GRB 160709A is one of the few bright short gamma-ray bursts detected by both the Gamma-ray Burst Monitor and the Large Area Telescope on board the "],["unlabeled sensing and shuffled linear regression is a problem in diverse","Homomorphic Sensing","summarize: A recent line of research termed unlabeled sensing and shuffled linear regression has been exploring under great generality the recovery of signals from subsampled and permuted measurements; a challenging problem in diverse fields of data science and machine learning. In this paper we introduce an abstraction of this problem which we call homomorphic sensing. Given a linear subspace and a finite set of linear transformations we develop an algebraic theory which establishes conditions guaranteeing that points in the subspace are uniquely determined from their homomorphic image under some transformation in the set. As a special case, we recover known conditions for unlabeled sensing, as well as new results and extensions. On the algorithmic level we exhibit two dynamic programming based algorithms, which to the best of our knowledge are the first working solutions for the unlabeled sensing problem for small dimensions. One of them, additionally based on branch-and-bound, when applied to image registration under affine transformations, performs on par with or outperforms state-of-the-art methods on benchmark datasets."],["the sign structure of the Jacobi matrix carries the information about which components of the network inhibit","Representing Model Ensembles as Boolean Functions","summarize: Families of ODE models characterized by a common sign structure of their Jacobi matrix are investigated within the formalism of qualitative differential equations. In the context of regulatory networks the sign structure of the Jacobi matrix carries the information about which components of the network inhibit or activate each other. Information about constraints on the behavior of models in this family is stored in a so called qualitative state transition graph. We showed previously that a similar approach can be used to analyze a model pool of Boolean functions characterized by a common interaction graph. Here we show that the opposite approach is fruitful as well. We show that the qualitative state transition graph can be reduced to a skeleton represented by a Boolean function conserving the reachability properties. This reduction has the advantage that approaches such as model checking and network inference methods can be applied to the skeleton within the framework of Boolean networks. Furthermore, our work constitutes an alternative to approach to link Boolean networks and differential equations."],["we compare abstract elementary classes of Shelah with accessible categories.","Abstract elementary classes and accessible categories","summarize: We compare abstract elementary classes of Shelah with accessible categories having directed colimits."],["convection caused by Joule heating of electrolyte during charging or dischar","Thermal convection in a liquid metal battery","summarize: Generation of thermal convection flow in the liquid metal battery, a device recently proposed as a promising solution for the problem of the short-term energy storage, is analyzed using a numerical model. It is found that convection caused by Joule heating of electrolyte during charging or discharging is virtually unavoidable. It exists in laboratory prototypes larger than a few cm in size and should become much stronger in larger-scale batteries. The phenomenon needs further investigation in view of its positive and negative effects."],["the levitation of condensed matter in vacuum allows the study of its physical properties under","Extending vacuum trapping to absorbing objects with hybrid Paul-optical traps","summarize: The levitation of condensed matter in vacuum allows the study of its physical properties under extreme isolation from the environment. It also offers a venue to investigate quantum mechanics with large systems, at the transition between the quantum and classical worlds. In this work, we study a novel hybrid levitation platform that combines a Paul trap with a weak but highly focused laser beam, a configuration that integrates a deep potential with excellent confinement and motion detection. We combine simulations and experiments to demonstrate the potential of this approach to extend vacuum trapping and interrogation to a broader range of nanomaterials, such as absorbing particles. We study the stability and dynamics of different specimens, like fluorescent dielectric crystals and gold nanorods, and demonstrate stable trapping down to pressures of 1 mbar."],["IoT devices running various applications can demonstrate a wide range of behaviors in the devices'","BEHAVE: Behavior-Aware, Intelligent and Fair Resource Management for Heterogeneous Edge-IoT Systems","summarize: Edge computing is an emerging solution to support the future Internet of Things applications that are delay-sensitive, processing-intensive or that require closer intelligence. Machine intelligence and data-driven approaches are envisioned to build future Edge-IoT systems that satisfy IoT devices' demands for edge resources. However, significant challenges and technical barriers exist which complicate the resource management for such Edge-IoT systems. IoT devices running various applications can demonstrate a wide range of behaviors in the devices' resource demand that are extremely difficult to manage. In addition, the management of multidimensional resources fairly and efficiently by the edge in such a setting is a challenging task. In this paper, we develop a novel data-driven resource management framework named BEHAVE that intelligently and fairly allocates edge resources to heterogeneous IoT devices with consideration of their behavior of resource demand . BEHAVE aims to holistically address the management technical barriers by: 1) building an efficient scheme for modeling and assessment of the BRD of IoT devices based on their resource requests and resource usage; 2) expanding a new Rational, Fair, and Truthful Resource Allocation model that binds the devices' BRD and resource allocation to achieve fair allocation and encourage truthfulness in resource demand; and 3) developing an enhanced deep reinforcement learning scheme to achieve the RFTA goals. The evaluation results demonstrate BEHAVE's capability to analyze the IoT devices' BRD and adjust its resource management policy accordingly."],["we present a framework facilitating the implementation and comparison of text compression algorithms. we evaluate","Compression with the tudocomp Framework","summarize: We present a framework facilitating the implementation and comparison of text compression algorithms. We evaluate its features by a case study on two novel compression algorithms based on the Lempel-Ziv compression schemes that perform well on highly repetitive texts."],["we correct the double spend race analysis given in Nakamoto's foundational Bitcoin article","Double spend races","summarize: We correct the double spend race analysis given in Nakamoto's foundational Bitcoin article and give a closed-form formula for the probability of success of a double spend attack using the Regularized Incomplete Beta Function. We give a proof of the exponential decay on the number of confirmations, often cited in the literature, and find an asymptotic formula. Larger number of confirmations are necessary compared to those given by Nakamoto. We also compute the probability conditional to the known validation time of the blocks. This provides a finer risk analysis than the classical one."],["we provide a characterization of the intersection of the two classes. we associate a","Linear perturbations of the Wigner transform and the Weyl quantization","summarize: We study a class of quadratic time-frequency representations that, roughly speaking, are obtained by linear perturbations of the Wigner transform. They satisfy Moyal's formula by default and share many other properties with the Wigner transform, but in general they do not belong to Cohen's class. We provide a characterization of the intersection of the two classes. To any such time-frequency representation, we associate a pseudodifferential calculus. We investigate the related quantization procedure, study the properties of the pseudodifferential operators, and compare the formalism with that of the Weyl calculus."],["independent vector analysis has emerged as an extension of independent component analysis into multiple sets of mixtures","The Extended Sequentially Drilled Joint Congruence Transformation and its Application in Gaussian Independent Vector Analysis","summarize: Independent Vector Analysis has emerged in recent years as an extension of Independent Component Analysis into multiple sets of mixtures, where the source signals in each set are independent, but may depend on source signals in the other sets. In a semi-blind IVA framework, information regarding the probability distributions of the sources may be available, giving rise to Maximum Likelihood separation. In recent work we have shown that under the multivariate Gaussian model, with arbitrary temporal covariance matrices of the source signals, ML separation requires the solution of a Sequentially Drilled Joint Congruence transformation of a set of matrices, which is reminiscent of classical joint diagonalization. In this paper we extend our results to the IVA problem, showing how the ML solution for the Gaussian model takes the form of an extended SeDJoCo problem. We formulate the extended problem, derive a condition for the existence of a solution, and propose two iterative solution algorithms. In addition, we derive the induced Cram\\'er-Rao Lower Bound on the resulting Interference-to-Source Ratios matrices, and demonstrate by simulation how the ML separation obtained by solving the extended SeDJoCo problem indeed attains the iCRLB , as opposed to other separation approaches, which cannot exploit prior knowledge regarding the sources' distributions."],["Dirac spinor fields in five and four dimensions share many features. in five dimensions we","Dirac spinors and their application to Bianchi-I space-times in 5 dimensions","summarize: We consider a five-dimensional Einstein--Cartan spacetime upon which Dirac spinor fields can be defined. Dirac spinor fields in five and four dimensions share many features, like the fact that both are described by four-component spinor fields, but they are also characterized by strong differences, like the fact that in five dimensions we do not have the possibility to project on left-handed and right-handed chiral parts unlike what happens in the four-dimensional instance: we conduct a polar decomposition of the spinorial fields, so to highlight all similarities and discrepancies. As an application of spinor fields in five dimensions, we study Bianchi-I spacetimes, verifying whether the Dirac fields in five dimensions can give rise to inflation or dark-energy dominated cosmological eras or not."],["the study employed six control variables: Cost, Time, Scope, Quality, Risk,","I.T. Project Success: Practical Frameworks based on key Project Control Variables","summarize: The objectives of this study were to research into the interdependencies IT project control variables, and also come out with frameworks to help IT project managers understand how to effectively control these variables to ensure the success of IT projects. The study employed six control variables: Cost, Time , Scope, Quality, Risk, and Benefits. A qualitative approach was adopted, where selected IT program and project managers of the Telecom industry in Ghana were interviewed individually and in a group based on a set of questions. The findings, espoused in the frameworks, reiterated the theory of the dependence of one control variable on the other, and the fact that varying one affects the others positively or negatively in relation to IT project success, as is the case for the iron triangle. Again, key activities of the control variables necessary to ensure IT project success were discovered."],["a family of operators on the Sturm-Liouville operator family is on the","Trace estimation of a family of periodic Sturm-Liouville operators with application to Robe's restricted three-body problem","summarize: In this paper, we consider a family of Sturm-Liouville operators on the "],["a movie description of a singular link cobordism in 4-space is a","Movie moves for singular link cobordisms in 4-dimensional space","summarize: Two singular links are cobordant if one can be obtained from the other by singular link isotopy together with a combination of births or deaths of simple unknotted curves, and saddle point transformations. A movie description of a singular link cobordism in 4-space is a sequence of singular link diagrams obtained from a projection of the cobordism into 3-space by taking 2-dimensional cross sections perpendicular to a fixed direction. We present a set of movie moves that are sufficient to connect any two movies of isotopic singular link cobordisms."],["TIBA - Tankette for Intelligent bioEnergy Agriculture - is the","Robotic Tankette for Intelligent BioEnergy Agriculture: Design, Development and Field Tests","summarize: In recent years, the use of robots in agriculture has been increasing mainly due to the high demand of productivity, precision and efficiency, which follow the climate change effects and world population growth. Unlike conventional agriculture, sugarcane farms are usually regions with dense vegetation, gigantic areas, and subjected to extreme weather conditions, such as intense heat, moisture and rain. TIBA - Tankette for Intelligent BioEnergy Agriculture - is the first result of an R&D project which strives to develop an autonomous mobile robotic system for carrying out a number of agricultural tasks in sugarcane fields. The proposed concept consists of a semi-autonomous, low-cost, dust and waterproof tankette-type vehicle, capable of infiltrating dense vegetation in plantation tunnels and carry several sensing systems, in order to perform mapping of hard-to-access areas and collecting samples. This paper presents an overview of the robot mechanical design, the embedded electronics and software architecture, and the construction of a first prototype. Preliminary results obtained in field tests validate the proposed conceptual design and bring about several challenges and potential applications for robot autonomous navigation, as well as to build a new prototype with additional functionality."],["a new texture feature-based approach for the estimation of crowd density is proposed. the","Crowd Density Estimation using Novel Feature Descriptor","summarize: Crowd density estimation is an important task for crowd monitoring. Many efforts have been done to automate the process of estimating crowd density from images and videos. Despite series of efforts, it remains a challenging task. In this paper, we proposes a new texture feature-based approach for the estimation of crowd density based on Completed Local Binary Pattern . We first divide the image into blocks and then re-divide the blocks into cells. For each cell, we compute CLBP and then concatenate them to describe the texture of the corresponding block. We then train a multi-class Support Vector Machine classifier, which classifies each block of image into one of four categories, i.e. Very Low, Low, Medium, and High. We evaluate our technique on the PETS 2009 dataset, and from the experiments, we show to achieve 95% accuracy for the proposed descriptor. We also compare other state-of-the-art texture descriptors and from the experimental results, we show that our proposed method outperforms other state-of-the-art methods."],["the elliptic Euler solutions are a complex complex complex. the","Maslov-type indices and linear stability of elliptic Euler solutions of the three-body problem","summarize: In this paper, we use the central configuration coordinate decomposition to study the linearized Hamiltonian system near the elliptic Euler solutions. Then using the Maslov-type \\omega-index theory of symplectic paths and the theory of linear operators we compute the \\omega-indices and obtain certain properties of linear stability of the Euler elliptic solutions of the classical three-body problem."],["slice Dirac operator over octonions involves a new theory of stem functions","Slice Dirac operator over octonions","summarize: The slice Dirac operator over octonions is a slice counterpart of the Dirac operator over quaternions. It involves a new theory of stem functions, which is the extension from the commutative "],["the resonant frequency analysis is done numerically through the determination of the eigen","Resonant frequency analysis of dental implants","summarize: Dental implant stability influences the decision on the determination of the duration between implant insertion and loading, This work investigates the resonant frequency analysis by means of a numerical model. The investigation is done numerically through the determination of the eigenfrequencies and performing a steady state response analyses using a commercial finite element package. A peri-implant interface, of simultaneously varying stiffness and layer thickness is introduced in the numerical 3D model in order to probe the sensitivity of the eigenfrequencies and steady state response to an evolving weakened layer, in an attempt to identify the bone reconstruction around the implant. For the first two modes, the resonant frequency is somewhat insensitive to the healing process, unless the weakened layer is rather large and compliant, like in the very early stages of the implantation. A Normalized Healing Factor is devised in the spirit of the Implant Stability Quotient, which can identify the healing process especially at the early stages after implantation. The sensitivity of the RFA to changes of mechanical properties of periprosthetic bone tissue seems relatively weak. Another indicator considering the amplitude as well as the resonance frequency might be more adapted to bone healing estimations. However, these results need to be verified experimentally as well as clinically."],["the paper derives the post-Newtonian Lagrangian of translational motion of N","Post-Newtonian Lagrangian of an N-body System with Arbitrary Mass and Spin Multipoles","summarize: The present paper derives the post-Newtonian Lagrangian of translational motion of N arbitrary-structured bodies with all mass and spin multipoles in a scalar-tensor theory of gravity. The multipoles depend on time and evolve in accordance with their own dynamic equations of motion. The Lagrangian is retrieved from the post-Newtonian equations of motion by solving the inverse problem of the Lagrangian mechanics and generalizes a well-known Lagrangian of pole-dipole-quadrupole massive particles to the particles of higher multipolarity. Analytic treatment of the higher-order multipole contributions is important for more rigorous computation of gravitational waveform of inspiralling compact binaries at the latest stage of their orbital evolution before merger when tidal and rotational deformations of stars are no longer small and rapidly change in time. The Lagrangian of an N-body system with arbitrary mass and spin multipoles is instrumental for formulation of the post-Newtonian conservation laws of energy, momenta and the integrals of the center of mass."],["agent-based models provide a flexible framework that is often used for modelling biological systems.","Learning differential equation models from stochastic agent-based model simulations","summarize: Agent-based models provide a flexible framework that is frequently used for modelling many biological systems, including cell migration, molecular dynamics, ecology, and epidemiology. Analysis of the model dynamics can be challenging due to their inherent stochasticity and heavy computational requirements. Common approaches to the analysis of agent-based models include extensive Monte Carlo simulation of the model or the derivation of coarse-grained differential equation models to predict the expected or averaged output from the agent-based model. Both of these approaches have limitations, however, as extensive computation of complex agent-based models may be infeasible, and coarse-grained differential equation models can fail to accurately describe model dynamics in certain parameter regimes. We propose that methods from the equation learning field provide a promising, novel, and unifying approach for agent-based model analysis. Equation learning is a recent field of research from data science that aims to infer differential equation models directly from data. We use this tutorial to review how methods from equation learning can be used to learn differential equation models from agent-based model simulations. We demonstrate that this framework is easy to use, requires few model simulations, and accurately predicts model dynamics in parameter regions where coarse-grained differential equation models fail to do so. We highlight these advantages through several case studies involving two agent-based models that are broadly applicable to biological phenomena: a birth-death-migration model commonly used to explore cell biology experiments and a susceptible-infected-recovered model of infectious disease spread."],["Let us know what you think about it!","On Monotonous Separately Continuous Functions","summarize: Let "],["QAPL 2019 was a satellite event of the european joint conference on theory and practice of","Proceedings 16th Workshop on Quantitative Aspects of Programming Languages and Systems","summarize: This EPTCS volume contains the proceedings of the 16th Workshop on Quantitative Aspects of Programming Languages and Systems held in Prague, Czech Republic, on Sunday 7 April 2019. QAPL 2019 was a satellite event of the European Joint Conferences on Theory and Practice of Software . QAPL focuses on quantitative aspects of computations, which may refer to the use of physical quantities as well as mathematical quantities for the characterisation of the behaviour and for determining the properties of systems. Such quantities play a central role in defining both the model of systems and the methodologies and tools for the analysis and verification of system properties. The aim of the QAPL workshop series is to discuss the explicit use of time and probability and general quantities either directly in the model or as a tool for the analysis or synthesis of systems. The 16th edition of QAPL also focuses on discussing the developments, challenges and results in this area covered by our workshop in its nearly 20-year history."],["MC is mostly studied in microscale but new practical applications emerge in macroscale. it","Fluid Dynamics-Based Distance Estimation Algorithm for Macroscale Molecular Communication","summarize: Many species, from single-cell bacteria to advanced animals, use molecular communication to share information with each other via chemical signals. Although MC is mostly studied in microscale, new practical applications emerge in macroscale. It is essential to derive an estimation method for channel parameters such as distance for practical macroscale MC systems which include a sprayer emitting molecules as a transmitter and a sensor as the receiver . In this paper, a novel approach based on fluid dynamics is proposed for the derivation of the distance estimation in practical MC systems. According to this approach, transmitted molecules are considered as moving droplets in the MC channel. With this approach, the Fluid Dynamics-Based Distance Estimation algorithm which predicts the propagation distance of the transmitted droplets by updating the diameter of evaporating droplets at each time step is proposed. FDDE algorithm is validated by experimental data. The results reveal that the distance can be estimated by the fluid dynamics approach which introduces novel parameters such as the volume fraction of droplets in a mixture of air and liquid droplets and the beamwidth of the TX. Furthermore, the effect of the evaporation is shown with the numerical results."],["the learning rate warm-up stage is crucial to the final performance. the transition is","On Layer Normalization in the Transformer Architecture","summarize: The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks , the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications."],["we propose an augmented gradient-play dynamics with correction. players communicate locally only with their","A Passivity-Based Approach to Nash Equilibrium Seeking over Networks","summarize: In this paper we consider the problem of distributed Nash equilibrium seeking over networks, a setting in which players have limited local information. We start from a continuous-time gradient-play dynamics that converges to an NE under strict monotonicity of the pseudo-gradient and assumes perfect information, i.e., instantaneous all-to-all player communication. We consider how to modify this gradient-play dynamics in the case of partial, or networked information between players. We propose an augmented gradient-play dynamics with correction in which players communicate locally only with their neighbours to compute an estimate of the other players' actions. We derive the new dynamics based on the reformulation as a multi-agent coordination problem over an undirected graph. We exploit incremental passivity properties and show that a synchronizing, distributed Laplacian feedback can be designed using relative estimates of the neighbours. Under a strict monotonicity property of the pseudo-gradient, we show that the augmented gradient-play dynamics converges to consensus on the NE of the game. We further discuss two cases that highlight the tradeoff between properties of the game and the communication graph."],["the paper derives the post-Newtonian Lagrangian of translational motion of N","Post-Newtonian Lagrangian of an N-body System with Arbitrary Mass and Spin Multipoles","summarize: The present paper derives the post-Newtonian Lagrangian of translational motion of N arbitrary-structured bodies with all mass and spin multipoles in a scalar-tensor theory of gravity. The multipoles depend on time and evolve in accordance with their own dynamic equations of motion. The Lagrangian is retrieved from the post-Newtonian equations of motion by solving the inverse problem of the Lagrangian mechanics and generalizes a well-known Lagrangian of pole-dipole-quadrupole massive particles to the particles of higher multipolarity. Analytic treatment of the higher-order multipole contributions is important for more rigorous computation of gravitational waveform of inspiralling compact binaries at the latest stage of their orbital evolution before merger when tidal and rotational deformations of stars are no longer small and rapidly change in time. The Lagrangian of an N-body system with arbitrary mass and spin multipoles is instrumental for formulation of the post-Newtonian conservation laws of energy, momenta and the integrals of the center of mass."],["ultrathin nonlinear metasurfaces composed of graphene micro-ribbons","Enhanced third harmonic generation with graphene metasurfaces","summarize: The nonlinear responses of different materials provide useful mechanisms for optical switching, low noise amplification, and harmonic frequency generation. However, the nonlinear processes usually have an extremely weak nature and require high input power to be excited. To alleviate this severe limitation, we propose new designs of ultrathin nonlinear metasurfaces composed of patterned graphene micro-ribbons to significantly enhance third harmonic generation at far-infrared and terahertz frequencies. The incident wave is tightly confined and significantly boosted along the surface of graphene in these configurations due to the excitation of highly localized plasmons. The bandwidth of the resonant response becomes narrower due to the introduction of a metallic substrate below the graphene micro-ribbons, which leads to zero transmission and standing waves inside the intermediate dielectric spacer layer. The enhancement of the incident field, combined with the large nonlinear conductivity of graphene, can dramatically increase the THG conversion efficiency by several orders of magnitude. In addition, the resonant frequency of the metasurface can be adjusted by dynamically tuning the Fermi energy of graphene via electrical or chemical doping. As a result, the third harmonic generated wave can be optimized and tuned to be emitted at different frequencies without the need to change the nonlinear metasurface geometry. The proposed nonlinear metasurfaces provide a new way to realize compact and efficient nonlinear sources at the far-infrared and THz frequency ranges, as well as new frequency generation and wave mixing devices which are expected to be useful for nonlinear THz spectroscopy and noninvasive THz imaging applications."],["we prove an analogue of Kirwan surjectivity in setting of equivari","Basic Kirwan Surjectivity for K-Contact Manifolds","summarize: We prove an analogue of Kirwan surjectivity in the setting of equivariant basic cohomology of K-contact manifolds. If the Reeb vector field induces a free "],["social media presence of peerJ articles is high. about 68.18% of the papers","Social Media Attention Increases Article Visits: An Investigation on Article-Level Referral Data of PeerJ","summarize: In order to better understand the effect of social media in the dissemination of scholarly articles, employing the daily updated referral data of 110 PeerJ articles collected over a period of 345 days, we analyze the relationship between social media attention and article visitors directed by social media. Our results show that social media presence of PeerJ articles is high. About 68.18% of the papers receive at least one tweet from Twitter accounts other than @PeerJ, the official account of the journal. Social media attention increases the dissemination of scholarly articles. Altmetrics could not only act as the complement of traditional citation measures but also play an important role in increasing the article downloads and promoting the impacts of scholarly articles. There also exists a significant correlation among the online attention from different social media platforms. Articles with more Facebook shares tend to get more tweets. The temporal trends show that social attention comes immediately following publication but does not last long, so do the social media directed article views."],["we investigate the nuclear modification factor of identified particles as a function of transverse momentum in collision","Nuclear modification factor in Pb-Pb and p-Pb collisions using Boltzmann transport equation","summarize: We investigate the nuclear modification factor of identified particles as a function of transverse momentum in Pb-Pb collisions at "],["model provides framework linking skills to her position in the collaboration network. a degree distribution of","A Model of Collaboration Network Formation with Heterogenous Skills","summarize: Collaboration networks provide a method for examining the highly heterogeneous structure of collaborative communities. However, we still have limited theoretical understanding of how individual heterogeneity relates to network heterogeneity. The model presented here provides a framework linking an individual's skill set to her position in the collaboration network, and the distribution of skills in the population to the structure of the collaboration network as a whole. This model suggests that there is a non-trivial relationship between skills and network position: individuals with a useful combination of skills will have a disproportionate number of links in the network. Indeed, in some cases, an individual's degree is non-monotonic in the number of skills she has--an individual with very few skills may outperform an individual with many. Special cases of the model suggest that the degree distribution of the network will be skewed, even when the distribution of skills is uniform in the population. The degree distribution becomes more skewed as problems become more difficult, leading to a community dominated by a few high-degree superstars. This has striking implications for labor market outcomes in industries where production is largely the result of collaborative effort."],["four circulant constructions for self-dual codes and bordered versions of the construction","New extremal binary self-dual codes from a modified four circulant construction","summarize: In this work, we propose a modified four circulant construction for self-dual codes and a bordered version of the construction using the properties of \\lambda-circulant and \\lambda-reverse circulant matrices. By using the constructions on "],["methyl vinyl ketone is a volatile organic compound of importance in atmospheric chemistry","Spatial Separation of the Conformers of Methyl Vinyl Ketone","summarize: Methyl vinyl ketone is a volatile, labile organic compound of importance in atmospheric chemistry. We prepared a molecular beam of methyl vinyl ketone with a rotational temperature of 1.2~K and demonstrated the spatial separation of the \\emph and \\emph conformers of methyl vinyl ketone using the electrostatic deflector. The resulting sample density was "],["the circuit combines a few-layer MoS2 n-MOSFET and","Symmetric Complementary Logic Inverter Using Integrated Black Phosphorus and MoS2 Transistors","summarize: The operation of an integrated two-dimensional complementary metal-oxide-semiconductor inverter with well-matched input\/output voltages is reported. The circuit combines a few-layer MoS2 n-MOSFET and a black phosphorus p-MOSFET fabricated using a common local backgate electrode with thin HfO2 gate dielectric. The constituent devices have linear threshold voltages of -0.8 V and +0.8 V and produce peak transconductances of 16 uS\/um and 41 uS\/um for the MoS2 n-MOSFET and BP p-MOSFET, respectively. The inverter shows a voltage gain of 3.5 at a supply voltage, VDD = 2.5 V, and has peak switching current of 108 uA and off-state current of 8.4 uA at VIN = 0 . In addition, the inverter has voltage gain greater than unity for VDD > 0.5 V, has open butterfly curves for VDD > 1 V, and achieves static noise margin over 500 mV at VDD = 2.5 V. The voltage gain was found to be insensitive to temperature between 270 K and 340 K, and AC large and small-signal operation was demonstrated at frequencies up to 100 kHz. The demonstration of a complementary 2D inverter which operates in a symmetric voltage window suitable for driving a subsequent logic stage is a significant step forward in developing practical applications for devices based upon 2D materials."],["we study the action of substitution maps between power series rings. this structure appears as a","On Hasse--Schmidt derivations: the action of substitution maps","summarize: We study the action of substitution maps between power series rings as an additional algebraic structure on the groups of Hasse--Schmidt derivations. This structure appears as a counterpart of the module structure on classical derivations."],["di-hadron correlations are used to compare low momentum jets. the distance-","Disappearance of the Mach Cone in heavy ion collisions","summarize: We present an analysis of di-hadron correlations using recently developed methods for background subtraction which allow for higher precision measurements with fewer assumptions about the background. These studies indicate that low momentum jets interacting with the medium do not equilibrate with the medium, but rather that interactions with the medium lead to more subtle increases in their widths and fragmentation functions, consistent with observations from studies of higher momentum fully reconstructed jets. The away-side shape is not consistent with a Mach cone."],["ANITA observes two upgoing ultra-high energy shower events. the dark matter","Upgoing ANITA events as evidence of the CPT symmetric universe","summarize: We explain the two upgoing ultra-high energy shower events observed by ANITA as arising from the decay in the Earth's interior of the quasi-stable dark matter candidate in the CPT symmetric universe. The dark matter particle is a 480 PeV right-handed neutrino that decays into a Higgs boson and a light Majorana neutrino. The latter interacts in the Earth's crust to produce a tau lepton that in turn initiates an atmospheric upgoing shower. The fact that both events emerge at the same angle from the Antarctic ice-cap suggests an atypical dark matter density distribution in the Earth."],["ab initio methods are used to study the absorption energy of atomic hydrogen at rotated","Selective Hydrogen Adsoprtion in Graphene Rotated Bilayers","summarize: The absorption energy of atomic hydrogen at rotated graphene bilayers is studied using ab initio methods based on the density functional theory including van der Waals interactions. We find that, due to the surface corrugation induced by the underneath rotated layer and the perturbation of the electronic density of states near the Fermi energy, the atoms with an almost AA stacking are the preferential ones for hydrogen chemisorption. The adsorption energy difference between different atoms can be as large as 80 meV. In addition, we find that, due to the logarithmic van Hove singularities in the electronic density of states at energies close to the Dirac point, the adsorption energy of either electron or hole doped samples is substantially increased. We also find that the adsorption energy increases with the decrease of the rotated angle between the layers. Finally, the large zero point energy of the C-H bond suggests adsorption and desorption of atomic hydrogen and deuterium should behave differently."],["the study confirms that the approximate results established by Kabanov and Safarian are still valid","Approximate hedging with proportional transaction costs in stochastic volatility models with jumps","summarize: We study the problem of option replication under constant proportional transaction costs in models where stochastic volatility and jumps are combined to capture the market's important features. Assuming some mild condition on the jump size distribution we show that transaction costs can be approximately compensated by applying the Leland adjusting volatility principle and the asymptotic property of the hedging error due to discrete readjustments is characterized. In particular, the jump risk can be approximately eliminated and the results established in continuous diffusion models are recovered. The study also confirms that for the case of constant trading cost rate, the approximate results established by Kabanov and Safarian and by Pergamenschikov are still valid in jump-diffusion models with deterministic volatility using the classical Leland parameter in Leland ."],["the profile development in the transition from hydraulically smooth to fully rough flow displays a propagating","Turbulence intensity and the friction factor for smooth- and rough-wall pipe flow","summarize: Turbulence intensity profiles are compared for smooth- and rough-wall pipe flow measurements made in the Princeton Superpipe. The profile development in the transition from hydraulically smooth to fully rough flow displays a propagating sequence from the pipe wall towards the pipe axis. The scaling of turbulence intensity with Reynolds number shows that the smooth- and rough wall level deviates with increasing Reynolds number. We quantify the correspondence between turbulence intensity and the friction factor."],["Arkan's polar coding technique is based on the idea of synthes","Alignment of Polarized Sets","summarize: Arkan's polar coding technique is based on the idea of synthesizing "],["in this paper, we give sufficient and necessary conditions for cosine operator functions on solid Ban","Linear dynamics of cosine operator functions on solid Banach function spaces","summarize: In this paper, we give some sufficient and necessary conditions for cosine operator functions on solid Banach function spaces to be chaotic or topologically transitive."],["we are motivated by a pressing need to address regulatory challenges. we introduce the internet of","Realising the Right to Data Portability for the Domestic Internet of Things","summarize: There is an increasing role for the IT design community to play in regulation of emerging IT. Article 25 of the EU General Data Protection Regulation 2016 puts this on a strict legal basis by establishing the need for information privacy by design and default for personal data-driven technologies. Against this backdrop, we examine legal, commercial and technical perspectives around the newly created legal right to data portability in GDPR. We are motivated by a pressing need to address regulatory challenges stemming from the Internet of Things . We need to find channels to support the protection of these new legal rights for users in practice. In Part I we introduce the internet of things and information PbD in more detail. We briefly consider regulatory challenges posed by the IoT and the nature and practical challenges surrounding the regulatory response of information privacy by design. In Part II, we look in depth at the legal nature of the RTDP, determining what it requires from IT designers in practice but also limitations on the right and how it relates to IoT. In Part III we focus on technical approaches that can support the realisation of the right. We consider the state of the art in data management architectures, tools and platforms that can provide portability, increased transparency and user control over the data flows. In Part IV, we bring our perspectives together to reflect on the technical, legal and business barriers and opportunities that will shape the implementation of the RTDP in practice, and how the relationships may shape emerging IoT innovation and business models. We finish with brief conclusions about the future for the RTDP and PbD in the IoT."],["we use the suite of AbacusCosmos Lamda-CDM simulations","Can Assembly Bias Explain the Lensing Amplitude of the BOSS CMASS Sample in a Planck Cosmology?","summarize: In this paper, we investigate whether galaxy assembly bias can reconcile the 20-40% disagreement between the observed galaxy projected clustering signal and the galaxy-galaxy lensing signal in the BOSS CMASS galaxy sample reported in Leauthaud et al. . We use the suite of AbacusCosmos Lamda-CDM simulations at Planck best-fit cosmology and two flexible implementations of extended halo occupation distribution models that incorporate galaxy assembly bias to build forward models and produce joint fits of the observed galaxy clustering signal and the galaxy-galaxy lensing signal. We find that our models using the standard HODs without any assembly bias generalizations continue to show a 20-40% over-prediction of the observed galaxy-galaxy lensing signal. We find that our implementations of galaxy assembly bias do not reconcile the two measurements at Planck best-fit cosmology. In fact, despite incorporating galaxy assembly bias, the satellite distribution parameter, and the satellite velocity bias parameter into our extended HOD model, our fits still strongly suggest a 31-34% discrepancy between the observed projected clustering and galaxy-galaxy lensing measurements. It remains to be seen whether a combination of other galaxy assembly bias models, alternative cosmological parameters, or baryonic effects can explain the amplitude difference between the two signals."],["nano-based nano-based devices are able to be deposited on metal surfaces.","Molecular anchoring stabilizes low valence NiTPP on copper against thermally induced chemical changes","summarize: Many applications of molecular layers deposited on metal surfaces, ranging from single-atom catalysis to on-surface magnetochemistry and biosensing, rely on the use of thermal cycles to regenerate the pristine properties of the system. Thus, understanding the microscopic origin behind the thermal stability of organic\/metal interfaces is fundamental for engineering reliable organic-based devices. Here, we study nickel porphyrin molecules on a copper surface as an archetypal system containing a metal center whose oxidation state can be controlled through the interaction with the metal substrate. We demonstrate that the strong molecule-surface interaction, followed by charge transfer at the interface, plays a fundamental role in the thermal stability of the layer by rigidly anchoring the porphyrin to the substrate. Upon thermal treatment, the molecules undergo an irreversible transition at 420 K, which is associated with an increase of the charge transfer from the substrate, mostly localized on the phenyl substituents, and a downward tilting of the latters without any chemical modification"],["superconductivity in Nd has been observed in recent years.","Doping evolution of the Mott-Hubbard landscape in infinite-layer nickelates","summarize: The recent observation of superconductivity in Nd"],["energy efficient power control for uplink two-tier networks is investigated. the algorithm is implemented","Energy Efficient Power Control for the Two-tier Networks with Small Cells and Massive MIMO","summarize: In this paper, energy efficient power control for the uplink two-tier networks where a macrocell tier with a massive multiple-input multiple-output base station is overlaid with a small cell tier is investigated. We propose a distributed energy efficient power control algorithm which allows each user in the two-tier network taking individual decisions to optimize its own energy efficiency for the multi-user and multi-cell scenario. The distributed power control algorithm is implemented by decoupling the EE optimization problem into two steps. In the first step, we propose to assign the users on the same resource into the same group and each group can optimize its own EE, respectively. In the second step, multiple power control games based on evolutionary game theory are formulated for each group, which allows each user optimizing its own EE. In the EGT-based power control games, each player selects a strategy giving a higher payoff than the average payoff, which can improve the fairness among the users. The proposed algorithm has a linear complexity with respect to the number of subcarriers and the number of cells in comparison with the brute force approach which has an exponential complexity. Simulation results show the remarkable improvements in terms of fairness by using the proposed algorithm."],["lower voltage swings and decreasing inter-wire spacing cause two distinct problems for communication in integrated circuit","Joint Crosstalk-Avoidance and Error-Correction Coding for Parallel Data Buses","summarize: Decreasing transistor sizes and lower voltage swings cause two distinct problems for communication in integrated circuits. First, decreasing inter-wire spacing increases interline capacitive coupling, which adversely affects transmission energy and delay. Second, lower voltage swings render the transmission susceptible to various noise sources. Coding can be used to address both these problems. So-called crosstalk-avoidance codes mitigate capacitive coupling, and traditional error-correction codes introduce resilience against channel errors. Unfortunately, crosstalk-avoidance and error-correction codes cannot be combined in a straightforward manner. On the one hand, crosstalk-avoidance encoding followed by error-correction encoding destroys the crosstalk-avoidance property. On the other hand, error-correction encoding followed by crosstalk-avoidance encoding causes the crosstalk-avoidance decoder to fail in the presence of errors. Existing approaches circumvent this difficulty by using additional bus wires to protect the parities generated from the output of the error-correction encoder, and are therefore inefficient. In this work we propose a novel joint crosstalk-avoidance and error-correction coding and decoding scheme that provides higher bus transmission rates compared to existing approaches. Our joint approach carefully embeds the parities such that the crosstalk-avoidance property is preserved. We analyze the rate and minimum distance of the proposed scheme. We also provide a density evolution analysis and predict iterative decoding thresholds for reliable communication under random bus erasures. This density evolution analysis is nonstandard, since the crosstalk-avoidance constraints are inherently nonlinear."],["coronagraph data shows a slow flux rope CME. the ejection","A model for stealth coronal mass ejections","summarize: Stealth coronal mass ejections are events in which there are almost no observable signatures of the CME eruption in the low corona but often a well-resolved slow flux rope CME observed in the coronagraph data. We present results from a three-dimensional numerical magnetohydrodynamics simulation of the 1--2 June 2008 slow streamer blowout CME that Robbrecht et al. called the CME from nowhere. We model the global coronal structure using a 1.4 MK isothermal solar wind and a low-order potential field source surface representation of the Carrington Rotation 2070 magnetogram synoptic map. The bipolar streamer belt arcade is energized by simple shearing flows applied in the vicinity of the helmet streamer's polarity inversion line. The flows are large scale and impart a shear typical of that expected from the differential rotation. The slow expansion of the energized helmet streamer arcade results in the formation of a radial current sheet. The subsequent onset of expansion-induced flare reconnection initiates the stealth CME while gradually releasing the stored magnetic energy. We present favorable comparisons between our simulation results and the multiviewpoint SOHO-LASCO and STEREO-SECCHI coronagraph observations of the preeruption streamer structure and the initiation and evolution of the stealth streamer blowout CME."],["Cremona group in 2 dimension is not simple, over any field. some elements satisfy","Non simplicit\\'e du groupe de Cremona sur tout corps","summarize: Using a theorem of Dahmani, Guirardel and Osin we prove that the Cremona group in 2 dimension is not simple, over any field. More precisely, we show that some elements of this group satisfy a weakened WPD property which is equivalent in our particular context to the Bestvina and Fujiwara's one."],["the theory of exact completions is used to study categorical properties of small setoids in","Exact completion and constructive theories of sets","summarize: In the present paper we use the theory of exact completions to study categorical properties of small setoids in Martin-L\\of type theory and, more generally, of models of the Constructive Elementary Theory of the Category of Sets, in terms of properties of their subcategories of choice objects . Because of these intended applications, we deal with categories that lack equalisers and just have weak ones, but whose objects can be regarded as collections of global elements.In this context, we study the internal logic of the categories involved, and employ this analysis to give a sufficient condition for the local cartesian closure of an exact completion. Finally, we apply this result to show when an exact completion produces a model of CETCS."],["a new emph problem setting aims at simultaneously recognizing and locating objects","Zero-Shot Object Detection: Learning to Simultaneously Recognize and Localize Novel Concepts","summarize: Current Zero-Shot Learning approaches are restricted to recognition of a single dominant unseen object category in a test image. We hypothesize that this setting is ill-suited for real-world applications where unseen objects appear only as a part of a complex scene, warranting both the `recognition' and `localization' of an unseen category. To address this limitation, we introduce a new \\emph problem setting, which aims at simultaneously recognizing and locating object instances belonging to novel categories without any training examples. We also propose a new experimental protocol for ZSD based on the highly challenging ILSVRC dataset, adhering to practical issues, e.g., the rarity of unseen objects. To the best of our knowledge, this is the first end-to-end deep network for ZSD that jointly models the interplay between visual and semantic domain information. To overcome the noise in the automatically derived semantic descriptions, we utilize the concept of meta-classes to design an original loss function that achieves synergy between max-margin class separation and semantic space clustering. Furthermore, we present a baseline approach extended from recognition to detection setting. Our extensive experiments show significant performance boost over the baseline on the imperative yet difficult ZSD problem."],["we give an introduction to the Mathematica packages masterPVA and masterPVAmulti","MasterPVA and WAlg: Mathematica packages for Poisson vertex algebras and classical affine ","summarize: We give an introduction to the Mathematica packages MasterPVA and MasterPVAmulti used to compute lambda-brackets in Poisson vertex algebras, which play an important role in the theory of infinite-dimensional Hamiltonian systems. As an application, we give an introduction to the Mathematica package WAlg aimed to compute the lambda-brackets among the generators of classical affine W-algebras. The use of these packages is shown by providing some explicit examples."],["the new calculus is very effective when it comes to computing drifts and expected values that possibly","Simplified stochastic calculus with applications in Economics and Finance","summarize: The paper introduces a simple way of recording and manipulating general stochastic processes without explicit reference to a probability measure. In the new calculus, operations traditionally presented in a measure-specific way are instead captured by tracing the behaviour of jumps . The calculus is fail-safe in that, under minimal assumptions, all informal calculations yield mathematically well-defined stochastic processes. The calculus is also intuitive as it allows the user to pretend all jumps are of compound Poisson type. The new calculus is very effective when it comes to computing drifts and expected values that possibly involve a change of measure. Such drift calculations yield, for example, partial integro-differential equations, Hamilton-Jacobi-Bellman equations, Feynman-Kac formulae, or exponential moments needed in numerous applications. We provide several illustrations of the new technique, among them a novel result on the Margrabe option to exchange one defaultable asset for another."],["K-essence Lagrangian studied in context of early universe epoch.","Some studies on k essence Lagrangian","summarize: K-essence Lagrangian is studied in the context of an early universe epoch when time is very small. Equation of state parameter as well as deceleration parameter , indicates an accelerated expansion of the universe at an early epoch of time driven by negative pressure generated by dark energy."],["we provide a pointwise bipolar theorem for liminf-close","A pointwise bipolar theorem","summarize: We provide a pointwise bipolar theorem for liminf-closed convex sets of positive Borel measurable functions on a sigma-compact metric space without the assumption that the polar is a tight set of measures. As applications we derive a version of the transport duality under non-tight marginals, and a superhedging duality for semistatic hedging in discrete time."],["2D deep learning methods are favored for their computational efficiency. but existing 2D deep","ACEnet: Anatomical Context-Encoding Network for Neuroanatomy Segmentation","summarize: Segmentation of brain structures from magnetic resonance scans plays an important role in the quantification of brain morphology. Since 3D deep learning models suffer from high computational cost, 2D deep learning methods are favored for their computational efficiency. However, existing 2D deep learning methods are not equipped to effectively capture 3D spatial contextual information that is needed to achieve accurate brain structure segmentation. In order to overcome this limitation, we develop an Anatomical Context-Encoding Network to incorporate 3D spatial and anatomical contexts in 2D convolutional neural networks for efficient and accurate segmentation of brain structures from MR scans, consisting of 1) an anatomical context encoding module to incorporate anatomical information in 2D CNNs and 2) a spatial context encoding module to integrate 3D image information in 2D CNNs. In addition, a skull stripping module is adopted to guide the 2D CNNs to attend to the brain. Extensive experiments on three benchmark datasets have demonstrated that our method achieves promising performance compared with state-of-the-art alternative methods for brain structure segmentation in terms of both computational efficiency and segmentation accuracy."],["this paper addresses an open problem in the area of linear quadratic optimal control. we","The regular indefinite linear quadratic optimal control problem: stabilizable case","summarize: This paper addresses an open problem in the area of linear quadratic optimal control. We consider the regular, infinite-horizon, stability-modulo-a-subspace, indefinite linear quadratic problem under the assumption that the dynamics are stabilizable. Our result generalizes previous works dealing with the same problem in the case of controllable dynamics. We explicitly characterize the unique solution of the algebraic Riccati equation that gives the optimal cost and optimal feedback control, as well as necessary and sufficient conditions for the existence of optimal controls."],["a physical theory loses its concrete applicability and thus verifiability of its","Examples of non-constructive proofs in quantum theory","summarize: Unlike mathematics, in which the notion of truth might be abstract, in physics, the emphasis must be placed on algorithmic procedures for obtaining numerical results subject to the experimental verifiability. For, a physical science is exactly that: algorithmic procedures for obtaining verifiable conclusions from a set of basic hypotheses. By admitting non-constructivist statements a physical theory loses its concrete applicability and thus verifiability of its predictions. Accordingly, the requirement of constructivism must be indispensable to any physical theory. Nevertheless, in at least some physical theories, and especially in quantum mechanics, one can find examples of non-constructive statements. The present paper demonstrates a couple of such examples dealing with macroscopic quantum states . As it is shown, in these examples the proofs of the existence of macroscopic quantum states are based on logical principles allowing one to decide the truth of predicates over an infinite number of things."],["creation operators act on symmetric functions to build Schur functions, Hall--Little","Abacus-histories and the combinatorics of creation operators","summarize: Creation operators act on symmetric functions to build Schur functions, Hall--Littlewood polynomials, and related symmetric functions one row at a time. Haglund, Morse, Zabrocki, and others have studied more general symmetric functions "],["composite system made up of distinguishable parties is investigated in the context of arbitrary convex","On the notion of composite system","summarize: The notion of composite system made up of distinguishable parties is investigated in the context of arbitrary convex spaces."],["entropy models are optimized for rate-distortion performance. ANN","Image-Dependent Local Entropy Models for Learned Image Compression","summarize: The leading approach for image compression with artificial neural networks is to learn a nonlinear transform and a fixed entropy model that are optimized for rate-distortion performance. We show that this approach can be significantly improved by incorporating spatially local, image-dependent entropy models. The key insight is that existing ANN-based methods learn an entropy model that is shared between the encoder and decoder, but they do not transmit any side information that would allow the model to adapt to the structure of a specific image. We present a method for augmenting ANN-based image coders with image-dependent side information that leads to a 17.8% rate reduction over a state-of-the-art ANN-based baseline model on a standard evaluation set, and 70-98% reductions on images with low visual complexity that are poorly captured by a fixed, global entropy model."],["in the present contribution, we introduce a wireless optical communication-based system architecture which is shown","Optical Wireless Cochlear Implants","summarize: In the present contribution, we introduce a wireless optical communication-based system architecture which is shown to significantly improve the reliability and the spectral and power efficiency of the transcutaneous link in cochlear implants . We refer to the proposed system as optical wireless cochlear implant .In order to provide a quantified understanding of its design parameters, we establish a theoretical framework that takes into account the channel particularities, the integration area of the internal unit, the transceivers misalignment, and the characteristics of the optical units. To this end, we derive explicit expressions for the corresponding average signal-to-noise-ratio, outage probability, ergodic spectral efficiency and capacity of the transcutaneous optical link . These expressions are subsequently used to assess the dependence of the TOL's communication quality on the transceivers design parameters and the corresponding channels characteristics. The offered analytic results are corroborated with respective results from Monte Carlo simulations. Our findings reveal that OWCI is a particularly promising architecture that drastically increases the reliability and effectiveness of the CI TOL, whilst it requires considerably lower transmit power compared to the corresponding widely-used radio frequency solution."],["optical tweezers based on strontium atoms achieve exceptional precision and","Seconds-scale coherence in a tweezer-array optical clock","summarize: Optical clocks based on atoms and ions achieve exceptional precision and accuracy, with applications to relativistic geodesy, tests of relativity, and searches for dark matter. Achieving such performance requires balancing competing desirable features, including a high particle number, isolation of atoms from collisions, insensitivity to motional effects, and high duty-cycle operation. Here we demonstrate a new platform based on arrays of ultracold strontium atoms confined within optical tweezers that realizes a novel combination of these features by providing a scalable platform for isolated atoms that can be interrogated multiple times. With this tweezer-array clock, we achieve greater than 3 second coherence times and record duty cycles up to 96%, as well as stability commensurate with leading platforms. By using optical tweezer arrays --- a proven platform for the controlled creation of entanglement through microscopic control --- this work further promises a new path toward combining entanglement enhanced sensitivities with the most precise optical clock transitions."],["mass discrepancy between lensing and baryonic mass is larger when acceleration of the","Mass Discrepancy-Acceleration Relation in Einstein Rings","summarize: We study the Mass Discrepancy-Acceleration Relation of 57 elliptical galaxies by their Einstein rings from the Sloan Lens ACS Survey . The mass discrepancy between the lensing mass and the baryonic mass derived from population synthesis is larger when the acceleration of the elliptical galaxy lenses is smaller. The MDAR is also related to surface mass density discrepancy. At the Einstein ring, these lenses belong to high-surface-mass density galaxies. Similarly, we find that the discrepancy between the lensing and stellar surface mass density is small. It is consistent with the recent discovery of dynamical surface mass density discrepancy in disk galaxies where the discrepancy is smaller when surface density is larger. We also find relativistic modified Newtonian dynamics can naturally explain the MDAR and surface mass density discrepancy in 57 Einstein rings. Moreover, the lensing mass, the dynamical mass and the stellar mass of these galaxies are consistent with each other in relativistic MOND."],["a quadratic objective function is used to formulate calibration as a quadratic program","Certifiably Globally Optimal Extrinsic Calibration from Per-Sensor Egomotion","summarize: We present a certifiably globally optimal algorithm for determining the extrinsic calibration between two sensors that are capable of producing independent egomotion estimates. This problem has been previously solved using a variety of techniques, including local optimization approaches that have no formal global optimality guarantees. We use a quadratic objective function to formulate calibration as a quadratically constrained quadratic program . By leveraging recent advances in the optimization of QCQPs, we are able to use existing semidefinite program solvers to obtain a certifiably global optimum via the Lagrangian dual problem. Our problem formulation can be globally optimized by existing general-purpose solvers in less than a second, regardless of the number of measurements available and the noise level. This enables a variety of robotic platforms to rapidly and robustly compute and certify a globally optimal set of calibration parameters without a prior estimate or operator intervention. We compare the performance of our approach with a local solver on extensive simulations and multiple real datasets. Finally, we present necessary observability conditions that connect our approach to recent theoretical results and analytically support the empirical performance of our system."],["a pig phantom maintained at 20 degree was a pig phan","Quantitative MRI molecular imaging in the evaluation of early post mortem changes in muscles. A feasibility study on a pig phantom","summarize: Estimating early postmortem interval EPI is a difficult task in daily forensic activity due to limitations of accurate and reliable methods. The aim of the present work is to describe a novel approach in the estimation of EPI based on quantitative magnetic resonance molecular imaging qMRMI using a pig phantom since post mortem degradation of pig meat is similar to that of human muscles. On a pig phantom maintained at 20 degree, using a 1.5 T MRI scanner we performed 10 scans, every 4 hours, monitoring apparent diffusion coefficient ADC, fractional anisotropy FA, magnetization transfer ration MTR, tractography and susceptibility weighted changes in muscles until 36 hours after death. Cooling of the phantom during the experiment was recorded. Histology was also obtained. Pearson's Test was carried out for statistical correlation. We found a significative statistical inverse correlation between ADC, FA, MT and PMI. Our preliminary data shows that post mortem qMRMI is a potential powerful tool in accurately determining EPI and is worth of further investigation."],["brachytherapy is a tumor treatment method where a highly radioactive source is brought","High-dose-rate prostate brachytherapy inverse planning on dose-volume criteria by simulated annealing","summarize: High-dose-rate brachytherapy is a tumor treatment method where a highly radioactive source is brought in close proximity to the tumor. In this paper we develop a simulated annealing algorithm to optimize the dwell times at preselected dwell positions to maximize tumor coverage under dose-volume constraints on the organs at risk. Compared to existing algorithms, our algorithm has advantages in terms of speed and objective value and does not require an expensive general purpose solver. Its success mainly depends on exploiting the efficiency of matrix multiplication and a careful selection of the neighboring states. In this paper we outline its details and make an in-depth comparison with existing methods using real patient data."],["gyroscopes use the physics of exceptional points. the measurable","Ultrasensitive micro-scale parity-time-symmetric ring laser gyroscope","summarize: We propose a new scheme for ultra-sensitive laser gyroscopes that utilizes the physics of exceptional points. By exploiting the properties of such non-Hermitian degeneracies, we show that the rotation-induced frequency splitting becomes proportional to the square root of the gyration speed- thus enhancing the sensitivity to low angular rotations by orders of magnitudes. In addition, at its maximum sensitivity limit, the measurable spectral splitting is independent of the radius of the rings involved. Our work paves the way towards a new class of ultra-sensitive miniature ring laser gyroscopes on chip."],["the image under this parametrization is the interior of a polytope. the","The Hilbert metric on Teichm\\uller space and Earthquake","summarize: Hamenst\\adt gave a parametrization of the Teichm\\uller space of punctured surfaces such that the image under this parametrization is the interior of a polytope. In this paper, we study the Hilbert metric on the Teichm\\uller space of punctured surfaces based on this parametrization. We prove that every earthquake ray is an almost geodesic under the Hilbert metric."],["model developed by a multilayer perceptron artificial neural network model. the model","An Artificial Neural Network-based Stock Trading System Using Technical Analysis and Big Data Framework","summarize: In this paper, a neural network-based stock price prediction and trading system using technical analysis indicators is presented. The model developed first converts the financial time series data into a series of buy-sell-hold trigger signals using the most commonly preferred technical analysis indicators. Then, a Multilayer Perceptron artificial neural network model is trained in the learning stage on the daily stock prices between 1997 and 2007 for all of the Dow30 stocks. Apache Spark big data framework is used in the training stage. The trained model is then tested with data from 2007 to 2017. The results indicate that by choosing the most appropriate technical indicators, the neural network model can achieve comparable results against the Buy and Hold strategy in most of the cases. Furthermore, fine tuning the technical indicators and\/or optimization strategy can enhance the overall trading performance."],["synchrotron external shock model predicts evolution of spectral and temporal indices","Closure relations of Gamma Ray Bursts in high energy emission","summarize: The synchrotron external shock model predicts the evolution of the spectral and temporal indices during the gamma-ray burst afterglow for different environmental density profiles, electron spectral indices, electron cooling regimes, and regions of the spectrum. We study the relationship between "],["cloud computing providers offer solutions based on bare metal or virtualization platforms. container placement","Containers Placement and Migration on Cloud System","summarize: Currently, many businesses are using cloud computing to obtain an entire IT infrastructure remotely while delegating its management to a third party. The provider of this architecture ensures the operation and maintenance of the services while offering management capabilities via web consoles.These providers offer solutions based on bare metal or virtualization platforms . Recently, a new type of virtualization-based on containerization technology has emerged. Containers can be deployed on bare metal servers or in virtual machines. The migration of virtual machines and Containers in Dynamic Resource Management is a crucial factor in minimizing the operating costs of data centers by reducing their energy consumption and subsequently limiting their impact on climate change.In this article, live migration for both types of virtualization will be studied. for that, container placement and migration algorithms are proposed, which takes into account the QoS requirements of different users in order to minimize energy consumption. Thus, a dynamic approach is suggested based on a threshold of RAM usage for host and virtual machines in the data center to avoid unnecessary power consumption. In this paper, the proposed work is compared with VM\/Container placement and migration methods, the results of the experiment indicate that using container migration instead of VMs demonstrates a reduction in power consumption, and also reduces the migration time which impacts QoS and reduces SLA violation."],["brownian dynamics simulations are used to study the detachment of a particle","Equilibrium binding energies from fluctuation theorems and force spectroscopy simulations","summarize: Brownian dynamics simulations are used to study the detachment of a particle from a substrate. Although the model is simple and generic, we attempt to map its energy, length and time scales onto a specific experimental system, namely a bead that is weakly bound to a cell and then removed by an optical tweezer. The external driving force arises from the combined optical tweezer and substrate potentials, and thermal fluctuations are taken into account by a Brownian force. The Jarzynski equality and Crooks' fluctuation theorem are applied to obtain the equilibrium free energy difference between the final and initial states. To this end, we sample non--equilibrium work trajectories for various tweezer pulling rates. We argue that this methodology should also be feasible experimentally for the envisioned system. Furthermore, we outline how the measurement of a whole free energy profile would allow the experimentalist to retrieve the unknown substrate potential by means of a suitable deconvolution. The influence of the pulling rate on the accuracy of the results is investigated, and umbrella sampling is used to obtain the equilibrium probability of particle escape for a variety of trap potentials."],["van der Waals heterostructures are a new approach to produce artificial systems. we","Synthetic Semimetals with van der Waals Interfaces","summarize: The assembly of suitably designed van der Waals heterostructures represents a new approach to produce artificial systems with engineered electronic properties. Here, we apply this strategy to realize synthetic semimetals based on vdW interfaces formed by two different semiconductors. Guided by existing ab-initio calculations, we select WSe"],["we consider application of a standard generic divisor doubling for construction of new auto transformations","Backlund transformations and divisor doubling","summarize: In classical mechanics well-known cryptographic algorithms and protocols can be very useful for construction canonical transformations preserving form of Hamiltonians. We consider application of a standard generic divisor doubling for construction of new auto B\\cklund transformations for the Lagrange top and H\\'non-Heiles system separable in parabolic coordinates."],["the resulting extractions are a valuable resource for downstream tasks such as knowledge base construction,","OPIEC: An Open Information Extraction Corpus","summarize: Open information extraction systems extract relations and their arguments from natural language text in an unsupervised manner. The resulting extractions are a valuable resource for downstream tasks such as knowledge base construction, open question answering, or event schema induction. In this paper, we release, describe, and analyze an OIE corpus called OPIEC, which was extracted from the text of English Wikipedia. OPIEC complements the available OIE resources: It is the largest OIE corpus publicly available to date and contains valuable metadata such as provenance information, confidence scores, linguistic annotations, and semantic annotations including spatial and temporal information. We analyze the OPIEC corpus by comparing its content with knowledge bases such as DBpedia or YAGO, which are also based on Wikipedia. We found that most of the facts between entities present in OPIEC cannot be found in DBpedia and\/or YAGO, that OIE facts often differ in the level of specificity compared to knowledge base facts, and that OIE open relations are generally highly polysemous. We believe that the OPIEC corpus is a valuable resource for future research on automated knowledge base construction."],["error detection problem in a study by a spokesman.","On Error Detection in Asymmetric Channels","summarize: We study the error detection problem in "],["concepts are made explicit to provide formality and a precise understanding of small, contingent universes","Thinging-Based Conceptual Modeling: Case Study of a Tendering System","summarize: In computer science, models are made explicit to provide formality and a precise understanding of small, contingent universes , as constructed from stakeholder requirements. Conceptual modeling is a fundamental discipline in this context whose main concerns are identifying, analyzing and describing the critical concepts of a universe of discourse. In the information systems field, one of the reasons why projects fail is an inability to capture requirements in a way that can be technically used to configure a system. This problem of requirements specification is considered to have deficiencies in theory. We apply a recently developed model called the Thinging Machine model which uniformly integrates static and dynamic modeling features to this problem of requirements specification. The object-Oriented approach to modeling, as applied in Unified Modeling Language, is by far the most applied and accepted standard in software engineering; nevertheless, new notions in the field may enhance and facilitate a supplementary understanding of the OO model itself. We aim to contribute to the field of conceptual modeling by introducing the TM model s philosophical foundation of requirements analysis. The TM model has only five generic processes of things , in which genericity indicates generality, as in the generic Aristotelian concepts based on abstraction. We show the TM model s viability by applying it to a real business system."],["study introduces notions of semi-homotopy of semi-continuous maps and","Semi-homotopy and semi-fundamental groups","summarize: In this study we introduce the notions of semi-homotopy of semi-continuous maps and of semi-paths. We also construct a group structure, which will be called semi-fundamental group, using semi-loops and explore some properties of semi-homotopy and semi-fundamental groups."],["creation operators act on symmetric functions to build Schur functions, Hall--Little","Abacus-histories and the combinatorics of creation operators","summarize: Creation operators act on symmetric functions to build Schur functions, Hall--Littlewood polynomials, and related symmetric functions one row at a time. Haglund, Morse, Zabrocki, and others have studied more general symmetric functions "],["the web 2.0 paradigm has changed the way businesses are run all around the world. e","Evaluation of Business-Oriented Performance Metrics in e-Commerce using Web-based Simulation","summarize: The Web 2.0 paradigm has radically changed the way businesses are run all around the world. Moreover, e-Commerce has overcome in daily shopping activities. For management teams, the assessment, evaluation, and forecasting of online incomes and other business-oriented performance measures have become 'a holy grail', the ultimate question imposing their current and future e-Commerce projects. Within the paper, we describe the development of a Web-based simulation model, suitable for their estimation, taking into account multiple operation profiles and scenarios. Specifically, we put focus on introducing specific classes of e-Customers, as well as the workload characterization of an arbitrary e-Commerce website. On the other hand, we employ and embed the principles of the system thinking approach and the system dynamics into the proposed solution. As a result, a complete simulation model has been developed, available online. The model, which includes numerous adjustable input variables, can be successfully utilized in making 'what-if'-like insights into a plethora of business-oriented performance metrics for an arbitrary e-Commerce website. This project is, also, a great example of the power delivered by InsightMaker, free-of-charge Web-based software, suitable for a collaborative online development of models following the systems thinking paradigm."],["this study uses deep learning based multiple regression algorithm. we predict the TCWV with","Deep Learning based Multiple Regression to Predict Total Column Water Vapor from Physical Parameters in West Africa by using Keras Library","summarize: Total column water vapor is an important factor for the weather and climate. This study apply deep learning based multiple regression to map the TCWV with elements that can improve spatiotemporal prediction. In this study, we predict the TCWV with the use of ERA5 that is the fifth generation ECMWF atmospheric reanalysis of the global climate. We use an appropriate deep learning based multiple regression algorithm using Keras library to improve nonlinear prediction between Total Column water vapor and predictors as Mean sea level pressure, Surface pressure, Sea surface temperature, 100 metre U wind component, 100 metre V wind component, 10 metre U wind component, 10 metre V wind component, 2 metre dew point temperature, 2 metre temperature. The results obtained permit to build a predictor which modelling TCWV with a mean abs error equal to 3.60 kg\/m2 and a coefficient of determination R2 equal to 0.90."],["the French writer colette has a question about the majesty of ce qui fin","Historical Developments of the Theory of Transcendence and Algebraic Independence","summarize: Qu'est la majest de ce qui finit, auprs des dparts titubants, des dsordres de l'aurore?. Let this question from the French writer Colette guide us to walk through the promising beginnings, fascinating evolutions and results, challenging open problems and perspectives of an exciting theory that is far from revealing all its secrets."],["the strategy is based on methods developed to analyze social networks. we illustrate the use of","Using Data Mining to Explore Calmodulin Bibliography","summarize: In this chapter, we present a strategy and the technics to approach a scientific field from a set of articles gathered from the bibliographic database, Web of Science. The strategy is based on methods developed to analyze social networks. We illustrate the use of such strategy in studying the calmodulin field. Such method allows to structure a huge number of articles when writing a review, to detect the key opinion leaders in a given field and to locate his own research topic in the landscape of the themes deciphered by our own community. We show that the free software VosViewer may be used without knowledge in computing science and with a short learning period. iii."],["method learns chemical reaction networks from trajectory data. method is based on trajectory data","Learning chemical reaction networks from trajectory data","summarize: We develop a data-driven method to learn chemical reaction networks from trajectory data. Modeling the reaction system as a continuous-time Markov chain and assuming the system is fully observed, our method learns the propensity functions of the system with predetermined basis functions by maximizing the likelihood function of the trajectory data under "],["entropic regularization has given rise to many applications. but the OT problem is","Entropic Optimal Transport between Unbalanced Gaussian Measures has a Closed Form","summarize: Although optimal transport problems admit closed form solutions in a very few notable cases, e.g. in 1D or between Gaussians, these closed forms have proved extremely fecund for practitioners to define tools inspired from the OT geometry. On the other hand, the numerical resolution of OT problems using entropic regularization has given rise to many applications, but because there are no known closed-form solutions for entropic regularized OT problems, these approaches are mostly algorithmic, not informed by elegant closed forms. In this paper, we propose to fill the void at the intersection between these two schools of thought in OT by proving that the entropy-regularized optimal transport problem between two Gaussian measures admits a closed form. Contrary to the unregularized case, for which the explicit form is given by the Wasserstein-Bures distance, the closed form we obtain is differentiable everywhere, even for Gaussians with degenerate covariance matrices. We obtain this closed form solution by solving the fixed-point equation behind Sinkhorn's algorithm, the default method for computing entropic regularized OT. Remarkably, this approach extends to the generalized unbalanced case -- where Gaussian measures are scaled by positive constants. This extension leads to a closed form expression for unbalanced Gaussians as well, and highlights the mass transportation \/ destruction trade-off seen in unbalanced optimal transport. Moreover, in both settings, we show that the optimal transportation plans are Gaussians and provide analytical formulas of their parameters. These formulas constitute the first non-trivial closed forms for entropy-regularized optimal transport, thus providing a ground truth for the analysis of entropic OT and Sinkhorn's algorithm."],["a residual-driven FCM framework is proposed by integrating into FCM a residual","Residual-driven Fuzzy C-Means Clustering for Image Segmentation","summarize: Due to its inferior characteristics, an observed image's direct use gives rise to poor segmentation results. Intuitively, using its noise-free image can favorably impact image segmentation. Hence, the accurate estimation of the residual between observed and noise-free images is an important task. To do so, we elaborate on residual-driven Fuzzy C-Means for image segmentation, which is the first approach that realizes accurate residual estimation and leads noise-free image to participate in clustering. We propose a residual-driven FCM framework by integrating into FCM a residual-related fidelity term derived from the distribution of different types of noise. Built on this framework, we present a weighted "],["bacteriotherapies or bugs as drugs are communities of bacteria administered to patients for specific therapeutic","Robust and Scalable Models of Microbiome Dynamics","summarize: Microbes are everywhere, including in and on our bodies, and have been shown to play key roles in a variety of prevalent human diseases. Consequently, there has been intense interest in the design of bacteriotherapies or bugs as drugs, which are communities of bacteria administered to patients for specific therapeutic applications. Central to the design of such therapeutics is an understanding of the causal microbial interaction network and the population dynamics of the organisms. In this work we present a Bayesian nonparametric model and associated efficient inference algorithm that addresses the key conceptual and practical challenges of learning microbial dynamics from time series microbe abundance data. These challenges include high-dimensional but temporally sparse and non-uniformly sampled data; high measurement noise; and, nonlinear and physically non-negative dynamics. Our contributions include a new type of dynamical systems model for microbial dynamics based on what we term interaction modules, or learned clusters of latent variables with redundant interaction structure "],["a case that Microscopic Black Holes of mass.","Ramjet Acceleration of Microscopic Black Holes Within Stellar Material","summarize: In this work we present a case that Microscopic Black Holes of mass "],["hybridization of atoms is the basis for the endless library of naturally occurring molecules.","Perspective on Coupled Colloidal Quantum Dot Molecules","summarize: Electronic coupling and hence hybridization of atoms serve as the basis for the rich properties of the endless library of naturally occurring molecules. Colloidal quantum dots manifesting quantum strong confinement, possess atomic like characteristics with s and p electronic levels, which popularized the notion of CQDs as artificial atoms. Continuing this analogy, when two atoms are close enough to form a molecule so that their orbitals start overlapping, the orbitals' energies start to split into bonding and anti-bonding states made out of hybridized orbitals. The same concept is also applicable for two fused core-shell nanocrystals in close proximity. Their band-edge states, which dictate the emitted photon energy, start to hybridize changing their electronic and optical properties. Thus, an exciting direction of artificial molecules emerges leading to a multitude of possibilities for creating a library of new hybrid nanostructures with novel optoelectronic properties with relevance towards diverse applications including quantum technologies. In a model fused core-shell homodimer molecule, the hybridization energy is strongly correlated with the extent of structural continuity, the delocalization of the exciton wavefunction, and the barrier thickness as calculated numerically. The hybridization impacts the emitted photon statistics manifesting a faster radiative decay rate, photon bunching effect, and modified Auger recombination pathway compared to the monomer artificial atoms. Future perspectives for the nanocrystals chemistry paradigm are highlighted."],["the study was conducted to develop an appropriate model that could predict the weekly reported Malaria incidence in","Malaria Incidence in the Philippines: Prediction using the Autoregressive Moving Average Models","summarize: The study was conducted to develop an appropriate model that could predict the weekly reported Malaria incidence in the Philippines using the Box-Jenkins method.The data were retrieved from the Department of Health website in the Philippines. It contains 70 data points of which 60 data points were used in model building and the remaining 10 data points were used for forecast evaluation. The R Statistical Software was used to do all the necessary computations in the study. Box-Cox Transformation and Differencing was done to make the series stationary. Based on the results of the analysis, ARIMA is the appropriate model for the weekly Malaria incidence in the Philippines."],["new theory of massive gravity with only two propagating degrees of freedom. we will perform","Minimal theory of massive gravity","summarize: We propose a new theory of massive gravity with only two propagating degrees of freedom. After defining the theory in the unitary gauge in the vielbein language, we shall perform a Hamiltonian analysis to count the number of physical degrees of freedom, and then study some phenomenologies. While the homogeneous and isotropic background cosmology and the tensor linear perturbations around it are described by exactly the same equations as those in the de Rham-Gabadadze-Tolley massive gravity, the scalar and vector gravitational degrees of freedom are absent in the new theory at the fully nonlinear level. Hence the new theory provides a stable nonlinear completion of the self-accelerating cosmological solution that was originally found in the dRGT theory."],["binary or the interaction with a circumbinary disk may efficiently drive the system to sub-","Detecting eccentric supermassive black hole binaries with pulsar timing arrays: Resolvable source strategies","summarize: The couplings between supermassive black-hole binaries and their environments within galactic nuclei have been well studied as part of the search for solutions to the final parsec problem. The scattering of stars by the binary or the interaction with a circumbinary disk may efficiently drive the system to sub-parsec separations, allowing the binary to enter a regime where the emission of gravitational waves can drive it to merger within a Hubble time. However, these interactions can also affect the orbital parameters of the binary. In particular, they may drive an increase in binary eccentricity which survives until the system's gravitational-wave signal enters the pulsar-timing array band. Therefore, if we can measure the eccentricity from observed signals, we can potentially deduce some of the properties of the binary environment. To this end, we build on previous techniques to present a general Bayesian pipeline with which we can detect and estimate the parameters of an eccentric supermassive black-hole binary system with pulsar-timing arrays. Additionally, we generalize the pulsar-timing array "],["discrete continuous method was used to compute the mechanical fields. original simulation strategy was modified to","Discrete dislocation dynamics simulations of dislocation-","summarize: The mechanisms of dislocation\/precipitate interaction were studied by means of discrete dislocation dynamics within a multiscale approach. Simulations were carried out using the discrete continuous method in combination with a fast Fourier transform solver to compute the mechanical fields. The original simulation strategy was modified to include straight dislocation segments by means of the field dislocation mechanics method and was applied to simulate the interaction of an edge dislocation with a "],["redundancy assumption claims large programs contain the seeds of their own repair. but most red","Sorting and Transforming Program Repair Ingredients via Deep Learning Code Similarities","summarize: In the field of automated program repair, the redundancy assumption claims large programs contain the seeds of their own repair. However, most redundancy-based program repair techniques do not reason about the repair ingredients---the code that is reused to craft a patch. We aim to reason about the repair ingredients by using code similarities to prioritize and transform statements in a codebase for patch generation. Our approach, DeepRepair, relies on deep learning to reason about code similarities. Code fragments at well-defined levels of granularity in a codebase can be sorted according to their similarity to suspicious elements and statements can be transformed by mapping out-of-scope identifiers to similar identifiers in scope. We examined these new search strategies for patch generation with respect to effectiveness from the viewpoint of a software maintainer. Our comparative experiments were executed on six open-source Java projects including 374 buggy program revisions and consisted of 19,949 trials spanning 2,616 days of computation time. DeepRepair's search strategy using code similarities generally found compilable ingredients faster than the baseline, jGenProg, but this improvement neither yielded test-adequate patches in fewer attempts nor found significantly more patches than the baseline. Although the patch counts were not statistically different, there were notable differences between the nature of DeepRepair patches and baseline patches. The results demonstrate that our learning-based approach finds patches that cannot be found by existing redundancy-based repair techniques."],["we introduce two new classes of directional distributions that extend the rotationally symmetric class","On optimal tests for rotational symmetry against new classes of hyperspherical distributions","summarize: Motivated by the central role played by rotationally symmetric distributions in directional statistics, we consider the problem of testing rotational symmetry on the hypersphere. We adopt a semiparametric approach and tackle problems where the location of the symmetry axis is either specified or unspecified. For each problem, we define two tests and study their asymptotic properties under very mild conditions. We introduce two new classes of directional distributions that extend the rotationally symmetric class and are of independent interest. We prove that each test is locally asymptotically maximin, in the Le Cam sense, for one kind of the alternatives given by the new classes of distributions, both for specified and unspecified symmetry axis. The tests, aimed to detect location-like and scatter-like alternatives, are combined into convenient hybrid tests that are consistent against both alternatives. We perform Monte Carlo experiments that illustrate the finite-sample performances of the proposed tests and their agreement with the asymptotic results. Finally, the practical relevance of our tests is illustrated on a real data application from astronomy. The R package rotasym implements the proposed tests and allows practitioners to reproduce the data application."],["neural networks have shown significant success in a wide range of AI tasks. large-scale","Differentially Private Model Publishing for Deep Learning","summarize: Deep learning techniques based on neural networks have shown significant success in a wide range of AI tasks. Large-scale training datasets are one of the critical factors for their success. However, when the training datasets are crowdsourced from individuals and contain sensitive information, the model parameters may encode private information and bear the risks of privacy leakage. The recent growing trend of the sharing and publishing of pre-trained models further aggravates such privacy risks. To tackle this problem, we propose a differentially private approach for training neural networks. Our approach includes several new techniques for optimizing both privacy loss and model accuracy. We employ a generalization of differential privacy called concentrated differential privacy, with both a formal and refined privacy loss analysis on two different data batching methods. We implement a dynamic privacy budget allocator over the course of training to improve model accuracy. Extensive experiments demonstrate that our approach effectively improves privacy loss accounting, training efficiency and model quality under a given privacy budget."],["we propose a novel and principled method to learn a nonparametric graph model called","Learning Graphons via Structured Gromov-Wasserstein Barycenters","summarize: We propose a novel and principled method to learn a nonparametric graph model called graphon, which is defined in an infinite-dimensional space and represents arbitrary-size graphs. Based on the weak regularity lemma from the theory of graphons, we leverage a step function to approximate a graphon. We show that the cut distance of graphons can be relaxed to the Gromov-Wasserstein distance of their step functions. Accordingly, given a set of graphs generated by an underlying graphon, we learn the corresponding step function as the Gromov-Wasserstein barycenter of the given graphs. Furthermore, we develop several enhancements and extensions of the basic algorithm, "],["additive partially linear model combines nonparametric regression with the parsimony of regression models","Sparse Model Identification and Learning for Ultra-high-dimensional Additive Partially Linear Models","summarize: The additive partially linear model combines the flexibility of nonparametric regression with the parsimony of regression models, and has been widely used as a popular tool in multivariate nonparametric regression to alleviate the curse of dimensionality. A natural question raised in practice is the choice of structure in the nonparametric part, that is, whether the continuous covariates enter into the model in linear or nonparametric form. In this paper, we present a comprehensive framework for simultaneous sparse model identification and learning for ultra-high-dimensional APLMs where both the linear and nonparametric components are possibly larger than the sample size. We propose a fast and efficient two-stage procedure. In the first stage, we decompose the nonparametric functions into a linear part and a nonlinear part. The nonlinear functions are approximated by constant spline bases, and a triple penalization procedure is proposed to select nonzero components using adaptive group LASSO. In the second stage, we refit data with selected covariates using higher order polynomial splines, and apply spline-backfitted local-linear smoothing to obtain asymptotic normality for the estimators. The procedure is shown to be consistent for model structure identification. It can identify zero, linear, and nonlinear components correctly and efficiently. Inference can be made on both linear coefficients and nonparametric functions. We conduct simulation studies to evaluate the performance of the method and apply the proposed method to a dataset on the Shoot Apical Meristem of maize genotypes for illustration."],["silicon wafer manufacturers are experimenting with new technology. the demand for longer mono-sili","Nonlinear dynamics in the flexible shaft rotating-lifting system of silicon crystal puller using Czochralski method","summarize: Silicon crystal puller is a key equipment in silicon wafer manufacture, which is, in turn, the base material for the most currently used integrated circuit chips. With the development of the techniques, the demand for longer mono-silicon crystal rod with larger diameter is continuously increasing in order to reduce the manufacture time and the price of the wafer. This demand calls for larger SCP with increasing height, however, it causes serious swing phenomenon of the crystal seed. The strong swing of the seed causes difficulty in the solidification and increases the risk of mono-silicon growth failure.The main aim of this paper is to analyze the nonlinear dynamics in the FSRL system of the SCP. A mathematical model for the swing motion of the FSRL system is derived. The influence of relevant parameters, such as system damping, excitation amplitude and rotation speed, on the stability and the responses of the system are analyzed. The stability of the equilibrium, bifurcation and chaotic motion are demonstrated, which are often observed in practical situations. Melnikov method is used to derive the possible parameter region that leads to chaotic motion. Three routes to chaos are identified in the FSRL system, including period doubling, symmetry-breaking bifurcation and interior crisis. The work in this paper explains the complex dynamics in the FSRL system of the SCP, which will be helpful for the SCP designers in order to avoid the swing phenomenon in the SCP."],["linear discrete ill-posed problem for large-scale linear discrete ill-posed","Approximation Accuracy of the Krylov Subspaces for Linear Discrete Ill-Posed Problems","summarize: For the large-scale linear discrete ill-posed problem "],["we present the novel approach to mathematical modeling of information processes in biosystems. it explore","Quantum-like modeling in biology with open quantum systems and instruments","summarize: We present the novel approach to mathematical modeling of information processes in biosystems. It explores the mathematical formalism and methodology of quantum theory, especially quantum measurement theory. This approach is known as and it should be distinguished from study of genuine quantum physical processes in biosystems . It is based on quantum information representation of biosystem's state and modeling its dynamics in the framework of theory of open quantum systems. This paper starts with the non-physicist friendly presentation of quantum measurement theory, from the original von Neumann formulation to modern theory of quantum instruments. Then, latter is applied to model combinations of cognitive effects and gene regulation of glucose\/lactose metabolism in Escherichia coli bacterium. The most general construction of quantum instruments is based on the scheme of indirect measurement, in that measurement apparatus plays the role of the environment for a biosystem. The biological essence of this scheme is illustrated by quantum formalization of Helmholtz sensation-perception theory. Then we move to open systems dynamics and consider quantum master equation, with concentrating on quantum Markov processes. In this framework, we model functioning of biological functions such as psychological functions and epigenetic mutation."],["the balance function measures the correlation between opposite sign charge pairs. the study of the balance function","Reaction Plane and Beam Energy Dependence Of The Balance Function at RHIC","summarize: The balance function, which measures the correlation between opposite sign charge pairs, is sensitive to the mechanisms of charge formation and the subsequent relative diffusion of the balancing charges. The study of the balance function can provide information about charge creation time as well as the subsequent collective behavior of particles. In this paper, we present a reaction-plane-dependent balance function study for Au+Au collisions at "],["this review focuses on the physics of Gamma Ray Bursts probed","Radio Afterglows of Gamma Ray Bursts","summarize: This review focuses on the physics of Gamma Ray Bursts probed through their radio afterglow emission. Even though radio band is the least explored of the afterglow spectrum, it has played an important role in the progress of GRB physics, specifically in confirming the hypothesized relativistic effects. Currently radio astronomy is in the beginning of a revolution. The high sensitive Square Kilometer Array is being planned, its precursors and pathfinders are about to be operational, and several existing instruments are undergoing upgradation. Thus, the afterglow detection statistics and results from follow up programs are expected to improve in the coming years. We list a few avenues unique to radio band which if explored to full potential have the promise to greatly contribute to the future of GRB physics."],["review of current state of the art in remote sensing based monitoring of forest disturbances and","Methods for Mapping Forest Disturbance and Degradation from Optical Earth Observation Data: a Review","summarize: Purpose of review: This paper presents a review of the current state of the art in remote sensing based monitoring of forest disturbances and forest degradation from optical Earth Observation data. Part one comprises an overview of currently available optical remote sensing sensors, which can be used for forest disturbance and degradation mapping. Part two reviews the two main categories of existing approaches: classical image-to-image change detection and time series analysis. Recent findings: With the launch of the Sentinel-2a satellite and available Landsat imagery, time series analysis has become the most promising but also most demanding category of degradation mapping approaches. Four time series classification methods are distinguished. The methods are explained and their benefits and drawbacks are discussed. A separate chapter presents a number of recent forest degradation mapping studies for two different ecosystems: temperate forests with a geographical focus on Europe and tropical forests with a geographical focus on Africa. Summary: The review revealed that a wide variety of methods for the detection of forest degradation is already available. Today, the main challenge is to transfer these approaches to high resolution time series data from multiple sensors. Future research should also focus on the classification of disturbance types and the development of robust up-scalable methods to enable near real time disturbance mapping in support of operational reactive measures."],["instrumental scenario has also attracted increasing attention in quantum physics. it allows the detection of","Exclusivity graph approach to Instrumental inequalities","summarize: Instrumental variables allow the estimation of cause and effect relations even in presence of unobserved latent factors, thus providing a powerful tool for any science wherein causal inference plays an important role. More recently, the instrumental scenario has also attracted increasing attention in quantum physics, since it is related to the seminal Bell's theorem and in fact allows the detection of even stronger quantum effects, thus enhancing our current capabilities to process information and becoming a valuable tool in quantum cryptography. In this work, we further explore this bridge between causality and quantum theory and apply a technique, originally developed in the field of quantum foundations, to express the constraints implied by causal relations in the language of graph theory. This new approach can be applied to any causal model containing a latent variable. Here, by focusing on the instrumental scenario, it allows us to easily reproduce known results as well as obtain new ones and gain new insights on the connections and differences between the instrumental and the Bell scenarios."],["dynamic regime literature aims to map characteristics of a unit to a action tailored to maximize","Estimation of Personalized Effects Associated With Causal Pathways","summarize: The goal of personalized decision making is to map a unit's characteristics to an action tailored to maximize the expected outcome for that unit. Obtaining high-quality mappings of this type is the goal of the dynamic regime literature. In healthcare settings, optimizing policies with respect to a particular causal pathway may be of interest as well. For example, we may wish to maximize the chemical effect of a drug given data from an observational study where the chemical effect of the drug on the outcome is entangled with the indirect effect mediated by differential adherence. In such cases, we may wish to optimize the direct effect of a drug, while keeping the indirect effect to that of some reference treatment. shows how to combine mediation analysis and dynamic treatment regime ideas to defines policies associated with causal pathways and counterfactual responses to these policies. In this paper, we derive a variety of methods for learning high quality policies of this type from data, in a causal model corresponding to a longitudinal setting of practical importance. We illustrate our methods via a dataset of HIV patients undergoing therapy, gathered in the Nigerian PEPFAR program."],["recent studies have emphasized the important role that a shape deformability of scal","Kink-antikink scattering-induced breathing bound states and oscillons in a parametrized ","summarize: Recent studies have emphasized the important role that a shape deformability of scalar-field models pertaining to the same class with the standard "],["decomposition of a non-empty simple graph.","Coloring decompositions of complete geometric graphs","summarize: A decomposition of a non-empty simple graph "],["we investigate the electric and thermoelectric transport coefficients of nanocomposites using the non-","Thermoelectric power factor in nanostructured materials with randomized nanoinclusions","summarize: We investigate the electric and thermoelectric transport coefficients of nanocomposites using the Non-Equilibrium Greens Function method, which can accurately capture the details of geometry and disorder in these structures. We consider here two dimensional channels with embedded nanoinclusions modelled as potential barriers of cylindrical shape and height VB. We investigate the effect of randomness of the NIs on the thermoelectric power factor by varying the positions, diameter, and heights of the barriers according to a Gaussian probability distribution. We find that the power factor shows indications of tolerance to variations in the parameters of the NIs when the Fermi level is placed into the bands and VB approx. kBT. These results could be experimentally relevant in the design of nanocomposites for thermoelectric applications."],["the black hole candidate in NGC 6946 was a yellow hypergiant.","Comments on the Progenitor of NGC6946-BH1","summarize: The broad-band colors of the progenitor of the black hole candidate in NGC 6946 suggest that it was a yellow hypergiant on a post-red supergiant track when the core collapse occurred."],["a new pipeline is a data-driven, learnable pipeline. the learned pipeline","Learning beamforming in ultrasound imaging","summarize: Medical ultrasound is a widespread imaging modality owing its popularity to cost efficiency, portability, speed, and lack of harmful ionizing radiation. In this paper, we demonstrate that replacing the traditional ultrasound processing pipeline with a data-driven, learnable counterpart leads to significant improvement in image quality. Moreover, we demonstrate that greater improvement can be achieved through a learning-based design of the transmitted beam patterns simultaneously with learning an image reconstruction pipeline. We evaluate our method on an in-vivo first-harmonic cardiac ultrasound dataset acquired from volunteers and demonstrate the significance of the learned pipeline and transmit beam patterns on the image quality when compared to standard transmit and receive beamformers used in high frame-rate US imaging. We believe that the presented methodology provides a fundamentally different perspective on the classical problem of ultrasound beam pattern design."],["real-world tasks would benefit from using multiagent reinforcement learning algorithms. if agents","R-MADDPG for Partially Observable Environments and Limited Communication","summarize: There are several real-world tasks that would benefit from applying multiagent reinforcement learning algorithms, including the coordination among self-driving cars. The real world has challenging conditions for multiagent learning systems, such as its partial observable and nonstationary nature. Moreover, if agents must share a limited resource they must all learn how to coordinate resource use. This paper introduces a deep recurrent multiagent actor-critic framework for handling multiagent coordination under partial observable set-tings and limited communication. We investigate recurrency effects on performance and communication use of a team of agents. We demonstrate that the resulting framework learns time dependencies for sharing missing observations, handling resource limitations, and developing different communication patterns among agents."],["we celebrate the 50th anniversary of one of the most classical models in probability theory. in","50 years of first passage percolation","summarize: We celebrate the 50th anniversary of one the most classical models in probability theory. In this survey, we describe the main results of first passage percolation, paying special attention to the recent burst of advances of the past 5 years. The purpose of these notes is twofold. In the first chapters, we give self-contained proofs of seminal results obtained in the '80s and '90s on limit shapes and geodesics, while covering the state of the art of these questions. Second, aside from these classical results, we discuss recent perspectives and directions including the connection between Busemann functions and geodesics, the proof of sublinear variance under 2+log moments of passage times and the role of growth and competition models. We also provide a collection of open questions, hoping to solve them before the 100th birthday."],["the optimal strategy is derived analytically under the worst-case scenario with or without derivative trading","Robust portfolio optimization with multi-factor stochastic volatility","summarize: This paper studies a robust portfolio optimization problem under the multi-factor volatility model introduced by Christoffersen et al. . The optimal strategy is derived analytically under the worst-case scenario with or without derivative trading. To illustrate the effects of ambiguity, we compare our optimal robust strategy with some strategies that ignore the information of uncertainty, and provide the corresponding welfare analysis. The effects of derivative trading to the optimal portfolio selection are also discussed by considering alternative strategies. Our study is further extended to the cases with jump risks in asset price and correlated volatility factors, respectively. Numerical experiments are provided to demonstrate the behavior of the optimal portfolio and utility loss."],["a novel recovery algorithm is proposed to leverage the correlation from multiple heterogeneous signals","Heterogeneous Networked Data Recovery from Compressive Measurements Using a Copula Prior","summarize: Large-scale data collection by means of wireless sensor network and internet-of-things technology poses various challenges in view of the limitations in transmission, computation, and energy resources of the associated wireless devices. Compressive data gathering based on compressed sensing has been proven a well-suited solution to the problem. Existing designs exploit the spatiotemporal correlations among data collected by a specific sensing modality. However, many applications, such as environmental monitoring, involve collecting heterogeneous data that are intrinsically correlated. In this study, we propose to leverage the correlation from multiple heterogeneous signals when recovering the data from compressive measurements. To this end, we propose a novel recovery algorithm---built upon belief-propagation principles---that leverages correlated information from multiple heterogeneous signals. To efficiently capture the statistical dependencies among diverse sensor data, the proposed algorithm uses the statistical model of copula functions. Experiments with heterogeneous air-pollution sensor measurements show that the proposed design provides significant performance improvements against state-of-the-art compressive data gathering and recovery schemes that use classical compressed sensing, compressed sensing with side information, and distributed compressed sensing."],["we leverage longitudinal data from 56 conspiracy communities on Reddit. we first identify 30K","What Makes People Join Conspiracy Communities?: Role of Social Factors in Conspiracy Engagement","summarize: Widespread conspiracy theories, like those motivating anti-vaccination attitudes or climate change denial, propel collective action and bear society-wide consequences. Yet, empirical research has largely studied conspiracy theory adoption as an individual pursuit, rather than as a socially mediated process. What makes users join communities endorsing and spreading conspiracy theories? We leverage longitudinal data from 56 conspiracy communities on Reddit to compare individual and social factors determining which users join the communities. Using a quasi-experimental approach, we first identify 30K future conspiracists- and 30K matched non-conspiracists-. We then provide empirical evidence of importance of social factors across six dimensions relative to the individual factors by analyzing 6 million Reddit comments and posts. Specifically in social factors, we find that dyadic interactions with members of the conspiracy communities and marginalization outside of the conspiracy communities, are the most important social precursors to conspiracy joining-even outperforming individual factor baselines. Our results offer quantitative backing to understand social processes and echo chamber effects in conspiratorial engagement, with important implications for democratic institutions and online communities."],["a new dynamic of scientific articles dissemination was initiated. the view profiles along time tend to","Classification of abrupt changes along viewing profiles of scientific articles","summarize: With the expansion of electronic publishing, a new dynamics of scientific articles dissemination was initiated. Nowadays, many works are widely disseminated even before publication, in the form of preprints. Another important new element concerns the views of published articles. Thanks to the availability of respective data by some journals, such as PLoS ONE, it became possible to develop investigations on how scientific works are viewed along time, often before the first citations appear. This provides the main theme of the present work. More specifically, our research was motivated by preliminary observations that the view profiles along time tend to present a piecewise linear nature. A methodology was then delineated in order to identify the main segments in the view profiles, which allowed several related measurements to be derived. In particular, we focused on the inclination and length of each subsequent segment. Basic statistics indicated that the inclination can vary substantially along subsequent segments, while the segment lengths resulted more stable. Complementary joint statistics analysis, considering pairwise correlations, provided further information about the properties of the views. In order to better understand the view profiles, we performed respective multivariate statistical analysis, including principal component analysis and hierarchical clustering. The results suggest that a portion of the polygonal views are organized into clusters or groups. These groups were characterized in terms of prototypes indicating the relative increase or decrease along subsequent segments. Four respective distinct models were then developed for representing the observed segments. It was found that models incorporating joint dependencies between the properties of the segments provided the most accurate results among the considered alternatives."],["G is an almost-connected Lie group or a discrete group. we establish","Positive Scalar Curvature and Poincare Duality for Proper Actions","summarize: For G an almost-connected Lie group, we study G-equivariant index theory for proper co-compact actions with various applications, including obstructions to and existence of G-invariant Riemannian metrics of positive scalar curvature. We prove a rigidity result for almost-complex manifolds, generalising Hattori's results, and an analogue of Petrie's conjecture. When G is an almost-connected Lie group or a discrete group, we establish Poincare duality between G-equivariant K-homology and K-theory, observing that Poincare duality does not necessarily hold for general G."],["the nonzero level sets of a homogeneous polynomial are proper","Functions dividing their Hessian determinants and affine spheres","summarize: The nonzero level sets of a homogeneous, logarithmically homogeneous, or translationally homogeneous function are affine spheres if and only if the Hessian determinant of the function is a multiple of a power or an exponential of the function. In particular, the nonzero level sets of a homogeneous polynomial are proper affine spheres if some power of it equals a nonzero multiple of its Hessian determinant. The relative invariants of real forms of regular irreducible prehomogeneous vector spaces yield many such polynomials which are moreover irreducible. For example, the nonzero level sets of the Cayley hyperdeterminant are affine spheres."],["we present results of high-resolution numerical simulations of compressible 2D turbul","Energy Transfer and Spectra in Simulations of Two-dimensional Compressible Turbulence","summarize: We present results of high-resolution numerical simulations of compressible 2D turbulence forced at intermediate spatial scales with a solenoidal white-in-time external acceleration. A case with an isothermal equation of state, low energy injection rate, and turbulent Mach number "],["the restricted Melikyan algebra of dimension 125 is a deformation of a","Melikyan algebra is a deformation of a Poisson algebra","summarize: We prove, using computer, that the restricted Melikyan algebra of dimension 125 is a deformation of a Poisson algebra."],["proposed energy trading scheme is a canonical coalition game. it is used to establish","Peer-to-Peer Energy Trading with Sustainable User Participation: A Game Theoretic Approach","summarize: This paper explores the feasibility of social cooperation between prosumers within an energy network in establishing their sustainable participation in peer-to-peer energy trading. In particular, a canonical coalition game is utilized to propose a P2P energy trading scheme, in which a set of participating prosumers form a coalition group to trade their energy, if there is any, with one another. By exploring the concept of the core of the designed CCG framework, the mid-market rate is utilized as a pricing mechanism of the proposed P2P trading to confirm the stability of the coalition as well as to guarantee the benefit to the prosumers for forming the social coalition. The paper further introduces the motivational psychology models that are relevant to the proposed P2P scheme and it is shown that the outcomes of proposed P2P energy trading scheme satisfy the discussed models. Consequently, it is proven that the proposed scheme is consumer-centric that has the potential to corroborate sustainable prosumers participation in P2P energy trading. Finally, some numerical examples are provided to demonstrate the beneficial properties of the proposed scheme."],["RankME significantly improves reliability and consistency of human ratings. ranked multiple NLG","RankME: Reliable Human Ratings for Natural Language Generation","summarize: Human evaluation for natural language generation often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method , which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems."],["the use of multivalued controls derived from a maximum monotone operator is studied","Robust Output Regulation of Linear Passive Systems with Multivalued Upper Semicontinuous Controls","summarize: The use of multivalued controls derived from a special maximal monotone operator are studied in this note. Starting with a strictly passive linear system a multivalued control law is derived, ensuring regulation of the output to a desired value. The methodology used falls in a passivity-based control context, where we study how the multivalued control affects the dissipation equation of the closed-loop system, from which we derive its robustness properties. Finally, some numerical examples together with implementation issues are presented to support the main result."],["we identify core challenges and sources of errors associated with sensing attention on mobile devices in the wild","How far are we from quantifying visual attention in mobile HCI?","summarize: With an ever-increasing number of mobile devices competing for our attention, quantifying when, how often, or for how long users visually attend to their devices has emerged as a core challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying visual attention during everyday mobile interactions. We identify core challenges and sources of errors associated with sensing attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head pose estimation, and the need for accurate gaze estimation. Based on this analysis, we propose future research directions and discuss how eye contact detection represents the foundation for exciting new applications towards next-generation pervasive attentive user interfaces."],["the BC score of a vertex is proportional to the number of all-pair","Algorithms and Heuristics for Scalable Betweenness Centrality Computation on Multi-GPU Systems","summarize: Betweenness Centrality is steadily growing in popularity as a metrics of the influence of a vertex in a graph. The BC score of a vertex is proportional to the number of all-pairs-shortest-paths passing through it. However, complete and exact BC computation for a large-scale graph is an extraordinary challenge that requires high performance computing techniques to provide results in a reasonable amount of time. Our approach combines bi-dimensional decomposition of the graph and multi-level parallelism together with a suitable data-thread mapping that overcomes most of the difficulties caused by the irregularity of the computation on GPUs. Furthermore, we propose novel heuristics which exploit the topology information of the graph in order to reduce time and space requirements of BC computation. Experimental results on synthetic and real-world graphs show that the proposed techniques allow the BC computation of graphs which are too large to fit in the memory of a single computational node along with a significant reduction of the computing time."],["a new analysis of basic Couette flow is based on an Action Principle for compressible","Stability analysis for cylindrical Couette flow of compressible fluids","summarize: A new analysis of basic Couette flow, is based on an Action Principle for compressible fluids, with a Hamiltonian as well as a kinetic potential. An effective criterion for stability recognizes the tensile strength of water. This interpretation relates the problem to capillary action and to metastable configurations . We calculate the pressure and density profiles and find that the first instability of basic Couette flow is localized near the bubble point. This theoretical prediction has been confirmed by recent experiments. The theory is the result of merging the two versions of classical hydrodynamics, as advocated by Landau for superfluid Helium II, but here applied to fluids in general, in accord with a widely held opinion concerning superfluidity. In this paper two-flow dynamics is created by merging two actions, not by choosing between them, nor by combining the two vector fields as in the Navier-Stokes equation . At rest, as contributions to the mass flow they cancel, but a non-zero kinetic energy and kinetic potential as well as non-zero angular momentum remain, manifest as liquid tension, as is well known to exist by observation of the meniscus and configurations with negative pressure. . This theory gives a very satisfactory characterization of the limit of stability of the most basic Couette flow. The inclusion of a vector field that is not a gradient has the additional affect of introducing spin, which explains a most perplexing experimental discovery: the ability of frozen Helium to remember its angular momentum."],["AA is used by dozens of researchers and software developers. it is used by different","The Algorithmic-Autoregulation Methodology and Software: a collective focus on self-transparency","summarize: There are numerous efforts to achieve a lightweight and systematic account of what is done by a group and its individuals. The Algorithmic-Autoregulation is a special case, in which a technical community embraced the challenge of registering their own dedication for sharing processes, self-transparency, and documenting the efforts. AA is used since June\/2011 by dozens of researchers and software developers, with the support of different software gadgets and for distinct tasks. This article describes these implementations and statistics of their usage including expected natural properties and ontological formalisms which eases comparative analysis and furthers integration."],["PFA uses the correlation between filter responses within network layers. it produces a compressed model","Filter Distillation for Network Compression","summarize: In this paper we introduce Principal Filter Analysis , an easy to use and effective method for neural network compression. PFA exploits the correlation between filter responses within network layers to recommend a smaller network that maintain as much as possible the accuracy of the full model. We propose two algorithms: the first allows users to target compression to specific network property, such as number of trainable variable , and produces a compressed model that satisfies the requested property while preserving the maximum amount of spectral energy in the responses of each layer, while the second is a parameter-free heuristic that selects the compression used at each layer by trying to mimic an ideal set of uncorrelated responses. Since PFA compresses networks based on the correlation of their responses we show in our experiments that it gains the additional flexibility of adapting each architecture to a specific domain while compressing. PFA is evaluated against several architectures and datasets, and shows considerable compression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10, CIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x with an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. Our tests show that PFA is competitive with state-of-the-art approaches while removing adoption barriers thanks to its practical implementation, intuitive philosophy and ease of use."],["we demonstrate up to 12 km, 56 Gb\/s DMT transmission using high-speed","56 Gb\/s DMT Transmission with VCSELs in 1.5 um Wavelength Range over up to 12 km for DWDM Intra-Data Center Connects","summarize: We demonstrate up to 12 km, 56 Gb\/s DMT transmission using high-speed VCSELs in the 1.5 um wavelength range for future 400Gb\/s intra-data center connects, enabled by vestigial sideband filtering of the transmit signal."],["model-mismatch problem for model-based subspace channel tracking in the correlated underwater","Dynamic Underwater Acoustic Channel Tracking for Correlated Rapidly Time-varying Channels","summarize: In this work, we focus on the model-mismatch problem for model-based subspace channel tracking in the correlated underwater acoustic channel. A model based on the underwater acoustic channel's correlation can be used as the state-space model in the Kalman filter to improve the underwater acoustic channel tracking compared that without a model. Even though the data support the assumption that the model is slow-varying and uncorrelated to some degree, to improve the tracking performance further, we can not ignore the model-mismatch problem because most channel models encounter this problem in the underwater acoustic channel. Therefore, in this work, we provide a dynamic time-variant state-space model for underwater acoustic channel tracking. This model is tolerant to the slight correlation after decorrelation. Moreover, a forward-backward Kalman filter is combined to further improve the tracking performance. The performance of our proposed algorithm is demonstrated with the same at-sea data as that used for conventional channel tracking. Compared with the conventional algorithms, the proposed algorithm shows significant improvement, especially in rough sea conditions in which the channels are fast-varying."],["the USP has a parameter that determines the shape of its density function. but","Frequency Coverage Properties of a Uniform Shrinkage Prior Distribution","summarize: A uniform shrinkage prior distribution on the unknown variance component of a random-effects model is known to produce good frequency properties. The USP has a parameter that determines the shape of its density function, but it has been neglected whether the USP can maintain such good frequency properties regardless of the choice for the shape parameter. We investigate which choice for the shape parameter of the USP produces Bayesian interval estimates of random effects that meet their nominal confidence levels better than several existent choices in the literature. Using univariate and multivariate Gaussian hierarchical models, we empirically show that the USP can achieve its best frequency properties when its shape parameter makes the USP behave similarly to an improper flat prior distribution on the unknown variance component."],["the French writer colette has a question about the majesty of ce qui fin","Historical Developments of the Theory of Transcendence and Algebraic Independence","summarize: Qu'est la majest de ce qui finit, auprs des dparts titubants, des dsordres de l'aurore?. Let this question from the French writer Colette guide us to walk through the promising beginnings, fascinating evolutions and results, challenging open problems and perspectives of an exciting theory that is far from revealing all its secrets."],["proposed energy trading scheme is a canonical coalition game. it is used to establish","Peer-to-Peer Energy Trading with Sustainable User Participation: A Game Theoretic Approach","summarize: This paper explores the feasibility of social cooperation between prosumers within an energy network in establishing their sustainable participation in peer-to-peer energy trading. In particular, a canonical coalition game is utilized to propose a P2P energy trading scheme, in which a set of participating prosumers form a coalition group to trade their energy, if there is any, with one another. By exploring the concept of the core of the designed CCG framework, the mid-market rate is utilized as a pricing mechanism of the proposed P2P trading to confirm the stability of the coalition as well as to guarantee the benefit to the prosumers for forming the social coalition. The paper further introduces the motivational psychology models that are relevant to the proposed P2P scheme and it is shown that the outcomes of proposed P2P energy trading scheme satisfy the discussed models. Consequently, it is proven that the proposed scheme is consumer-centric that has the potential to corroborate sustainable prosumers participation in P2P energy trading. Finally, some numerical examples are provided to demonstrate the beneficial properties of the proposed scheme."],["polarization theory solves this problem by providing a necessary and sufficient condition for a","Ergodic Theory Meets Polarization. I: An Ergodic Theory for Binary Operations","summarize: An open problem in polarization theory is to determine the binary operations that always lead to polarization when they are used in Arkan style constructions. This paper, which is presented in two parts, solves this problem by providing a necessary and sufficient condition for a binary operation to be polarizing. This part of the paper introduces the mathematical framework that we will use in the second part to characterize the polarizing operations. We define uniformity preserving, irreducible, ergodic and strongly ergodic operations and we study their properties. The concepts of a stable partition and the residue of a stable partition are introduced. We show that an ergodic operation is strongly ergodic if and only if all its stable partitions are their own residues. We also study the products of binary operations and the structure of their stable partitions. We show that the product of a sequence of binary operations is strongly ergodic if and only if all the operations in the sequence are strongly ergodic. In the second part of the paper, we provide a foundation of polarization theory based on the ergodic theory of binary operations that we develop in this part."],["X-ray variability characteristics of the Narrow Line Seyfert 1 galaxy RE J","Long term X-ray variability characteristics of the narrow-line Seyfert 1 galaxy RE~J1034+396","summarize: We present the results of our study of the long term X-ray variability characteristics of the Narrow Line Seyfert 1 galaxy RE J1034+396. We use data obtained from the AstroSat satellite along with the light curves obtained from XMM-Newton and Swift-XRT. We use the 0.3 - 7.0 keV and 3 - 20 keV data, respectively, from the SXT and the LAXPC of AstroSat. The X-ray spectra in the 0.3 - 20 keV region are well fit with a model consisting of a power-law and a soft excess described by a thermal-Compton emission with a large optical depth, consistent with the earlier reported results. We have examined the X-ray light curves in the soft and hard X-ray bands of SXT and LAXPC, respectively, and find that the variability is slightly larger in the hard band. To investigate the variability characteristics of this source at different time scales, we have used X-ray light curves obtained from XMM-Newton data and Swift-XRT data and find that there are evidences to suggest that the variability sharply increases at longer time scales. We argue that the mass of the black hole in RE J1034+396 is likely to be "],["to every half-translation surface, we associate a saddle connection graph. we prove","Affine equivalence and saddle connection graphs of half-translation surfaces","summarize: To every half-translation surface, we associate a saddle connection graph, which is a subgraph of the arc graph. We prove that every isomorphism between two saddle connection graphs is induced by an affine homeomorphism between the underlying half-translation surfaces. We also investigate the automorphism group of the saddle connection graph, and the corresponding quotient graph."],["the SEROA signal is proportional to the magnetic polarizability of the substrate","Strongly Enhanced Raman Optical Activity in Molecules by Magnetic Response of Nanoparticles","summarize: An analytical theory for the surface-enhanced Raman optical activity with the magnetic response of the substrate particle has been presented. We have demonstrated that the SEROA signal is proportional to the magnetic polarizability of the substrate particle, which can be significantly enhanced due to the existence of the magnetic response. At the same time, a large circular intensity difference for the SEROA can also be achieved in the presence of the magnetic response. Taking Si nanoparticles as examples, we have found that the CID enhanced by a Si nanoparticle is 10 times larger than that of Au. Furthermore, when the molecule is located in the hotspot of a Si dimer, CID can be 60 times larger. The phenomena originate from large magnetic fields concentrated near the nanoparticle and boosted magnetic dipole emission of the molecule. The symmetric breaking of the electric fields caused by the magnetic dipole response of the nanoparticle also plays an important role. Our findings provide a new way to tailor theRaman optical activity by designing metamaterials with the strong magnetic response."],["game-theoretic calculations serve as a useful tool for assisting cyber wargaming","Game-Theoretic Model and Experimental Investigation of Cyber Wargaming","summarize: We demonstrate that game-theoretic calculations serve as a useful tool for assisting cyber wargaming teams in identifying useful strategies. We note a significant similarity between formulating cyber wargaming strategies and the methodology known in military practice as Course of Action generation. For scenarios in which the attacker must penetrate multiple layers in a defense-in-depth security configuration, an accounting of attacker and defender costs and penetration probabilities provides cost-utility payoff matrices and penetration probability matrices. These can be used as decision tools by both the defender and attacker. Inspection of the matrices allows players to deduce preferred strategies based on game-theoretical equilibrium solutions. The matrices also help in analyzing anticipated effects of potential human-based choices of wargame strategies and counter-strategies. We describe a mathematical game-theoretic formalism and offer detailed analysis of a table-top cyber wargame executed at the US Army Research Laboratory. Our analysis shows how game-theoretical calculations can provide an effective tool for decision-making during cyber wargames."],["we leverage longitudinal data from 56 conspiracy communities on Reddit. we first identify 30K","What Makes People Join Conspiracy Communities?: Role of Social Factors in Conspiracy Engagement","summarize: Widespread conspiracy theories, like those motivating anti-vaccination attitudes or climate change denial, propel collective action and bear society-wide consequences. Yet, empirical research has largely studied conspiracy theory adoption as an individual pursuit, rather than as a socially mediated process. What makes users join communities endorsing and spreading conspiracy theories? We leverage longitudinal data from 56 conspiracy communities on Reddit to compare individual and social factors determining which users join the communities. Using a quasi-experimental approach, we first identify 30K future conspiracists- and 30K matched non-conspiracists-. We then provide empirical evidence of importance of social factors across six dimensions relative to the individual factors by analyzing 6 million Reddit comments and posts. Specifically in social factors, we find that dyadic interactions with members of the conspiracy communities and marginalization outside of the conspiracy communities, are the most important social precursors to conspiracy joining-even outperforming individual factor baselines. Our results offer quantitative backing to understand social processes and echo chamber effects in conspiratorial engagement, with important implications for democratic institutions and online communities."],["ab initio computed Potential Energy Surface is used to compute the potential energy surface.","Collisional cooling of internal rotation in MgH","summarize: Using the ab initio computed Potential Energy Surface for the electronic interaction of the MgH"],["a mismatch may lead to problems during the optimization process. a natural question is","","summarize: It is well known that neural networks with rectified linear units activation functions are positively scale-invariant. Conventional algorithms like stochastic gradient descent optimize the neural networks in the vector space of weights, which is, however, not positively scale-invariant. This mismatch may lead to problems during the optimization process. Then, a natural question is: \\emph? In this paper, we provide our positive answer to this question. First, we conduct a formal study on the positive scaling operators which forms a transformation group, denoted as "],["study examines the effect of different feedback modalities in a table setting robot assistant for","Feedback modalities for a table setting robot assistant for elder care","summarize: The interaction of Older adults with robots requires effective feedback to keep them aware of the state of the interaction for optimum interaction quality. This study examines the effect of different feedback modalities in a table setting robot assistant for elder care. Two different feedback modalities and their combination were evaluated for three complexity levels. The visual feedback included the use of LEDs and a GUI screen. The auditory feedback included alerts and verbal commands. The results revealed that the quality of interaction was influenced mainly by the feedback modality, and complexity had less influence. The verbal feedback was significantly preferable and increased the involvement of the participants during the experiment. The combination of LED lights and verbal commands increased participants' understanding contributing to the quality of interaction."],["GMRES solver solves a system of constant state, singularly perturbed","Multilevel Schwarz preconditioners for singularly perturbed symmetric reaction-diffusion systems","summarize: We present robust and highly parallel multilevel non-overlapping Schwarz preconditioners, to solve an interior penalty discontinuous Galerkin finite element discretization of a system of steady state, singularly perturbed reaction-diffusion equations with a singular reaction operator, using a GMRES solver. We provide proofs of convergence for the two-level setting and the multigrid V-cycle as well as numerical results for a wide range of regimes."],["semantics of sentiment prediction are a key factor in the prediction. a self-","Enhanced Aspect-Based Sentiment Analysis Models with Progressive Self-supervised Attention Learning","summarize: In aspect-based sentiment analysis , many neural models are equipped with an attention mechanism to quantify the contribution of each context word to sentiment prediction. However, such a mechanism suffers from one drawback: only a few frequent words with sentiment polarities are tended to be taken into consideration for final sentiment decision while abundant infrequent sentiment words are ignored by models. To deal with this issue, we propose a progressive self-supervised attention learning approach for attentional ABSA models. In this approach, we iteratively perform sentiment prediction on all training instances, and continually learn useful attention supervision information in the meantime. During training, at each iteration, context words with the highest impact on sentiment prediction, identified based on their attention weights or gradients, are extracted as words with active\/misleading influence on the correct\/incorrect prediction for each instance. Words extracted in this way are masked for subsequent iterations. To exploit these extracted words for refining ABSA models, we augment the conventional training objective with a regularization term that encourages ABSA models to not only take full advantage of the extracted active context words but also decrease the weights of those misleading words. We integrate the proposed approach into three state-of-the-art neural ABSA models. Experiment results and in-depth analyses show that our approach yields better attention results and significantly enhances the performance of all three models. We release the source code and trained models at https:\/\/github.com\/DeepLearnXMU\/PSSAttention."],["network virtualization suffers from the problem of mapping virtual links and nodes to physical network in","Energy-Aware Virtual Network Embedding Approach for Distributed Cloud","summarize: Network virtualization has caught the attention of many researchers in recent years. It facilitates the process of creating several virtual networks over a single physical network. Despite this advantage, however, network virtualization suffers from the problem of mapping virtual links and nodes to physical network in most efficient way. This problem is called virtual network embedding . Many researches have been proposed in an attempt to solve this problem, which have many optimization aspects, such as improving embedding strategies in a way that preserves energy, reducing embedding cost and increasing embedding revenue. Moreover, some researchers have extended their algorithms to be more compatible with the distributed clouds instead of a single infrastructure provider . This paper proposes energy aware particle swarm optimization algorithm for distributed clouds. This algorithm aims to partition each virtual network request to subgraphs, using the Heavy Clique Matching technique to generate a coarsened graph. Each coarsened node in the coarsened graph is assigned to a suitable data center . Inside each DC, a modified particle swarm optimization algorithm is initiated to find the near optimal solution for the VNE problem. The proposed algorithm was tested and evaluated against existing algorithms using extensive simulations, which shows that the proposed algorithm outperforms other algorithms."],["gyroscopes use the physics of exceptional points. the measurable","Ultrasensitive micro-scale parity-time-symmetric ring laser gyroscope","summarize: We propose a new scheme for ultra-sensitive laser gyroscopes that utilizes the physics of exceptional points. By exploiting the properties of such non-Hermitian degeneracies, we show that the rotation-induced frequency splitting becomes proportional to the square root of the gyration speed- thus enhancing the sensitivity to low angular rotations by orders of magnitudes. In addition, at its maximum sensitivity limit, the measurable spectral splitting is independent of the radius of the rings involved. Our work paves the way towards a new class of ultra-sensitive miniature ring laser gyroscopes on chip."],["a base station needs to mitigate interference to users associated with other coexisting networks in the","Unsupervised frequency clustering algorithm for null space estimation in wideband spectrum sharing networks","summarize: In spectrum sharing networks, a base station needs to mitigate the interference to users associated with other coexisting network in the same band. The BS can achieve this by transmitting its downlink signal in the null space of channels to such users. However, under a wideband scenario, the BS needs to estimate null space matrices using the received signal from such non-cooperative users in each frequency bin where the users are active. To reduce the computational complexity of this operation, we propose a frequency clustering algorithm that exploits the channel correlations among adjacent frequency bins. The proposed algorithm forms clusters of frequency bins with correlated channel vectors without prior knowledge of the channels and obtains a single null space matrix for each cluster. We show that the number of matrices and the number of eigenvalue decompositions required to obtain the null space significantly reduce using the proposed clustering algorithm."],["converged trajectory sets provide dynamical information that is discarded in free energy calculations.","Configurational space discretization and free energy calculation in complex molecular systems","summarize: Trajectories provide dynamical information that is discarded in free energy calculations, for which we sought to design a scheme with the hope of saving cost for generating dynamical information. We first demonstrated that snapshots in a converged trajectory set are associated with implicit conformers that have invariant statistical weight distribution . Based on the thought that infinite number of sets of implicit conformers with ISWD may be created through independent converged trajectory sets, we hypothesized that explicit conformers with ISWD may be constructed for complex molecular systems through systematic increase of conformer fineness, and tested the hypothesis in lipid molecule palmitoyloleoylphosphatidylcholine . Furthermore, when explicit conformers with ISWD were utilized as basic states to define conformational entropy, change of which between two given macrostates was found to be equivalent to change of free energy except a mere difference of a negative temperature factor, and change of enthalpy essentially cancels corresponding change of average intra-conformer entropy. These findings suggest that entropy enthalpy compensation is inherently a local phenomenon in configurational space. By implicitly taking advantage of entropy enthalpy compensation and forgoing all dynamical information, constructing explicit conformers with ISWD and counting thermally accessible number of which for interested end macrostates is likely to be an efficient and reliable alternative end point free energy calculation strategy."],["geodesics in the space of signature analysis of the geodesics in the space","Integrability of geodesics of totally geodesic metrics","summarize: Analysis of the geodesics in the space of signature "],["trajectory extraction and prediction is a method used to predict moving objects. the method is the","AutoTrajectory: Label-free Trajectory Extraction and Prediction from Videos using Dynamic Points","summarize: Current methods for trajectory prediction operate in supervised manners, and therefore require vast quantities of corresponding ground truth data for training. In this paper, we present a novel, label-free algorithm, AutoTrajectory, for trajectory extraction and prediction to use raw videos directly. To better capture the moving objects in videos, we introduce dynamic points. We use them to model dynamic motions by using a forward-backward extractor to keep temporal consistency and using image reconstruction to keep spatial consistency in an unsupervised manner. Then we aggregate dynamic points to instance points, which stand for moving objects such as pedestrians in videos. Finally, we extract trajectories by matching instance points for prediction training. To the best of our knowledge, our method is the first to achieve unsupervised learning of trajectory extraction and prediction. We evaluate the performance on well-known trajectory datasets and show that our method is effective for real-world videos and can use raw videos to further improve the performance of existing models."],["a 9-DoF MEMS IMU is a popular system design. it","Statistical Sensor Fusion of a 9-DoF MEMS IMU for Indoor Navigation","summarize: Sensor fusion of a MEMS IMU with a magnetometer is a popular system design, because such 9-DoF systems are capable of achieving drift-free 3D orientation tracking. However, these systems are often vulnerable to ambient magnetic distortions and lack useful position information; in the absence of external position aiding the dead-reckoned position accuracy from a 9-DoF MEMS IMU deteriorates rapidly due to unmodelled errors. Positioning information is valuable in many satellite-denied geomatics applications . This paper proposes an improved 9-DoF IMU indoor pose tracking method using batch optimization. By adopting a robust in-situ user self-calibration approach to model the systematic errors of the accelerometer, gyroscope, and magnetometer simultaneously in a tightly-coupled post-processed least-squares framework, the accuracy of the estimated trajectory from a 9-DoF MEMS IMU can be improved. Through a combination of relative magnetic measurement updates and a robust weight function, the method is able to tolerate a high level of magnetic distortions. The proposed auto-calibration method was tested in-use under various heterogeneous magnetic field conditions to mimic a person walking with the sensor in their pocket, a person checking their phone, and a person walking with a smartwatch. In these experiments, the presented algorithm improved the in-situ dead-reckoning orientation accuracy by 79.8 - 89.5% and the dead-reckoned positioning accuracy by 72.9 - 92.8%, thus reducing the relative positioning error from metre-level to decimetre-level after ten seconds of integration, without making assumptions about the user's dynamics."],["Einstein Yang-Mills Higgs action with the Higgs field in the","Einstein Yang-Mills Higgs dark energy revisited","summarize: Inspired in the Standard Model of Elementary Particles, the Einstein Yang-Mills Higgs action with the Higgs field in the SU representation was proposed in Class. Quantum Grav. 32 045002 as the element responsible for the dark energy phenomenon. We revisit this action emphasizing in a very important aspect not sufficiently explored in the original work and that substantially changes its conclusions. This aspect is the role that the Yang-Mills Higgs interaction plays at fixing the gauge for the Higgs field, in order to sustain a homogeneous and isotropic background, and at driving the late accelerated expansion of the Universe by moving the Higgs field away of the minimum of its potential and holding it towards an asymptotic finite value. We analyse the dynamical behaviour of this system and supplement this analysis with a numerical solution whose initial conditions are in agreement with the current observed values for the density parameters. This scenario represents a step towards a successful merging of cosmology and well-tested particle physics phenomenology."],["network traffic forensic plays an important role in cybercrime investigation. we used a","Network Traffic Forensics on Firefox Mobile OS: Facebook, Twitter and Telegram as Case Studies","summarize: Development of mobile web-centric OS such as Firefox OS has created new challenges, and opportunities for digital investigators. Network traffic forensic plays an important role in cybercrime investigation to detect subject and object of the crime. In this chapter, we detect and analyze residual network traffic artefacts of Firefox OS in relation to two popular social networking applications and one instant messaging application . We utilized a Firefox OS simulator to generate relevant traffic while all communication data were captured using network monitoring tools. Captured network packets were examined and remnants with forensic value were reported. This paper as the first focused study on mobile Firefox OS network traffic analysis should pave the way for the future research in this direction."],["black hole candidate 1E 1740.7-2942 is one of the strongest hard X","Tandem Swift and INTEGRAL Data to Revisit the Orbital and Superorbital Periods of 1E 1740.7-2942","summarize: The black hole candidate 1E 1740.7-2942 is one of the strongest hard X-ray sources in the Galactic Center region. No counterparts in longer wavelengths have been identified for this object yet. The presence of characteristic timing signatures in the flux history of X-ray sources has been shown to be an important diagnostic tool for the properties of these systems. Using simultaneous data from NASA's Swift and ESA's INTEGRAL missions, we have found two periodic signatures at 12.61 "],["5G is designed to be an essential enabler and a leading infrastructure provider in the communication","Vision-Aided Radio: User Identity Match in Radio and Video Domains Using Machine Learning","summarize: 5G is designed to be an essential enabler and a leading infrastructure provider in the communication technology industry by supporting the demand for the growing data traffic and a variety of services with distinct requirements. The use of deep learning and computer vision tools has the means to increase the environmental awareness of the network with information from visual data. Information extracted via computer vision tools such as user position, movement direction, and speed can be promptly available for the network. However, the network must have a mechanism to match the identity of a user in both visual and radio systems. This mechanism is absent in the present literature. Therefore, we propose a framework to match the information from both visual and radio domains. This is an essential step to practical applications of computer vision tools in communications. We detail the proposed framework training and deployment phases for a presented setup. We carried out practical experiments using data collected in different types of environments. The work compares the use of Deep Neural Network and Random Forest classifiers and shows that the former performed better across all experiments, achieving classification accuracy greater than 99%."],["fractional mean curvature is a fractional mean curvature. fractional","Complete stickiness of nonlocal minimal surfaces for small values of the fractional parameter","summarize: In this paper, we consider the asymptotic behavior of the fractional mean curvature when "],["artificial neural networks are among the most used artificial intelligence methods. we propose sparse evolutionary","Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science","summarize: Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks , we argue that artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible."],["van der Waals heterostructures are twisted bilayers of transition metal dichalc","Broken mirror symmetry in excitonic response of reconstructed domains in twisted MoSe","summarize: Structural engineering of van der Waals heterostructures via stacking and twisting has recently been used to create moir\\'e superlattices, enabling the realization of new optical and electronic properties in solid-state systems. In particular, moir\\'e lattices in twisted bilayers of transition metal dichalcogenides have been shown to lead to exciton trapping, host Mott insulating and superconducting states, and act as unique Hubbard systems whose correlated electronic states can be detected and manipulated optically. Structurally, these twisted heterostructures also feature atomic reconstruction and domain formation. Unfortunately, due to the nanoscale sizes of typical moir\\'e domains, the effects of atomic reconstruction on the electronic and excitonic properties of these heterostructures could not be investigated systematically and have often been ignored. Here, we use near-0"],["the method is exact for all convex quadratic polynomials. the","Pointwise rates of convergence for the Oliker-Prussner method for the Monge-Amp\\`re equation","summarize: We study the Oliker-Prussner method exploiting its geometric nature. We derive discrete stability and continuous dependence estimates in the max-norm by using a discrete Alexandroff estimate and the Brunn-Minkowski inequality. We show that the method is exact for all convex quadratic polynomials provided the underlying set of nodes is translation invariant within the domain; nodes still conform to the domain boundary. This gives a suitable notion of operator consistency which, combined with stability, leads to pointwise rates of convergence for classical and non-classical solutions of the Monge-Amp\\`re equation."],["the famous continuous time random walk model with power law waiting time distribution describes this phenomenon. the","Well-posedness and numerical algorithm for the tempered fractional ordinary differential equations","summarize: Trapped dynamics widely appears in nature, e.g., the motion of particles in viscous cytoplasm. The famous continuous time random walk model with power law waiting time distribution describes this phenomenon. Because of the finite lifetime of biological particles, sometimes it is necessary to temper the power law measure such that the waiting time measure has convergent first moment. Then the time operator of the Fokker-Planck equation corresponding to the CTRW model with tempered waiting time measure is the so-called tempered fractional derivative. This paper focus on discussing the properties of the time tempered fractional derivative, and studying the well-posedness and the Jacobi-predictor-corrector algorithm for the tempered fractional ordinary differential equation. By adjusting the parameter of the proposed algorithm, any desired convergence order can be obtained and the computational cost linearly increases with time. And the effectiveness of the algorithm is numerically confirmed."],["angular momentum of gas in molecular cloud core will slow its collapse.","The role of magnetic fields in the formation of protostellar discs","summarize: Truncated abstract: The formation of a protostellar disc is a natural outcome during the star formation process. As gas in a molecular cloud core collapses under self-gravity, the angular momentum of the gas will slow its collapse on small scales and promote the formation of a protostellar disc. Although the angular momenta of dense star-forming cores remain to be fully characterized observationally, existing data indicates that typical cores have enough angular momenta to form relatively large, rotationally supported discs. However, molecular clouds are observed to be permeated by magnetic fields, which can strongly affect the evolution of angular momentum through magnetic braking. Indeed, in the ideal MHD limit, magnetic braking has been shown to be so efficient as to remove essentially all of the angular momentum of the material close to the forming star such that disc formation is suppressed. This is known as the magnetic braking catastrophe. The catastrophe must be averted in order for the all-important rotationally supported discs to appear, but when and how this happens remains debated. We review the resolutions proposed to date, with emphasis on misalignment, turbulence and especially non-ideal effects. The dissipative non-ideal effects weaken the magnetic field, and the dispersive term redirects it to promote or hinder disc formation. When self-consistently applying non-ideal processes, rotationally supported discs of at least tens of au form, thus preventing the magnetic braking catastrophe. The non-ideal processes are sensitive to the magnetic field strength, cosmic ray ionization rate, and gas and dust grain properties, thus a complete understanding of the host molecular cloud is required. Therefore, the properties of the host molecular cloud -- and especially its magnetic field -- cannot be ignored when numerically modelling the formation and evolution of protostellar discs."],["a new video dataset, called the Miss Universe dataset, is proposed. the results are","Towards Miss Universe Automatic Prediction: The Evening Gown Competition","summarize: Can we predict the winner of Miss Universe after watching how they stride down the catwalk during the evening gown competition? Fashion gurus say they can! In our work, we study this question from the perspective of computer vision. In particular, we want to understand whether existing computer vision approaches can be used to automatically extract the qualities exhibited by the Miss Universe winners during their catwalk. This study can pave the way towards new vision-based applications for the fashion industry. To this end, we propose a novel video dataset, called the Miss Universe dataset, comprising 10 years of the evening gown competition selected between 1996-2010. We further propose two ranking-related problems: Miss Universe Listwise Ranking and Miss Universe Pairwise Ranking. In addition, we also develop an approach that simultaneously addresses the two proposed problems. To describe the videos we employ the recently proposed Stacked Fisher Vectors in conjunction with robust local spatio-temporal features. From our evaluation we found that although the addressed problems are extremely challenging, the proposed system is able to rank the winner in the top 3 best predicted scores for 5 out of 10 Miss Universe competitions."],["experimental observation and analyses of supercritical bifurcation of combustion instabilities triggered by","Inlet temperature driven supercritical bifurcation of combustion instabilities in a lean premixed prevaporized combustor","summarize: The present article reports experimental observation and analyses of a supercritical bifurcation of combustion instabilities triggered by the air inlet temperature . The studies are performed with a pressurised kerosene fuelled Lean Premixed Prevaporized combustor operated under elevated temperature. Unlike some previous studies, starting from an unstable condition of the system, the amplitude of combustion instabilities suddenly decrease when Ta exceeds a critical value of Ta =570 K. When the temperature is lowered back the system returns to being unstable without featuring any hysteresis behaviour, as expected in case of a supercritical bifurcation. The unstable flames feature a periodic axial motion of lift-off and re-ignition, characterized as Helmholtz mode. The phase difference between chemiluminescence and pressure signals is found to increase with Ta, exceeding 90 degrees for temperatures higher than 570 K. A low order network framework is conducted, illustrating that when Ta is increased a stability shift of this mode is predicted at Ta near 570 K, in good agreement with the experimental observations. The impact of Ta on the spray characteristics is also examined, finding that higher Ta promotes fuel evaporation and reduces equivalence ratio fluctuation at the exit of the swirler."],["only lens spaces and prism manifolds admit several Seifert fibrations","On the diffeomorphism type of Seifert fibered spherical 3-orbifolds","summarize: It is well known that, among closed spherical Seifert three-manifolds, only lens spaces and prism manifolds admit several Seifert fibrations which are not equivalent up to diffeomorphism. Moreover the former admit infinitely many fibrations, and the latter exactly two. In this work, we analyse the non-uniqueness phenomenon for orbifold Seifert fibrations. For any closed spherical Seifert three-orbifold, we determine the number of its inequivalent fibrations. When these are in a finite number we provide a complete list. In case of infinitely many fibrations, we describe instead an algorithmic procedure to determine whether two closed spherical Seifert orbifolds are diffeomorphic."],["we look at the performance and cost implication of running analytics on IoT data.","JANUS: Benchmarking Commercial and Open-Source Cloud and Edge Platforms for Object and Anomaly Detection Workloads","summarize: With diverse IoT workloads, placing compute and analytics close to where data is collected is becoming increasingly important. We seek to understand what is the performance and the cost implication of running analytics on IoT data at the various available platforms. These workloads can be compute-light, such as outlier detection on sensor data, or compute-intensive, such as object detection from video feeds obtained from drones. In our paper, JANUS, we profile the performance\/"],["the most popular face recognition benchmarks assume a distribution of subjects without much attention to their demographic","Algorithmic Discrimination: Formulation and Exploration in Deep Learning-based Face Biometrics","summarize: The most popular face recognition benchmarks assume a distribution of subjects without much attention to their demographic attributes. In this work, we perform a comprehensive discrimination-aware experimentation of deep learning-based face recognition. The main aim of this study is focused on a better understanding of the feature space generated by deep models, and the performance achieved over different demographic groups. We also propose a general formulation of algorithmic discrimination with application to face biometrics. The experiments are conducted over the new DiveFace database composed of 24K identities from six different demographic groups. Two popular face recognition models are considered in the experimental framework: ResNet-50 and VGG-Face. We experimentally show that demographic groups highly represented in popular face databases have led to popular pre-trained deep face models presenting strong algorithmic discrimination. That discrimination can be observed both qualitatively at the feature space of the deep models and quantitatively in large performance differences when applying those models in different demographic groups, e.g. for face biometrics."],["our CACHEFIX framework verifies the cache side-channel freedom of a program","Symbolic Verification of Cache Side-channel Freedom","summarize: Cache timing attacks allow third-party observers to retrieve sensitive information from program executions. But, is it possible to automatically check the vulnerability of a program against cache timing attacks and then, automatically shield program executions against these attacks? For a given program, a cache configuration and an attack model, our CACHEFIX framework either verifies the cache side-channel freedom of the program or synthesizes a series of patches to ensure cache side-channel freedom during program execution. At the core of our framework is a novel symbolic verification technique based on automated abstraction refinement of cache semantics. The power of such a framework is to allow symbolic reasoning over counterexample traces and to combine it with runtime monitoring for eliminating cache side channels during program execution. Our evaluation with routines from OpenSSL, libfixedtimefixedpoint, GDK and FourQlib libraries reveals that our CACHEFIX approach proves cache sidechannel freedom within an average of 75 seconds. Besides, in all except one case, CACHEFIX synthesizes all patches within 20 minutes to ensure cache side-channel freedom of the respective routines during execution."],["proposed framework includes a network-based model that connected sentences based on their semantic similarity","Semantic flow in language networks","summarize: In this study we propose a framework to characterize documents based on their semantic flow. The proposed framework encompasses a network-based model that connected sentences based on their semantic similarity. Semantic fields are detected using standard community detection methods. as the story unfolds, transitions between semantic fields are represent in Markov networks, which in turned are characterized via network motifs . Here we show that the proposed framework can be used to classify books according to their style and publication dates. Remarkably, even without a systematic optimization of parameters, philosophy and investigative books were discriminated with an accuracy rate of 92.5%. Because this model captures semantic features of texts, it could be used as an additional feature in traditional network-based models of texts that capture only syntactical\/stylistic information, as it is the case of word adjacency networks."],["rBPF is a register-based VM based on extended Berkeley Packe","Minimal Virtual Machines on IoT Microcontrollers: The Case of Berkeley Packet Filters with rBPF","summarize: Virtual machines are widely used to host and isolate software modules. However, extremely small memory and low-energy budgets have so far prevented wide use of VMs on typical microcontroller-based IoT devices. In this paper, we explore the potential of two minimal VM approaches on such low-power hardware. We design rBPF, a register-based VM based on extended Berkeley Packet Filters . We compare it with a stack-based VM based on WebAssembly adapted for embedded systems. We implement prototypes of each VM, hosted in the IoT operating system RIOT. We perform measurements on commercial off-the-shelf IoT hardware. Unsurprisingly, we observe that both Wasm and rBPF virtual machines yield execution time and memory overhead, compared to not using a VM. We show however that this execution time overhead is tolerable for low-throughput, low-energy IoT devices. We further show that, while using a VM based on Wasm entails doubling the memory budget for a simple networked IoT application using a 6LoWPAN\/CoAP stack, using a VM based on rBPF requires only negligible memory overhead . rBPF is thus a promising approach to host small software modules, isolated from OS software, and updatable on-demand, over low-power networks."],["the primary goal of a Smart City is to counteract these problems and mitigate their effects by","MONICA in Hamburg: Towards Large-Scale IoT Deployments in a Smart City","summarize: Modern cities and metropolitan areas all over the world face new management challenges in the 21st century primarily due to increasing demands on living standards by the urban population. These challenges range from climate change, pollution, transportation, and citizen engagement, to urban planning, and security threats. The primary goal of a Smart City is to counteract these problems and mitigate their effects by means of modern ICT to improve urban administration and infrastructure. Key ideas are to utilise network communication to inter-connect public authorities; but also to deploy and integrate numerous sensors and actuators throughout the city infrastructure - which is also widely known as the Internet of Things . Thus, IoT technologies will be an integral part and key enabler to achieve many objectives of the Smart City vision. The contributions of this paper are as follows. We first examine a number of IoT platforms, technologies and network standards that can help to foster a Smart City environment. Second, we introduce the EU project MONICA which aims for demonstration of large-scale IoT deployments at public, inner-city events and give an overview on its IoT platform architecture. And third, we provide a case-study report on SmartCity activities by the City of Hamburg and provide insights on recent field tests of a vertically integrated, end-to-end IoT sensor application."],["financial intelligence is the core technology of the AI 2.0 era. it has elicit","FinBrain: When Finance Meets AI 2.0","summarize: Artificial intelligence is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a financial brain. In this work, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field."],["nanoporous gold can be used as plasmonic material for the mid-inf","Boosting infrared energy transfer in 3D nanoporous gold antennas","summarize: The applications of plasmonics to energy transfer from free-space radiation to molecules are currently limited to the visible region of the electromagnetic spectrum due to the intrinsic optical properties of bulk noble metals that support strong electromagnetic field confinement only close to their plasma frequency in the visible\/ultraviolet range. In this work, we show that nanoporous gold can be exploited as plasmonic material for the mid-infrared region to obtain strong electromagnetic field confinement, co-localized with target molecules into the nanopores and resonant with their vibrational frequency. The effective optical response of the nanoporous metal enables the penetration of optical fields deep into the nanopores, where molecules can be loaded thus achieving a more efficient light-matter coupling if compared to bulk gold. In order to realize plasmonic resonators made of nanoporous gold, we develop a nanofabrication method based on polymeric templates for metal deposition and we obtain antenna arrays resonating at mid-infrared wavelengths selected by design. We then coat the antennas with a thin silica layer acting as target dielectric layer for optical energy transfer. We study the strength of the light-matter coupling at the vibrational absorption frequency of silica at 1250 cm-1 through the analysis of the experimental Fano lineshape that is benchmarked against identical structures made of bulk gold. The boost of optical energy transfer from free-space mid-infrared radiation to molecular vibrations in nanoporous 3D nanoantenna arrays can open new application routes for plasmon-enhanced physical-chemical reactions."],["nonnegativity bounds problems are solved by scaling operator. a block-","Scaled Projected-Directions Methods with Application to Transmission Tomography","summarize: Statistical image reconstruction in X-Ray computed tomography yields large-scale regularized linear least-squares problems with nonnegativity bounds, where the memory footprint of the operator is a concern. Discretizing images in cylindrical coordinates results in significant memory savings, and allows parallel operator-vector products without on-the-fly computation of the operator, without necessarily decreasing image quality. However, it deteriorates the conditioning of the operator. We improve the Hessian conditioning by way of a block-circulant scaling operator and we propose a strategy to handle nondiagonal scaling in the context of projected-directions methods for bound-constrained problems. We describe our implementation of the scaling strategy using two algorithms: TRON, a trust-region method with exact second derivatives, and L-BFGS-B, a linesearch method with a limited-memory quasi-Newton Hessian approximation. We compare our approach with one where a change of variable is made in the problem. On two reconstruction problems, our approach converges faster than the change of variable approach, and achieves much tighter accuracy in terms of optimality residual than a first-order method."],["non-reciprocal wave dispersion in locally resonant acous","On the wave dispersion and non-reciprocal power flow in space-time traveling acoustic metamaterials","summarize: This note analytically investigates non-reciprocal wave dispersion in locally resonant acoustic metamaterials. Dispersion relations associated with space-time varying modulations of inertial and stiffness parameters of the base material and the resonant components are derived. It is shown that the resultant dispersion bias onsets intriguing features culminating in a break-up of both acoustic and optic propagation modes and one-way local resonance band gaps. The derived band structures are validated using the full transient displacement response of a finite metamaterial. A mathematical framework is presented to characterize power flow in the modulated acoustic metamaterials to quantify energy transmission patterns associated with the non-reciprocal response. Since local resonance band gaps are size-independent and frequency tunable, the outcome enables the synthesis of a new class of sub-wavelength low-frequency one-way wave guides."],["the technique of coded caching proposed by Maddah-Ali and Nie","Placement Delivery Array Design through Strong Edge Coloring of Bipartite Graphs","summarize: The technique of coded caching proposed by Madddah-Ali and Niesen is a promising approach to alleviate the load of networks during busy times. Recently, placement delivery array was presented to characterize both the placement and delivery phase in a single array for the centralized coded caching algorithm. In this paper, we interpret PDA from a new perspective, i.e., the strong edge coloring of bipartite graph. We prove that, a PDA is equivalent to a strong edge colored bipartite graph. Thus, we can construct a class of PDAs from existing structures in bipartite graphs. The class includes the scheme proposed by Maddah-Ali \\textit and a more general class of PDAs proposed by Shangguan \\textit as special cases. Moreover, it is capable of generating a lot of PDAs with flexible tradeoff between the sub-packet level and load."],["X-ray variability characteristics of the Narrow Line Seyfert 1 galaxy RE J","Long term X-ray variability characteristics of the narrow-line Seyfert 1 galaxy RE~J1034+396","summarize: We present the results of our study of the long term X-ray variability characteristics of the Narrow Line Seyfert 1 galaxy RE J1034+396. We use data obtained from the AstroSat satellite along with the light curves obtained from XMM-Newton and Swift-XRT. We use the 0.3 - 7.0 keV and 3 - 20 keV data, respectively, from the SXT and the LAXPC of AstroSat. The X-ray spectra in the 0.3 - 20 keV region are well fit with a model consisting of a power-law and a soft excess described by a thermal-Compton emission with a large optical depth, consistent with the earlier reported results. We have examined the X-ray light curves in the soft and hard X-ray bands of SXT and LAXPC, respectively, and find that the variability is slightly larger in the hard band. To investigate the variability characteristics of this source at different time scales, we have used X-ray light curves obtained from XMM-Newton data and Swift-XRT data and find that there are evidences to suggest that the variability sharply increases at longer time scales. We argue that the mass of the black hole in RE J1034+396 is likely to be "],["remote sensing image retrieval pipeline uses local convolutional features. we propose a","Aggregated Deep Local Features for Remote Sensing Image Retrieval","summarize: Remote Sensing Image Retrieval remains a challenging topic due to the special nature of Remote Sensing Imagery. Such images contain various different semantic objects, which clearly complicates the retrieval task. In this paper, we present an image retrieval pipeline that uses attentive, local convolutional features and aggregates them using the Vector of Locally Aggregated Descriptors to produce a global descriptor. We study various system parameters such as the multiplicative and additive attention mechanisms and descriptor dimensionality. We propose a query expansion method that requires no external inputs. Experiments demonstrate that even without training, the local convolutional features and global representation outperform other systems. After system tuning, we can achieve state-of-the-art or competitive results. Furthermore, we observe that our query expansion method increases overall system performance by about 3%, using only the top-three retrieved images. Finally, we show how dimensionality reduction produces compact descriptors with increased retrieval performance and fast retrieval computation times, e.g. 50% faster than the current systems."],["agent-based models provide a flexible framework that is often used for modelling biological systems.","Learning differential equation models from stochastic agent-based model simulations","summarize: Agent-based models provide a flexible framework that is frequently used for modelling many biological systems, including cell migration, molecular dynamics, ecology, and epidemiology. Analysis of the model dynamics can be challenging due to their inherent stochasticity and heavy computational requirements. Common approaches to the analysis of agent-based models include extensive Monte Carlo simulation of the model or the derivation of coarse-grained differential equation models to predict the expected or averaged output from the agent-based model. Both of these approaches have limitations, however, as extensive computation of complex agent-based models may be infeasible, and coarse-grained differential equation models can fail to accurately describe model dynamics in certain parameter regimes. We propose that methods from the equation learning field provide a promising, novel, and unifying approach for agent-based model analysis. Equation learning is a recent field of research from data science that aims to infer differential equation models directly from data. We use this tutorial to review how methods from equation learning can be used to learn differential equation models from agent-based model simulations. We demonstrate that this framework is easy to use, requires few model simulations, and accurately predicts model dynamics in parameter regions where coarse-grained differential equation models fail to do so. We highlight these advantages through several case studies involving two agent-based models that are broadly applicable to biological phenomena: a birth-death-migration model commonly used to explore cell biology experiments and a susceptible-infected-recovered model of infectious disease spread."],["complexity of many of these systems poses accountability challenges. data provenance methods show much promise as","Decision Provenance: Harnessing data flow for accountable systems","summarize: Demand is growing for more accountability regarding the technological systems that increasingly occupy our world. However, the complexity of many of these systems - often systems-of-systems - poses accountability challenges. A key reason for this is because the details and nature of the information flows that interconnect and drive systems, which often occur across technical and organisational boundaries, tend to be invisible or opaque. This paper argues that data provenance methods show much promise as a technical means for increasing the transparency of these interconnected systems. Specifically, given the concerns regarding ever-increasing levels of automated and algorithmic decision-making, and so-called 'algorithmic systems' in general, we propose decision provenance as a concept showing much promise. Decision provenance entails using provenance methods to provide information exposing decision pipelines: chains of inputs to, the nature of, and the flow-on effects from the decisions and actions taken throughout systems. This paper introduces the concept of decision provenance, and takes an interdisciplinary exploration into its potential for assisting accountability in algorithmic systems. We argue that decision provenance can help facilitate oversight, audit, compliance, risk mitigation, and user empowerment, and we also indicate the implementation considerations and areas for research necessary for realising its vision. More generally, we make the case that considerations of data flow, and systems more broadly, are important to discussions of accountability, and complement the considerable attention already given to algorithmic specifics."],["neural networks have shown significant success in a wide range of AI tasks. large-scale","Differentially Private Model Publishing for Deep Learning","summarize: Deep learning techniques based on neural networks have shown significant success in a wide range of AI tasks. Large-scale training datasets are one of the critical factors for their success. However, when the training datasets are crowdsourced from individuals and contain sensitive information, the model parameters may encode private information and bear the risks of privacy leakage. The recent growing trend of the sharing and publishing of pre-trained models further aggravates such privacy risks. To tackle this problem, we propose a differentially private approach for training neural networks. Our approach includes several new techniques for optimizing both privacy loss and model accuracy. We employ a generalization of differential privacy called concentrated differential privacy, with both a formal and refined privacy loss analysis on two different data batching methods. We implement a dynamic privacy budget allocator over the course of training to improve model accuracy. Extensive experiments demonstrate that our approach effectively improves privacy loss accounting, training efficiency and model quality under a given privacy budget."],["golfarshchi and Khalilzadeh disprove two results.","On the paper: Numerical radius preserving linear maps on Banach algebras","summarize: We give an example of a unital commutative complex Banach algebra having a normalized state which is not a spectral state and admitting an extreme normalized state which is not multiplicative. This disproves two results by Golfarshchi and Khalilzadeh."],["we present the novel approach to mathematical modeling of information processes in biosystems. it explore","Quantum-like modeling in biology with open quantum systems and instruments","summarize: We present the novel approach to mathematical modeling of information processes in biosystems. It explores the mathematical formalism and methodology of quantum theory, especially quantum measurement theory. This approach is known as and it should be distinguished from study of genuine quantum physical processes in biosystems . It is based on quantum information representation of biosystem's state and modeling its dynamics in the framework of theory of open quantum systems. This paper starts with the non-physicist friendly presentation of quantum measurement theory, from the original von Neumann formulation to modern theory of quantum instruments. Then, latter is applied to model combinations of cognitive effects and gene regulation of glucose\/lactose metabolism in Escherichia coli bacterium. The most general construction of quantum instruments is based on the scheme of indirect measurement, in that measurement apparatus plays the role of the environment for a biosystem. The biological essence of this scheme is illustrated by quantum formalization of Helmholtz sensation-perception theory. Then we move to open systems dynamics and consider quantum master equation, with concentrating on quantum Markov processes. In this framework, we model functioning of biological functions such as psychological functions and epigenetic mutation."],["ytterbium dopant concentrations in high power fiber amplifiers have been","Study of dopant concentrations on thermal induced mode instability in high power fiber amplifiers","summarize: Dependence of mode instabilities on ytterbium dopant concentrations in high power fiber amplifiers has been investigated. It is theoretically shown that, by only varying the fiber length to maintain the same total small-signal pump absorption, the MI threshold is independent of dopant concentration. MI thresholds of gain fibers with ytterbium dopant concentration of 5.93X10^25\/m3 and 1.02X10^26\/m3 have been measured, which exhibit similar thresholds and agree with theoretical results. The result indicates that heavy doping of active fiber can be adopted to suppress nonlinear effects without decreasing MI threshold, which provides a method of maximizing the power output of fiber laser, taking into account the stimulated Brillouin scattering, stimulated Raman Scattering, and MI thresholds simultaneously."],["a large-scale aerialterrestrial heterogenous cellular network can be either ground","Coverage Performance of Aerial-Terrestrial HetNets","summarize: Providing seamless coverage under current cellular network technologies is surmountable only through gross overengineering. Alternatively, as an economically effective solution, the use of unmanned aerial vehicles , augmented with the functionalities of terrestrial base stations , is recently advocated. In this paper we investigate the effect that the incorporation of UAV-mounted BSs poses on the coverage probability of cellular networks. To this end, we focus on the evaluation of the coverage probability of a large-scale aerialterrestrial heterogenous cellular network , in which BSs of each technology\/tier can be either ground or UBS. Our analysis incorporates the impact of Line-of-Sight and non-LOS path-loss attenuations of both ground-toground and Air-to-Ground links. Adopting tools of stochastic geometry we provide an expression for the coverage probability based on main system parameters and percentage of BSs in each tier that are aerial. We confirm the accuracy of our analysis. Using our analysis, we observe that for several common communication environments, e.g., high-rise and dense urban environments, the inclusion of U-BSs can be detrimental to the coverage probability. Nevertheless, it is still possible to minimize the coverage cost by turning off a percentage of G-BSs. Interestingly, for urban and sub-urban areas one can adjust the altitude of U-BSs in order to increase the coverage probability."],["124 industry practitioners in agile software developing companies answered questionnaire. results show few practitioners indicated widespread","The Unfulfilled Potential of Data-Driven Decision Making in Agile Software Development","summarize: With the general trend towards data-driven decision making , organizations are looking for ways to use DDDM to improve their decisions. However, few studies have looked into the practitioners view of DDDM, in particular for agile organizations. In this paper we investigated the experiences of using DDDM, and how data can improve decision making. An emailed questionnaire was sent out to 124 industry practitioners in agile software developing companies, of which 84 answered. The results show that few practitioners indicated a widespread use of DDDM in their current decision making practices. The practitioners were more positive to its future use for higher-level and more general decision making, fairly positive to its use for requirements elicitation and prioritization decisions, while being less positive to its future use at the team level. The practitioners do see a lot of potential for DDDM in an agile context; however, currently unfulfilled."],["a panel of 14 observers graded 160 biopsies with and without AI assistance","Artificial Intelligence Assistance Significantly Improves Gleason Grading of Prostate Biopsies by Pathologists","summarize: While the Gleason score is the most important prognostic marker for prostate cancer patients, it suffers from significant observer variability. Artificial Intelligence systems, based on deep learning, have proven to achieve pathologist-level performance at Gleason grading. However, the performance of such systems can degrade in the presence of artifacts, foreign tissue, or other anomalies. Pathologists integrating their expertise with feedback from an AI system could result in a synergy that outperforms both the individual pathologist and the system. Despite the hype around AI assistance, existing literature on this topic within the pathology domain is limited. We investigated the value of AI assistance for grading prostate biopsies. A panel of fourteen observers graded 160 biopsies with and without AI assistance. Using AI, the agreement of the panel with an expert reference standard significantly increased . Our results show the added value of AI systems for Gleason grading, but more importantly, show the benefits of pathologist-AI synergy."],["OATM can significantly save tuning time compared to state-of-the-art methods","Deep Neural Network Hyperparameter Optimization with Orthogonal Array Tuning","summarize: Deep learning algorithms have achieved excellent performance lately in a wide range of fields . However, a severe challenge faced by deep learning is the high dependency on hyper-parameters. The algorithm results may fluctuate dramatically under the different configuration of hyper-parameters. Addressing the above issue, this paper presents an efficient Orthogonal Array Tuning Method for deep learning hyper-parameter tuning. We describe the OATM approach in five detailed steps and elaborate on it using two widely used deep neural network structures . The proposed method is compared to the state-of-the-art hyper-parameter tuning methods including manually and automatically ones. The experiment results state that OATM can significantly save the tuning time compared to the state-of-the-art methods while preserving the satisfying performance. The codes are open in GitHub "],["XACMET defines a typed graph that models the XACML policy evaluation","An automated model-based test oracle for access control systems","summarize: In the context of XACML-based access control systems, an intensive testing activity is among the most adopted means to assure that sensible information or resources are correctly accessed. Unfortunately, it requires a huge effort for manual inspection of results: thus automated verdict derivation is a key aspect for improving the cost-effectiveness of testing. To this purpose, we introduce XACMET, a novel approach for automated model-based oracle definition. XACMET defines a typed graph, called the XAC-Graph, that models the XACML policy evaluation. The expected verdict of a specific request execution can thus be automatically derived by executing the corresponding path in such graph. Our validation of the XACMET prototype implementation confirms the effectiveness of the proposed approach."],["we compare Particle-in-Cell simulation results of relativistic electron-ion she","Scaling of Relativistic Shear Flows with Bulk Lorentz Factor","summarize: We compare Particle-in-Cell simulation results of relativistic electron-ion shear flows with different bulk Lorentz factors, and discuss their implications for spine-sheath models of blazar versus gamma-ray burst jets. Specifically, we find that most properties of the shear boundary layer scale with the bulk Lorentz factor: the lower the Lorentz factor, the thinner the boundary layer, and the weaker the self-generated fields. Similarly, the energized electron spectrum peaks at an energy near the ion drift energy, which increases with bulk Lorentz factor, and the beaming of the accelerated electrons gets narrower with increasing Lorentz factor. This predicts a strong correlation between emitted photon energy, angular beaming and temporal variability with the bulk Lorentz factor. Observationally, we expect systematic differences between the high-energy emissions of blazars and GRB jets."],["multiwavelength monitoring of the blazar 3C 279 observed a very bright,","Analyzing the December 2013 Orphan Gamma-Ray Flare From 3C 279","summarize: Multiwavelength monitoring of the blazar 3C 279 observed a very bright, 12-hour, orphan gamma-ray flare on 20 Dec 2013 with a uniquely hard Fermi-LAT spectrum and high Compton dominance. We work with a one-zone, leptonic model with both first- and second-order Fermi acceleration, which now reproduces the unique flaring behavior. We present a simplified analytic electron energy distribution to provide intuition about how particle acceleration shapes multi-wavelength blazar jet emission spectra. The contributions of individual processes in relativistic jets is fundamental to understanding the particle energy budget in the formation and propagation of astrophysical jets. We show that first- and second-order Fermi acceleration are sufficient to explain the flare, and that magnetic reconnection is not needed. Our analysis suggests that the flare is initiated by an increase in the particle energies due to shock acceleration, which also increases the stochastic acceleration. The higher energy particle preferentially occupy the outer jet, along the sheath, which decreases the apparent magnetic field and synchrotron radiation, while increasing electron exposure to the broad line region photon fields, driving up the external Compton emission."],["the weak order on the lattice is a weak order. the weak order","Quotientopes","summarize: For any lattice congruence of the weak order on "],["the master wants to distribute the data to untrusted workers who have volunteered or are in","Minimizing Latency for Secure Coded Computing Using Secret Sharing via Staircase Codes","summarize: We consider the setting of a Master server, M, who possesses confidential data and wants to run intensive computations on it, as part of a machine learning algorithm for example. The Master wants to distribute these computations to untrusted workers who have volunteered or are incentivized to help with this task. However, the data must be kept private and not revealed to the individual workers. Some of the workers may be stragglers, e.g., slow or busy, and will take a random time to finish the task assigned to them. We are interested in reducing the delays experienced by the Master. We focus on linear computations as an essential operation in many iterative algorithms such as principal component analysis, support vector machines and other gradient-descent based algorithms. A classical solution is to use a linear secret sharing scheme, such as Shamir's scheme, to divide the data into secret shares on which the workers can perform linear computations. However, classical codes can provide straggler mitigation assuming a worst-case scenario of a fixed number of stragglers. We propose a solution based on new secure codes, called Staircase codes, introduced previously by two of the authors. Staircase codes allow flexibility in the number of stragglers up to a given maximum, and universally achieve the information theoretic limit on the download cost by the Master, leading to latency reduction. Under the shifted exponential model, we find upper and lower bounds on the Master's mean waiting time. We derive the distribution of the Master's waiting time, and its mean, for systems with up to two stragglers. For systems with any number of stragglers, we derive an expression that can give the exact distribution, and the mean, of the waiting time of the Master. We show that Staircase codes always outperform classical secret sharing codes."],["new measurements of luminosity function at quasar luminosity function.","Subaru High-z Exploration of Low-Luminosity Quasars . V. Quasar Luminosity Function and Contribution to Cosmic Reionization at z = 6","summarize: We present new measurements of the quasar luminosity function at "],["axion Bose stars can engender bursts when undergoing conversion","May axion clusters be sources of fast radio bursts?","summarize: Fast radio bursts can be caused by some phenomena related to 'new physics'.One of the most prominent candidates of the kind are axion Bose stars which can engender bursts when undergoing conversion into photons in magnetospheres of neutron stars or during their collapse. In this short research note an importance of three observational criteria is outlined, namely total energetic, "],["the conductivity tensor is calculated based on a linear response theory.","Optical responses induced by spin gauge field at the second order","summarize: Optical responses of ferromagnetic materials with spin gauge field that drives intrinsic spin curren is theoretically studied. The conductivity tensor is calculated based on a linear response theory to the applied electric field taking account of the non-linear effects of the spin gauge field to the second order. We consider the case where the spin gauge field is uniform, realized for spiral magnetization structure or uniform spin-orbit interaction. The spin gauge field, or an intrinsic spin current, turns out to give rise to anisotropic optical responses, which is expected to be useful to experimental detection of magnetization structures."],["smart grid addresses several security challenges of the traditional grid. it has exposed the system to numerous","Cyber-Security in Smart Grid: Survey and Challenges","summarize: Smart grid uses the power of information technology to intelligently deliver energy to customers by using a two-way communication, and wisely meet the environmental requirements by facilitating the integration of green technologies. Although smart grid addresses several problems of the traditional grid, it faces a number of security challenges. Because communication has been incorporated into the electrical power with its inherent weaknesses, it has exposed the system to numerous risks. Several research papers have discussed these problems. However, most of them classified attacks based on confidentiality, integrity, and availability, and they excluded attacks which compromise other security criteria such as accountability. In addition, the existed security countermeasures focus on countering some specific attacks or protecting some specific components, but there is no global approach which combines these solutions to secure the entire system. The purpose of this paper is to provide a comprehensive overview of the relevant published works. First, we review the security requirements. Then, we investigate in depth a number of important cyber-attacks in smart grid to diagnose the potential vulnerabilities along with their impact. In addition, we proposed a cyber security strategy as a solution to address breaches, counter attacks, and deploy appropriate countermeasures. Finally, we provide some future research directions."],["we propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving","Softwarization of Internet of Things Infrastructure for Secure and Smart Healthcare","summarize: We propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving deployment of Internet of Things for smart healthcare applications and services. It integrates state-of-the-art networking and virtualization techniques across IoT, fog and cloud domains, employing Blockchain, Tor and message brokers to provide security and privacy for patients and healthcare providers. We propose a novel platform using Machine-to-Machine messaging and rule-based beacons for seamless data management and discuss the role of data and decision fusion in the cloud and the fog, respectively, for smart healthcare applications and services."],["a large family of inhomogeneous random digraphs is based on","PageRank on inhomogeneous random digraphs","summarize: We study the typical behavior of a generalized version of Google's PageRank algorithm on a large family of inhomogeneous random digraphs. This family includes as special cases directed versions of classical models such as the Erd\\os-R\\'enyi model, the Chung-Lu model, the Poissonian random graph and the generalized random graph, and is suitable for modeling scale-free directed complex networks where the number of neighbors a vertex has is related to its attributes. In particular, we show that the rank of a randomly chosen node in a graph from this family converges weakly to the attracting endogenous solution to the stochastic fixed-point equation "],["generative model of point clouds in form of energy-based model. energy function learns","Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification","summarize: We propose a generative model of unordered point sets, such as point clouds, in the form of an energy-based model, where the energy function is parameterized by an input-permutation-invariant bottom-up neural network. The energy function learns a coordinate encoding of each point and then aggregates all individual point features into an energy for the whole point cloud. We call our model the Generative PointNet because it can be derived from the discriminative PointNet. Our model can be trained by MCMC-based maximum likelihood learning , without the help of any assisting networks like those in GANs and VAEs. Unlike most point cloud generators that rely on hand-crafted distance metrics, our model does not require any hand-crafted distance metric for the point cloud generation, because it synthesizes point clouds by matching observed examples in terms of statistical properties defined by the energy function. Furthermore, we can learn a short-run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpolation. The learned point cloud representation can be useful for point cloud classification. Experiments demonstrate the advantages of the proposed generative model of point clouds."],["generative model generates statistically independent samples for molecules. model learns a low","A Generative Model for Molecular Distance Geometry","summarize: Great computational effort is invested in generating equilibrium states for molecular systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statistically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties."],["we consider the critical behaviour of the continuous-time weakly self-avoiding walk with contact self","Four-dimensional weakly self-avoiding walk with contact self-attraction","summarize: We consider the critical behaviour of the continuous-time weakly self-avoiding walk with contact self-attraction on "],["localized plasmon resonances can be seen from the STEM images in cuP hexagonal","Imaging localized plasmon resonances in vacancy doped Cu3-xP semiconductor nanocrystals with STEM-EELS","summarize: Copper binary compounds are often intrinsic p-type semiconductors due to the presence of Cu vacancies, with corresponding hole carriers in the valence band. If the free carrier concentration is high enough, localized plasmon resonances can be sustained in nanocrystals, with frequencies in the infra-red , with respect to the typical resonances seen in the visible range in the case of metals . The localization of the resonances can be demonstrated with scanning transmission electron energy loss spectroscopy by combining high spatial and high energy resolutions. Here we demonstrate that Cu vacancies can be directly measured from the STEM images in CuP hexagonal nanocrystals. Two localized resonances can be seen from STEM-EELS, which are in agreement with the resonances calculated from the vacancy concentration obtained from the STEM."],["a novel and general task-agnostic search space is proposed. the","MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning","summarize: We propose to incorporate neural architecture search into general-purpose multi-task learning . Existing NAS methods typically define different search spaces according to different tasks. In order to adapt to different task combinations , we disentangle the GP-MTL networks into single-task backbones , and a hierarchical and layerwise features sharing\/fusing scheme across them. This enables us to design a novel and general task-agnostic search space, which inserts cross-task edges into fixed single-task network backbones. Moreover, we also propose a novel single-shot gradient-based search algorithm that closes the performance gap between the searched architectures and the final evaluation architecture. This is realized with a minimum entropy regularization on the architecture weights during the search phase, which makes the architecture weights converge to near-discrete values and therefore achieves a single model. As a result, our searched model can be directly used for evaluation without training from scratch. We perform extensive experiments using different single-task backbones on various task sets, demonstrating the promising performance obtained by exploiting the hierarchical and layerwise features, as well as the desirable generalizability to different i) task sets and ii) single-task backbones. The code of our paper is available at https:\/\/github.com\/bhpfelix\/MTLNAS."],["results on Harnack type inequalities for matrices are more general than","Harnack type inequalities for matrices in majorization","summarize: Following the recent work of Jiang and Lin 45--49), we present more results on Harnack type inequalities for matrices in terms of majorization of eigenvalues and singular values. We discuss and compare the bounds derived through different ways. Jiang and Lin's results imply Tung's version of Harnack's inequality 375--381); our results %with simpler proofs are stronger and more general than Jiang and Lin's. We also show some majorization inequalities concerning Cayley transforms. Some open problems on spectral norm and eigenvalues are proposed."],["a multi-head attention Molecular Transformer model outperforms all algorithms in the literature","Molecular Transformer - A Model for Uncertainty-Calibrated Chemical Reaction Prediction","summarize: Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between SMILES strings of reactants-reagents and the products. We show that a multi-head attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90% on a common benchmark dataset. Our algorithm requires no handcrafted rules, and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without reactant-reagent split and including stereochemistry, which makes our method universally applicable."],["pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from","Pseudo-Outcrop Visualization of Borehole Images and Core Scans","summarize: A pseudo-outcrop visualization is demonstrated for borehole and full-diameter rock core images to augment the ubiquitous unwrapped cylinder view and thereby to assist non-specialist interpreters. The pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from borehole to earth frame of reference that creates a solid volume sliced longitudinally to reveal two or more faces in which the orientations of geological features indicate what is observed in the subsurface. A proxy for grain size is used to modulate the external dimensions of the plot to mimic profiles seen in real outcrops. The volume is created from a mixture of geological boundary elements and texture, the latter being the residue after the sum of boundary elements is subtracted from the original data. In the case of measurements from wireline microresistivity tools, whose circumferential coverage is substantially less than 100%, the missing circumferential data is first inpainted using multiscale directional transforms, which decompose the image into its elemental building structures, before reconstructing the full image. The pseudo-outcrop view enables direct observation of the angular relationships between features and aids visual comparison between borehole and core images, especially for the interested non-specialist."],["a distributed computational productive laziness approach is proposed in this paper.","Ethically Aligned Opportunistic Scheduling for Productive Laziness","summarize: In artificial intelligence mediated workforce management systems , long-term success depends on workers accomplishing tasks productively and resting well. This dual objective can be summarized by the concept of productive laziness. Existing scheduling approaches mostly focus on efficiency but overlook worker wellbeing through proper rest. In order to enable workforce management systems to follow the IEEE Ethically Aligned Design guidelines to prioritize worker wellbeing, we propose a distributed Computational Productive Laziness approach in this paper. It intelligently recommends personalized work-rest schedules based on local data concerning a worker's capabilities and situational factors to incorporate opportunistic resting and achieve superlinear collective productivity without the need for explicit coordination messages. Extensive experiments based on a real-world dataset of over 5,000 workers demonstrate that CPL enables workers to spend 70% of the effort to complete 90% of the tasks on average, providing more ethically aligned scheduling than existing approaches."],["the recent outbreak of COVID-19 pandemic has exposed and highlighted the limitations of current","Future Smart Connected Communities to Fight COVID-19 Outbreak","summarize: Internet of Things has grown rapidly in the last decade and continue to develop in terms of dimension and complexity offering wide range of devices to support diverse set of applications. With ubiquitous Internet, connected sensors and actuators, networking and communication technology, and artificial intelligence , smart cyber-physical systems provide services rendering assistance to humans in their daily lives. However, the recent outbreak of COVID-19 pandemic has exposed and highlighted the limitations of current technological deployments to curtail this disease. IoT and smart connected technologies together with data-driven applications can play a crucial role not only in prevention, continuous monitoring, and mitigation of the disease, but also enable prompt enforcement of guidelines, rules and government orders to contain such future outbreaks. In this paper, we envision an IoT-enabled ecosystem for intelligent monitoring, pro-active prevention and control, and mitigation of COVID-19. We propose different architectures, applications and technology systems for various smart infrastructures including E-health, smart home, smart supply chain management, smart locality, and smart city, to develop future connected communities to manage and mitigate similar outbreaks. Furthermore, we present research challenges together with future directions to enable and develop these smart communities and infrastructures to fight and prepare against such outbreaks."],["byte-addressable non-volatile memory features high density, D","System Evaluation of the Intel Optane Byte-addressable NVM","summarize: Byte-addressable non-volatile memory features high density, DRAM comparable performance, and persistence. These characteristics position NVM as a promising new tier in the memory hierarchy. Nevertheless, NVM has asymmetric read and write performance, and considerably higher write energy than DRAM. Our work provides an in-depth evaluation of the first commercially available byte-addressable NVM -- the Intel Optane DC persistent memory. The first part of our study quantifies the latency, bandwidth, power efficiency, and energy consumption under eight memory configurations. We also evaluate the real impact on in-memory graph processing workloads. Our results show that augmenting NVM with DRAM is essential, and the combination can effectively bridge the performance gap and provide reasonable performance with higher capacity. We also identify NUMA-related performance characteristics for accesses to memory on a remote socket. In the second part, we employ two fine-grained allocation policies to control traffic distribution between DRAM and NVM. Our results show that bandwidth spilling between DRAM and NVM could provide 2.0x bandwidth and enable "],["a challenge in fundamental physics is to understand emergent order in far-from-","Coexisting Ordered States, Local Equilibrium-like Domains, and Broken Ergodicity in a Non-turbulent Rayleigh-B\\'enard Convection at Steady-state","summarize: A challenge in fundamental physics and especially in thermodynamics is to understand emergent order in far-from-equilibrium systems. While at equilibrium, temperature plays the role of a key thermodynamic variable whose uniformity in space and time defines the equilibrium state the system is in, this is not the case in a far-from-equilibrium driven system. When energy flows through a finite system at steady-state, temperature takes on a time-independent but spatially varying character. In this study, the convection patterns of a Rayleigh-Bnard fluid cell at steady-state is used as a prototype system where the temperature profile and fluctuations are measured spatio-temporally. The thermal data is obtained by performing high-resolution real-time infrared calorimetry on the convection system as it is first driven out-of-equilibrium when the power is applied, achieves steady-state, and then as it gradually relaxes back to room temperature equilibrium when the power is removed. Our study provides new experimental data on the non-trivial nature of thermal fluctuations when stable complex convective structures emerge. The thermal analysis of these convective cells at steady-state further yield local equilibrium-like statistics. In conclusion, these results correlate the spatial ordering of the convective cells with the evolution of the system's temperature manifold."],["contract verification aims to guarantee all or most of these properties ahead of time. existing methods","Soft Contract Verification for Higher-Order Stateful Programs","summarize: Software contracts allow programmers to state rich program properties using the full expressive power of an object language. However, since they are enforced at runtime, monitoring contracts imposes significant overhead and delays error discovery. So contract verification aims to guarantee all or most of these properties ahead of time, enabling valuable optimizations and yielding a more general assurance of correctness. Existing methods for static contract verification satisfy the needs of more restricted target languages, but fail to address the challenges unique to those conjoining untyped, dynamic programming, higher-order functions, modularity, and statefulness. Our approach tackles all these features at once, in the context of the full Racket system---a mature environment for stateful, higher-order, multi-paradigm programming with or without types. Evaluating our method using a set of both pure and stateful benchmarks, we are able to verify 99.94% of checks statically . Stateful, higher-order functions pose significant challenges for static contract verification in particular. In the presence of these features, a modular analysis must permit code from the current module to escape permanently to an opaque context that may be stateful and therefore store a reference to the escaped closure. Also, contracts themselves, being predicates wri en in unrestricted Racket, may exhibit stateful behavior; a sound approach must be robust to contracts which are arbitrarily expressive and interwoven with the code they monitor. In this paper, we present and evaluate our solution based on higher-order symbolic execution, explain the techniques we used to address such thorny issues, formalize a notion of behavioral approximation, and use it to provide a mechanized proof of soundness."],["in this paper, we investigate the moduli of continuity for viscosity solutions.","Moduli of Continuity for Viscosity Solutions","summarize: In this paper, we investigate the moduli of continuity for viscosity solutions of a wide class of nonsingular quasilinear evolution equations and also for the level set mean curvature flow, which is an example of singular degenerate equations. We prove that the modulus of continuity is a viscosity subsolution of some one dimensional equation. This work extends B. Andrews' recent result on moduli of continuity for smooth spatially periodic solutions."],["two players alternate moves in the following impartial combinatorial game. given a finite","Algebraic games - Playing with groups and rings","summarize: Two players alternate moves in the following impartial combinatorial game: Given a finitely generated abelian group "],["the charged anisotropic star on paraboloidal spacetime is reported by choosing particular form","Charged Anisotropic Star on Paraboloidal Spacetime","summarize: The charged anisotropic star on paraboloidal spacetime is reported by choosing particular form of radial pressure and electric field intensity. The non-singular solution of Einstein-Maxwell system of equation have been derived and it is shown that model satisfy all the physical plausibility conditions. It is observed that in the absence of electric field intensity, model reduces to particular case of uncharged Sharma \\& Ratanpal model. It is also observed that the parameter used in electric field intensity directly effects the mass of the star."],["a priori estimates for the Keller-Segel system are applied to the homogen","Stochastic homogenization of the Keller-Segel chemotaxis system","summarize: In this paper, we focus on the Keller-Segel chemotaxis system in a random heterogeneous domain. We assume that the corresponding diffusion and chemotaxis coefficients are given by stationary ergodic random fields and apply stochastic two-scale convergence methods to derive the homogenized macroscopic equations. In establishing our results, we also derive a priori estimates for the Keller-Segel system that rely only on the boundedness of the coefficients; in particular, no differentiability assumption on the diffusion and chemotaxis coefficients for the chemotactic species is required. Finally, we prove the convergence of a periodization procedure for approximating the homogenized macroscopic coefficients."],["intuitionistic logic introduces an epistemic operator to intuitionistic logic. the a","An Arithmetical Interpretation of Verification and Intuitionistic Knowledge","summarize: Intuitionistic epistemic logic introduces an epistemic operator, which reflects the intended BHK semantics of intuitionism, to intuitionistic logic. The fundamental assumption concerning intuitionistic knowledge and belief is that it is the product of verification. The BHK interpretation of intuitionistic logic has a precise formulation in the Logic of Proofs and its arithmetical semantics. We show here that this interpretation can be extended to the notion of verification upon which intuitionistic knowledge is based, thereby providing the systems of intuitionistic epistemic logic extended by an epistemic operator based on verification with an arithmetical semantics too."],["a special class of standard gaussian Autoregressive Hilbertian processes of","Classical and bayesian componentwise predictors for non-compact correlated ARH processes","summarize: A special class of standard Gaussian Autoregressive Hilbertian processes of order one processes), with bounded linear autocorrelation operator, which does not satisfy the usual Hilbert-Schmidt assumption, is considered. To compensate the slow decay of the diagonal coefficients of the autocorrelation operator, a faster decay velocity of the eigenvalues of the trace autocovariance operator of the innovation process is assumed. As usual, the eigenvectors of the autocovariance operator of the ARH process are considered for projection, since, here, they are assumed to be known. Diagonal componentwise classical and bayesian estimation of the autocorrelation operator is studied for prediction. The asymptotic efficiency and equivalence of both estimators is proved, as well as of their associated componentwise ARH plugin predictors. A simulation study is undertaken to illustrate the theoretical results derived."],["trajectory optimization is solved in the time domain. the optimization is solved in the time domain","Frequency-Aware Model Predictive Control","summarize: Transferring solutions found by trajectory optimization to robotic hardware remains a challenging task. When the optimization fully exploits the provided model to perform dynamic tasks, the presence of unmodeled dynamics renders the motion infeasible on the real system. Model errors can be a result of model simplifications, but also naturally arise when deploying the robot in unstructured and nondeterministic environments. Predominantly, compliant contacts and actuator dynamics lead to bandwidth limitations. While classical control methods provide tools to synthesize controllers that are robust to a class of model errors, such a notion is missing in modern trajectory optimization, which is solved in the time domain. We propose frequency-shaped cost functions to achieve robust solutions in the context of optimal control for legged robots. Through simulation and hardware experiments we show that motion plans can be made compatible with bandwidth limits set by actuators and contact dynamics. The smoothness of the model predictive solutions can be continuously tuned without compromising the feasibility of the problem. Experiments with the quadrupedal robot ANYmal, which is driven by highly-compliant series elastic actuators, showed significantly improved tracking performance of the planned motion, torque, and force trajectories and enabled the machine to walk robustly on terrain with unmodeled compliance."],["we introduce a spin-flip model for a broad-area Vertical-Ca","Localized chaos of elliptically polarized cavity solitons in broad-area VCSEL with saturable absorber","summarize: We introduce a spin-flip model for a broad-area Vertical-Cavity Surface-Emitting Laser with a saturable absorber. We demonstrate simultaneous existence of orthogonally linearly polarized and elliptically polarized cavity solitons. We show that polarization degree of freedom leads to period doubling route to spatially localized chaos of the elliptically polarized cavity solitons."],["49 new times of minimum were obtained between 2005 and 2020. this allows the derivation","CK Aqr time keeping. Evidence for a third body","summarize: Photometric measurements of the contact binary system CK Aqr are presented. 49 new times of minimum were obtained between 2005 and 2020. Along with already published observations, this allows the derivation of an improved ephemeris. The resulting O-C diagram shows oscillations which are interpreted as the light time travel effect due to a third component, with a period of 8.2 years."],["discrete continuous method was used to compute the mechanical fields. original simulation strategy was modified to","Discrete dislocation dynamics simulations of dislocation-","summarize: The mechanisms of dislocation\/precipitate interaction were studied by means of discrete dislocation dynamics within a multiscale approach. Simulations were carried out using the discrete continuous method in combination with a fast Fourier transform solver to compute the mechanical fields. The original simulation strategy was modified to include straight dislocation segments by means of the field dislocation mechanics method and was applied to simulate the interaction of an edge dislocation with a "],["the data analysis is performed using location data of the bus system. the data is collected using","Los Angeles Metro Bus Data Analysis Using GPS Trajectory and Schedule Data ","summarize: With the widespread installation of location-enabled devices on public transportation, public vehicles are generating massive amounts of trajectory data in real time. However, using these trajectory data for meaningful analysis requires careful considerations in storing, managing, processing, and visualizing the data. Using the location data of the Los Angeles Metro bus system, along with publicly available bus schedule data, we conduct a data processing and analyses study to measure the performance of the public transportation system in Los Angeles utilizing a number of metrics including travel-time reliability, on-time performance, bus bunching, and travel-time estimation. We demonstrate the visualization of the data analysis results through an interactive web-based application. The developed algorithms and system provide powerful tools to detect issues and improve the efficiency of public transportation systems."],["work presents robust and efficient sharp interface immersed boundary framework. the work deploys an in","A Novel Sharp Interface Immersed Boundary Framework for Viscous Flow Simulations at Arbitrary Mach Number Involving Complex and Moving Boundaries","summarize: This work presents a robust and efficient sharp interface immersed boundary framework, which is applicable for all-speed flow regimes and is capable of handling arbitrarily complex bodies . The work deploys an in-house, parallel, multi-block structured finite volume flow solver, which employs a 3D unsteady Favre averaged Navier Stokes equations in a generalized curvilinear coordinate system; while we employ a combination of HCIB method and GC for solution reconstruction near immersed boundary interface. A significant difficulty for these sharp interface approaches is of handling sharp features\/edges of complex geometries. In this study, we observe that apart from the need for robust node classification strategy and higher order boundary formulations, the direction in which the reconstruction procedures are performed plays an important role in handling sharp edges. Taking this into account we present a versatile interface tracking procedure based on ray tracing algorithm and a novel three step solution reconstruction procedure that computes pseudo-normals in the regions where the normal is not well-defined and reconstructs the flow field along those directions. We demonstrate that this procedure enables solver to efficiently handle and accurately represent sharp-edged regions. A fifth-order weighted essentially non-oscillatory scheme is used for capturing shock-induced discontinuities and complex fluid-solid interactions with high resolution. The developed IBM framework is applied to a wide range of flow phenomena encompassing all-speed regimes . A total of seven benchmark cases are presented involving various geometries and the predictions are found to be in excellent agreement with the published results."],["the weak order on the lattice is a weak order. the weak order","Quotientopes","summarize: For any lattice congruence of the weak order on "],["many organizations wish to train machine learning models on their combined datasets. but they often cannot","Helen: Maliciously Secure Coopetitive Learning for Linear Models","summarize: Many organizations wish to collaboratively train machine learning models on their combined datasets for a common benefit . However, they often cannot share their plaintext datasets due to privacy concerns and\/or business competition. In this paper, we design and build Helen, a system that allows multiple parties to train a linear model without revealing their data, a setting we call coopetitive learning. Compared to prior secure training systems, Helen protects against a much stronger adversary who is malicious and can compromise m-1 out of m parties. Our evaluation shows that Helen can achieve up to five orders of magnitude of performance improvement when compared to training using an existing state-of-the-art secure multi-party computation framework."],["we consider the problem of learning stabilizable systems governed by nonlinear state equation","Non-asymptotic and Accurate Learning of Nonlinear Dynamical Systems","summarize: We consider the problem of learning stabilizable systems governed by nonlinear state equation "],["wireless sensor networks are being deployed for different applications. each protocol has its own structure, goals","A General Model for MAC Protocol Selection in Wireless Sensor Networks","summarize: Wireless Sensor Networks are being deployed for different applications, each having its own structure, goals and requirements. Medium access control protocols play a significant role in WSNs and hence should be tuned to the applications. However, there is no for selecting MAC protocols for different situations. Therefore, it is hard to decide which MAC protocol is good for a given situation. Having a precise model for each MAC protocol, on the other hand, is almost impossible. Using the intuition that the protocols in the same behavioral category perform similarly, our goal in this paper is to introduce a general model that selects the protocol that satisfy the given requirements from the category that performs better for a given context. We define the Combined Performance Function to demonstrate the performance of different categories protocols for different contexts. Having the general model, we then discuss the model scalability for adding new protocols, categories, requirements, and performance criteria. Considering energy consumption and delay as the initial performance criteria of the model, we focus on deriving mathematical models for them. The results extracted from CPF are the same as the well-known rule of thumb for the MAC protocols that verifies our model. We validate our models with the help of simulation study. We also implemented the current CPF model in a web page to make the model online and useful."],["chroma intra-prediction is a complex and computationally intensive coding scheme","Attention-Based Neural Networks for Chroma Intra Prediction in Video Coding","summarize: Neural networks can be successfully used to improve several modules of advanced video coding schemes. In particular, compression of colour components was shown to greatly benefit from usage of machine learning models, thanks to the design of appropriate attention-based architectures that allow the prediction to exploit specific samples in the reference region. However, such architectures tend to be complex and computationally intense, and may be difficult to deploy in a practical video coding pipeline. This work focuses on reducing the complexity of such methodologies, to design a set of simplified and cost-effective attention-based architectures for chroma intra-prediction. A novel size-agnostic multi-model approach is proposed to reduce the complexity of the inference process. The resulting simplified architecture is still capable of outperforming state-of-the-art methods. Moreover, a collection of simplifications is presented in this paper, to further reduce the complexity overhead of the proposed prediction architecture. Thanks to these simplifications, a reduction in the number of parameters of around 90% is achieved with respect to the original attention-based methodologies. Simplifications include a framework for reducing the overhead of the convolutional operations, a simplified cross-component processing model integrated into the original architecture, and a methodology to perform integer-precision approximations with the aim to obtain fast and hardware-aware implementations. The proposed schemes are integrated into the Versatile Video Coding prediction pipeline, retaining compression efficiency of state-of-the-art chroma intra-prediction methods based on neural networks, while offering different directions for significantly reducing coding complexity."],["distributed speech recognition requires to aggregate outputs of distributed deep neural network -based models.","Submodular Rank Aggregation on Score-based Permutations for Distributed Automatic Speech Recognition","summarize: Distributed automatic speech recognition requires to aggregate outputs of distributed deep neural network -based models. This work studies the use of submodular functions to design a rank aggregation on score-based permutations, which can be used for distributed ASR systems in both supervised and unsupervised modes. Specifically, we compose an aggregation rank function based on the Lovasz Bregman divergence for setting up linear structured convex and nested structured concave functions. The algorithm is based on stochastic gradient descent and can obtain well-trained aggregation models. Our experiments on the distributed ASR system show that the submodular rank aggregation can obtain higher speech recognition accuracy than traditional aggregation methods like Adaboost. Code is available online~\\footnote."],["light cone grows logarithmically with time. local changes in energy or other conserve","Out-of-time-ordered correlators in many-body localized systems","summarize: In many-body localized systems, propagation of information forms a light cone that grows logarithmically with time. However, local changes in energy or other conserved quantities typically spread only within a finite distance. Is it possible to detect the logarithmic light cone generated by a local perturbation from the response of a local operator at a later time? We numerically calculate various correlators in the random-field Heisenberg chain. While the equilibrium retarded correlator "],["parallelization program, called MPI_XSTAR, has been developed and implemented in the C","MPI_XSTAR: MPI-based Parallelization of the XSTAR Photoionization Program","summarize: We describe a program for the parallel implementation of multiple runs of XSTAR, a photoionization code that is used to predict the physical properties of an ionized gas from its emission and\/or absorption lines. The parallelization program, called MPI_XSTAR, has been developed and implemented in the C++ language by using the Message Passing Interface protocol, a conventional standard of parallel computing. We have benchmarked parallel multiprocessing executions of XSTAR, using MPI_XSTAR, against a serial execution of XSTAR, in terms of the parallelization speedup and the computing resource efficiency. Our experience indicates that the parallel execution runs significantly faster than the serial execution, however, the efficiency in terms of the computing resource usage decreases with increasing the number of processors used in the parallel computing."],["we prove strong convergence of order.","Convergence of the Euler-Maruyama method for multidimensional SDEs with discontinuous drift and degenerate diffusion coefficient","summarize: We prove strong convergence of order "],["In","A unified approach to convergence rates for ","summarize: In "],["despite the success of embedding graph analytics, we notice that an important concept of embed","From Node Embedding To Community Embedding","summarize: Most of the existing graph embedding methods focus on nodes, which aim to output a vector representation for each node in the graph such that two nodes being close on the graph are close too in the low-dimensional space. Despite the success of embedding individual nodes for graph analytics, we notice that an important concept of embedding communities is missing. Embedding communities is useful, not only for supporting various community-level applications, but also to help preserve community structure in graph embedding. In fact, we see community embedding as providing a higher-order proximity to define the node closeness, whereas most of the popular graph embedding methods focus on first-order and\/or second-order proximities. To learn the community embedding, we hinge upon the insight that community embedding and node embedding reinforce with each other. As a result, we propose ComEmbed, the first community embedding method, which jointly optimizes the community embedding and node embedding together. We evaluate ComEmbed on real-world data sets. We show it outperforms the state-of-the-art baselines in both tasks of node classification and community prediction."],["the SIMPLS or NIPALS algorithm provides estimates on incomplete data. the algorithm is","Determining the Number of Components in PLS Regression on Incomplete Data","summarize: Partial least squares regression---or PLS---is a multivariate method in which models are estimated using either the SIMPLS or NIPALS algorithm. PLS regression has been extensively used in applied research because of its effectiveness in analysing relationships between an outcome and one or several components. Note that the NIPALS algorithm is able to provide estimates on incomplete data. Selection of the number of components used to build a representative model in PLS regression is an important problem. However, how to deal with missing data when using PLS regression remains a matter of debate. Several approaches have been proposed in the literature, including the "],["experimental observation and analyses of supercritical bifurcation of combustion instabilities triggered by","Inlet temperature driven supercritical bifurcation of combustion instabilities in a lean premixed prevaporized combustor","summarize: The present article reports experimental observation and analyses of a supercritical bifurcation of combustion instabilities triggered by the air inlet temperature . The studies are performed with a pressurised kerosene fuelled Lean Premixed Prevaporized combustor operated under elevated temperature. Unlike some previous studies, starting from an unstable condition of the system, the amplitude of combustion instabilities suddenly decrease when Ta exceeds a critical value of Ta =570 K. When the temperature is lowered back the system returns to being unstable without featuring any hysteresis behaviour, as expected in case of a supercritical bifurcation. The unstable flames feature a periodic axial motion of lift-off and re-ignition, characterized as Helmholtz mode. The phase difference between chemiluminescence and pressure signals is found to increase with Ta, exceeding 90 degrees for temperatures higher than 570 K. A low order network framework is conducted, illustrating that when Ta is increased a stability shift of this mode is predicted at Ta near 570 K, in good agreement with the experimental observations. The impact of Ta on the spray characteristics is also examined, finding that higher Ta promotes fuel evaporation and reduces equivalence ratio fluctuation at the exit of the swirler."],["full flux loop signals provide a global measurement proportional to the voltage induced by changes in","The global build-up to intrinsic ELM bursts seen in divertor full flux loops in Jet","summarize: A global signature of the build-up to an intrinsic ELM is found in the phase of signals measured in full flux azimuthal loops in the divertor region of JET. Full flux loop signals provide a global measurement proportional to the voltage induced by changes in poloidal magnetic flux; they are electromagnetically induced by the dynamics of spatially integrated current density. We perform direct time-domain analysis of the high time-resolution full flux loop signals VLD2 and VLD3. We analyze plasmas where a steady H-mode is sustained over several seconds, during which all the observed ELMs are intrinsic; there is no deliberate intent to pace the ELMing process by external means. ELM occurrence times are determined from the Be II emission at the divertor. We previously found that the occurrence times of intrinsic ELMs correlate with specific phases of the VLD2 and VLD3 signals. Here, we investigate how the VLD2 and VLD3 phases vary with time in advance of the ELM occurrence time. We identify a build-up to the ELM in which the VLD2 and VLD3 signals progressively align to the phase at which ELMs preferentially occur, on a ~ 2 -5ms timescale. At the same time, the VLD2 and VLD3 signals become phase synchronized with each other, consistent with the emergence of coherent global dynamics in the integrated current density. In a plasma that remains close to a global magnetic equilibrium, this can reflect bulk displacement or motion of the plasma. This build-up signature to an intrinsic ELM can be extracted from a time interval of data that does not extend beyond the ELM occurrence time, so that these full flux loop signals could assist in ELM prediction or mitigation."],["a successful conversational Recommender System requires proper handling of interactions between conversation and recommendation","Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems","summarize: Recommender systems are embracing conversational technologies to obtain user preferences dynamically, and to overcome inherent limitations of their static models. A successful Conversational Recommender System requires proper handling of interactions between conversation and recommendation. We argue that three fundamental problems need to be solved: 1) what questions to ask regarding item attributes, 2) when to recommend items, and 3) how to adapt to the users' online feedback. To the best of our knowledge, there lacks a unified framework that addresses these problems. In this work, we fill this missing interaction framework gap by proposing a new CRS framework named Estimation-Action-Reflection, or EAR, which consists of three stages to better converse with users. Estimation, which builds predictive models to estimate user preference on both items and item attributes; Action, which learns a dialogue policy to determine whether to ask attributes or recommend items, based on Estimation stage and conversation history; and Reflection, which updates the recommender model when a user rejects the recommendations made by the Action stage. We present two conversation scenarios on binary and enumerated questions, and conduct extensive experiments on two datasets from Yelp and LastFM, for each scenario, respectively. Our experiments demonstrate significant improvements over the state-of-the-art method CRM , corresponding to fewer conversation turns and a higher level of recommendation hits."],["classical strings rotate in deformed three-sphere truncation of the double Yang-","Double Yang-Baxter deformation of spinning strings","summarize: We study the reduction of classical strings rotating in the deformed three-sphere truncation of the double Yang-Baxter deformation of the "],["zigzag nanotubes show stronger binding energies for the single Na atom","Hydrogen adsorption on Na-SWCNT systems","summarize: We investigate the hydrogen adsorption capacity of Na-coated carbon nanotubes using first-principles electronic structure calculations at absolute temperature and pressure. A single Na atom is always found to occupy the hollow site of a hexagonal carbon ring in all the six different SWCNTs considered, with a nearly uniform Na-C bond length of 2.5 A. Semiconducting zigzag nanotubes, and , show stronger binding energies for the Na atom , as compared to metallic SWCNTs with armchair and chiral geometries. The single Na atom can further adsorb up to six hydrogen molecules with a relatively constant binding energy of -0.26 eV\/H"],["the paper derives the post-Newtonian Lagrangian of translational motion of N","Post-Newtonian Lagrangian of an N-body System with Arbitrary Mass and Spin Multipoles","summarize: The present paper derives the post-Newtonian Lagrangian of translational motion of N arbitrary-structured bodies with all mass and spin multipoles in a scalar-tensor theory of gravity. The multipoles depend on time and evolve in accordance with their own dynamic equations of motion. The Lagrangian is retrieved from the post-Newtonian equations of motion by solving the inverse problem of the Lagrangian mechanics and generalizes a well-known Lagrangian of pole-dipole-quadrupole massive particles to the particles of higher multipolarity. Analytic treatment of the higher-order multipole contributions is important for more rigorous computation of gravitational waveform of inspiralling compact binaries at the latest stage of their orbital evolution before merger when tidal and rotational deformations of stars are no longer small and rapidly change in time. The Lagrangian of an N-body system with arbitrary mass and spin multipoles is instrumental for formulation of the post-Newtonian conservation laws of energy, momenta and the integrals of the center of mass."],["Travis CI handles thousands of builds every day to provide valuable feedback to thousands of open-source","An Analysis of 35+ Million Jobs of Travis CI","summarize: Travis CI handles automatically thousands of builds every day to, amongst other things, provide valuable feedback to thousands of open-source developers. In this paper, we investigate Travis CI to firstly understand who is using it, and when they start to use it. Secondly, we investigate how the developers use Travis CI and finally, how frequently the developers change the Travis CI configurations. We observed during our analysis that the main users of Travis CI are corporate users such as Microsoft. And the programming languages used in Travis CI by those users do not follow the same popularity trend than on GitHub, for example, Python is the most popular language on Travis CI, but it is only the third one on GitHub. We also observe that Travis CI is set up on average seven days after the creation of the repository and the jobs are still mainly used to run tests. And finally, we observe that 7.34% of the commits modify the Travis CI configuration. We share the biggest benchmark of Travis CI jobs : it contains 35,793,144 jobs from 272,917 different GitHub projects."],["phylogeny is the field of modelling the temporal discrete dynamics of speciation","Modelling trait dependent speciation with Approximate Bayesian Computation","summarize: Phylogeny is the field of modelling the temporal discrete dynamics of speciation. Complex models can nowadays be studied using the Approximate Bayesian Computation approach which avoids likelihood calculations. The field's progression is hampered by the lack of robust software to estimate the numerous parameters of the speciation process. In this work we present an R package, pcmabc, based on Approximate Bayesian Computations, that implements three novel phylogenetic algorithms for trait-dependent speciation modelling. Our phylogenetic comparative methodology takes into account both the simulated traits and phylogeny, attempting to estimate the parameters of the processes generating the phenotype and the trait. The user is not restricted to a predefined set of models and can specify a variety of evolutionary and branching models. We illustrate the software with a simulation-reestimation study focused around the branching Ornstein-Uhlenbeck process, where the branching rate depends non-linearly on the value of the driving Ornstein-Uhlenbeck process. Included in this work is a tutorial on how to use the software."],["the proposed Res2Net block can be plugged into the state-of-the-art","Res2Net: A New Multi-scale Backbone Architecture","summarize: Representing features at multiple scales is of great importance for numerous vision tasks. Recent advances in backbone convolutional neural networks continually demonstrate stronger multi-scale representation ability, leading to consistent performance gains on a wide range of applications. However, most existing methods represent the multi-scale features in a layer-wise manner. In this paper, we propose a novel building block for CNNs, namely Res2Net, by constructing hierarchical residual-like connections within one single residual block. The Res2Net represents multi-scale features at a granular level and increases the range of receptive fields for each network layer. The proposed Res2Net block can be plugged into the state-of-the-art backbone CNN models, e.g., ResNet, ResNeXt, and DLA. We evaluate the Res2Net block on all these models and demonstrate consistent performance gains over baseline models on widely-used datasets, e.g., CIFAR-100 and ImageNet. Further ablation studies and experimental results on representative computer vision tasks, i.e., object detection, class activation mapping, and salient object detection, further verify the superiority of the Res2Net over the state-of-the-art baseline methods. The source code and trained models are available on https:\/\/mmcheng.net\/res2net\/."],["the proposed system is in pilot phase where 4 sensor nodes are deployed in indoor environment.","A Low Cost ZigBee Sensor Network Architecture for Indoor Air Quality Monitoring","summarize: This paper presents a low-cost system architecture that has been proposed for automatically monitoring air quality indoors and continuously in real-time. The designed system is in pilot phase where 4 sensor nodes are deployed in indoor environment, and data over 4 weeks has been collected and performance analysis and assessment are performed. Environmental data from sensor nodes are sent through ZigBee communication protocol. The proposed system is low in cost, and achieves low power consumption. Hardware and network architecture are presented in addition to real-world deployment."],["a comparative study of stellar population profiles of 635 disk galaxies.","New Constraints on the Origin of Surface Brightness Profile Breaks of Disk Galaxies from MaNGA","summarize: In an effort to probe the origin of surface brightness profile breaks widely observed in nearby disk galaxies, we carry out a comparative study of stellar population profiles of 635 disk galaxies selected from the MaNGA spectroscopic survey. We classify our galaxies into single exponential , down-bending and up-bending SBP types, and derive their spin parameters and radial profiles of age\/metallicity-sensitive spectral features. Most TII galaxies have down-bending star formation rate radial profiles, implying that abrupt radial changes of SFR intensities contribute to the formation of both TII and TIII breaks. Nevertheless, a comparison between our galaxies and simulations suggests that stellar migration plays a significant role in weakening down-bending "],["the elliptic Euler solutions are a complex complex complex. the","Maslov-type indices and linear stability of elliptic Euler solutions of the three-body problem","summarize: In this paper, we use the central configuration coordinate decomposition to study the linearized Hamiltonian system near the elliptic Euler solutions. Then using the Maslov-type \\omega-index theory of symplectic paths and the theory of linear operators we compute the \\omega-indices and obtain certain properties of linear stability of the Euler elliptic solutions of the classical three-body problem."],["search engine combines visual and textual cues to retrieve items from a multimedia database","DeepStyle: Multimodal Search Engine for Fashion and Interior Design","summarize: In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21% on the tested datasets. Our search engine is commercially deployed and available through a Web-based application."],["bio-inspired workpiece structural optimization approach is presented in this paper. aim of this method","Bio-inspired method based on bone architecture to optimize the structure of mechanical workspieces","summarize: Nowadays, additive manufacturing processes greatly simplify the production of openwork workpiece providing new opportunities for workpieces design. Based on Nature knowledge, a new bio-inspired workpiece structural optimization approach is presented in this paper. This approach is derived from bones structure. The aim of this method is to reduce the workpiece weight maintaining an acceptable resistance. Like in bones, the porosity of the part to optimize was controlled by a bio-inspired method as function of the local stress field. Shape, size and orientation of the porosities were derived from bone structure; two main strategies were used: one inspired of avian species and other inspired of terrestrial mammalian. Subsequently, to validate this method, an experimental test was carried out for comparing a topological optimization and the proposed bio-inspired designs. This test was conducted on a beam part in 2.5D subjected to a static three-point bending with 65% of density. Three beams were manufactured by 3D metal printing: two bio-inspired beams and the last designed using a topological optimization method. Experimental test results demonstrated the usefulness of the proposed method. This bio-inspired structural optimization approach opens up new prospects in design of openwork workpiece."],["work studies the denoising of piecewise smooth graph signals. graph trend filtering framework","Vector-Valued Graph Trend Filtering with Non-Convex Penalties","summarize: This work studies the denoising of piecewise smooth graph signals that exhibit inhomogeneous levels of smoothness over a graph, where the value at each node can be vector-valued. We extend the graph trend filtering framework to denoising vector-valued graph signals with a family of non-convex regularizers, which exhibit superior recovery performance over existing convex regularizers. Using an oracle inequality, we establish the statistical error rates of first-order stationary points of the proposed non-convex method for generic graphs. Furthermore, we present an ADMM-based algorithm to solve the proposed method and establish its convergence. Numerical experiments are conducted on both synthetic and real-world data for denoising, support recovery, event detection, and semi-supervised classification."],["no-go theorems have played an important role in the development and assessment of","No-Go Theorems: What Are They Good For?","summarize: No-go theorems have played an important role in the development and assessment of scientific theories. They have stopped whole research programs and have given rise to strong ontological commitments. Given the importance they obviously have had in physics and philosophy of physics and the huge amount of literature on the consequences of specific no-go theorems, there has been relatively little attention to the more abstract assessment of no-go theorems as a tool in theory development. We will here provide this abstract assessment of no-go theorems and conclude that the methodological implications one may draw from no-go theorems are in disagreement with the implications that have often been drawn from them in the history of science."],["model trees are a way to combine simple and highly transparent models. the underlying simple","A Gradient-Based Split Criterion for Highly Accurate and Transparent Model Trees","summarize: Machine learning algorithms aim at minimizing the number of false decisions and increasing the accuracy of predictions. However, the high predictive power of advanced algorithms comes at the costs of transparency. State-of-the-art methods, such as neural networks and ensemble methods, often result in highly complex models that offer little transparency. We propose shallow model trees as a way to combine simple and highly transparent predictive models for higher predictive power without losing the transparency of the original models. We present a novel split criterion for model trees that allows for significantly higher predictive power than state-of-the-art model trees while maintaining the same level of simplicity. This novel approach finds split points which allow the underlying simple models to make better predictions on the corresponding data. In addition, we introduce multiple mechanisms to increase the transparency of the resulting trees."],["unsupervised anomaly detection approaches have been made in the medical domain. previously, deep spatial","Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images","summarize: Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning."],["gyroscopes use the physics of exceptional points. the measurable","Ultrasensitive micro-scale parity-time-symmetric ring laser gyroscope","summarize: We propose a new scheme for ultra-sensitive laser gyroscopes that utilizes the physics of exceptional points. By exploiting the properties of such non-Hermitian degeneracies, we show that the rotation-induced frequency splitting becomes proportional to the square root of the gyration speed- thus enhancing the sensitivity to low angular rotations by orders of magnitudes. In addition, at its maximum sensitivity limit, the measurable spectral splitting is independent of the radius of the rings involved. Our work paves the way towards a new class of ultra-sensitive miniature ring laser gyroscopes on chip."],["a network of two nodes separated by a noisy channel with two-sided state information","Strong coordination of signals and actions over noisy channels with two-sided state information","summarize: We consider a network of two nodes separated by a noisy channel with two-sided state information, in which the input and output signals have to be coordinated with the source and its reconstruction. In the case of non-causal encoding and decoding, we propose a joint source-channel coding scheme and develop inner and outer bounds for the strong coordination region. While the inner and outer bounds do not match in general, we provide a complete characterization of the strong coordination region in three particular cases: i) when the channel is perfect; ii) when the decoder is lossless; and iii) when the random variables of the channel are independent from the random variables of the source. Through the study of these special cases, we prove that the separation principle does not hold for joint source-channel strong coordination. Finally, in the absence of state information, we show that polar codes achieve the best known inner bound for the strong coordination region, which therefore offers a constructive alternative to random binning and coding proofs."],["kinetic functions uniquely characterize macro-scale dynamics of small-scale dependent, undercompressive","Kinetic functions for nonclassical shocks, entropy stability, and discrete summation by parts","summarize: We study nonlinear hyperbolic conservation laws with non-convex flux in one space dimension and, for a broad class of numerical methods based on summation by parts operators, we compute numerically the kinetic functions associated with each scheme. As established by LeFloch and collaborators, kinetic functions uniquely characterize the macro-scale dynamics of small-scale dependent, undercompressive, nonclassical shock waves. We show here that various entropy-dissipative numerical schemes can yield nonclassical solutions containing classical shocks, including Fourier methods with spectral viscosity, finite difference schemes with artificial dissipation, discontinuous Galerkin schemes with or without modal filtering, and TeCNO schemes. We demonstrate numerically that entropy stability does not imply uniqueness of the limiting numerical solutions for scalar conservation laws in one space dimension, and we compute the associated kinetic functions in order to distinguish between these schemes. In addition, we design entropy-dissipative schemes for the Keyfitz-Kranzer system whose solutions are measures with delta shocks. This system illustrates the fact that entropy stability does not imply boundedness under grid refinement."],["the recently introduced 'neural-style' algorithm succeeds in merging the perceived","Neural Style Representations and the Large-Scale Classification of Artistic Style","summarize: The artistic style of a painting is a subtle aesthetic judgment used by art historians for grouping and classifying artwork. The recently introduced `neural-style' algorithm substantially succeeds in merging the perceived artistic style of one image or set of images with the perceived content of another. In light of this and other recent developments in image analysis via convolutional neural networks, we investigate the effectiveness of a `neural-style' representation for classifying the artistic style of paintings."],["the French writer colette has a question about the majesty of ce qui fin","Historical Developments of the Theory of Transcendence and Algebraic Independence","summarize: Qu'est la majest de ce qui finit, auprs des dparts titubants, des dsordres de l'aurore?. Let this question from the French writer Colette guide us to walk through the promising beginnings, fascinating evolutions and results, challenging open problems and perspectives of an exciting theory that is far from revealing all its secrets."],["a planar superfluid predicts vortex clusters to reorder above","Giant Vortex Clusters in a Two-Dimensional Quantum Fluid","summarize: Adding energy to a system through transient stirring usually leads to more disorder. In contrast, point-like vortices in a bounded two-dimensional fluid are predicted to reorder above a certain energy, forming persistent vortex clusters. Here we realize experimentally these vortex clusters in a planar superfluid: a "],["a clique in a link stream is defined as a set of nodes and","Enumerating maximal cliques in link streams with durations","summarize: Link streams model interactions over time, and a clique in a link stream is defined as a set of nodes and a time interval such that all pairs of nodes in this set interact permanently during this time interval. This notion was introduced recently in the case where interactions are instantaneous. We generalize it to the case of interactions with durations and show that the instantaneous case actually is a particular case of the case with durations. We propose an algorithm to detect maximal cliques that improves our previous one for instantaneous link streams, and performs better than the state of the art algorithms in several cases of interest."],["traditional solutions to HDR imaging are designed for and applied to CMOS image sensors.","HDR Imaging with Quanta Image Sensors: Theoretical Limits and Optimal Reconstruction","summarize: High dynamic range imaging is one of the biggest achievements in modern photography. Traditional solutions to HDR imaging are designed for and applied to CMOS image sensors . However, the mainstream one-micron CIS cameras today generally have a high read noise and low frame-rate. These, in turn, limit the acquisition speed and quality, making the cameras slow in the HDR mode. In this paper, we propose a new computational photography technique for HDR imaging. Recognizing the limitations of CIS, we use the Quanta Image Sensor to trade the spatial-temporal resolution with bit-depth. QIS is a single-photon image sensor that has comparable pixel pitch to CIS but substantially lower dark current and read noise. We provide a complete theoretical characterization of the sensor in the context of HDR imaging, by proving the fundamental limits in the dynamic range that QIS can offer and the trade-offs with noise and speed. In addition, we derive an optimal reconstruction algorithm for single-bit and multi-bit QIS. Our algorithm is theoretically optimal for \\emph linear reconstruction schemes based on exposure bracketing. Experimental results confirm the validity of the theory and algorithm, based on synthetic and real QIS data."],["a device which can generate, manipulate, and analyse two-qubit entangled states","Silicon photonic processor of two-qubit entangling quantum logic","summarize: Entanglement is a fundamental property of quantum mechanics, and is a primary resource in quantum information systems. Its manipulation remains a central challenge in the development of quantum technology. In this work, we demonstrate a device which can generate, manipulate, and analyse two-qubit entangled states, using miniature and mass-manufacturable silicon photonics. By combining four photon-pair sources with a reconfigurable six-mode interferometer, embedding a switchable entangling gate, we generate two-qubit entangled states, manipulate their entanglement, and analyse them, all in the same silicon chip. Using quantum state tomography, we show how our source can produce a range of entangled and separable states, and how our switchable controlled-Z gate operates on them, entangling them or making them separable depending on its configuration."],["electronic voting system deals with selection, casting of votes with embedded security mechanism. it can include","A Unique One-Time Password Table Sequence Pattern Authentication: Application to Bicol University Union of Federated Faculty Association, Inc. eVoting System","summarize: Electronic Voting System is a type of voting program that deals primarily with the selection, the casting of votes with embedded security mechanism that detects errors, and the tamper-proof election of results done through the use of an electronic system. It can include optical scan, specialized voting kiosks and Internet voting approach. Most organizations have difficulties when it comes to voting and the Bicol University Union of Federated Faculty Association Incorporated is not an exception. Some of the problems involved include convenience, cost, geographical location of the polling precinct, and voting turnouts. This study extends the scope of the current BUUFFAI eVoting system to address such issues and to eliminate inconvenience both to the faculty voters and the facilitators. This voting scheme used an algorithmic OTP scheme based on table sequence pattern schedule that randomly generates an XY coordinate unique to voters that will be sent to voter registered email address. This study addressed the security requirements and maintained election procedures with confidentiality, integrity and availability."],["a type A in homotopy type theory defines the free infinity-group on A","Free Higher Groups in Homotopy Type Theory","summarize: Given a type A in homotopy type theory , we can define the free infinity-group on A as the loop space of the suspension of A+1. Equivalently, this free higher group can be defined as a higher inductive type F with constructors unit : F, cons : A -> F -> F, and conditions saying that every cons is an auto-equivalence on F. Assuming that A is a set , we are interested in the question whether F is a set as well, which is very much related to an open problem in the HoTT book. We show an approximation to the question, namely that the fundamental groups of F are trivial, i.e. that the 1-truncation of F is a set."],["agent-based models provide a flexible framework that is often used for modelling biological systems.","Learning differential equation models from stochastic agent-based model simulations","summarize: Agent-based models provide a flexible framework that is frequently used for modelling many biological systems, including cell migration, molecular dynamics, ecology, and epidemiology. Analysis of the model dynamics can be challenging due to their inherent stochasticity and heavy computational requirements. Common approaches to the analysis of agent-based models include extensive Monte Carlo simulation of the model or the derivation of coarse-grained differential equation models to predict the expected or averaged output from the agent-based model. Both of these approaches have limitations, however, as extensive computation of complex agent-based models may be infeasible, and coarse-grained differential equation models can fail to accurately describe model dynamics in certain parameter regimes. We propose that methods from the equation learning field provide a promising, novel, and unifying approach for agent-based model analysis. Equation learning is a recent field of research from data science that aims to infer differential equation models directly from data. We use this tutorial to review how methods from equation learning can be used to learn differential equation models from agent-based model simulations. We demonstrate that this framework is easy to use, requires few model simulations, and accurately predicts model dynamics in parameter regions where coarse-grained differential equation models fail to do so. We highlight these advantages through several case studies involving two agent-based models that are broadly applicable to biological phenomena: a birth-death-migration model commonly used to explore cell biology experiments and a susceptible-infected-recovered model of infectious disease spread."],["this paper also includes some criteria used for the construction of this modeling platform. the power of","Basis to develop a platform for multiple-scale complex systems modeling and visualization: MoNet","summarize: This work presents some characteristics of MoNet, a digital platform for the modeling and visualization of complex systems. Emphasis is on the ideas that allowed the successful progressive development of this modeling platform, which goes along with the implementation of applications to the modeling of several studied systems. The platform can represent different aspects of systems modeled at different observation scales. This tool offers advantages in the sense of favoring the perception of the phenomenon of the emergence of information, associated with changes of scale. This paper also includes some criteria used for the construction of this modeling platform. The power of current computers has made practical representing graphic resources such as shapes, line thickness, overlaying-text tags, colors, and transparencies, in the graphical modeling of systems. By visualizing diagrams conveniently designed to highlight contrasts, these modeling platforms allow the recognition of patterns that drive our understanding of systems and their structure. Graphs reflecting the benefits of the tool regarding the visualization of systems at different scales of observation are presented to illustrate the application of the platform."],["image forensic plays a crucial role in both criminal investigations and civil litigation. there are","A Survey of Machine Learning Techniques in Adversarial Image Forensics","summarize: Image forensic plays a crucial role in both criminal investigations and civil litigation . Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial examples, with real-world consequences . Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios."],["low-frequency, GMRT observations at 240, 610 and 1300 MHz","GMRT observations of IC 711 -- The longest head-tail radio galaxy known","summarize: We present low-frequency, GMRT observations at 240, 610 and 1300 MHz of IC~711, a narrow angle tail radio galaxy. The total angular extent of the radio emission, "],["polynomials defined via fillings of diagrams satisfy linear recurrence","Polynomials defined by tableaux and linear recurrences","summarize: We show that several families of polynomials defined via fillings of diagrams satisfy linear recurrences under a natural operation on the shape of the diagram. We focus on key polynomials, , and Demazure atoms. The same technique can be applied to Hall-Littlewood polynomials and dual Grothendieck polynomials. The motivation behind this is that such recurrences are strongly connected with other nice properties, such as interpretations in terms of lattice points in polytopes and divided difference operators."],["we introduce a novel RGB-D patch descriptor designed for SLAM reconstruction","PlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction","summarize: We introduce a novel RGB-D patch descriptor designed for detecting coplanar surfaces in SLAM reconstruction. The core of our method is a deep convolutional neural net that takes in RGB, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to find coplanar patches from other images.We train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity RGB-D scans. Experiments show that our learned descriptor outperforms alternatives extended for this new task by a significant margin. In addition, we demonstrate the benefits of coplanarity matching in a robust RGBD reconstruction formulation.We find that coplanarity constraints detected with our method are sufficient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on standard benchmarks when combined with a simple keypoint method."],["we demonstrate how the distributions of link probabilities can be used to generate various complex networks simply","A Simple and Generic Paradigm for Creating Complex Networks Using the Strategy of Vertex Selecting-and-Pairing","summarize: In many networks of scientific interest we know that the link between any pair of vertices conforms to a specific probability, such as the link probability in the Barab\\'asi-Albert scale-free networks. Here we demonstrate how the distributions of link probabilities can be utilized to generate various complex networks simply and effectively. We focus in particular on the problem of complex network generation and develop a straightforward paradigm by using the strategy of vertex selecting-and-pairing to create complex networks more generic than other relevant approaches. Crucially, our paradigm is capable of generating various complex networks with varied degree distributions by using different probabilities for selecting vertices, while in contrast other relevant approaches can only be used to generate a specific type of complex networks. We demonstrate our paradigm on four synthetic Barab\\'asi-Albert scale-free networks, four synthetic Watts-Strogatz small-world networks, and on a real email network with known degree distributions."],["we determine the relationship between the turnaround radius and the turnaround radius.","Turnaround radius in ","summarize: We determine the relationship between the turnaround radius, "],["the approach is hybrid: discrete at cellular scale and continuous at molecular level","A discrete in continuous mathematical model of cardiac progenitor cells formation and growth as spheroid clusters ","summarize: We propose a discrete in continuous mathematical model describing the in vitro growth process of biophsy-derived mammalian cardiac progenitor cells growing as clusters in the form of spheres . The approach is hybrid: discrete at cellular scale and continuous at molecular level. In the present model cells are subject to the self-organizing collective dynamics mechanism and, additionally, they can proliferate and differentiate, also depending on stochastic processes. The two latter processes are triggered and regulated by chemical signals present in the environment. Numerical simulations show the structure and the development of the clustered progenitors and are in a good agreement with the results obtained from in vitro experiments."],["neural networks have a long history of logical compositionality. the new techniques are","Neural Multi-Step Reasoning for Question Answering on Semi-Structured Tables","summarize: Advances in natural language processing tasks have gained momentum in recent years due to the increasingly popular neural network methods. In this paper, we explore deep learning techniques for answering multi-step reasoning questions that operate on semi-structured tables. Challenges here arise from the level of logical compositionality expressed by questions, as well as the domain openness. Our approach is weakly supervised, trained on question-answer-table triples without requiring intermediate strong supervision. It performs two phases: first, machine understandable logical forms are generated from natural language questions following the work of . Second, paraphrases of logical forms and questions are embedded in a jointly learned vector space using word and character convolutional neural networks. A neural scoring function is further used to rank and retrieve the most probable logical form of a question. Our best single model achieves 34.8% accuracy on the WikiTableQuestions dataset, while the best ensemble of our models pushes the state-of-the-art score on this task to 38.7%, thus slightly surpassing both the engineered feature scoring baseline, as well as the Neural Programmer model of ."],["econometric model shifts day-ahead supply curves to calculate intraday","The Impact of Renewable Energy Forecasts on Intraday Electricity Prices","summarize: In this paper we study the impact of errors in wind and solar power forecasts on intraday electricity prices. We develop a novel econometric model which is based on day-ahead wholesale auction curves data and errors in wind and solar power forecasts. The model shifts day-ahead supply curves to calculate intraday prices. We apply our model to the German EPEX SPOT SE data. Our model outperforms both linear and non-linear benchmarks. Our study allows us to conclude that errors in renewable energy forecasts exert a non-linear impact on intraday prices. We demonstrate that additional wind and solar power capacities induce non-linear changes in the intraday price volatility. Finally, we comment on economical and policy implications of our findings."],["initialization of weights in backpropagation neural net is heavily affected by initialization of","A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition","summarize: Convergence rate of training algorithms for neural networks is heavily affected by initialization of weights. In this paper, an original algorithm for initialization of weights in backpropagation neural net is presented with application to character recognition. The initialization method is mainly based on a customization of the Kalman filter, translating it into Bayesian statistics terms. A metrological approach is used in this context considering weights as measurements modeled by mutually dependent normal random variables. The algorithm performance is demonstrated by reporting and discussing results of simulation trials. Results are compared with random weights initialization and other methods. The proposed method shows an improved convergence rate for the backpropagation training algorithm."],["multiple mobile agents need to collaborate to move the message. each agent has a limited energy","Collaborative Delivery with Energy-Constrained Mobile Robots","summarize: We consider the problem of collectively delivering some message from a specified source to a designated target location in a graph, using multiple mobile agents. Each agent has a limited energy which constrains the distance it can move. Hence multiple agents need to collaborate to move the message, each agent handing over the message to the next agent to carry it forward. Given the positions of the agents in the graph and their respective budgets, the problem of finding a feasible movement schedule for the agents can be challenging. We consider two variants of the problem: in non-returning delivery, the agents can stop anywhere; whereas in returning delivery, each agent needs to return to its starting location, a variant which has not been studied before. We first provide a polynomial-time algorithm for returning delivery on trees, which is in contrast to the known NP-hardness of the non-returning version. In addition, we give resource-augmented algorithms for returning delivery in general graphs. Finally, we give tight lower bounds on the required resource augmentation for both variants of the problem. In this sense, our results close the gap left by previous research."],["the Vertex Reinforced Jump Process introduces a continuous space limit in dimension one","Fine mesh limit of the VRJP in dimension one and Bass-Burdzy flow","summarize: We introduce a continuous space limit of the Vertex Reinforced Jump Process in dimension one, which we call Linearly Reinforced Motion on "],["octahedra bonding geometry engineering is a simple way of tailoring","Propagation control of octahedral tilt in SrRuO3 via artificial heterostructuring","summarize: Bonding geometry engineering of metal-oxygen octahedra is a facile way of tailoring various functional properties of transition metal oxides. Several approaches, including epitaxial strain, thickness, and stoichiometry control, have been proposed to efficiently tune the rotation and tilting of the octahedra, but these approaches are inevitably accompanied by unnecessary structural modifications such as changes in thin-film lattice parameters. In this study, we propose a method to selectively engineer the octahedral bonding geometries, while maintaining other parameters that might implicitly influence the functional properties. A concept of octahedral tilt propagation engineering has been developed using atomically designed SrRuO3\/SrTiO3 superlattices. In particular, the propagation of RuO6 octahedral tilting within the SrRuO3 layers having identical thicknesses was systematically controlled by varying the thickness of adjacent SrTiO3 layers. This led to a substantial modification in the electromagnetic properties of the SrRuO3 layer, significantly enhancing the magnetic moment of Ru. Our approach provides a method to selectively manipulate the bonding geometry of strongly correlated oxides, thereby enabling a better understanding and greater controllability of their functional properties."],["2DCCA is incapable of extracting sufficient discriminatory representations. this is a","A Complete Discriminative Tensor Representation Learning for Two-Dimensional Correlation Analysis","summarize: As an effective tool for two-dimensional data analysis, two-dimensional canonical correlation analysis is not only capable of preserving the intrinsic structural information of original two-dimensional data, but also reduces the computational complexity effectively. However, due to the unsupervised nature, 2DCCA is incapable of extracting sufficient discriminatory representations, resulting in an unsatisfying performance. In this letter, we propose a complete discriminative tensor representation learning method based on linear correlation analysis for analyzing 2D signals . This letter shows that the introduction of the complete discriminatory tensor representation strategy provides an effective vehicle for revealing, and extracting the discriminant representations across the 2D data sets, leading to improved results. Experimental results show that the proposed CDTRL outperforms state-of-the-art methods on the evaluated data sets."],["we develop a general term structure framework taking stochastic discontinuities explicitly into account.","Term structure modeling for multiple curves with stochastic discontinuities","summarize: We develop a general term structure framework taking stochastic discontinuities explicitly into account. Stochastic discontinuities are a key feature in interest rate markets, as for example the jumps of the term structures in correspondence to monetary policy meetings of the ECB show. We provide a general analysis of multiple curve markets under minimal assumptions in an extended HJM framework and provide a fundamental theorem of asset pricing based on NAFLVR. The approach with stochastic discontinuities permits to embed market models directly, unifying seemingly different modeling philosophies. We also develop a tractable class of models, based on affine semimartingales, going beyond the requirement of stochastic continuity."],["i-TED consists of a total energy detector and a Compton camera","First i-TED demonstrator: a Compton imager with Dynamic Electronic Collimation","summarize: i-TED consists of both a total energy detector and a Compton camera primarily intended for the measurement of neutron capture cross sections by means of the simultaneous combination of neutron time-of-flight and "],["68 solar mass black hole detected in binary system LB-1. black hole is twice","No signature of the orbital motion of a putative 70 solar mass black hole in LB-1","summarize: Liu et al. recently reported the detection of a 68 solar mass black hole paired with an 8.2 Msun B-type sub-giant star in the 78.9-day spectroscopic binary system LB-1. Such a black hole is over twice as massive as any other known stellar-mass black hole with non-compact companions2 and its mass approaches those that result from BH-BH coalescences that are detected by gravitational wave interferometers. Its presence in a solar-like metallicity environment challenges conventional theories of massive binary evolution, stellar winds and core-collapse supernovae, so that more exotic scenarios seem to be needed to explain the existence and properties of LB-1. Here, we show that the observational diagnostics used to derive the BH mass results from the orbital motion of the B-type star, not that of the BH. As a consequence, no evidence for a massive BH remains in the data, therefore solving the existing tension with formation models of such a massive BH at solar metallicity and with theories of massive star evolution in general."],["Lieb-Liniger model for bosonic gas of bosonic gas.","Density form factors of the 1D Bose gas for finite entropy states","summarize: We consider the Lieb-Liniger model for a gas of bosonic "],["experiments show a massive acceleration of the annealing of a monolayer of passive","Activity-controlled Annealing of Colloidal Monolayers","summarize: Molecular motors are essential to the living, they generate additional fluctuations that boost transport and assist assembly. Self-propelled colloids, that consume energy to move, hold similar potential for the man-made assembly of microparticles. Yet, experiments showing their use as a powerhouse in materials science lack. Our work explores the design of man-made materials controlled by fluctuations, arising from the internal forces generated by active colloids. Here we show a massive acceleration of the annealing of a monolayer of passive beads by moderate addition of self-propelled microparticles. We rationalize our observations with a model of collisions that drive active fluctuations to overcome kinetic barriers and activate the annealing. The experiment is quantitatively compared with Brownian dynamic simulations that further unveil a dynamical transition in the mechanism of annealing. Active dopants travel uniformly in the system or co-localize at the grain boundaries as a result of the persistence of their motion. Our findings uncover the potential of man-made materials controlled by internal activity and lay the groundwork for the rise of materials science beyond equilibrium."],["affine connections on manifolds are determined by the 1-connection form and the fundamental","Flat Affine Manifolds And Their Transformations","summarize: We give a characterization of flat affine connections on manifolds by means of a natural affine representation of the universal covering of the Lie group of diffeomorphisms preserving the connection. From the infinitesimal point of view, this representation is determined by the 1-connection form and the fundamental form of the bundle of linear frames of the manifold. We show that the group of affine transformations of a real flat affine "],["nonreciprocal devices are available for free-space and fibre-optic communication systems","Inverse-designed photonic circuits for fully passive, bias-free Kerr-based nonreciprocal transmission and routing","summarize: Nonreciprocal devices such as isolators and circulators are key enabling technologies for communication systems, both at microwave and optical frequencies. While nonreciprocal devices based on magnetic effects are available for free-space and fibre-optic communication systems, their on-chip integration has been challenging, primarily due to the concomitant high insertion loss, weak magneto-optical effects, and material incompatibility. We show that Kerr nonlinear resonators can be used to achieve all-passive, low-loss, bias-free, broadband nonreciprocal transmission and routing for applications in photonic systems such as chip-scale LIDAR. A multi-port nonlinear Fano resonator is used as an on-chip, all-optical router for frequency comb based distance measurement. Since time-reversal symmetry imposes stringent limitations on the operating power range and transmission of a single nonlinear resonator, we implement a cascaded Fano-Lorentzian resonator system that overcomes these limitations and significantly improves the insertion loss, bandwidth and non-reciprocal power range of current state-of-the-art devices. This work provides a platform-independent design for nonreciprocal transmission and routing that are ideally suited for photonic integration."],["linear differential operators are defined on domains, which degenerate fast enough toward the boundary","Continuous maximal regularity on singular manifolds and its applications","summarize: In this article, we set up the continuous maximal regularity theory for a class of linear differential operators on manifolds with singularities. These operators exhibit degenerate or singular behaviors while approaching the singular ends. Particular examples of such operators include differential operators defined on domains, which degenerate fast enough toward the boundary. Applications of the theory established herein are shown to the Yamabe flow, the porous medium equation, the parabolic "],["a robust and stable relationship between a country's productive structure and its economic growth has","The Impact of Services on Economic Complexity: Service Sophistication as Route for Economic Growth","summarize: Economic complexity reflects the amount of knowledge that is embedded in the productive structure of an economy. By combining tools from network science and econometrics, a robust and stable relationship between a country's productive structure and its economic growth has been established. Here we report that not only goods but also services are important for predicting the rate at which countries will grow. By adopting a terminology which classifies manufactured goods and delivered services as products, we investigate the influence of services on the country's productive structure. In particular, we provide evidence that complexity indices for services are in general higher than those for goods, which is reflected in a general tendency to rank countries with developed service sector higher than countries with economy centred on manufacturing of goods. By focusing on country dynamics based on experimental data, we investigate the impact of services on the economic complexity of countries measured in the product space . Importantly, we show that diversification of service exports and its sophistication can provide an additional route for economic growth in both developing and developed countries."],["a study of the relation between the charge density at a point on a conducting surface","On the Dependence of Charge Density on Surface Curvature of an Isolated Conductor","summarize: A study of the relation between the electrostatic charge density at a point on a conducting surface and the curvature of the surface is presented. Two major scientific literature on this topic are reviewed and the apparent discrepancy between them is resolved. Hence, a step is taken towards obtaining a general analytic formula for relating the charge density with surface curvature of conductors. The merit of this formula and its limitations are discussed."],["the interaction between edge dislocations and Guinier-Preston zones was analyzed.","An atomistic investigation of the interaction of dislocations with Guinier-Preston zones in Al-Cu alloys","summarize: The interaction between edge dislocations and Guinier-Preston zones in an Al-Cu alloy was analyzed by means of atomistic simulations. The different thermodynamic functions that determine the features of these obstacles for the dislocation glide were computed using molecular statics, molecular dynamics and the nudged elastic band method. It was found that Guinier-Preston zones are sheared by dislocations and the rate at which dislocations overcome the precipitate is controlled by the activation energy, "],["dialogue policy learning for task-oriented dialogue systems has enjoyed great progress lately. but these approaches","Rethinking Supervised Learning and Reinforcement Learning in Task-Oriented Dialogue Systems","summarize: Dialogue policy learning for task-oriented dialogue systems has enjoyed great progress recently mostly through employing reinforcement learning methods. However, these approaches have become very sophisticated. It is time to re-evaluate it. Are we really making progress developing dialogue agents only based on reinforcement learning? We demonstrate how ~traditional supervised learning together with ~a simulator-free adversarial learning method can be used to achieve performance comparable to state-of-the-art RL-based methods. First, we introduce a simple dialogue action decoder to predict the appropriate actions. Then, the traditional multi-label classification solution for dialogue policy learning is extended by adding dense layers to improve the dialogue agent performance. Finally, we employ the Gumbel-Softmax estimator to alternatively train the dialogue agent and the dialogue reward model without using reinforcement learning. Based on our extensive experimentation, we can conclude the proposed methods can achieve more stable and higher performance with fewer efforts, such as the domain knowledge required to design a user simulator and the intractable parameter tuning in reinforcement learning. Our main goal is not to beat reinforcement learning with supervised learning, but to demonstrate the value of rethinking the role of reinforcement learning and supervised learning in optimizing task-oriented dialogue systems."],["magnetic dressing phenomenon occurs when spins precessing in a static field are subject to","Studying and applying magnetic dressing with a Bell and Bloom magnetometer","summarize: The magnetic dressing phenomenon occurs when spins precessing in a static field are subject to an additional, strong, alternating field. It is usually studied when such extra field is homogeneous and oscillates in one direction. We study the dynamics of spins under dressing condition in two unusual configurations. In the first instance, an inhomogeneous dressing field produces space dependent dressing phenomenon, which helps to operate the magnetometer in strongly inhomogeneous static field. In the second instance, beside the usual configuration with static and the strong orthogonal oscillating magnetic fields, we add a secondary oscillating field, which is perpendicular to both. The system shows novel and interesting features that are accurately explained and modelled theoretically. Possible applications of these novel features are briefly discussed."],["the link uses an active telecommunication fiber network with parallel data traffic. it is equipped","First industrial-grade coherent fiber link for optical frequency standard dissemination","summarize: We report on a fully bi-directional 680~km fiber link connecting two cities for which the equipment, the set up and the characterization are managed for the first time by an industrial consortium. The link uses an active telecommunication fiber network with parallel data traffic and is equipped with three repeater laser stations and four remote double bi-directional Erbium-doped fiber amplifiers. We report a short term stability at 1~s integration time of "],["two common spatial pattern filters for regression problems in BCI. they are extended from the C","Spatial Filtering for EEG-Based Regression Problems in Brain-Computer Interface ","summarize: Electroencephalogram signals are frequently used in brain-computer interfaces , but they are easily contaminated by artifacts and noises, so preprocessing must be done before they are fed into a machine learning algorithm for classification or regression. Spatial filters have been widely used to increase the signal-to-noise ratio of EEG for BCI classification problems, but their applications in BCI regression problems have been very limited. This paper proposes two common spatial pattern filters for EEG-based regression problems in BCI, which are extended from the CSP filter for classification, by making use of fuzzy sets. Experimental results on EEG-based response speed estimation from a large-scale study, which collected 143 sessions of sustained-attention psychomotor vigilance task data from 17 subjects during a 5-month period, demonstrate that the two proposed spatial filters can significantly increase the EEG signal quality. When used in LASSO and k-nearest neighbors regression for user response speed estimation, the spatial filters can reduce the root mean square estimation error by 10.02-19.77%, and at the same time increase the correlation to the true response speed by 19.39-86.47%."],["paper addresses the problem of imaging in the presence of diffraction-photons","Imaging with SPADs and DMDs: Seeing through Diffraction-Photons","summarize: This paper addresses the problem of imaging in the presence of diffraction-photons. Diffraction-photons arise from the low contrast ratio of DMDs , and very much degrade the quality of images captured by SPAD-based systems. Herein, a joint illumination-deconvolution scheme is designed to overcome diffraction-photons, enabling the acquisition of intensity and depth images. Additionally, a proof-of-concept experiment is conducted to demonstrate the viability of the designed scheme. It is shown that by co-designing the illumination and deconvolution phases of imaging, one can substantially overcome diffraction-photons."],["a novel and general task-agnostic search space is proposed. the","MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning","summarize: We propose to incorporate neural architecture search into general-purpose multi-task learning . Existing NAS methods typically define different search spaces according to different tasks. In order to adapt to different task combinations , we disentangle the GP-MTL networks into single-task backbones , and a hierarchical and layerwise features sharing\/fusing scheme across them. This enables us to design a novel and general task-agnostic search space, which inserts cross-task edges into fixed single-task network backbones. Moreover, we also propose a novel single-shot gradient-based search algorithm that closes the performance gap between the searched architectures and the final evaluation architecture. This is realized with a minimum entropy regularization on the architecture weights during the search phase, which makes the architecture weights converge to near-discrete values and therefore achieves a single model. As a result, our searched model can be directly used for evaluation without training from scratch. We perform extensive experiments using different single-task backbones on various task sets, demonstrating the promising performance obtained by exploiting the hierarchical and layerwise features, as well as the desirable generalizability to different i) task sets and ii) single-task backbones. The code of our paper is available at https:\/\/github.com\/bhpfelix\/MTLNAS."],["the solar system motivates students to interest themselves in sciences. the solar system is a","Enacting Planets to Understand Occultation Phenomena","summarize: The Solar System motivates students to interest themselves in sciences, as a large number of concepts may be easily introduced through the observation and understanding of planet's motion. Using a large representation of the Solar System at a human scale , we have conducted different activities with 10 to 16 years old children. In this contribution, we discuss the different scientific concepts covered by the Human Orrery, allowing the connection of both science and mathematics subjects in schools. We then detail how this pedagogical tool may serve to introduce abstract concepts required to understand occultation phenomena through a modelling activity."],["symmetry breaks from a group G to a group H. the deep reason for","The geometric role of symmetry breaking in gravity","summarize: In gravity, breaking symmetry from a group G to a group H plays the role of describing geometry in relation to the geometry the homogeneous space G\/H. The deep reason for this is Cartan's method of equivalence, giving, in particular, an exact correspondence between metrics and Cartan connections. I argue that broken symmetry is thus implicit in any gravity theory, for purely geometric reasons. As an application, I explain how this kind of thinking gives a new approach to Hamiltonian gravity in which an observer field spontaneously breaks Lorentz symmetry and gives a Cartan connection on space."],["fluorescein-a fluorescent marker used as a proxy for drug molecules.","Quantitative Kinetic Models from Intravital Microcopy: A Case Study Using Hepatic Transport","summarize: The liver performs critical physiological functions, including metabolizing and removing substances, such as toxins and drugs, from the bloodstream. Hepatotoxicity itself is intimately linked to abnormal hepatic transport and hepatotoxicity remains the primary reason drugs in development fail and approved drugs are withdrawn from the market. For this reason, we propose to analyze, across liver compartments, the transport kinetics of fluorescein-a fluorescent marker used as a proxy for drug molecules-using intravital microscopy data. To resolve the transport kinetics quantitatively from fluorescence data, we account for the effect that different liver compartments have on fluorescein's emission rate. To do so, we develop ordinary differential equation transport models from the data where the kinetics are related to the observable fluorescence levels by measurement parameters that vary across different liver compartments. On account of the steep non-linearities in the kinetics and stochasticity inherent to the model, we infer kinetic and measurement parameters by generalizing the method of parameter cascades. For this application, the method of parameter cascades ensures fast and precise parameter estimates from noisy time traces."],["a structure theorem for invertible skew-symmetric operators is proved","Real Normal Operators and Williamson's Normal Form","summarize: A simple proof is provided to show that any bounded normal operator on a real Hilbert space is orthogonally equivalent to its transpose. A structure theorem for invertible skew-symmetric operators, which is analogous to the finite dimensional situation is also proved using elementary techniques. The second result is used to establish the main theorem of this article, which is a generalization of Williamson's normal form for bounded positive operators on infinite dimensional separable Hilbert spaces. This has applications in the study of infinite mode Gaussian states."],["astrophysics is a promising source for ground-based GW detectors.","Early Advanced LIGO binary neutron-star sky localization and parameter estimation","summarize: 2015 will see the first observations of Advanced LIGO and the start of the gravitational-wave advanced-detector era. One of the most promising sources for ground-based GW detectors are binary neutron-star coalescences. In order to use any detections for astrophysics, we must understand the capabilities of our parameter-estimation analysis. By simulating the GWs from an astrophysically motivated population of BNSs, we examine the accuracy of parameter inferences in the early advanced-detector era. We find that sky location, which is important for electromagnetic follow-up, can be determined rapidly , but that sky areas may be hundreds of square degrees. The degeneracy between component mass and spin means there is significant uncertainty for measurements of the individual masses and spins; however, the chirp mass is well measured ."],["GW170104 was measured by the twin advanced detectors of the laser interferometer","GW170104: Observation of a 50-Solar-Mass Binary Black Hole Coalescence at Redshift 0.2","summarize: We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are "],["this paper presents a review of the characteristics of the multiple stellar populations observed in globular","Multiple stellar populations and their evolution in globular clusters: A nucleosynthesis perspective","summarize: This paper presents a review of the characteristics of the multiple stellar populations observed in globular clusters, and of their possible origin. The current theoretical issues and the many open questions are discussed."],["the feasible region can be employed for the selection of feasible footholds and CoM tra","Feasible Region: an Actuation-Aware Extension of the Support Region","summarize: In legged locomotion the projection of the robot Center of Mass being inside the convex hull of the contact points is a commonly accepted sufficient condition to achieve static balancing. However, some of these configurations cannot be realized because the joint torques required to sustain them would be above their limits . In this manuscript we rule out such configurations and define the Feasible Region, a revisited support region that guarantees both global static stability in the sense of tipover and slippage avoidance and of existence of a set of joint-torques that are able to sustain the robot body weight. We show that the feasible region can be employed for the selection of feasible footholds and CoM trajectories to achieve static locomotion on rough terrains, also in presence of load intensive tasks. Key results of our approach include the efficiency in the computation of the feasible region thanks to an Iterative Projection algorithm. This allowed us to carry out successful experiments on the HyQ robot, that was able to negotiate obstacles of moderate dimensions while carrying an extra 10 kg payload."],["the stochastic Galerkin method is based on the linear semiconductor Boltzmann","Uniform Spectral Convergence of the Stochastic Galerkin Method for the Linear Semiconductor Boltzmann Equation with Random Inputs and Diffusive Scalings","summarize: In this paper, we study the generalized polynomial chaos based stochastic Galerkin method for the linear semiconductor Boltzmann equation under diffusive scaling and with random inputs from an anisotropic collision kernel and the random initial condition. While the numerical scheme and the proof of uniform-in-Knudsen-number regularity of the distribution function in the random space has been introduced in , the main goal of this paper is to first obtain a sharper estimate on the regularity of the solution-an exponential decay towards its local equilibrium, which then lead to the uniform spectral convergence of the stochastic Galerkin method for the problem under study."],["simple models are preferred over complex models, but over-simplistic models could lead to","Top-down Transformation Choice","summarize: Simple models are preferred over complex models, but over-simplistic models could lead to erroneous interpretations. The classical approach is to start with a simple model, whose shortcomings are assessed in residual-based model diagnostics. Eventually, one increases the complexity of this initial overly simple model and obtains a better-fitting model. I illustrate how transformation analysis can be used as an alternative approach to model choice. Instead of adding complexity to simple models, step-wise complexity reduction is used to help identify simpler and better-interpretable models. As an example, body mass index distributions in Switzerland are modelled by means of transformation models to understand the impact of sex, age, smoking and other lifestyle factors on a person's body mass index. In this process, I searched for a compromise between model fit and model interpretability. Special emphasis is given to the understanding of the connections between transformation models of increasing complexity. The models used in this analysis ranged from evergreens, such as the normal linear regression model with constant variance, to novel models with extremely flexible conditional distribution functions, such as transformation trees and transformation forests."],["abstract syntax tree mapping algorithms are widely used to analyze changes in source code. AST mapping","A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms","summarize: Abstract syntax tree mapping algorithms are widely used to analyze changes in source code. Despite the foundational role of AST mapping algorithms, little effort has been made to evaluate the accuracy of AST mapping algorithms, i.e., the extent to which an algorihtm captures the evolution of code. We observe that a program element often has only one best-mapped program element. Based on this observation, we propose a hierarchical approach to automatically compare the similarity of mapped statements and tokens by different algorithms. By performing the comparison, we determine if each of the compared algorithms generates inaccurate mappings for a statement or its tokens. We invite 12 external experts to determine if three commonly used AST mapping algorithms generate accurate mappings for a statement and its tokens for 200 statements. Based on the experts' feedback,we observe that our approach achieves a precision of 0.98--1.00 and a recall of 0.65--0.75. Furthermore, we conduct a large-scale study with a dataset of ten Java projects, containing a total of 263,165 file revisions. Our approach determines that GumTree, MTDiff and IJM generate inaccurate mappings for 20%--29%, 25%--36% and 21%--30% of the file revisions, respectively. Our experimental results show that state-of-art AST mapping agorithms still need improvements."],["active learning frameworks select data points that can accelerate learning process of a model. the","Diminishing Uncertainty within the Training Pool: Active Learning for Medical Image Segmentation","summarize: Active learning is a unique abstraction of machine learning techniques where the model\/algorithm could guide users for annotation of a set of data points that would be beneficial to the model, unlike passive machine learning. The primary advantage being that active learning frameworks select data points that can accelerate the learning process of a model and can reduce the amount of data needed to achieve full accuracy as compared to a model trained on a randomly acquired data set. Multiple frameworks for active learning combined with deep learning have been proposed, and the majority of them are dedicated to classification tasks. Herein, we explore active learning for the task of segmentation of medical imaging data sets. We investigate our proposed framework using two datasets: 1.) MRI scans of the hippocampus, 2.) CT scans of pancreas and tumors. This work presents a query-by-committee approach for active learning where a joint optimizer is used for the committee. At the same time, we propose three new strategies for active learning: 1.) increasing frequency of uncertain data to bias the training data set; 2.) Using mutual information among the input images as a regularizer for acquisition to ensure diversity in the training dataset; 3.) adaptation of Dice log-likelihood for Stein variational gradient descent . The results indicate an improvement in terms of data reduction by achieving full accuracy while only using 22.69 % and 48.85 % of the available data for each dataset, respectively."],["four Bloch modes form the degenerate band edge condition. the four modes","Experimental Demonstration of Degenerate Band Edge in Metallic Periodically-Loaded Circular Waveguide","summarize: We experimentally demonstrate for the first time the degenerate band edge condition, namely the degeneracy of four Bloch modes, in loaded circular metallic waveguides. The four modes forming the DBE represent a degeneracy of fourth order occurring in a periodic structure where four Bloch modes, two propagating and two evanescent, coalesce. It leads to a very flat wavenumber-frequency dispersion relation, and the finite length structure's quality factor scales as "],["we consider the problem of designing a stabilizing and optimal static controller. this problem is","On Separable Quadratic Lyapunov Functions for Convex Design of Distributed Controllers","summarize: We consider the problem of designing a stabilizing and optimal static controller with a pre-specified sparsity pattern. Since this problem is NP-hard in general, it is necessary to resort to approximation approaches. In this paper, we characterize a class of convex restrictions of this problem that are based on designing a separable quadratic Lyapunov function for the closed-loop system. This approach generalizes previous results based on optimizing over diagonal Lyapunov functions, thus allowing for improved feasibility and performance. Moreover, we suggest a simple procedure to compute favourable structures for the Lyapunov function yielding high-performance distributed controllers. Numerical examples validate our results."],["the aim of this paper is to extend the concept of regional exponential general observability to the","Regional exponential general observability in diffusion problem","summarize: The aim of this paper is to extend the concept of regional exponential general observability to the case of Neumann boundary conditions problem in diffusion system. More precisely, for linear distributed parameter diffusion systems, we show that the sensors characterizations allow to reconstruct the regional exponential state in a sub-region of the considered systems domain. Moreover, various interesting results associated with the choice of sensors are given and illustrated in specific situations in order regional exponential general observability notion to be achieved. Finally, we also show that, there exists a dynamical system for diffusion system is not exponential general observable in the usual sense, but it may be regional exponential general observable."],["the ALICE collaboration has observed an interesting systematic behaviour of ratios of identified particles to pion","Hadronic Resonance Gas Model and Multiplicity Dependence in p-p, p-Pb, Pb-Pb collisions: Strangeness Enhancement","summarize: Recently the ALICE collaboration has observed an interesting systematic behaviour of ratios of identified particles to pions yields at the LHC, showing that they depend solely on the charged-particle multiplicity in pp, pPb and PbPb collisions. In particular, the yields of strange particles relative to pions, increases with multiplicity and the enhancement is mode pronounced with increasing strangeness content. We will argue, that such a pattern of arises naturally in the thermal model taking into account exact strangeness conservation. Furthermore, extending the thermal model by including hadron interactions within the S-matrix approach, the ALICE data can be well quantified by the thermal particle yields at the chiral-crossover temperature, as previously found in central Pb-Pb collisions."],["the experimental properties of intrinsic localized modes have long been compared with theoretical dynamical lat","Inductive intrinsic localized modes in a 1D nonlinear electric transmission line","summarize: The experimental properties of intrinsic localized modes have long been compared with theoretical dynamical lattice models that make use of nonlinear onsite and\/or nearest neighbor intersite potentials. Here it is shown for a 1-D lumped electrical transmission line a nonlinear inductive component in an otherwise linear parallel capacitor lattice makes possible a new kind of ILM outside the plane wave spectrum. To simplify the analysis the nonlinear inductive current equations are transformed to flux transmission line equations with analogue onsite hard potential nonlinearities. Approximate analytic results compare favorably with those obtained from a driven damped lattice model and with eigenvalue simulations. For this mono-element lattice ILMs above the top of the plane wave spectrum are the result. We find that the current ILM is spatially compressed relative to the corresponding flux ILM. Finally this study makes the connection between the dynamics of mass and force constant defects in the harmonic lattice and ILMs in a strongly anharmonic lattice."],["a paper focuses on the world of CAPs, based exclusively on the analysis","Results of a Collective Awareness Platforms Investigation","summarize: In this paper we provide two introductory analyses of CAPs, based exclusively on the analysis of documents found on the Internet. The first analysis allowed us to investigate the world of CAPs, in particular for what concerned their status , the scope of those platforms and the typology of users. In order to develop a more accurate model of CAPs, and to understand more deeply the motivation of the users and the type of expected payoff, we analysed those CAPs from the above list that are still alive and we used two models developed for what concerned the virtual community and the collective intelligence."],["the physical domain of the system can be a bounded region of the system.","Local thermal equilibrium for certain stochastic models of heat transport","summarize: This paper is about nonequilibrium steady states of a class of stochastic models in which particles exchange energy with their local environments rather than directly with one another. The physical domain of the system can be a bounded region of "],["black hole solution is a new charge for black hole solution. the topological charge is","The Last Lost Charge And Phase Transition In Schwarzschild AdS Minimally Coupled to a Cloud of Strings","summarize: In this paper we study the Schwarzschild AdS black hole with a cloud of string background in an extended phase space and investigate a new phase transition related to the topological charge. By treating the topological charge as a new charge for black hole solution we study its thermodynamics in this new extended phase space. We treat by two approaches to study the phase transition behavior via both "],["the triple scissor extender is a 6 Degree-Of-Free","Design and Analysis of 6-DOF Triple Scissor Extender Robots with Applications in Aircraft Assembly","summarize: A new type of parallel robot mechanism with an extendable structure is presented, and its kinematic properties and design parameters are analyzed. The Triple Scissor Extender is a 6 Degree-Of-Freedom robotic mechanism for reaching high ceilings and positioning an end effector. Three scissor mechanisms are arranged in parallel, with the bottom ends coupled to linear slides, and the top vertex attached to an end effector plate. Arbitrary positions and orientations of the end effector can be achieved through the coordinated motion of the six linear actuators located at the base. By changing key geometric parameters, the TSE's design can yield a specific desired workspace volume and differential motion behavior. A general kinematic model for diverse TSEs is derived, and the kinematic properties, including workspace, singularity, and the Jacobian singular values, are evaluated. From these expressions, four key design parameters are identified, and their sensitivity upon the workspace volume and the Jacobian singular values is analyzed. A case study in autonomous aircraft assembly is presented using the insights gained from the design parameter studies."],["Cumulative Prospect Theory is a modeling tool widely used in behavioral economics and cognitive psychology","Cumulative Prospect Theory Based Dynamic Pricing for Shared Mobility on Demand Services","summarize: Cumulative Prospect Theory is a modeling tool widely used in behavioral economics and cognitive psychology that captures subjective decision making of individuals under risk or uncertainty. In this paper, we propose a dynamic pricing strategy for Shared Mobility on Demand Services using a passenger behavioral model based on CPT. This dynamic pricing strategy together with dynamic routing via a constrained optimization algorithm that we have developed earlier, provide a complete solution customized for SMoDS of multi-passenger transportation. The basic principles of CPT and the derivation of the passenger behavioral model in the SMoDS context are described in detail. The implications of CPT on dynamic pricing of the SMoDS are delineated using computational experiments involving passenger preferences. These implications include interpretation of the classic fourfold pattern of risk attitudes, strong risk aversion over mixed prospects, and behavioral preferences of self reference. Overall, it is argued that the use of the CPT framework corresponds to a crucial building block in designing socio-technical systems by allowing quantification of subjective decision making under risk or uncertainty that is perceived to be otherwise qualitative."],["probability of stochastic processes is higher than probability. probability of stochastic processes is","Survival probability of stochastic processes beyond persistence exponents","summarize: For many stochastic processes, the probability "],["a macroscopic theory of interfacial interactions targeting terrestrial applications has been developed","Exploratory numerical experiments with a macroscopic theory of interfacial interactions","summarize: Phenomenological theories of interfacial interactions have targeted terrestrial applications since long time and their exploitation has inspired our research programme to build up a macroscopic theory of gas-surface interactions targeting the complex phenomenology of hypersonic reentry flows as alternative to standard methods based on accommodation coefficients. The objective of this paper is the description of methods employed and results achieved in an exploratory study, that is, the unsteady heat transfer between two solids in contact with and without interface. It is a simple numerical-demonstrator test case designed to facilitate quick numerical calculations and to bring forth already sufficiently meaningful aspects relevant to thermal protection due to the formation of the interface. The paper begins with a brief introduction on the subject matter and a review of relevant literature. Then the case is considered in which the interface is absent. The importance of tension continuity as boundary condition on the same footing of heat-flux continuity is recognised and the role of the former in governing the establishment of the temperature-difference distribution over the separation surface is explicitly shown. Evidence is given that the standard temperature-continuity boundary condition is just a particular case. Subsequently the case in which the interface is formed between the solids is analysed. The coupling among the heat-transfer equations applicable in the solids and the balance equation for the surface thermodynamic energy formulated in terms of the surface temperature is discussed. Results are illustrated for planar and cylindrical configuration; they show unequivocally that the thermal-protection action of the interface turns out to be driven exclusively by thermophysical properties of the solids and of the interface; accommodation coefficients are not needed."],["this survey studies potential data link candidates for unmanned aircraft vehicles. the data links for these","Potential Data Link Candidates for Civilian Unmanned Aircraft Systems: A Survey","summarize: This survey studies the potential data link candidates for unmanned aircraft vehicles . There has been tremendous growth in different applications of UAVs such as lifesaving and rescue missions, commercial use, recreations, etc. Unlike the traditional wireless communications, the data links for these systems do not have any general standardized framework yet to ensure safe co-existence of UAVs with other flying vehicles. This motivated us to provide a comprehensive survey of potential data link technologies available for UAVs. Our goal is to study the current trends and available candidates and carry out a comprehensive comparison among them. The contribution of this survey is to highlight the strength and weakness of the current data link options and their suitability to satisfy the UAVs communication requirements. Satellite links, cellular technologies, Wi-Fi and several similar wireless technologies are studied thoroughly in this paper. We also focus on several available promising standards that can be modified for these data links. Then, we discuss standard-related organizations that are working actively in the area of civilian unmanned systems. Finally, we bring up some future challenges in this area with several potential solutions to motivate further research work."],["a human-human interaction is a complex and complex process. a human-","A Review on Learning Planning Action Models for Socio-Communicative HRI","summarize: For social robots to be brought more into widespread use in the fields of companionship, care taking and domestic help, they must be capable of demonstrating social intelligence. In order to be acceptable, they must exhibit socio-communicative skills. Classic approaches to program HRI from observed human-human interactions fails to capture the subtlety of multimodal interactions as well as the key structural differences between robots and humans. The former arises due to a difficulty in quantifying and coding multimodal behaviours, while the latter due to a difference of the degrees of liberty between a robot and a human. However, the notion of reverse engineering from multimodal HRI traces to learn the underlying behavioral blueprint of the robot given multimodal traces seems an option worth exploring. With this spirit, the entire HRI can be seen as a sequence of exchanges of speech acts between the robot and human, each act treated as an action, bearing in mind that the entire sequence is goal-driven. Thus, this entire interaction can be treated as a sequence of actions propelling the interaction from its initial to goal state, also known as a plan in the domain of AI planning. In the same domain, this action sequence that stems from plan execution can be represented as a trace. AI techniques, such as machine learning, can be used to learn behavioral models , intended to be reusable for AI planning, from the aforementioned multimodal traces. This article reviews recent machine learning techniques for learning planning action models which can be applied to the field of HRI with the intent of rendering robots as socio-communicative."],["end-to-end acoustic speech recognition has gained widespread popularity. but","Fusing information streams in end-to-end audio-visual speech recognition","summarize: End-to-end acoustic speech recognition has quickly gained widespread popularity and shows promising results in many studies. Specifically the joint transformer\/CTC model provides very good performance in many tasks. However, under noisy and distorted conditions, the performance still degrades notably. While audio-visual speech recognition can significantly improve the recognition rate of end-to-end models in such poor conditions, it is not obvious how to best utilize any available information on acoustic and visual signal quality and reliability in these models. We thus consider the question of how to optimally inform the transformer\/CTC model of any time-variant reliability of the acoustic and visual information streams. We propose a new fusion strategy, incorporating reliability information in a decision fusion net that considers the temporal effects of the attention mechanism. This approach yields significant improvements compared to a state-of-the-art baseline model on the Lip Reading Sentences 2 and 3 corpus. On average, the new system achieves a relative word error rate reduction of 43% compared to the audio-only setup and 31% compared to the audiovisual end-to-end baseline."],["the results of an experimental study show that the hydrodynamic force acting on a plate is","Hydroelastic effects during the fast lifting of a disc from a water surface","summarize: Here we report the results of an experimental study where we measure the hydrodynamic force acting on a plate which is lifted from a water surface, suddently starting to move upwards with an acceleration much larger than gravity. Our work focuses on the early stage of the plate motion, when the hydrodynamic suction forces due to the liquid inertia are the most relevant ones. Besides the force, we measure as well the acceleration at the center of the plate and the time evolution of the wetted area. The results of this study show that, at very early stages, the hydrodynamic force can be estimated by a simple extension to the linear exit theory by Korobkin , which incorporates an added mass to the body dynamics. However, at longer times, the measured acceleration decays even though the applied external force continues to increase. Moreover, high-speed recordings of the disc displacement and the radius of the wetted area reveal that the latter does not change before the disc acceleration reaches its maximum value. We show in this paper that these phenomena are caused by the elastic deflection of the disc during the initial transient stage of water exit. We present a linearised model of water exit that accounts for the elastic behaviour of the lifted body. The results obtained with this new model agree fairly well with the experimental results."],["real-world tasks would benefit from using multiagent reinforcement learning algorithms. if agents","R-MADDPG for Partially Observable Environments and Limited Communication","summarize: There are several real-world tasks that would benefit from applying multiagent reinforcement learning algorithms, including the coordination among self-driving cars. The real world has challenging conditions for multiagent learning systems, such as its partial observable and nonstationary nature. Moreover, if agents must share a limited resource they must all learn how to coordinate resource use. This paper introduces a deep recurrent multiagent actor-critic framework for handling multiagent coordination under partial observable set-tings and limited communication. We investigate recurrency effects on performance and communication use of a team of agents. We demonstrate that the resulting framework learns time dependencies for sharing missing observations, handling resource limitations, and developing different communication patterns among agents."],["clinical dermoscopic features may indicate melanoma. a neural network architecture","Fully Convolutional Neural Networks to Detect Clinical Dermoscopic Features","summarize: The presence of certain clinical dermoscopic features within a skin lesion may indicate melanoma, and automatically detecting these features may lead to more quantitative and reproducible diagnoses. We reformulate the task of classifying clinical dermoscopic features within superpixels as a segmentation problem, and propose a fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images. Our neural network architecture uses interpolated feature maps from several intermediate network layers, and addresses imbalanced labels by minimizing a negative multi-label Dice-F"],["nonlinear optics is an increasingly important field for scientific and technological applications. there is","Grating-graphene metamaterial as a platform for terahertz nonlinear photonics","summarize: Nonlinear optics is an increasingly important field for scientific and technological applications, owing to its relevance and potential for optical and optoelectronic technologies. Currently, there is an active search for suitable nonlinear material systems with efficient conversion and small material footprint. Ideally, the material system should allow for chip-integration and room-temperature operation. Two-dimensional materials are highly interesting in this regard. Particularly promising is graphene, which has demonstrated an exceptionally large nonlinearity in the terahertz regime. Yet, the light-matter interaction length in two-dimensional materials is inherently minimal, thus limiting the overall nonlinear-optical conversion efficiency. Here we overcome this challenge using a metamaterial platform that combines graphene with a photonic grating structure providing field enhancement. We measure terahertz third-harmonic generation in this metamaterial and obtain an effective third-order nonlinear susceptibility with a magnitude as large as 3"],["we consider two-body and quasi-two-body decays of the type.","Polarization in two-body decays and new physics","summarize: We consider two-body and quasi-two-body decays of the type "],["texture images are first represented by tree of shapes, each of which is associated with several geometrical","Texture Characterization by Using Shape Co-occurrence Patterns","summarize: Texture characterization is a key problem in image understanding and pattern recognition. In this paper, we present a flexible shape-based texture representation using shape co-occurrence patterns. More precisely, texture images are first represented by tree of shapes, each of which is associated with several geometrical and radiometric attributes. Then four typical kinds of shape co-occurrence patterns based on the hierarchical relationship of the shapes in the tree are learned as codewords. Three different coding methods are investigated to learn the codewords, with which, any given texture image can be encoded into a descriptive vector. In contrast with existing works, the proposed method not only inherits the strong ability to depict geometrical aspects of textures and the high robustness to variations of imaging conditions from the shape-based method, but also provides a flexible way to consider shape relationships and to compute high-order statistics on the tree. To our knowledge, this is the first time to use co-occurrence patterns of explicit shapes as a tool for texture analysis. Experiments on various texture datasets and scene datasets demonstrate the efficiency of the proposed method."],["Janossy pooling expresses permutation-invariant function as","Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs","summarize: We consider a simple and overarching representation for permutation-invariant functions of sequences . Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with "],["the FAIR facility is an international accelerator centre for research with ion and antiprot","Twin GEM-TPC Prototype Beam Test at GSI and Jyv\\askyl\\a - a Development for the Super-FRS at FAIR","summarize: The FAIR facility is an international accelerator centre for research with ion and antiproton beams. It is being built at Darmstadt, Germany as an extension to the current GSI research institute. One major part of the facility will be the Super-FRS separator, which will be include in phase one of the project construction. The NUSTAR experiments will benefit from the Super-FRS, which will deliver an unprecedented range of radioactive ion beams . These experiments will use beams of different energies and characteristics in three different branches; the high-energy which utilizes the RIB at relativistic energies 300-1500 MeV\/u as created in the production process, the low-energy branch aims to use beams in the range of 0-150 MeV\/u whereas the ring branch will cool and store beams in the NESR ring. The main tasks for the Super-FRS beam diagnostics chambers will be for the set up and adjustment of the separator as well as to provide tracking and event-by-event particle identification. The Helsinki Institute of Physics, and the Detector Laboratory and Experimental Electronics at GSI are in a joint R&D of a GEM-TPC detector which could satisfy the requirements of such tracking detectors, in terms of tracking efficiency, space resolution, count rate capability and momenta resolution. The current prototype, which is the generation four of this type, is two GEM-TPCs in twin configuration inside the same vessel. This means that one of the GEM-TPC is flipped on the middle plane w.r.t. the other one. This chamber was tested at Jyv\\askyl\\a accelerator with protons projectiles and at GSI with Uranium, fragments and Carbon beams during this year 2016."],["intra-variable handwriting is a challenge for automatic writer inspection. the state-","Intra-Variable Handwriting Inspection Reinforced with Idiosyncrasy Analysis","summarize: In this paper, we work on intra-variable handwriting, where the writing samples of an individual can vary significantly. Such within-writer variation throws a challenge for automatic writer inspection, where the state-of-the-art methods do not perform well. To deal with intra-variability, we analyze the idiosyncrasy in individual handwriting. We identify\/verify the writer from highly idiosyncratic text-patches. Such patches are detected using a deep recurrent reinforcement learning-based architecture. An idiosyncratic score is assigned to every patch, which is predicted by employing deep regression analysis. For writer identification, we propose a deep neural architecture, which makes the final decision by the idiosyncratic score-induced weighted average of patch-based decisions. For writer verification, we propose two algorithms for patch-fed deep feature aggregation, which assist in authentication using a triplet network. The experiments were performed on two databases, where we obtained encouraging results."],["the information acquisition problem is based on a planning algorithm on known models. the proposed","Learning Q-network for Active Information Acquisition","summarize: In this paper, we propose a novel Reinforcement Learning approach for solving the Active Information Acquisition problem, which requires an agent to choose a sequence of actions in order to acquire information about a process of interest using on-board sensors. The classic challenges in the information acquisition problem are the dependence of a planning algorithm on known models and the difficulty of computing information-theoretic cost functions over arbitrary distributions. In contrast, the proposed framework of reinforcement learning does not require any knowledge on models and alleviates the problems during an extended training stage. It results in policies that are efficient to execute online and applicable for real-time control of robotic systems. Furthermore, the state-of-the-art planning methods are typically restricted to short horizons, which may become problematic with local minima. Reinforcement learning naturally handles the issue of planning horizon in information problems as it maximizes a discounted sum of rewards over a long finite or infinite time horizon. We discuss the potential benefits of the proposed framework and compare the performance of the novel algorithm to an existing information acquisition method for multi-target tracking scenarios."],["two parallel time-varying metasurfaces are used for realizing a region of space","Electromagnetic Isolation Induced by Time-Varying Metasurfaces: Non-Reciprocal Bragg Grating","summarize: In this letter, we propose a magnet-less non-reciprocal isolating system based on time-varying metasurfaces. Two parallel time-varying metasurfaces, one for frequency up-conversion and one for down-conversion by the same amount, are used for realizing a region of space where incident waves from opposite directions experience an opposite Doppler frequency shift. As a result, any device within this region becomes sensitive to the illumination direction, exhibiting a different scattering response from opposite directions and thus breaking reciprocity. Very importantly, thanks to the opposite frequency shift of the metasurfaces, the frequency of the transmitted electromagnetic field is the same as for the incident one. Here, we demonstrate this general approach by using a Bragg grating as the device between the time-varying metasurfaces. The combined structure of the metasurfaces and the grating exhibits different transmission and reflection properties for opposite illumination direction, thereby realizing an isolator. More broadly, this letter presents a strategy for converting any conventional electromagnetic device to a non-reciprocal one by placing it between two time-varying metasurfaces. This approach opens the door to several new non-reciprocal components based on thin and lightweight metasurfaces, which are simpler to realize compared to their volumetric counterparts."],["68 solar mass black hole detected in binary system LB-1. black hole is twice","No signature of the orbital motion of a putative 70 solar mass black hole in LB-1","summarize: Liu et al. recently reported the detection of a 68 solar mass black hole paired with an 8.2 Msun B-type sub-giant star in the 78.9-day spectroscopic binary system LB-1. Such a black hole is over twice as massive as any other known stellar-mass black hole with non-compact companions2 and its mass approaches those that result from BH-BH coalescences that are detected by gravitational wave interferometers. Its presence in a solar-like metallicity environment challenges conventional theories of massive binary evolution, stellar winds and core-collapse supernovae, so that more exotic scenarios seem to be needed to explain the existence and properties of LB-1. Here, we show that the observational diagnostics used to derive the BH mass results from the orbital motion of the B-type star, not that of the BH. As a consequence, no evidence for a massive BH remains in the data, therefore solving the existing tension with formation models of such a massive BH at solar metallicity and with theories of massive star evolution in general."],["in the last few years, telecom and computer networks have witnessed new concepts and technologies through Network Function","Enhancing Middleware-based IoT Applications through Run-Time Pluggable QoS Management Mechanisms. Application to a oneM2M compliant IoT Middleware","summarize: In the recent years, telecom and computer networks have witnessed new concepts and technologies through Network Function Virtualization and Software-Defined Networking . SDN, which allows applications to have a control over the network, and NFV, which allows deploying network functions in virtualized environments, are two paradigms that are increasingly used for the Internet of Things . This Internet brings the promise to interconnect billions of devices in the next few years rises several scientific challenges in particular those of the satisfaction of the quality of service required by the IoT applications. In order to address this problem, we have identified two bottlenecks with respect to the QoS: the traversed networks and the intermediate entities that allows the application to interact with the IoT devices. In this paper, we first present an innovative vision of a network function with respect to their deployment and runtime environment. Then, we describe our general approach of a solution that consists in the dynamic, autonomous, and seamless deployment of QoS management mechanisms. We also describe the requirements for the implementation of such approach. Finally, we present a redirection mechanism, implemented as a network function, allowing the seamless control of the data path of a given middleware traffic. This mechanism is assessed through a use case related to vehicular transportation."],["graphene sensors were used to detect ultra-low concentrations of NO2. the","Detection of ultra-low concentration NO2 in complex environment using epitaxial graphene sensors","summarize: We demonstrate proof-of-concept graphene sensors for environmental monitoring of ultra-low concentration NO2 in complex environments. Robust detection in a wide range of NO2 concentrations, 10-154 ppb, was achieved, highlighting the great potential for graphene-based NO2 sensors, with applications in environmental pollution monitoring, portable monitors, automotive and mobile sensors for a global real-time monitoring network. The measurements were performed in a complex environment, combining NO2\/synthetic air\/water vapour, traces of other contaminants and variable temperature in an attempt to fully replicate the environmental conditions of a working sensor. It is shown that the performance of the graphene-based sensor can be affected by co-adsorption of NO2 and water on the surface at low temperatures . However, the sensitivity to NO2 increases significantly when the sensor operates at 150 C and the cross-selectivity to water, sulphur dioxide and carbon monoxide is minimized. Additionally, it is demonstrated that single-layer graphene exhibits two times higher carrier concentration response upon exposure to NO2 than bilayer graphene."],["Yanson point-contact spectroscopy has carried out an investigation. the spect","Electron-Phonon Interaction in Ternary Rare-Earth Copper Antimonides LaCuSb2 and LaSb2 probed by Yanson Point-Contact Spectroscopy","summarize: Investigation of the electron-phonon interaction in LaCuSb2 and LaSb2 compounds by Yanson point-contact spectroscopy has been carried out. Point-contact spectra display a pronounced broad maximum in the range of 1020 mV caused by EPI. Variation of the position of this maximum is likely connected with anisotropic phonon spectrum in these layered compounds. The absence of phonon features after the main maximum allows the assessment of the Debye energy of about 40 meV. The EPI constant for the LaCuSb2 compound was estimated to be =0.2+\/-0.03. A zero-bias minimum in differential resistance for the latter compound is observed for some point contacts, which vanishes at about 6 K, pointing to the formation of superconducting phase under point contact, while superconducting critical temperature of the bulk sample is only 1K."],["traditional plane-based clustering methods measure cost of within-cluster and between-cluster","Ramp-based Twin Support Vector Clustering","summarize: Traditional plane-based clustering methods measure the cost of within-cluster and between-cluster by quadratic, linear or some other unbounded functions, which may amplify the impact of cost. This letter introduces a ramp cost function into the plane-based clustering to propose a new clustering method, called ramp-based twin support vector clustering . RampTWSVC is more robust because of its boundness, and thus it is more easier to find the intrinsic clusters than other plane-based clustering methods. The non-convex programming problem in RampTWSVC is solved efficiently through an alternating iteration algorithm, and its local solution can be obtained in a finite number of iterations theoretically. In addition, the nonlinear manifold-based formation of RampTWSVC is also proposed by kernel trick. Experimental results on several benchmark datasets show the better performance of our RampTWSVC compared with other plane-based clustering methods."],["factorizations are a factoring factor.","Set-Direct Factorizations of Groups","summarize: We consider factorizations "],["we discuss software design issues related to the development of parallel computational intelligence algorithms on multi-core CPU","Probabilistic Graphical Models on Multi-Core CPUs using Java 8","summarize: In this paper, we discuss software design issues related to the development of parallel computational intelligence algorithms on multi-core CPUs, using the new Java 8 functional programming features. In particular, we focus on probabilistic graphical models and present the parallelisation of a collection of algorithms that deal with inference and learning of PGMs from data. Namely, maximum likelihood estimation, importance sampling, and greedy search for solving combinatorial optimisation problems. Through these concrete examples, we tackle the problem of defining efficient data structures for PGMs and parallel processing of same-size batches of data sets using Java 8 features. We also provide straightforward techniques to code parallel algorithms that seamlessly exploit multi-core processors. The experimental analysis, carried out using our open source AMIDST Java toolbox, shows the merits of the proposed solutions."],["chroma intra-prediction is a complex and computationally intensive coding scheme","Attention-Based Neural Networks for Chroma Intra Prediction in Video Coding","summarize: Neural networks can be successfully used to improve several modules of advanced video coding schemes. In particular, compression of colour components was shown to greatly benefit from usage of machine learning models, thanks to the design of appropriate attention-based architectures that allow the prediction to exploit specific samples in the reference region. However, such architectures tend to be complex and computationally intense, and may be difficult to deploy in a practical video coding pipeline. This work focuses on reducing the complexity of such methodologies, to design a set of simplified and cost-effective attention-based architectures for chroma intra-prediction. A novel size-agnostic multi-model approach is proposed to reduce the complexity of the inference process. The resulting simplified architecture is still capable of outperforming state-of-the-art methods. Moreover, a collection of simplifications is presented in this paper, to further reduce the complexity overhead of the proposed prediction architecture. Thanks to these simplifications, a reduction in the number of parameters of around 90% is achieved with respect to the original attention-based methodologies. Simplifications include a framework for reducing the overhead of the convolutional operations, a simplified cross-component processing model integrated into the original architecture, and a methodology to perform integer-precision approximations with the aim to obtain fast and hardware-aware implementations. The proposed schemes are integrated into the Versatile Video Coding prediction pipeline, retaining compression efficiency of state-of-the-art chroma intra-prediction methods based on neural networks, while offering different directions for significantly reducing coding complexity."],["a new combination of optimization tools with learning theory bounds. this contrasts the typical","Optimality Implies Kernel Sum Classifiers are Statistically Efficient","summarize: We propose a novel combination of optimization tools with learning theory bounds in order to analyze the sample complexity of optimal kernel sum classifiers. This contrasts the typical learning theoretic results which hold for all classifiers. Our work also justifies assumptions made in prior work on multiple kernel learning. As a byproduct of our analysis, we also provide a new form of Rademacher complexity for hypothesis classes containing only optimal classifiers."],["a new model of responsibility quantification predicts actual human responsibility. participants performed a","Theoretical, Measured and Subjective Responsibility in Aided Decision Making","summarize: When humans interact with intelligent systems, their causal responsibility for outcomes becomes equivocal. We analyze the descriptive abilities of a newly developed responsibility quantification model to predict actual human responsibility and perceptions of responsibility in the interaction with intelligent systems. In two laboratory experiments, participants performed a classification task. They were aided by classification systems with different capabilities. We compared the predicted theoretical responsibility values to the actual measured responsibility participants took on and to their subjective rankings of responsibility. The model predictions were strongly correlated with both measured and subjective responsibility. A bias existed only when participants with poor classification capabilities relied less-than-optimally on a system that had superior classification capabilities and assumed higher-than-optimal responsibility. The study implies that when humans interact with advanced intelligent systems, with capabilities that greatly exceed their own, their comparative causal responsibility will be small, even if formally the human is assigned major roles. Simply putting a human into the loop does not assure that the human will meaningfully contribute to the outcomes. The results demonstrate the descriptive value of the ResQu model to predict behavior and perceptions of responsibility by considering the characteristics of the human, the intelligent system, the environment and some systematic behavioral biases. The ResQu model is a new quantitative method that can be used in system design and can guide policy and legal decisions regarding human responsibility in events involving intelligent systems."],["this survey studies potential data link candidates for unmanned aircraft vehicles. the data links for these","Potential Data Link Candidates for Civilian Unmanned Aircraft Systems: A Survey","summarize: This survey studies the potential data link candidates for unmanned aircraft vehicles . There has been tremendous growth in different applications of UAVs such as lifesaving and rescue missions, commercial use, recreations, etc. Unlike the traditional wireless communications, the data links for these systems do not have any general standardized framework yet to ensure safe co-existence of UAVs with other flying vehicles. This motivated us to provide a comprehensive survey of potential data link technologies available for UAVs. Our goal is to study the current trends and available candidates and carry out a comprehensive comparison among them. The contribution of this survey is to highlight the strength and weakness of the current data link options and their suitability to satisfy the UAVs communication requirements. Satellite links, cellular technologies, Wi-Fi and several similar wireless technologies are studied thoroughly in this paper. We also focus on several available promising standards that can be modified for these data links. Then, we discuss standard-related organizations that are working actively in the area of civilian unmanned systems. Finally, we bring up some future challenges in this area with several potential solutions to motivate further research work."],["real-world AI systems have to coordinate and communicate with other agents in cooperative partially observable","Improving Policies via Search in Cooperative Partially Observable Games","summarize: Recent superhuman results in games have largely been achieved in a variety of zero-sum settings, such as Go and Poker, in which agents need to compete against others. However, just like humans, real-world AI systems have to coordinate and communicate with other agents in cooperative partially observable environments as well. These settings commonly require participants to both interpret the actions of others and to act in a way that is informative when being interpreted. Those abilities are typically summarized as theory f mind and are seen as crucial for social interactions. In this paper we propose two different search techniques that can be applied to improve an arbitrary agreed-upon policy in a cooperative partially observable game. The first one, single-agent search, effectively converts the problem into a single agent setting by making all but one of the agents play according to the agreed-upon policy. In contrast, in multi-agent search all agents carry out the same common-knowledge search procedure whenever doing so is computationally feasible, and fall back to playing according to the agreed-upon policy otherwise. We prove that these search procedures are theoretically guaranteed to at least maintain the original performance of the agreed-upon policy . In the benchmark challenge problem of Hanabi, our search technique greatly improves the performance of every agent we tested and when applied to a policy trained using RL achieves a new state-of-the-art score of 24.61 \/ 25 in the game, compared to a previous-best of 24.08 \/ 25."],["new work on adversary-aware classifiers has been focusing on evasion","Adversarial Feature Selection against Evasion Attacks","summarize: Pattern recognition and machine learning techniques have been increasingly adopted in adversarial settings such as spam, intrusion and malware detection, although their security against well-crafted attacks that aim to evade detection by manipulating data at test time has not yet been thoroughly assessed. While previous work has been mainly focused on devising adversary-aware classification algorithms to counter evasion attempts, only few authors have considered the impact of using reduced feature sets on classifier security against the same attacks. An interesting, preliminary result is that classifier security to evasion may be even worsened by the application of feature selection. In this paper, we provide a more detailed investigation of this aspect, shedding some light on the security properties of feature selection against evasion attacks. Inspired by previous work on adversary-aware classifiers, we propose a novel adversary-aware feature selection model that can improve classifier security against evasion attacks, by incorporating specific assumptions on the adversary's data manipulation strategy. We focus on an efficient, wrapper-based implementation of our approach, and experimentally validate its soundness on different application examples, including spam and malware detection."],["linear discrete ill-posed problem for large-scale linear discrete ill-posed","Approximation Accuracy of the Krylov Subspaces for Linear Discrete Ill-Posed Problems","summarize: For the large-scale linear discrete ill-posed problem "],["Arkan's polar coding technique is based on the idea of synthes","Alignment of Polarized Sets","summarize: Arkan's polar coding technique is based on the idea of synthesizing "],["the hydroxyl groups have been introduced on the nanorods surface. the dye-","The effect of hydroxyl on dye-sensitized solar cells assembled with TiO2 nanorods","summarize: TiO2 nanorods have been prepared on ITO substrates by dc reactive magnetron sputtering technique. The hydroxyl groups have been introduced on the nanorods surface. The structure and the optical properties of these nanorods have been studied. The dye-sensitized solar cells have been assembled using these TiO2 nanorods as photoelectrode. And the effect of the hydroxyl groups on the properties of the photoelectric conversion of the DSSCs has been studied."],["a variety of applications are envisioned to enable real-time control of metamaterials","Toward Localization in Terahertz-Operating Energy Harvesting Software-Defined Metamaterials: Context Analysis","summarize: Software-defined metamaterials represent a novel paradigm for real-time control of metamaterials. SDMs are envisioned to enable a variety of exciting applications in the domains such as smart textiles and sensing in challenging conditions. Many of these applications envisage deformations of the SDM structure . This affects the relative position of the metamaterial elements and requires their localization relative to each other. The question of how to perform such localization is, however, yet to spark in the community. We consider that the metamaterial elements are controlled wirelessly through a Terahertz -operating nanonetwork. Moreover, we consider the elements to be energy constrained, with their sole powering option being to harvest environmental energy. For such a setup, we demonstrate sub-millimeter accuracy of the two-way Time of Flight -based localization, as well as high availability of the service , which is a result of the low energy consumed in localization. Finally, we provide the localization context for a number of relevant system parameters such as operational frequency, bandwidth, and harvesting rate."],["the unique information measure quantifies a deviation from the blackwell order. the quantity is","Unique Information and Secret Key Decompositions","summarize: The unique information is an information measure that quantifies a deviation from the Blackwell order. We have recently shown that this quantity is an upper bound on the one-way secret key rate. In this paper, we prove a triangle inequality for the "],["the proposed scheme models a Bayesian Network using the observed port utilization and residual bandwidth to","A Proactive Flow Admission and Re-Routing Scheme for Load Balancing and Mitigation of Congestion Propagation in SDN Data Plane","summarize: The centralized architecture in software-defined network provides a global view of the underlying network, paving the way for enormous research in the area of SDN traffic engineering . This research focuses on the load balancing aspects of SDN TE, given that the existing reactive methods for data-plane load balancing eventually result in packet loss and proactive schemes for data plane load balancing do not address congestion propagation. In the proposed work, the SDN controller periodically monitors flow level statistics and utilization on each link in the network and over-utilized links that cause network congestion and packet loss are identified as bottleneck links. For load balancing the identified largest flow and further traffic through these bottleneck links are rerouted through the lightly-loaded alternate path. The proposed scheme models a Bayesian Network using the observed port utilization and residual bandwidth to decide whether the newly computed alternate path can handle the new flow load before flow admission which in turn reduces congestion propagation. The simulation results show that when the network traffic increases the proposed method efficiently re-routes the flows and balance the network load which substantially improves the network efficiency and the quality of service parameters."],["the existing models rely on Sommerfeld's free-electron theory. the","Fractional Fowler-Nordheim Law for Field Emission from Rough Surface with Nonparabolic Energy Dispersion","summarize: The theories of field electron emission from perfectly planar and smooth canonical surfaces are well understood, but they are not suitable for describing emission from rough, irregular surfaces arising in modern nanoscale electron sources. Moreover, the existing models rely on Sommerfeld's free-electron theory for the description of electronic distribution which is not a valid assumption for modern materials with nonparabolic energy dispersion. In this paper, we derive analytically a generalized Fowler-Nordheim type equation that takes into account the reduced space-dimensionality seen by the quantum mechanically tunneling electron at a rough, irregular emission surface. We also consider the effects of non-parabolic energy dispersion on field-emission from narrow-gap semiconductors and few-layer graphene using Kane's band model. The traditional FN equation is shown to be a limiting case of our model in the limit of a perfectly flat surface of a material with parabolic dispersion. The fractional-dimension parameter used in this model can be experimentally calculated from appropriate current-voltage data plot. By applying this model to experimental data, the standard field-emission parameters can be deduced with better accuracy than by using the conventional FN equation."],["we aim to train deep neural networks that are robust to adversarial perturbations. we","Training for Faster Adversarial Robustness Verification via Inducing ReLU Stability","summarize: We explore the concept of co-design in the context of neural network verification. Specifically, we aim to train deep neural networks that not only are robust to adversarial perturbations but also whose robustness can be verified more easily. To this end, we identify two properties of network models - weight sparsity and so-called ReLU stability - that turn out to significantly impact the complexity of the corresponding verification task. We demonstrate that improving weight sparsity alone already enables us to turn computationally intractable verification problems into tractable ones. Then, improving ReLU stability leads to an additional 4-13x speedup in verification times. An important feature of our methodology is its universality, in the sense that it can be used with a broad range of training procedures and verification approaches."],["the pathwise sensitivities approach is well established for smooth payoff functions. the","Monte Carlo pathwise sensitivities for barrier options","summarize: The Monte Carlo pathwise sensitivities approach is well established for smooth payoff functions. In this work, we present a new Monte Carlo algorithm that is able to calculate the pathwise sensitivities for discontinuous payoff functions. Our main tool is to combine the one-step survival idea of Glasserman and Staum with the stable differentiation approach of Alm, Harrach, Harrach and Keller. As an application we use the derived results for a two-dimensional calibration of a CoCo-Bond, which we model with different types of discretely monitored barrier options."],["we equip a knot.","An invariant for colored bonded knots","summarize: We equip a knot "],["the aim is to detect the onset of a pre-specified drift in the standard","Exact Distribution of the Generalized Shiryaev-Roberts Stopping Time Under the Minimax Brownian Motion Setup","summarize: We consider the quickest change-point detection problem where the aim is to detect the onset of a pre-specified drift in live-monitored standard Brownian motion; the change-point is assumed unknown . The object of interest is the distribution of the stopping time associated with the Generalized Shryaev-Roberts detection procedure set up to sense the presence of the drift in the Brownian motion under surveillance. Specifically, we seek the GSR stopping time's survival function , and distinguish two scenarios: when the drift never sets in and when the drift is in effect ab initio . Under each scenario, we obtain a closed-form formula for the respective survival function, with the GSR statistic's nonnegative headstart assumed arbitrarily given. The two formulae are found analytically, through direct solution of the respective Kolmogorov forward equation via the Fourier spectral method to achieve separation of the spacial and temporal variables. We then exploit the obtained formulae numerically and characterize the pre- and post-change distributions of the GSR stopping time depending on three factors: magnitude of the drift, detection threshold, and the GSR statistic's headstart."],["a symmetric Kohn-Sham potential can be used in density functional theory.","Degenerate Density Perturbation Theory","summarize: Fractional occupation numbers can be used in density functional theory to create a symmetric Kohn-Sham potential, resulting in orbitals with degenerate eigenvalues. We develop the corresponding perturbation theory and apply it to a system of "],["image classification for the English language is an ongoing research challenge. the method used in this study","Image Classification for Arabic: Assessing the Accuracy of Direct English to Arabic Translations","summarize: Image classification is an ongoing research challenge. Most of the available research focuses on image classification for the English language, however there is very little research on image classification for the Arabic language. Expanding image classification to Arabic has several applications. The present study investigated a method for generating Arabic labels for images of objects. The method used in this study involved a direct English to Arabic translation of the labels that are currently available on ImageNet, a database commonly used in image classification research. The purpose of this study was to test the accuracy of this method. In this study, 2,887 labeled images were randomly selected from ImageNet. All of the labels were translated from English to Arabic using Google Translate. The accuracy of the translations was evaluated. Results indicated that that 65.6% of the Arabic labels were accurate. This study makes three important contributions to the image classification literature: it determined the baseline level of accuracy for algorithms that provide Arabic labels for images, it provided 1,895 images that are tagged with accurate Arabic labels, and provided the accuracy of translations of image labels from English to Arabic."],["energy storage units can use excess electricity from RES efficiently and to prevent curtailment.","Cost-Optimal Operation of Energy Storage Units: Benefits of a Problem-Specific Approach","summarize: The integration of large shares of electricity produced by non-dispatchable Renewable Energy Sources leads to an increasingly volatile energy generation side, with temporary local overproduction. The application of energy storage units has the potential to use this excess electricity from RES efficiently and to prevent curtailment. The objective of this work is to calculate cost-optimal charging strategies for energy storage units used as buffers. For this purpose, a new mathematical optimization method is presented that is applicable to general storage-related problems. Due to a tremendous gain in efficiency of this method compared with standard solvers and proven optimality, calculations of complex problems as well as a high-resolution sensitivity analysis of multiple system combinations are feasible within a very short time. As an example technology, Power-to-Heat converters used in combination with thermal storage units are investigated in detail and optimal system configurations, including storage units with and without energy losses, are calculated and evaluated. The benefits of a problem-specific approach are demonstrated by the mathematical simplicity of our approach as well as the general applicability of the proposed method."],["the restricted Melikyan algebra of dimension 125 is a deformation of a","Melikyan algebra is a deformation of a Poisson algebra","summarize: We prove, using computer, that the restricted Melikyan algebra of dimension 125 is a deformation of a Poisson algebra."],["transmitter observing a sequence of independent and identically distributed random variables seeks to keep","Optimal Source Codes for Timely Updates","summarize: A transmitter observing a sequence of independent and identically distributed random variables seeks to keep a receiver updated about its latest observations. The receiver need not be apprised about each symbol seen by the transmitter, but needs to output a symbol at each time instant "],["polynomials defined via fillings of diagrams satisfy linear recurrence","Polynomials defined by tableaux and linear recurrences","summarize: We show that several families of polynomials defined via fillings of diagrams satisfy linear recurrences under a natural operation on the shape of the diagram. We focus on key polynomials, , and Demazure atoms. The same technique can be applied to Hall-Littlewood polynomials and dual Grothendieck polynomials. The motivation behind this is that such recurrences are strongly connected with other nice properties, such as interpretations in terms of lattice points in polytopes and divided difference operators."],["CYGNO will use a gaseous TPC with optical readout to detect","CYGNO: a gaseous TPC with optical readout for dark matter directional search","summarize: The CYGNO project has the goal to use a gaseous TPC with optical readout to detect dark matter and solar neutrinos with low energy threshold and directionality. The CYGNO demonstrator will consist of 1 m 3 volume filled with He:CF 4 gas mixture at atmospheric pressure. Optical readout with high granularity CMOS sensors, combined with fast light detectors, will provide a detailed reconstruction of the event topology. This will allow to discriminate the nuclear recoil signal from the background, mainly represented by low energy electron recoils induced by radioactivity. Thanks to the high reconstruction efficiency, CYGNO will be sensitive to low mass dark matter, and will have the potential to overcome the neutrino floor, that ultimately limits non-directional dark matter searches."],["simulations are chosen adaptively using an acquisition function. simulations are chosen adaptively using","Likelihood-free inference with emulator networks","summarize: Approximate Bayesian Computation provides methods for Bayesian inference in simulation-based stochastic models which do not permit tractable likelihoods. We present a new ABC method which uses probabilistic neural emulator networks to learn synthetic likelihoods on simulated data -- both local emulators which approximate the likelihood for specific observed data, as well as global ones which are applicable to a range of data. Simulations are chosen adaptively using an acquisition function which takes into account uncertainty about either the posterior distribution of interest, or the parameters of the emulator. Our approach does not rely on user-defined rejection thresholds or distance functions. We illustrate inference with emulator networks on synthetic examples and on a biophysical neuron model, and show that emulators allow accurate and efficient inference even on high-dimensional problems which are challenging for conventional ABC approaches."],["the ALICE collaboration measured the recent results for D-meson production.","Mid-rapidity D-meson production in pp, Pb-Pb and p-Pb collisions at the LHC","summarize: We present the recent results for D-meson production measured by the ALICE collaboration in pp collisions at "],["conversion prediction plays an important role in online advertising. MT-FwFM improve the","Predicting Different Types of Conversions with Multi-Task Learning in Online Advertising","summarize: Conversion prediction plays an important role in online advertising since Cost-Per-Action has become one of the primary campaign performance objectives in the industry. Unlike click prediction, conversions have different types in nature, and each type may be associated with different decisive factors. In this paper, we formulate conversion prediction as a multi-task learning problem, so that the prediction models for different types of conversions can be learned together. These models share feature representations, but have their specific parameters, providing the benefit of information-sharing across all tasks. We then propose Multi-Task Field-weighted Factorization Machine to solve these tasks jointly. Our experiment results show that, compared with two state-of-the-art models, MT-FwFM improve the AUC by 0.74% and 0.84% on two conversion types, and the weighted AUC across all conversion types is also improved by 0.50%."],["a new mixed-signal approach is proposed to design a power-elastic","A Pulse Width Modulation based Power-elastic and Robust Mixed-signal Perceptron Design","summarize: Neural networks are exerting burgeoning influence in emerging artificial intelligence applications at the micro-edge, such as sensing systems and image processing. As many of these systems are typically self-powered, their circuits are expected to be resilient and efficient in the presence of continuous power variations caused by the harvesters. In this paper, we propose a novel mixed-signal approach of designing a power-elastic perceptron using the principle of pulse width modulation . Fundamental to the design are a number of parallel inverters that transcode the input-weight pairs based on the principle of PWM duty cycle. Since PWM-based inverters are typically agnostic to amplitude and frequency variations, the perceptron shows a high degree of power elasticity and robustness under these variations. We show extensive design analysis in Cadence Analog Design Environment tool using a 3x3 perceptron circuit as a case study to demonstrate the resilience in the presence of parameric variations."],["k-NN search library NMSLIB is a new retrieval toolkit","Flexible retrieval with NMSLIB and FlexNeuART","summarize: Our objective is to introduce to the NLP community an existing k-NN search library NMSLIB, a new retrieval toolkit FlexNeuART, as well as their integration capabilities. NMSLIB, while being one the fastest k-NN search libraries, is quite generic and supports a variety of distance\/similarity functions. Because the library relies on the distance-based structure-agnostic algorithms, it can be further extended by adding new distances. FlexNeuART is a modular, extendible and flexible toolkit for candidate generation in IR and QA applications, which supports mixing of classic and neural ranking signals. FlexNeuART can efficiently retrieve mixed dense and sparse representations , which is achieved by extending NMSLIB. In that, other retrieval systems work with purely sparse representations , purely dense representations , or only perform mixing at the re-ranking stage."],["the mass of the black hole is around the mass of the black hole. the mass of","Determining the nature of white dwarfs from low-frequency gravitational waves","summarize: An extreme-mass-ratio system composed of a white dwarf and a massive black hole can be observed by the low-frequency gravitational wave detectors, such as the Laser Interferometer Space Antenna . When the mass of the black hole is around "],["the coCoA project dates back to 1987. it was designed to create a computational","Groebner Bases for Everyone with CoCoA-5 and CoCoALib","summarize: We present a survey on the developments related to Groebner bases, and show explicit examples in CoCoA. The CoCoA project dates back to 1987: its aim was to create a mathematician-friendly computational laboratory for studying Commutative Algebra, most especially Groebner bases. Always maintaining this friendly tradition, the project has grown and evolved, and the software has been completely rewritten. CoCoA offers Groebner bases for all levels of interest: from the basic, explicit call in the interactive system CoCoA-5, to problem-specific optimized implementations, to the computer--computer communication with the open source C++ software library, CoCoALib, or the prototype OpenMath-based server. The openness and clean design of CoCoALib and CoCoA-5 are intended to offer different levels of usage, and to encourage external contributions."],["graphene sensors were used to detect ultra-low concentrations of NO2. the","Detection of ultra-low concentration NO2 in complex environment using epitaxial graphene sensors","summarize: We demonstrate proof-of-concept graphene sensors for environmental monitoring of ultra-low concentration NO2 in complex environments. Robust detection in a wide range of NO2 concentrations, 10-154 ppb, was achieved, highlighting the great potential for graphene-based NO2 sensors, with applications in environmental pollution monitoring, portable monitors, automotive and mobile sensors for a global real-time monitoring network. The measurements were performed in a complex environment, combining NO2\/synthetic air\/water vapour, traces of other contaminants and variable temperature in an attempt to fully replicate the environmental conditions of a working sensor. It is shown that the performance of the graphene-based sensor can be affected by co-adsorption of NO2 and water on the surface at low temperatures . However, the sensitivity to NO2 increases significantly when the sensor operates at 150 C and the cross-selectivity to water, sulphur dioxide and carbon monoxide is minimized. Additionally, it is demonstrated that single-layer graphene exhibits two times higher carrier concentration response upon exposure to NO2 than bilayer graphene."],["TIBA - Tankette for Intelligent bioEnergy Agriculture - is the","Robotic Tankette for Intelligent BioEnergy Agriculture: Design, Development and Field Tests","summarize: In recent years, the use of robots in agriculture has been increasing mainly due to the high demand of productivity, precision and efficiency, which follow the climate change effects and world population growth. Unlike conventional agriculture, sugarcane farms are usually regions with dense vegetation, gigantic areas, and subjected to extreme weather conditions, such as intense heat, moisture and rain. TIBA - Tankette for Intelligent BioEnergy Agriculture - is the first result of an R&D project which strives to develop an autonomous mobile robotic system for carrying out a number of agricultural tasks in sugarcane fields. The proposed concept consists of a semi-autonomous, low-cost, dust and waterproof tankette-type vehicle, capable of infiltrating dense vegetation in plantation tunnels and carry several sensing systems, in order to perform mapping of hard-to-access areas and collecting samples. This paper presents an overview of the robot mechanical design, the embedded electronics and software architecture, and the construction of a first prototype. Preliminary results obtained in field tests validate the proposed conceptual design and bring about several challenges and potential applications for robot autonomous navigation, as well as to build a new prototype with additional functionality."],["the paper presents standard and handcrafted features. the influence of various objective factors on the subjective","Study on the Assessment of the Quality of Experience of Streaming Video","summarize: Dynamic adaptive streaming over HTTP provides the work of most multimedia services, however, the nature of this technology further complicates the assessment of the QoE . In this paper, the influence of various objective factors on the subjective estimation of the QoE of streaming video is studied. The paper presents standard and handcrafted features, shows their correlation and p-Value of significance. VQA models based on regression and gradient boosting with SRCC reaching up to 0.9647 on the validation subsample are proposed. The proposed regression models are adapted for applied applications ; the Gradient Boosting Regressor model is perspective for further improvement of the quality estimation model. We take SQoE-III database, so far the largest and most realistic of its kind. The VQA models are available at https:\/\/github.com\/AleksandrIvchenko\/QoE-assesment"],["we use tunable, vacuum ultraviolet laser-based angle-resolved photoe","Fragility of Fermi arcs in Dirac semimetals","summarize: We use tunable, vacuum ultraviolet laser-based angle-resolved photoemission spectroscopy and density functional theory calculations to study the electronic properties of Dirac semimetal candidate cubic PtBi"],["paper introduces non-stationary adversarial cost with a variation constraint.","Bayesian adversarial multi-node bandit for optimal smart grid protection against cyber attacks","summarize: The cybersecurity of smart grids has become one of key problems in developing reliable modern power and energy systems. This paper introduces a non-stationary adversarial cost with a variation constraint for smart grids and enables us to investigate the problem of optimal smart grid protection against cyber attacks in a relatively practical scenario. In particular, a Bayesian multi-node bandit model with adversarial costs is constructed and a new regret function is defined for this model. An algorithm called Thompson-Hedge algorithm is presented to solve the problem and the superior performance of the proposed algorithm is proven in terms of the convergence rate of the regret function. The applicability of the algorithm to real smart grid scenarios is verified and the performance of the algorithm is also demonstrated by numerical examples."],["a novel superpixel-guided two-view geometric model fitting method is proposed.","Superpixel-guided Two-view Deterministic Geometric Model Fitting","summarize: Geometric model fitting is a fundamental research topic in computer vision and it aims to fit and segment multiple-structure data. In this paper, we propose a novel superpixel-guided two-view geometric model fitting method , which can obtain reliable and consistent results for real images. Specifically, SDF includes three main parts: a deterministic sampling algorithm, a model hypothesis updating strategy and a novel model selection algorithm. The proposed deterministic sampling algorithm generates a set of initial model hypotheses according to the prior information of superpixels. Then the proposed updating strategy further improves the quality of model hypotheses. After that, by analyzing the properties of the updated model hypotheses, the proposed model selection algorithm extends the conventional fit-and-remove framework to estimate model instances in multiple-structure data. The three parts are tightly coupled to boost the performance of SDF in both speed and accuracy, and SDF has the deterministic nature. Experimental results show that the proposed SDF has significant advantages over several state-of-the-art fitting methods when it is applied to real images with single-structure and multiple-structure data."],["bounded curvature path is a continuously differentiable piecewise.","The classification of homotopy classes of bounded curvature paths","summarize: A bounded curvature path is a continuously differentiable piecewise "],["the cobra maneuver requires the aircraft to fly at extremely high angle of attacks. the feedback","Learning Pugachev's Cobra Maneuver for Tail-sitter UAVs Using Acceleration Model","summarize: The Pugachev's cobra maneuver is a dramatic and demanding maneuver requiring the aircraft to fly at extremely high Angle of Attacks where stalling occurs. This paper considers this maneuver on tail-sitter UAVs. We present a simple yet very effective feedback-iterative learning position control structure to regulate the altitude error and lateral displacement during the maneuver. Both the feedback controller and the iterative learning controller are based on the aircraft acceleration model, which is directly measurable by the onboard accelerometer. Moreover, the acceleration model leads to an extremely simple dynamic model that does not require any model identification in designing the position controller, greatly simplifying the implementation of the iterative learning control. Real-world outdoor flight experiments on the Hong Hu UAV, an aerobatic yet efficient quadrotor tail-sitter UAV of small-size, are provided to show the effectiveness of the proposed controller."],["the discovery of the gravitational-wave source GW150914 with the Advanced LI","Astrophysical Implications of the Binary Black-Hole Merger GW150914","summarize: The discovery of the gravitational-wave source GW150914 with the Advanced LIGO detectors provides the first observational evidence for the existence of binary black-hole systems that inspiral and merge within the age of the Universe. Such black-hole mergers have been predicted in two main types of formation models, involving isolated binaries in galactic fields or dynamical interactions in young and old dense stellar environments. The measured masses robustly demonstrate that relatively heavy black holes can form in nature. This discovery implies relatively weak massive-star winds and thus the formation of GW150914 in an environment with metallicity lower than "],["one often encounters numerical difficulties in solving linear matrix inequality problems obtained from linear matrix inequality problems","Application of Facial Reduction to ","summarize: One often encounters numerical difficulties in solving linear matrix inequality problems obtained from "],["the non-commutative black hole can reduce to the Schwarzschild black hole.","Shadow cast of non-commutative black holes in Rastall gravity","summarize: We study the shadow and energy emission rate of a spherically symmetric non-commutative black hole in Rastall gravity. Depending on the model parameters, the non-commutative black hole can reduce to the Schwarzschild black hole. Since the non-vanishing non-commutative parameter affects the formation of event horizon, the visibility of the resulting shadow depends on the non-commutative parameter in Rastall gravity. The obtained sectional shadows respect the unstable circular orbit condition, which is crucial for physical validity of the black hole image model."],["the proposed FT protocol can support an unbounded number of faulty nodes as long","Toward Fault-Tolerant Deadlock-Free Routing in HyperSurface-Embedded Controller Networks","summarize: HyperSurfaces consist of structurally reconfigurable metasurfaces whose electromagnetic properties can be changed via a software interface, using an embedded miniaturized network of controllers. With the HSF controllers, interconnected in an irregular, near-Manhattan geometry, we propose a robust, deterministic Fault-Tolerant , deadlock- and livelock-free routing protocol where faults are contained in a set of disjointed rectangular regions called faulty blocks. The proposed FT protocol can support an unbounded number of faulty nodes as long as nodes outside the faulty blocks are connected. Simulation results show the efficacy of the proposed FT protocol under various faulty node distribution scenarios."],["stellar debris spirals inwards and emits x-rays when near the black","A Remarkably Loud Quasi-Periodicity after a Star is Disrupted by a Massive Black Hole","summarize: The immense tidal forces of massive black holes can rip apart stars that come too close to them. As the resulting stellar debris spirals inwards, it heats up and emits x-rays when near the black hole. Here, we report the discovery of an exceptionally stable 131-second x-ray quasi-periodicity from a black hole after it disrupted a star. Using a black hole mass indicated from host galaxy scaling relations implies that, this periodicity originates from very close to the black hole's event horizon, and the black hole is rapidly spinning. Our findings suggest that other disruption events with similar highly sensitive observations likely also exhibit quasi-periodicities that encode information about the fundamental properties of their black holes."],["RankME significantly improves reliability and consistency of human ratings. ranked multiple NLG","RankME: Reliable Human Ratings for Natural Language Generation","summarize: Human evaluation for natural language generation often suffers from inconsistent user ratings. While previous research tends to attribute this problem to individual user preferences, we show that the quality of human judgements can also be improved by experimental design. We present a novel rank-based magnitude estimation method , which combines the use of continuous scales and relative assessments. We show that RankME significantly improves the reliability and consistency of human ratings compared to traditional evaluation methods. In addition, we show that it is possible to evaluate NLG systems according to multiple, distinct criteria, which is important for error analysis. Finally, we demonstrate that RankME, in combination with Bayesian estimation of system quality, is a cost-effective alternative for ranking multiple NLG systems."],["the casimir-like force arises when long-ranged fluctuations are geometrically","Casimir effect between pinned particles in two-dimensional jammed systems","summarize: The Casimir effect arises when long-ranged fluctuations are geometrically confined between two surfaces, leading to a macroscopic force. Traditionally, these forces have been observed in quantum systems and near critical points in classical systems. Here we show the existence of Casimir-like forces between two pinned particles immersed in two-dimensional systems near the jamming transition. We observe two components to the total force: a short-ranged, depletion force and a long-ranged, repulsive Casimir-like force. The Casimir-like force dominates as the jamming transition is approached, and when the pinned particles are much larger than the ambient jammed particles. We show that this repulsive force arises due to a clustering of particles with strong contact forces around the perimeter of the pinned particles. As the separation between the pinned particles decreases, a region of high-pressure develops between them, leading to a net repulsive force."],["a number of works have observed that Convolutional Neural Nets are invertible","Towards Understanding the Invertibility of Convolutional Neural Networks","summarize: Several recent works have empirically observed that Convolutional Neural Nets are invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable re- construction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios."],["a half-century journey of great complexity has been a journey of great complexity.","Core-Collapse Supernova Explosion Theory","summarize: Most supernova explosions accompany the death of a massive star. These explosions give birth to neutron stars and black holes and eject solar masses of heavy elements. However, determining the mechanism of explosion has been a half-century journey of great complexity. In this paper, we present our perspective of the status of this theoretical quest and the physics and astrophysics upon which its resolution seems to depend. The delayed neutrino-heating mechanism is emerging as a robust solution, but there remain many issues to address, not the least of which involves the chaos of the dynamics, before victory can unambiguously be declared. It is impossible to review in detail all aspects of this multi-faceted, more-than-half-century-long theoretical quest. Rather, we here map out the major ingredients of explosion and the emerging systematics of the observables with progenitor mass, as we currently see them. Our discussion will of necessity be speculative in parts, and many of the ideas may not survive future scrutiny. Some statements may be viewed as informed predictions concerning the numerous observables that rightly exercise astronomers witnessing and diagnosing the supernova Universe. Importantly, the same explosion in the inside, by the same mechanism, can look very different in photons, depending upon the mass and radius of the star upon explosion. A 10"],["the recovery in consumer spending in the united kingdom through the second half of 2020 is unevenly distributed","Levelling Down and the COVID-19 Lockdowns: Uneven Regional Recovery in UK Consumer Spending","summarize: We show the recovery in consumer spending in the United Kingdom through the second half of 2020 is unevenly distributed across regions. We utilise Fable Data: a real-time source of consumption data that is a highly correlated, leading indicator of Bank of England and Office for National Statistics data. The UK's recovery is heavily weighted towards the home counties around outer London and the South. We observe a stark contrast between strong online spending growth while offline spending contracts. The strongest recovery in spending is seen in online spending in the commuter belt areas in outer London and the surrounding localities and also in areas of high second home ownership, where working from home has significantly displaced the location of spending. Year-on-year spending growth in November 2020 in localities facing the UK's new tighter Tier 3 restrictions was 38.4% lower compared with areas facing the less restrictive Tier 2 . These patterns had been further exacerbated during November 2020 when a second national lockdown was imposed. To prevent such COVID-19-driven regional inequalities from becoming persistent we propose governments introduce temporary, regionally-targeted interventions in 2021. The availability of real-time, regional data enables policymakers to efficiently decide when, where and how to implement such regional interventions and to be able to rapidly evaluate their effectiveness to consider whether to expand, modify or remove them."],["the logarithm is a law of the iterated logarithm","On the law of the iterated logarithm for continued fractions with sequentially restricted partial quotients","summarize: We establish a law of the iterated logarithm for the set of real numbers whose "],["a semi-infinite crack in infinite square lattice is subjecte","Scattering on square lattice from crack with damage zone","summarize: A semi-infinite crack in infinite square lattice is subjected to a wave coming from infinity, thereby leading to its scattering by the crack surfaces. A partially damaged zone ahead of the crack-tip is modeled by an arbitrarily distributed stiffness of the damaged links. While the open crack, with an atomically sharp crack-tip, in the lattice has been solved in closed form with help of scalar Wiener-Hopf formulation , the problem considered here becomes very intricate depending on the nature of damaged links. For instance, in the case of partially bridged finite zone it involves a "],["multi-modal data set containing cardiovascular and brain electrical activities. we provide technical validation using","Multimodal pathophysiological dataset of gradual cerebral ischemia in a cohort of juvenile pigs","summarize: Ischemic brain injuries are frequent and difficult to detect reliably or early. We present the multi-modal data set containing cardiovascular and brain electrical activities to derive electroencephalogram biomarkers of corticothalamic communication under normal, sedation and hypoxic\/ischemic conditions with ensuing recovery. We provide technical validation using EEGLAB. We also delineate the corresponding changes in the electrocardiogram -derived heart rate variability with the potential for future in-depth analyses of joint EEG-ECG dynamics. We review an open-source methodology to derive signatures of coupling between the ECoG and electrothalamogram signals contained in the presented data set to better characterize the dynamics of thalamocortical communication during these clinically relevant states. The data set is presented in full band sampled at 2000 Hz, so the additional potential exists for insights from the full-band EEG and high-frequency oscillations under the bespoke experimental conditions. Future studies on the dataset may contribute to the development of new brain monitoring technologies, which will facilitate the prevention of neurological injuries."],["the tetrahedron is a special case of four triangular faces with","The stray- and demagnetizing field from a homogeneously magnetized tetrahedron","summarize: The stray- and demagnetization tensor field for a homogeneously magnetized tetrahedron is found analytically. The tetrahedron is a special case of four triangular faces with constant magnetization-charge surface density, for which we also determine the tensor field. The tensor field is implemented in the open source micromagnetic and magnetostatic simulation framework MagTense and compared with the obtained magnetic field from an FEM solution, showing excellent agreement. This result is important for modeling magnetostatics in general and for micromagnetism in particular as the demagnetizing field of an arbitrary body discretized using conventional meshing techniques is significantly simplified with this approach."],["a distribution over kernels formed by modelling a spectral mixture density with a","Scalable L\\'evy Process Priors for Spectral Kernel Learning","summarize: Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a L\\'evy process. The resulting distribution has support for all stationary covariances--including the popular RBF, periodic, and Mat\\'ern kernels--combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the L\\'evy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for "],["the class of generalized shearlet dilation groups has recently been developed to allow the","Recent Progress in Shearlet Theory: Systematic Construction of Shearlet Dilation Groups, Characterization of Wavefront Sets, and New Embeddings","summarize: The class of generalized shearlet dilation groups has recently been developed to allow the unified treatment of various shearlet groups and associated shearlet transforms that had previously been studied on a case-by-case basis. We consider several aspects of these groups: First, their systematic construction from associative algebras, secondly, their suitability for the characterization of wavefront sets, and finally, the question of constructing embeddings into the symplectic group in a way that intertwines the quasi-regular representation with the metaplectic one. For all questions, it is possible to treat the full class of generalized shearlet groups in a comprehensive and unified way, thus generalizing known results to an infinity of new cases. Our presentation emphasizes the interplay between the algebraic structure underlying the construction of the shearlet dilation groups, the geometric properties of the dual action, and the analytic properties of the associated shearlet transforms."],["software needs to evolve over the time to meet the new user's requirements. software companies","The Impact of the Object-Oriented Software Evolution on Software Metrics: The Iris Approach","summarize: The Object-Oriented software system evolves over the time to meet the new requirements. Based on the initial release of software, the continuous modification of software code leads to software evolution. Software needs to evolve over the time to meet the new user's requirements. Software companies often develop variant software of the original one depends on customers' needs. The main hypothesis of this paper states that the software when it evolves over the time, its code continues to grow, change and become more complex. This paper proposes an automatic approach to examine the proposed hypothesis. Originality of this approach is the exploiting of the software variants to study the impact of software evolution on the software metrics. This paper presents the results of experiments conducted on three releases of drawing shapes software, sixteen releases of rhino software, eight releases of mobile media software and ten releases of ArgoUML software. Based on the extracted software metrics, It has been found that Iris hypothesis is supported by the computed metrics."],["non-Hermitian optics is considered as one of the frontiers areas of research","Non-Hermitian Optics","summarize: Non-Hermitian optics, mostly known as 'Parity-time symmetric optics', is considered as one of the frontiers areas of research in optical sciences at present. This area is largely inspired by the so-called non-Hermitian quantum physics. While the non-Hermitian quantum mechanics is yet to be accepted widely, parity-time symmetric optics is already a craze among physicists with many experimental results and demonstrations to its support. In this article, keeping under graduate students in mind, in particular, we are giving a brief introduction to this promising area of research in physics."],["dg2pix is a novel pixel-based visualization technique. it","dg2pix: Pixel-Based Visual Analysis of Dynamic Graphs","summarize: Presenting long sequences of dynamic graphs remains challenging due to the underlying large-scale and high-dimensional data. We propose dg2pix, a novel pixel-based visualization technique, to visually explore temporal and structural properties in long sequences of large-scale graphs. The approach consists of three main steps: the multiscale modeling of the temporal dimension; unsupervised graph embeddings to learn low-dimensional representations of the dynamic graph data; and an interactive pixel-based visualization to simultaneously explore the evolving data at different temporal aggregation scales. dg2pix provides a scalable overview of a dynamic graph, supports the exploration of long sequences of high-dimensional graph data, and enables the identification and comparison of similar temporal states. We show the applicability of the technique to synthetic and real-world datasets, demonstrating that temporal patterns in dynamic graphs can be identified and interpreted over time. dg2pix contributes a suitable intermediate representation between node-link diagrams at the high detail end and matrix representations on the low detail end."],["we introduce a new invariant for triangulated categories. the poset","Spherical subcategories in representation theory","summarize: We introduce a new invariant for triangulated categories: the poset of spherical subcategories ordered by inclusion. This yields several numerical invariants, like the cardinality and the height of the poset. We explicitly describe spherical subcategories and their poset structure for derived categories of certain finite-dimensional algebras."],["a new video dataset, called the Miss Universe dataset, is proposed. the results are","Towards Miss Universe Automatic Prediction: The Evening Gown Competition","summarize: Can we predict the winner of Miss Universe after watching how they stride down the catwalk during the evening gown competition? Fashion gurus say they can! In our work, we study this question from the perspective of computer vision. In particular, we want to understand whether existing computer vision approaches can be used to automatically extract the qualities exhibited by the Miss Universe winners during their catwalk. This study can pave the way towards new vision-based applications for the fashion industry. To this end, we propose a novel video dataset, called the Miss Universe dataset, comprising 10 years of the evening gown competition selected between 1996-2010. We further propose two ranking-related problems: Miss Universe Listwise Ranking and Miss Universe Pairwise Ranking. In addition, we also develop an approach that simultaneously addresses the two proposed problems. To describe the videos we employ the recently proposed Stacked Fisher Vectors in conjunction with robust local spatio-temporal features. From our evaluation we found that although the addressed problems are extremely challenging, the proposed system is able to rank the winner in the top 3 best predicted scores for 5 out of 10 Miss Universe competitions."],["four circulant constructions for self-dual codes and bordered versions of the construction","New extremal binary self-dual codes from a modified four circulant construction","summarize: In this work, we propose a modified four circulant construction for self-dual codes and a bordered version of the construction using the properties of \\lambda-circulant and \\lambda-reverse circulant matrices. By using the constructions on "],["a gas storage valuation problem is a regime-switching model. this model","Gas Storage valuation with regime switching","summarize: In this paper we treat a gas storage valuation problem as a Markov Decision Process. As opposed to existing literature we model the gas price process as a regime-switching model. Such a model has shown to fit market data quite well in Chen and Forsyth . Before we apply a numerical algorithm to solve the problem, we first identify the structure of the optimal injection and withdraw policy. This part extends results in Secomandi . Knowing the structure reduces the complexity of the involved recursion in the algorithms by one variable. We explain the usage and implementation of two algorithms: A Multinomial-Tree Algorithm and a Least-Square Monte Carlo Algorithm. Both algorithms are shown to work for the regime-switching extension. In a numerical study we compare these two algorithms."],["lower bound given by lowest eigenvalue of one-dimensional operator. if reference","A lower bound to the spectral threshold in curved quantum layers","summarize: We derive a lower bound to the spectral threshold of the Dirichlet Laplacian in tubular neighbourhoods of constant radius about complete surfaces. This lower bound is given by the lowest eigenvalue of a one-dimensional operator depending on the radius and principal curvatures of the reference surface. Moreover, we show that it is optimal if the reference surface is non-negatively curved."],["Thurston maps are branched covering maps under iteration.","Expanding Thurston Maps","summarize: We study the dynamics of Thurston maps under iteration. These are branched covering maps "],["trajectory optimization is solved in the time domain. the optimization is solved in the time domain","Frequency-Aware Model Predictive Control","summarize: Transferring solutions found by trajectory optimization to robotic hardware remains a challenging task. When the optimization fully exploits the provided model to perform dynamic tasks, the presence of unmodeled dynamics renders the motion infeasible on the real system. Model errors can be a result of model simplifications, but also naturally arise when deploying the robot in unstructured and nondeterministic environments. Predominantly, compliant contacts and actuator dynamics lead to bandwidth limitations. While classical control methods provide tools to synthesize controllers that are robust to a class of model errors, such a notion is missing in modern trajectory optimization, which is solved in the time domain. We propose frequency-shaped cost functions to achieve robust solutions in the context of optimal control for legged robots. Through simulation and hardware experiments we show that motion plans can be made compatible with bandwidth limits set by actuators and contact dynamics. The smoothness of the model predictive solutions can be continuously tuned without compromising the feasibility of the problem. Experiments with the quadrupedal robot ANYmal, which is driven by highly-compliant series elastic actuators, showed significantly improved tracking performance of the planned motion, torque, and force trajectories and enabled the machine to walk robustly on terrain with unmodeled compliance."],["Locatello et al. demonstrated that unsupervised disentanglement learning without inductive","Disentangling Factors of Variation Using Few Labels","summarize: Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a limited amount of supervision, for example through manual labeling of factors of variation in a few training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 52000 models under well-defined and reproducible experimental conditions. We observe that a small number of labeled examples , with potentially imprecise and incomplete labels, is sufficient to perform model selection on state-of-the-art unsupervised models. Further, we investigate the benefit of incorporating supervision into the training process. Overall, we empirically validate that with little and imprecise supervision it is possible to reliably learn disentangled representations."],["the famous continuous time random walk model with power law waiting time distribution describes this phenomenon. the","Well-posedness and numerical algorithm for the tempered fractional ordinary differential equations","summarize: Trapped dynamics widely appears in nature, e.g., the motion of particles in viscous cytoplasm. The famous continuous time random walk model with power law waiting time distribution describes this phenomenon. Because of the finite lifetime of biological particles, sometimes it is necessary to temper the power law measure such that the waiting time measure has convergent first moment. Then the time operator of the Fokker-Planck equation corresponding to the CTRW model with tempered waiting time measure is the so-called tempered fractional derivative. This paper focus on discussing the properties of the time tempered fractional derivative, and studying the well-posedness and the Jacobi-predictor-corrector algorithm for the tempered fractional ordinary differential equation. By adjusting the parameter of the proposed algorithm, any desired convergence order can be obtained and the computational cost linearly increases with time. And the effectiveness of the algorithm is numerically confirmed."],["generative model generates statistically independent samples for molecules. model learns a low","A Generative Model for Molecular Distance Geometry","summarize: Great computational effort is invested in generating equilibrium states for molecular systems using, for example, Markov chain Monte Carlo. We present a probabilistic model that generates statistically independent samples for molecules from their graph representations. Our model learns a low-dimensional manifold that preserves the geometry of local atomic neighborhoods through a principled learning representation that is based on Euclidean distance geometry. In a new benchmark for molecular conformation generation, we show experimentally that our generative model achieves state-of-the-art accuracy. Finally, we show how to use our model as a proposal distribution in an importance sampling scheme to compute molecular properties."],["billiard table made of two circular cavities connected by a straight channel. determin","Deterministic reversible model of non-equilibrium phase transitions and stochastic counterpart","summarize: N point particles move within a billiard table made of two circular cavities connected by a straight channel. The usual billiard dynamics is modified so that it remains deterministic, phase space volumes preserving and time reversal invariant. Particles move in straight lines and are elastically reflected at the boundary of the table, as usual, but those in a channel that are moving away from a cavity invert their motion , if their number exceeds a given threshold T. When the geometrical parameters of the billiard table are fixed, this mechanism gives rise to non--equilibrium phase transitions in the large N limit: letting T\/N decrease, the homogeneous particle distribution abruptly turns into a stationary inhomogeneous one. The equivalence with a modified Ehrenfest two urn model, motivated by the ergodicity of the billiard with no rebound, allows us to obtain analytical results that accurately describe the numerical billiard simulation results. Thus, a stochastic exactly solvable model that exhibits non-equilibrium phase transitions is also introduced."],["ML-based solutions address the efficient computing requirements of big data. but they introduce security","Security for Machine Learning-based Systems: Attacks and Challenges during Training and Inference","summarize: The exponential increase in dependencies between the cyber and physical world leads to an enormous amount of data which must be efficiently processed and stored. Therefore, computing paradigms are evolving towards machine learning -based systems because of their ability to efficiently and accurately process the enormous amount of data. Although ML-based solutions address the efficient computing requirements of big data, they introduce security vulnerabilities into the systems, which cannot be addressed by traditional monitoring-based security measures. Therefore, this paper first presents a brief overview of various security threats in machine learning, their respective threat models and associated research challenges to develop robust security measures. To illustrate the security vulnerabilities of ML during training, inferencing and hardware implementation, we demonstrate some key security threats on ML using LeNet and VGGNet for MNIST and German Traffic Sign Recognition Benchmarks , respectively. Moreover, based on the security analysis of ML-training, we also propose an attack that has a very less impact on the inference accuracy. Towards the end, we highlight the associated research challenges in developing security measures and provide a brief overview of the techniques used to mitigate such security threats."],["paper documents our attempt to computationally model the creative process of a portrait painter.","Empathic AI Painter: A Computational Creativity System with Embodied Conversational Interaction","summarize: There is a growing recognition that artists use valuable ways to understand and work with cognitive and perceptual mechanisms to convey desired experiences and narrative in their created artworks . This paper documents our attempt to computationally model the creative process of a portrait painter, who relies on understanding human traits to inform their art. Our system includes an empathic conversational interaction component to capture the dominant personality category of the user and a generative AI Portraiture system that uses this categorization to create a personalized stylization of the user's portrait. This paper includes the description of our systems and the real-time interaction results obtained during the demonstration session of the NeurIPS 2019 Conference."],["a graph of the graph shows the sex of the sex of the","The vertex Folkman numbers ","summarize: For a graph "],["we introduce a robust optimization framework that accounts for uncertain model parameters. the resulting non","A Certified Model Reduction Approach for Robust Parameter Optimization with PDE Constraints","summarize: We investigate an optimization problem governed by an elliptic partial differential equation with uncertain parameters. We introduce a robust optimization framework that accounts for uncertain model parameters. The resulting non-linear optimization problem has a bi-level structure due to the min-max formulation. To approximate the worst-case in the optimization problem we propose linear and quadratic approximations. However, this approach still turns out to be very expensive, therefore we propose an adaptive model order reduction technique which avoids long offline stages and provides a certified reduced order surrogate model for the parametrized PDE which is then utilized in the numerical optimization. Numerical results are presented to validate the presented approach."],["proposed method simplifies SDPs with no strictly feasible solution. proposed method could be","Partial facial reduction: simplified, equivalent SDPs via approximations of the PSD cone","summarize: We develop a practical semidefinite programming facial reduction procedure that utilizes computationally efficient approximations of the positive semidefinite cone. The proposed method simplifies SDPs with no strictly feasible solution by solving a sequence of easier optimization problems and could be a useful pre-processing technique for SDP solvers. We demonstrate effectiveness of the method on SDPs arising in practice, and describe our publicly-available software implementation. We also show how to find maximum rank matrices in our PSD cone approximations , and we give a post-processing procedure for dual solution recovery that generally applies to facial-reduction-based pre-processing techniques. Finally, we show how approximations can be chosen to preserve problem sparsity."],["a new attack detection algorithm has been developed for better recommendations. the unorganized malicious attacks","Unorganized Malicious Attacks Detection","summarize: Recommender system has attracted much attention during the past decade. Many attack detection algorithms have been developed for better recommendations, mostly focusing on shilling attacks, where an attack organizer produces a large number of user profiles by the same strategy to promote or demote an item. This work considers a different attack style: unorganized malicious attacks, where attackers individually utilize a small number of user profiles to attack different items without any organizer. This attack style occurs in many real applications, yet relevant study remains open. We first formulate the unorganized malicious attacks detection as a matrix completion problem, and propose the Unorganized Malicious Attacks detection approach, a proximal alternating splitting augmented Lagrangian method. We verify, both theoretically and empirically, the effectiveness of our proposed approach."],["artificial intelligence has been applied to control players' decisions in board games for over half a century","Collaborative Agent Gameplay in the Pandemic Board Game","summarize: While artificial intelligence has been applied to control players' decisions in board games for over half a century, little attention is given to games with no player competition. Pandemic is an exemplar collaborative board game where all players coordinate to overcome challenges posed by events occurring during the game's progression. This paper proposes an artificial agent which controls all players' actions and balances chances of winning versus risk of losing in this highly stochastic environment. The agent applies a Rolling Horizon Evolutionary Algorithm on an abstraction of the game-state that lowers the branching factor and simulates the game's stochasticity. Results show that the proposed algorithm can find winning strategies more consistently in different games of varying difficulty. The impact of a number of state evaluation metrics is explored, balancing between optimistic strategies that favor winning and pessimistic strategies that guard against losing."],["penguins are divided into groups and tasked with finding optimal solutions. the pen","PeSOA: Penguins Search Optimisation Algorithm for Global Optimisation Problems","summarize: This paper develops Penguin search Optimisation Algorithm , a new metaheuristic algorithm which is inspired by the foraging behaviours of penguins. A population of penguins located in the solution space of the given search and optimisation problem is divided into groups and tasked with finding optimal solutions. The penguins of a group perform simultaneous dives and work as a team to collaboratively feed on fish the energy content of which corresponds to the fitness of candidate solutions. Fish stocks have higher fitness and concentration near areas of solution optima and thus drive the search. Penguins can migrate to other places if their original habitat lacks food. We identify two forms of penguin communication both intra-group and inter-group which are useful in designing intensification and diversification strategies. An efficient intensification strategy allows fast convergence to a local optimum, whereas an effective diversification strategy avoids cyclic behaviour around local optima and explores more effectively the space of potential solutions. The proposed PeSOA algorithm has been validated on a well-known set of benchmark functions. Comparative performances with six other nature-inspired metaheuristics show that the PeSOA performs favourably in these tests. A run-time analysis shows that the performance obtained by the PeSOA is very stable at any time of the evolution horizon, making the PeSOA a viable approach for real world applications."],["european leaders doubt waiving intellectual property rights for vaccines. proposal recently supported by the","EU leaders raise doubts over U.S. plan to waive Covid vaccine patents","summarize: European leaders have doubts that waiving intellectual property rights for Covid-19 vaccines, a proposal recently supported by the United States, is the way to go."],["Worcester polytechnic institute was founded in 1865 to create and convey the latest science and engineering","wpi provide an education that balances theory with practice","summarize: Worcester Polytechnic Institute was founded in 1865 to create and convey the latest science and engineering knowledge in ways that are most beneficial to society. Today, WPI holds firm to its founding mission to provide an education that balances theory with practice."],["WPI was among the first universities in the united states to incorporate project-based learning in its","The Global Leader in Project-Based Education","summarize: WPI was among the first universities in the United States to incorporate project-based learning in its undergraduate curriculum (known as the WPI Plan), requiring students to apply their acquired skills, knowledge, and abilities to develop solutions for real-world problems. Now WPI is helping other colleges and universities implement experiential learning on their own campuses through the Center for Project-Based Learning."]]}