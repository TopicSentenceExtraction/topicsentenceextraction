{"columns":["predict_title","actual_title","actual_abstract","bleu"],"index":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],"data":[["expression trees are the most frequently used visualization of the structure of mathematical expressions. a","VMEXT: A Visualization Tool for Mathematical Expression Trees","summarize: Mathematical expressions can be represented as a tree consisting of terminal symbols, such as identifiers or numbers , and functions or operators . Expression trees are an important mechanism for storing and processing mathematical expressions as well as the most frequently used visualization of the structure of mathematical expressions. Typically, researchers and practitioners manually visualize expression trees using general-purpose tools. This approach is laborious, redundant, and error-prone. Manual visualizations represent a user's notion of what the markup of an expression should be, but not necessarily what the actual markup is. This paper presents VMEXT - a free and open source tool to directly visualize expression trees from parallel MathML. VMEXT simultaneously visualizes the presentation elements and the semantic structure of mathematical expressions to enable users to quickly spot deficiencies in the Content MathML markup that does not affect the presentation of the expression. Identifying such discrepancies previously required reading the verbose and complex MathML markup. VMEXT also allows one to visualize similar and identical elements of two expressions. Visualizing expression similarity can support support developers in designing retrieval approaches and enable improved interaction concepts for users of mathematical information retrieval systems. We demonstrate VMEXT's visualizations in two web-based applications. The first application presents the visualizations alone. The second application shows a possible integration of the visualizations in systems for mathematical knowledge management and mathematical information retrieval. The application converts LaTeX input to parallel MathML, computes basic similarity measures for mathematical expressions, and visualizes the results using VMEXT.",0.25],["the paper examines the method of decomposing a spectral multiplier phi","Admissible decomposition for spectral multipliers on Gaussian L^p","summarize: This paper concerns harmonic analysis of the Ornstein--Uhlenbeck operator L on the Euclidean space. We examine the method of decomposing a spectral multiplier \\phi into three parts according to the notion of admissibility, which quantifies the doubling behaviour of the underlying Gaussian measure \\gamma. We prove that the above-mentioned admissible decomposition is bounded in L^p for 1 < p \\leq 2 in a certain sense involving the Gaussian conical square function. The proof relates admissibility with E. Nelson's hypercontractivity theorem in a novel way.",0.3125],["the most distant Kuiper belt objects appear to be clustered in longitude of","Orbital clustering in the distant solar system","summarize: The most distant Kuiper belt objects appear to be clustered in longitude of perihelion and in orbital pole position. To date, the only two suggestions for the cause of these apparent clusterings have been either the effects of observational bias or the existence of the distant giant planet in an eccentric inclined orbit known as Planet Nine. To determine if observational bias can be the cause of these apparent clusterings, we develop a rigorous method of quantifying the observational biases in the observations of longitude of perihelion and orbital pole position. From this now more complete understanding of the biases we calculate that the probability that these distant Kuiper belt objects would be clustered as strongly as observed in both longitude of perihelion and in orbital pole position is only 0.2%. While explanations other than Planet Nine may someday be found, the statistical significance of this clustering is now difficult to discount.",0.2142857143],["linear infinite dimensional distributed parameter systems in a space in Hilbert. the characterization","Regional Boundary Strategic Sensors Characterizations","summarize: This paper, deals with the linear infinite dimensional distributed parameter systems in a Hilbert space where the dynamics of the system is governed by strongly continuous semi-groups. More precisely, for parabolic distributed systems the characterizations of regional boundary strategic sensors have been discussed and analyzed in different cases of regional boundary observability in infinite time interval. Furthermore, the results so obtained are applied in two-dimensional systems and the sensors are studied under which conditions guarantee regional boundary observability in a sub-region of the system domain boundary. Also, the authors show that, the existence of a given sensor for the diffusion system is not strategic in the usual sense, but it may be regional boundary strategic of this system.",0.2068965517],["neural networks are generally designed as a stack of differentiable layers. they are designed","Why should we add early exits to neural networks?","summarize: Deep neural networks are generally designed as a stack of differentiable layers, in which a prediction is obtained only after running the full stack. Recently, some contributions have proposed techniques to endow the networks with early exits, allowing to obtain predictions at intermediate points of the stack. These multi-output networks have a number of advantages, including: significant reductions of the inference time, reduced tendency to overfitting and vanishing gradients, and capability of being distributed over multi-tier computation platforms. In addition, they connect to the wider themes of biological plausibility and layered cognitive reasoning. In this paper, we provide a comprehensive introduction to this family of neural networks, by describing in a unified fashion the way these architectures can be designed, trained, and actually deployed in time-constrained scenarios. We also describe in-depth their application scenarios in 5G and Fog computing environments, as long as some of the open research questions connected to them.",0.1923076923],["two new coding\/decoding algorithms are used to improve information security. the algorithm is","A New Cryptography Model via Fibonacci and Lucas Numbers","summarize: Coding\/decoding algorithms are of great importance to help in improving information security since information security is a more significiant problem in recent years. In this paper we introduce two new coding\/decoding algorithms using Fibonacci ",0.0714285714],["topological properties such as Chern number and momentum-space properties are used to explain back-","Momentum-Space Topological Effects of Nonreciprocity","summarize: The connection between topology and nonreciprocity in photonic systems is reviewed. Topological properties such as Chern number, and momentum-space properties such as Berry phase and Berry connection, are used to explain back-scattering immune edge states and their topological protection. We consider several examples to illustrate the role of momentum-space topology on wave propagation, and discus recent magnet-less approaches.",0.0],["a new algorithm for finding stable fixed fixed fixed fixed fixed fixed fixed fixed fixed fixed fixed fixed","The Mechanics of n-Player Differentiable Games","summarize: The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood -- and is becoming increasingly important as adversarial and multi-objective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment , a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -- whilst at the same time being applicable to -- and having guarantees in -- much more general games.",0.0197368421],["Gerards and Seymour conjectured that every graph with no odd odd","Improper coloring of graphs with no odd clique minor","summarize: As a strengthening of Hadwiger's conjecture, Gerards and Seymour conjectured that every graph with no odd ",0.3846153846],["the Brazilian Multipurpose Reactor will provide 3 thermal guides and 3 cold guides. the","Neutron Guide Building Instruments of the Brazilian Multipurpose Reactor Project","summarize: A growing community of scientists has been using neutrons in the most diverse areas of science. In order to meet the researchers demand in the areas of physics, chemistry, materials sciences, engineering, cultural heritage, biology and earth sciences, the Brazilian Multipurpose Reactor will provide 3 thermal guides and 3 cold guides, with the installation of several instruments for materials characterization. In this study, we present a standard design requirement of two primordial instruments, namely Sabi\\'a and Araponga. They are, respectively, cold and thermal neutron instruments and correspond to a Small-Angle Neutron Scattering and High-Resolution Powder Neutron Diffractometer to be installed in the Neutron Guide Building of RMB. To provide adequate flux for both instruments, we propose here an initial investigation of the use of simple and split guides to transport neutron beams to two different instruments on the same guide. For this purpose, we use Monte Carlo simulations utilizing McStas software to check the efficiency of thermal neutron transport for different basic configuration and sources. By considering these results, it is possible to conclude that the split guide configuration is, in most cases, more efficient than cases that use transmitted neutron beams independently of source. We also verify that the employment of different coating indexes for concave and convex surfaces on curved guides is crucial, at least on simulated cases, to optimise neutron flux and diminish facility installation cost.",0.2272727273],["we implement and evaluate practical randomized algorithms for accurately approximating the volume of a","Efficient Random-Walk Methods for Approximating Polytope Volume","summarize: We experimentally study the fundamental problem of computing the volume of a convex polytope given as an intersection of linear inequalities. We implement and evaluate practical randomized algorithms for accurately approximating the polytope's volume in high dimensions . To carry out this efficiently we experimentally correlate the effect of parameters, such as random walk length and number of sample points, on accuracy and runtime. Moreover, we exploit the problem's geometry by implementing an iterative rounding procedure, computing partial generations of random points and designing fast polytope boundary oracles. Our publicly available code is significantly faster than exact computation and more accurate than existing approximation methods. We provide volume approximations for the Birkhoff polytopes B_11,...,B_15, whereas exact methods have only computed that of B_10.",0.1923076923],["5G networks can provide high compatibility for applications, industries, and business models. they","A Survey on the Security and the Evolution of Osmotic and Catalytic Computing for 5G Networks","summarize: The 5G networks have the capability to provide high compatibility for the new applications, industries, and business models. These networks can tremendously improve the quality of life by enabling various use cases that require high data-rate, low latency, and continuous connectivity for applications pertaining to eHealth, automatic vehicles, smart cities, smart grid, and the Internet of Things . However, these applications need secure servicing as well as resource policing for effective network formations. There have been a lot of studies, which emphasized the security aspects of 5G networks while focusing only on the adaptability features of these networks. However, there is a gap in the literature which particularly needs to follow recent computing paradigms as alternative mechanisms for the enhancement of security. To cover this, a detailed description of the security for the 5G networks is presented in this article along with the discussions on the evolution of osmotic and catalytic computing-based security modules. The taxonomy on the basis of security requirements is presented, which also includes the comparison of the existing state-of-the-art solutions. This article also provides a security model, CATMOSIS, which idealizes the incorporation of security features on the basis of catalytic and osmotic computing in the 5G networks. Finally, various security challenges and open issues are discussed to emphasize the works to follow in this direction of research.",0.183212921],["a graph-based model of programmable environments is proposed. the results are","Modeling, Simulating and Configuring Programmable Wireless Environments for Multi-User Multi-Objective Networking","summarize: Programmable wireless environments enable the software-defined propagation of waves within them, yielding exceptional performance potential. Several building-block technologies have been implemented and evaluated at the physical layer. The present work contributes a network-layer scheme to configure such environments for multiple users and objectives, and for any physical-layer technology. Supported objectives include any combination of Quality of Service and power transfer optimization, eavesdropping and Doppler effect mitigation, in multi-cast or uni-cast settings. Additionally, a graph-based model of programmable environments is proposed, which incorporates core physical observations and efficiently separates physical and networking concerns. Evaluation takes place in a specially developed, free simulation tool, and in a variety of environments. Performance gains over regular propagation are highlighted, reaching important insights on the user capacity of programmable environments.",0.25],["leading scientists argue that global warming is a serious concern. a global warming denier","Detecting Stance in Media on Global Warming","summarize: Citing opinions is a powerful yet understudied strategy in argumentation. For example, an environmental activist might say, Leading scientists agree that global warming is a serious concern, framing a clause which affirms their own stance as an opinion endorsed by a reputable source . In contrast, a global warming denier might frame the same clause as the opinion of an untrustworthy source with a predicate connoting doubt: Mistaken scientists claim . Our work studies opinion-framing in the global warming debate, an increasingly partisan issue that has received little attention in NLP. We introduce Global Warming Stance Dataset , a dataset of stance-labeled GW sentences, and train a BERT classifier to study novel aspects of argumentation in how different sides of a debate represent their own and each other's opinions. From 56K news articles, we find that similar linguistic devices for self-affirming and opponent-doubting discourse are used across GW-accepting and skeptic media, though GW-skeptical media shows more opponent-doubt. We also find that authors often characterize sources as hypocritical, by ascribing opinions expressing the author's own view to source entities known to publicly endorse the opposing view. We release our stance dataset, model, and lexicons of framing devices for future work on opinion-framing and the automatic detection of GW stance.",0.2285714286],["the geometric assertion is modulo a hypothesis on root systems. the result extends the","Absolute convergence of the twisted Arthur-Selberg trace formula","summarize: We show that the distributions occurring in the geometric and spectral side of the twisted Arthur-Selberg trace formula extend to non-compactly supported test functions. The geometric assertion is modulo a hypothesis on root systems proven when the group is split. The result extends the work of Finis-Lapid to the twisted setting. We use the absolute convergence to give a geometric interpretation of sums of residues of certain Rankin-Selberg L-functions.",0.2692307692],["domain adaptation investigates cross-domain knowledge transfer. source domain and target domain have distinctive data","Cycle-consistent Conditional Adversarial Transfer Networks","summarize: Domain adaptation investigates the problem of cross-domain knowledge transfer where the labeled source domain and unlabeled target domain have distinctive data distributions. Recently, adversarial training have been successfully applied to domain adaptation and achieved state-of-the-art performance. However, there is still a fatal weakness existing in current adversarial models which is raised from the equilibrium challenge of adversarial training. Specifically, although most of existing methods are able to confuse the domain discriminator, they cannot guarantee that the source domain and target domain are sufficiently similar. In this paper, we propose a novel approach named to handle this issue. Our approach takes care of the domain alignment by leveraging adversarial training. Specifically, we condition the adversarial networks with the cross-covariance of learned features and classifier predictions to capture the multimodal structures of data distributions. However, since the classifier predictions are not certainty information, a strong condition with the predictions is risky when the predictions are not accurate. We, therefore, further propose that the truly domain-invariant features should be able to be translated from one domain to the other. To this end, we introduce two feature translation losses and one cycle-consistent loss into the conditional adversarial domain adaptation networks. Extensive experiments on both classical and large-scale datasets verify that our model is able to outperform previous state-of-the-arts with significant improvements.",0.0],["we consider merges where the gap between cars is smaller than the size of the ego vehicle","Interactive Decision Making for Autonomous Vehicles in Dense Traffic","summarize: Dense urban traffic environments can produce situations where accurate prediction and dynamic models are insufficient for successful autonomous vehicle motion planning. We investigate how an autonomous agent can safely negotiate with other traffic participants, enabling the agent to handle potential deadlocks. Specifically we consider merges where the gap between cars is smaller than the size of the ego vehicle. We propose a game theoretic framework capable of generating and responding to interactive behaviors. Our main contribution is to show how game-tree decision making can be executed by an autonomous vehicle, including approximations and reasoning that make the tree-search computationally tractable. Additionally, to test our model we develop a stochastic rule-based traffic agent capable of generating interactive behaviors that can be used as a benchmark for simulating traffic participants in a crowded merge setting.",0.0416666667],["correlators have a topological expansion in terms of weakly or strictly monotone","Laguerre Ensemble: Correlators, Hurwitz Numbers and Hodge Integrals","summarize: We consider the Laguerre partition function, and derive explicit generating functions for connected correlators with arbitrary integer powers of traces in terms of products of Hahn polynomials. It was recently proven that correlators have a topological expansion in terms of weakly or strictly monotone Hurwitz numbers, that can be explicitly computed from our formulae. As a second result we identify the Laguerre partition function with only positive couplings and a special value of the parameter ",0.3157894737],["uniform confidence sets for the single edge are based on rotated differences of two one-sided","Asymptotic confidence sets for the jump curve in bivariate regression problems","summarize: We construct uniform and point-wise asymptotic confidence sets for the single edge in an otherwise smooth image function which are based on rotated differences of two one-sided kernel estimators. Using methods from M-estimation, we show consistency of the estimators of location, slope and height of the edge function and develop a uniform linearization of the contrast process. The uniform confidence bands then rely on a Gaussian approximation of the score process together with anti-concentration results for suprema of Gaussian processes, while point-wise bands are based on asymptotic normality. The finite-sample performance of the point-wise proposed methods is investigated in a simulation study. An illustration to real-world image processing is also given.",0.3333333333],["the gauge field is valued on an octonionic algebra. the gauge","Yang-Mills connections valued on the octonionic algebra","summarize: We consider a formulation of Yang-Mills theory where the gauge field is valued on an octonionic algebra and the gauge transformation is the group of automorphisms of it. We show, under mild assumptions, that the only possible gauge formulations are the usual ",0.4705882353],["NNs are increasingly being deployed in safety critical domains. we propose a novel","Deep Evidential Regression","summarize: Deterministic neural networks are increasingly being deployed in safety critical domains, where calibrated, robust, and efficient measures of uncertainty are crucial. In this paper, we propose a novel method for training non-Bayesian NNs to estimate a continuous target as well as its associated evidence in order to learn both aleatoric and epistemic uncertainty. We accomplish this by placing evidential priors over the original Gaussian likelihood function and training the NN to infer the hyperparameters of the evidential distribution. We additionally impose priors during training such that the model is regularized when its predicted evidence is not aligned with the correct output. Our method does not rely on sampling during inference or on out-of-distribution examples for training, thus enabling efficient and scalable uncertainty learning. We demonstrate learning well-calibrated measures of uncertainty on various benchmarks, scaling to complex computer vision tasks, as well as robustness to adversarial and OOD test samples.",0.0454545455],["proposed method relies on the construction of time-dependent reduced spaces. the approxim","Dynamical model reduction method for solving parameter-dependent dynamical systems","summarize: We propose a projection-based model order reduction method for the solution of parameter-dependent dynamical systems. The proposed method relies on the construction of time-dependent reduced spaces generated from evaluations of the solution of the full-order model at some selected parameters values. The approximation obtained by Galerkin projection is the solution of a reduced dynamical system with a modified flux which takes into account the time dependency of the reduced spaces. An a posteriori error estimate is derived and a greedy algorithm using this error estimate is proposed for the adaptive selection of parameters values. The resulting method can be interpreted as a dynamical low-rank approximation method with a subspace point of view and a uniform control of the error over the parameter set.",0.125],["the detailed observation of the distribution of redshifts and chirp masses of binary black hole","Effect of gravitational lensing on the distribution of gravitational waves from distant binary black hole mergers","summarize: The detailed observation of the distribution of redshifts and chirp masses of binary black hole mergers is expected to provide a clue to their origin. In this paper, we develop a hybrid model of the probability distribution function of gravitational lensing magnification taking account of both strong and weak gravitational lensing, and use it to study the effect of gravitational lensing magnification on the distribution of gravitational waves from distant binary black hole mergers detected in ongoing and future gravitational wave observations. We find that the effect of gravitational lensing magnification is significant at high ends of observed chirp mass and redshift distributions. While a high mass tail in the observed chirp mass distribution is produced by highly magnified gravitational lensing events, we find that highly demagnified images of strong lensing events produce a high redshift tail in the observed redshift distribution, which can easily be observed in the third-generation gravitational wave observatories. Such a demagnified, apparently high redshift event is expected to be accompanied by a magnified image that is observed typically ",0.4880906009],["tensor-valued data is vectorized and subjected to one of the","Independent component analysis for tensor-valued data","summarize: In preprocessing tensor-valued data, e.g. images and videos, a common procedure is to vectorize the observations and subject the resulting vectors to one of the many methods used for independent component analysis . However, the tensor structure of the original data is lost in the vectorization and, as a more suitable alternative, we propose the matrix- and tensor fourth order blind identification . In these tensorial extensions of the classic fourth order blind identification we assume a Kronecker structure for the mixing and perform FOBI simultaneously on each direction of the observed tensors. We discuss the theory and assumptions behind MFOBI and TFOBI and provide two different algorithms and related estimates of the unmixing matrices along with their asymptotic properties. Finally, simulations are used to compare the method's performance with that of classical FOBI for vectorized data and we end with a real data clustering example.",0.3636363636],["constant feedback conjugates are the only commutative feedback pairs. constant feedback conjugates","Commutativity of Systems with their Feedback Conjugates","summarize: After introducing commutativity concept and summarizing the relevant literature, this work is focused on the commutativity of feedback conjugates. It is already known that a linear time-varying differential system describing a single input-single output dynamical system is always commutative with its constant gain feedback pairs. In this article, it is proven that among the time-varying feedback conjugates of a linear time-varying system, constant feedback conjugates are the only commutative feedback pairs and any of the time-varying feedback conjugates cannot constitutes a commutative pair of a linear time-varying system.",0.0454545455],["a MCTS variant may only encounter states with an explicit, extrinsic","Preference-Based Monte Carlo Tree Search","summarize: Monte Carlo tree search is a popular choice for solving sequential anytime problems. However, it depends on a numeric feedback signal, which can be difficult to define. Real-time MCTS is a variant which may only rarely encounter states with an explicit, extrinsic reward. To deal with such cases, the experimenter has to supply an additional numeric feedback signal in the form of a heuristic, which intrinsically guides the agent. Recent work has shown evidence that in different areas the underlying structure is ordinal and not numerical. Hence erroneous and biased heuristics are inevitable, especially in such domains. In this paper, we propose a MCTS variant which only depends on qualitative feedback, and therefore opens up new applications for MCTS. We also find indications that translating absolute into ordinal feedback may be beneficial. Using a puzzle domain, we show that our preference-based MCTS variant, wich only receives qualitative feedback, is able to reach a performance level comparable to a regular MCTS baseline, which obtains quantitative feedback.",0.1764705882],["this work investigated the stability and asymptotic behavior of some Lotka Volterra type","Mathematical Modeling and Stability of Predator-Prey Systems","summarize: This work investigated the stability and asymptotic behavior of some Lotka Volterra type models. We used the Liapunov method which consists in analyzing the stability of systems of ordinary differential equations around the equilibrium when they submitted to perturbations in the initial conditions",0.2307692308],["model predictive control framework for quadrotors combines control and planning with action and perception objectives","PAMPC: Perception-Aware Model Predictive Control for Quadrotors","summarize: We present the first perception-aware model predictive control framework for quadrotors that unifies control and planning with respect to action and perception objectives. Our framework leverages numerical optimization to compute trajectories that satisfy the system dynamics and require control inputs within the limits of the platform. Simultaneously, it optimizes perception objectives for robust and reliable sens- ing by maximizing the visibility of a point of interest and minimizing its velocity in the image plane. Considering both perception and action objectives for motion planning and control is challenging due to the possible conflicts arising from their respective requirements. For example, for a quadrotor to track a reference trajectory, it needs to rotate to align its thrust with the direction of the desired acceleration. However, the perception objective might require to minimize such rotation to maximize the visibility of a point of interest. A model-based optimization framework, able to consider both perception and action objectives and couple them through the system dynamics, is therefore necessary. Our perception-aware model predictive control framework works in a receding-horizon fashion by iteratively solving a non-linear optimization problem. It is capable of running in real-time, fully onboard our lightweight, small-scale quadrotor using a low-power ARM computer, to- gether with a visual-inertial odometry pipeline. We validate our approach in experiments demonstrating the contradiction between perception and action objectives, and improved behavior in extremely challenging lighting conditions.",0.0526315789],["two sets of sets of sets of sets of sets of sets of sets of sets of sets of","A positive fraction mutually avoiding sets theorem","summarize: Two sets ",0.0552147239],["a slotted-ALOHA overlay on LoRaWAN networks is proposed.","Slotted ALOHA on LoRaWAN - Design, Analysis, and Deployment","summarize: LoRaWAN is one of the most promising standards for long-range sensing applications. However, the high number of end devices expected in at-scale deployment, combined with the absence of an effective synchronization scheme, challenge the scalability of this standard. In this paper, we present an approach to increase network throughput through a Slotted-ALOHA overlay on LoRaWAN networks. To increase the single channel capacity, we propose to regulate the communication of LoRaWAN networks using a Slotted-ALOHA variant on the top of the Pure-ALOHA approach used by the standard; thus, no modification in pre-existing libraries is necessary. Our method is based on an innovative synchronization service that is suitable for low-cost wireless sensor nodes. We modelled the LoRaWAN channel with extensive measurement on hardware platforms, and we quantified the impact of tuning parameters on physical and medium access control layers, as well as the packet collision rate. Results show that Slotted-ALOHA supported by our synchronization service significantly improves the performance of traditional LoRaWAN networks regarding packet loss rate and network throughput.",0.5294981416],["smart contracts handle and transfer assets of values, offering strong incentives for malicious attacks. contractGu","ContractGuard: Defend Ethereum Smart Contracts with Embedded Intrusion Detection","summarize: Ethereum smart contracts are programs that can be collectively executed by a network of mutually untrusted nodes. Smart contracts handle and transfer assets of values, offering strong incentives for malicious attacks. Intrusion attacks are a popular type of malicious attacks. In this paper, we propose ContractGuard, the first intrusion detection system to defend Ethereum smart contracts against such attacks. Like IDSs for conventional programs, ContractGuard detects intrusion attempts as abnormal control flow. However, existing IDS techniques\/tools are inapplicable to Ethereum smart contracts due to Ethereum's decentralized nature and its highly restrictive execution environment. To address these issues, we design ContractGuard by embedding it in the contracts to profile context-tagged acyclic paths, and optimizing it under the Ethereum gas-oriented performance model. The main goal is to minimize the overheads, to which the users will be extremely sensitive since the cost needs to be paid upfront in digital concurrency. Empirical investigation using real-life contracts deployed in the Ethereum mainnet shows that on average, ContractGuard only adds to 36.14% of the deployment overhead and 28.27% of the runtime overhead. Furthermore, we conducted controlled experiments and show that ContractGuard successfully guard against attacks on all real-world vulnerabilities and 83% of the seeded vulnerabilities.",0.0],["the CE is built from the Fourier spectrum of fluctuations around the mean-field. it","Information Dynamics at a Phase Transition","summarize: We propose a new way of investigating phase transitions in the context of information theory. We use an information-entropic measure of spatial complexity known as configurational entropy to quantify both the storage and exchange of information in a lattice simulation of a Ginzburg-Landau model with a scalar order parameter coupled to a heat bath. The CE is built from the Fourier spectrum of fluctuations around the mean-field and reaches a minimum at criticality. In particular, we investigate the behavior of CE near and at criticality, exploring the relation between information and the emergence of ordered domains. We show that as the temperature is increased from below, the CE displays three essential scaling regimes at different spatial scales: scale free, turbulent, and critical. Together, they offer an information-entropic characterization of critical behavior where the storage and processing of information is maximized at criticality.",0.05],["a new benchmark we call Sequence labellIng evaLuatIon","Hierarchical Pre-training for Sequence Labelling in Spoken Dialog","summarize: Sequence labelling tasks like Dialog Act and Emotion\/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE benchmark . \\texttt is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pre-training objectives. Pre-training is performed on OpenSubtitles: a large corpus of spoken dialog containing over ",0.4615384615],["a Gaussian mixture model is adopted, whose components correspond to all possible candidate source","Multiple-Speaker Localization Based on Direct-Path Features and Likelihood Maximization with Spatial Sparsity Regularization","summarize: This paper addresses the problem of multiple-speaker localization in noisy and reverberant environments, using binaural recordings of an acoustic scene. A Gaussian mixture model is adopted, whose components correspond to all the possible candidate source locations defined on a grid. After optimizing the GMM-based objective function, given an observed set of binaural features, both the number of sources and their locations are estimated by selecting the GMM components with the largest priors. This is achieved by enforcing a sparse solution, thus favoring a small number of speakers with respect to the large number of initial candidate source locations. An entropy-based penalty term is added to the likelihood, thus imposing sparsity over the set of GMM priors. In addition, the direct-path relative transfer function is used to build robust binaural features. The DP-RTF, recently proposed for single-source localization, was shown to be robust to reverberations, since it encodes inter-channel information corresponding to the direct-path of sound propagation. In this paper, we extend the DP-RTF estimation to the case of multiple sources. In the short-time Fourier transform domain, a consistency test is proposed to check whether a set of consecutive frames is associated to the same source or not. Reliable DP-RTF features are selected from the frames that pass the consistency test to be used for source localization. Experiments carried out using both simulation data and real data gathered with a robotic head confirm the efficiency of the proposed multi-source localization method.",0.35],["the truncation is determined by a random stopping rule that achieves an almost","Stochastic approximations to the Pitman-Yor process","summarize: In this paper we consider approximations to the popular Pitman-Yor process obtained by truncating the stick-breaking representation. The truncation is determined by a random stopping rule that achieves an almost sure control on the approximation error in total variation distance. We derive the asymptotic distribution of the random truncation point as the approximation error epsilon goes to zero in terms of a polynomially tilted positive stable distribution. The practical usefulness and effectiveness of this theoretical result is demonstrated by devising a sampling algorithm to approximate functionals of the epsilon-version of the Pitman-Yor process.",0.3],["a Discrete Boltzmann Model is roughly equivalent to a hydrodynamic model","Discrete Boltzmann Modeling of Compressible Flows","summarize: Mathematically, the typical difference of Discrete Boltzmann Model from the traditional hydrodynamic one is that the Navier-Stokes equations are replaced by a discrete Boltzmann equation. But physically, this replacement has a significant gain: a DBM is roughly equivalent to a hydrodynamic model supplemented by a coarse-grained model of the Thermodynamic Non-Equilibrium effects, where the hydrodynamic model can be and can also beyond the NS. Via the DBM, it is convenient to perform simulations on systems with flexible Knudsen number. The observations on TNE are being obtaining more applications with time.",0.3],["one-sided t-tests are commonly used in the neuroimaging field.","Reply to Chen et al.: Parametric methods for cluster inference perform worse for two-sided t-tests","summarize: One-sided t-tests are commonly used in the neuroimaging field, but two-sided tests should be the default unless a researcher has a strong reason for using a one-sided test. Here we extend our previous work on cluster false positive rates, which used one-sided tests, to two-sided tests. Briefly, we found that parametric methods perform worse for two-sided t-tests, and that non-parametric methods perform equally well for one-sided and two-sided tests.",0.1026834238],["a plasma state has been found in the presence of a uniform applied axial magnetic field","Electrostatically driven helical plasma state","summarize: A novel plasma state has been found in the presence of a uniform applied axial magnetic field in periodic cylindrical geometry. This state is driven electrostatically by helical electrodes, providing a driving field that depends on radius and ",0.4117647059],["the original UKF propagates multiple sigma points to compute the a priori mean","A Novel a priori State Computation Strategy for the Unscented Kalman Filter to Improve Computational Efficiency","summarize: A priori state vector and error covariance computation for the Unscented Kalman Filter is described. The original UKF propagates multiple sigma points to compute the a priori mean state vector and the error covariance, resulting in a higher computational time compared to the Extended Kalman Filter . In the proposed method, the posterior mean state vector is propagated and then the sigma points at the current time step are calculated using the first-order Taylor Series approximation. This reduces the computation time significantly, as demonstrated using two example applications which show improvements of 90.5% and 92.6%. This method shows the estimated state vector and the error covariance are accurate to the first-order Taylor series terms. A second method using Richardson Extrapolation improves prediction accuracy to the second-order Taylor series terms. This is implemented on the two examples, improving efficiency by 85.5% and 86.8%.",0.3969613289],["ionization is a cosmology reionization. it is","Spectroscopic confirmation of an ultra-faint galaxy at the epoch of reionization","summarize: Within one billion years of the Big Bang, intergalactic hydrogen was ionized by sources emitting ultraviolet and higher energy photons. This was the final phenomenon to globally affect all the baryons in the Universe. It is referred to as cosmic reionization and is an integral component of cosmology. It is broadly expected that intrinsically faint galaxies were the primary ionizing sources due to their abundance in this epoch. However, at the highest redshifts , all galaxies with spectroscopic confirmations to date are intrinsically bright and, therefore, not necessarily representative of the general population. Here, we report the unequivocal spectroscopic detection of a low luminosity galaxy at ",0.188239374],["conductor formula gives the geometric case of a formula conjectured by Bloch","Characteristic cycles and the conductor of direct image","summarize: We prove the functoriality for proper push-forward of the characteristic cycles of constructible complexes by morphisms of smooth projective schemes over a perfect field, under the assumption that the direct image of the singular support has the dimension at most that of the target of the morphism. The functoriality is deduced from a conductor formula which is a special case for morphisms to curves. The conductor formula in the constant coefficient case gives the geometric case of a formula conjectured by Bloch.",0.4117647059],["polarimetric analysis of white-light images makes use of polarized sequences","Coronal Photopolarimetry with the LASCO-C2 Coronagraph over 24 Years -- Application to the K\/F Separation and to the Determination of the Electron Density","summarize: We present an in-depth characterization of the polarimetric channel of the Large-Angle Spectrometric COronagraph LASCO-C2 onboard the Solar and Heliospheric Observatory . The polarimetric analysis of the white-light images makes use of polarized sequences composed of three images obtained though three polarizers oriented at +60, 0 and -60, complemented by a neighboring unpolarized image, and relies on the formalism of Mueller. The Mueller matrix characterizing the C2 instrument was obtained through extensive ground-based calibrations of the optical components and global laboratory tests. Additional critical corrections were derived from in-flight tests relying prominently on roll sequences and on consistency criteria, mainly the tangential direction of polarization. Our final results encompass the characterization of the polarization of the white-light corona, of its polarized radiance, of the two-dimensional electron density, and of the K-corona over two solar cycles. They are in excellent agreement with measurements obtained at several solar eclipses except for slight discrepancies affecting the innermost part of the C2 field-of-view, probably resulting from an imperfect removal of the bright diffraction fringe surrounding the occulter.",0.0454219655],["this paper provides a study on the synchronization aspect of star connected star connected star connected","A Study on the Synchronization Aspect of Star Connected Identical Chua Circuits","summarize: This paper provides a study on the synchronization aspect of star connected ",0.2564102564],["a work with some ambiguity engages a viewer more than one that does not","Toward Quantifying Ambiguities in Artistic Images","summarize: It has long been hypothesized that perceptual ambiguities play an important role in aesthetic experience: a work with some ambiguity engages a viewer more than one that does not. However, current frameworks for testing this theory are limited by the availability of stimuli and data collection methods. This paper presents an approach to measuring the perceptual ambiguity of a collection of images. Crowdworkers are asked to describe image content, after different viewing durations. Experiments are performed using images created with Generative Adversarial Networks, using the Artbreeder website. We show that text processing of viewer responses can provide a fine-grained way to measure and describe image ambiguities.",0.25],["root cause analysis in large-scale production environment is challenging due to complexity of services running across global","Fast Dimensional Analysis for Root Cause Investigation in a Large-Scale Service Environment","summarize: Root cause analysis in a large-scale production environment is challenging due to the complexity of services running across global data centers. Due to the distributed nature of a large-scale system, the various hardware, software, and tooling logs are often maintained separately, making it difficult to review the logs jointly for understanding production issues. Another challenge in reviewing the logs for identifying issues is the scale - there could easily be millions of entities, each described by hundreds of features. In this paper we present a fast dimensional analysis framework that automates the root cause analysis on structured logs with improved scalability. We first explore item-sets, i.e. combinations of feature values, that could identify groups of samples with sufficient support for the target failures using the Apriori algorithm and a subsequent improvement, FP-Growth. These algorithms were designed for frequent item-set mining and association rule learning over transactional databases. After applying them on structured logs, we select the item-sets that are most unique to the target failures based on lift. We propose pre-processing steps with the use of a large-scale real-time database and post-processing techniques and parallelism to further speed up the analysis and improve interpretability, and demonstrate that such optimization is necessary for handling large-scale production datasets. We have successfully rolled out this approach for root cause investigation purposes in a large-scale infrastructure. We also present the setup and results from multiple production use cases in this paper.",0.1],["deep models are state-of-the-art for many vision tasks including video action recognition and","Excitation Backprop for RNNs","summarize: Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification\/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing\/training for these tasks.",0.0769230769],["new model is called VGG Data STN with CNN. the new model is applied to","Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images","summarize: Lung disease is common throughout the world. These include chronic obstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc. Timely diagnosis of lung disease is essential. Many image processing and machine learning models have been developed for this purpose. Different forms of existing deep learning techniques including convolutional neural network , vanilla neural network, visual geometry group based neural network , and capsule network are applied for lung disease prediction.The basic CNN has poor performance for rotated, tilted, or other abnormal image orientation. Therefore, we propose a new hybrid deep learning framework by combining VGG, data augmentation and spatial transformer network with CNN. This new hybrid method is termed here as VGG Data STN with CNN . As implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The new model is applied to NIH chest X-ray image dataset collected from Kaggle repository. Full and sample versions of the dataset are considered. For both full and sample datasets, VDSNet outperforms existing methods in terms of a number of metrics including precision, recall, F0.5 score and validation accuracy. For the case of full dataset, VDSNet exhibits a validation accuracy of 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified capsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%, respectively. When sample dataset rather than full dataset is used, VDSNet requires much lower training time at the expense of a slightly lower validation accuracy. Hence, the proposed VDSNet framework will simplify the detection of lung disease for experts as well as for doctors.",0.0952380952],["miquel dynamics relies on the six circles theorem of the square grid.","Miquel dynamics for circle patterns","summarize: We study a new discrete-time dynamical system on circle patterns with the combinatorics of the square grid. This dynamics, called Miquel dynamics, relies on Miquel's six circles theorem. We provide a coordinatization of the appropriate space of circle patterns on which the dynamics acts and use it to derive local recurrence formulas. Isoradial circle patterns arise as periodic points of Miquel dynamics. Furthermore, we prove that certain signed sums of intersection angles are preserved by the dynamics. Finally, when the initial circle pattern is spatially biperiodic with a fundamental domain of size two by two, we show that the appropriately normalized motion of intersection points of circles takes place along an explicit quartic curve.",0.0625],["we consider cooperative communications with energy harvesting relays. we aim at optimizing the long","Distributed Power Control for Delay Optimization in Energy Harvesting Cooperative Relay Networks","summarize: We consider cooperative communications with energy harvesting relays, and develop a distributed power control mechanism for the relaying terminals. Unlike prior art which mainly deal with single-relay systems with saturated traffic flow, we address the case of bursty data arrival at the source cooperatively forwarded by multiple half-duplex EH relays. We aim at optimizing the long-run average delay of the source packets under the energy neutrality constraint on power consumption of each relay. While EH relay systems have been predominantly optimized using either offline or online methodologies, we take on a more realistic learning-theoretic approach. Hence, our scheme can be deployed for real-time operation without assuming acausal information on channel realizations, data\/energy arrivals as required by offline optimization, nor does it rely on precise statistics of the system processes as is the case with online optimization. We formulate the problem as a partially observable identical payoff stochastic game with factored controllers, in which the power control policy of each relay is adaptive to its local source-to-relay\/relay-to-destination channel states, its local energy state as well as to the source buffer state information. We derive a multi-agent reinforcement learning algorithm which is convergent to a locally optimal solution of the formulated PO-IPSG. The proposed algorithm operates without explicit message exchange between the relays, while inducing only little source-relay signaling overhead. By simulation, we contrast the delay performance of the proposed method against existing heuristics for throughput maximization. It is shown that compared with these heuristics, the systematic approach adopted in this paper has a smaller sub-optimality gap once evaluated against a centralized optimal policy armed with perfect statistics.",0.2222222222],["nitrogen-vacancy center in diamond has a charge state dynamics. the charge state dynamics of","Charge State Dynamics During Excitation and Depletion of the Nitrogen Vacancy Center in Diamond","summarize: The charge state dynamics of the nitrogen-vacancy center in diamond play a key role in a wide range of applications, yet remain imperfectly understood. Using single ps-pulses and pulse pairs, we quantitatively investigate the charge dynamics associated with excitation and fluorescence depletion of a single NV center. Our pulsed excitation approach permits significant modeling simplifications, and allows us to extract relative rates of excitation, stimulated emission, ionization, and recombination under 531 nm and 766 nm illumination. By varying the duration between paired pulses, we can also investigate ionization and recombination out of metastable states. Our results are directly applicable to experiments employing stimulated emission-depletion imaging, and can be used to predict optimal operating regimes where excitation and stimulated emission are maximized relative to charge-state-switching processes.",0.3666666667],["quantum computer architecture requires millions of qubits. a 130 nm siG","30 GHz-voltage controlled oscillator operating at 4 K","summarize: Solid-state qubit manipulation and read-out fidelities are reaching fault-tolerance, but quantum error correction requires millions of physical qubits and thus a scalable quantum computer architecture. To solve signal-line bandwidth and fan-out problems, microwave sources required for qubit manipulation might be embedded close to the qubit chip, typically operating at temperatures below 4 K. Here, we perform the first low temperature measurements of a 130 nm BiCMOS based SiGe voltage controlled oscillator. The device maintains its functionality from 300 K to 4 K. We determined the dependence of frequency and output power on temperature and magnetic field up to 5 T and measured the temperature influence on noise performance. While the output power tends to increase, the frequency shift is 3 % for temperature and 0.02 % for the field dependence, respectively, both relevant for highly coherent spin qubit applications. We observe no improvement on output noise, but increased output flickering.",0.2307692308],["a number of works have studied different aspects of drone package delivery service by a supplier","Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach","summarize: Unmanned aerial vehicles , also known as drones, have emerged as a promising mode of fast, energy-efficient, and cost-effective package delivery. A considerable number of works have studied different aspects of drone package delivery service by a supplier, one of which is delivery planning. However, existing works addressing the planning issues consider a simple case of perfect delivery without service interruption, e.g., due to accident which is common and realistic. Therefore, this paper introduces the joint ground and aerial delivery service optimization and planning framework. The framework explicitly incorporates uncertainty of drone package delivery, i.e., takeoff and breakdown conditions. The GADOP framework aims to minimize the total delivery cost given practical constraints, e.g., traveling distance limit. Specifically, we formulate the GADOP framework as a three-stage stochastic integer programming model. To deal with the high complexity issue of the problem, a decomposition method is adopted. Then, the performance of the GADOP framework is evaluated by using two data sets including Solomon benchmark suite and the real data from one of the Singapore logistics companies. The performance evaluation clearly shows that the GADOP framework can achieve significantly lower total payment than that of the baseline methods which do not take uncertainty into account.",0.4285714286],["a stochastic geometry framework has been used to extract the results. first,","Energy Efficiency Optimization for Device-to-Device Communication Underlaying Cellular Networks in Millimeter-Wave","summarize: This paper studies energy efficiency maximization in device-to-device communications underlaying cellular networks in millimeter-wave band. A stochastic geometry framework has been used to extract the results. First, cellular and D2D users are modeled by independent homogeneous Poisson point process; then, exact expressions for successful transmission probability of D2D and cellular users have been derived. Furthermore, the average sum rate and energy efficiency for a typical D2D scenario have been presented. An optimization problem subject to transmission power and quality of service constraints for both cellular and D2D users has been defined and energy efficiency of D2D communication is maximized. Simulation results reveal that by working in millimeter-wave, significant energy efficiency improvement can be attained, e.g., 20\\% energy efficiency improvement compared to Rayleigh distribution in the practical scenarios by considering circuit power. Finally, to verify our analytical expressions, the simulation studies are carried out and the excellent agreements have been achieved.",0.3333333333],["polaritons can propagate along anisotropic metasurfaces with either hyperbol","Collective near-field coupling in infrared-phononic metasurfaces for nano-light canalization","summarize: Polaritons, coupled excitations of photons and dipolar matter excitations, can propagate along anisotropic metasurfaces with either hyperbolic or elliptical dispersion. At the transition from hyperbolic to elliptical dispersion , various intriguing phenomena are found, such as an enhancement of the photonic density of states, polariton canalization and hyperlensing. Here we investigate theoretically and experimentally the topological transition and the polaritonic coupling of deeply subwavelength elements in a uniaxial infrared-phononic metasurface, a grating of hexagonal boron nitride nanoribbons. By hyperspectral infrared nanoimaging, we observe, for the first time, a synthetic transverse optical phonon resonance in the middle of the hBN Reststrahlen band, yielding a topological transition from hyperbolic to elliptical dispersion. We further visualize and characterize the spatial evolution of a deeply subwavelength canalization mode near the transition frequency, which is a collimated polariton that is the basis for hyperlensing and diffraction-less propagation. Our results provide fundamental insights into the role of polaritonic near-field coupling in metasurfaces for creating topological transitions and polariton canalization.",0.2222222222],["a substantial percentage of hard X-ray sources discovered with the BAT instrument onboard","Optical spectroscopic classification of 35 hard X-ray sources from the Swift-BAT 70-month catalogue","summarize: The nature of a substantial percentage of hard X-ray sources discovered with the BAT instrument onboard the Neil Gehrels Swift Observatory is unknown because of the lack of an identified longer-wavelength counterpart. Without such follow-up, an X-ray catalogue is of limited astrophysical value: we therefore embarked, since 2009, on a long-term project to uncover the optical properties of sources identified by Swift by using a large suite of ground-based telescopes and instruments. In this work, we continue our programme of characterization of unidentified or poorly studied hard X-ray sources by presenting the results of an optical spectroscopic campaign aimed at pinpointing and classifying the optical counterparts of 35 hard X-ray sources taken from the 70-month BAT catalogue. With the use of optical spectra taken at six different telescopes we were able to identify the main spectral characteristics of the observed objects, and determined their nature. We identify and characterize a total of 41 optical candidate counterparts corresponding to 35 hard X-ray sources given that, because of positional uncertainties, multiple lower energy counterparts can sometimes be associated with higher energy detections. We discuss which ones are the actual counterparts based on our observational results. In particular, 31 sources in our sample are active galactic nuclei: 16 are classified as Type 1 and 13 are classified as Type 2 ; two more are BL Lac-type objects. We also identify one LINER, one starburst, and 3 elliptical galaxies. The remaining 5 objects are galactic sources: we identify 4 of them as cataclysmic variables, whereas one is a low mass X-ray binary.",0.6842105263],["the circuit level design and implementation of backpropagation learning circuits is an open problem","Learning in Memristive Neural Network Architectures using Analog Backpropagation Circuits","summarize: The on-chip implementation of learning algorithms would speed-up the training of neural networks in crossbar arrays. The circuit level design and implementation of backpropagation algorithm using gradient descent operation for neural network architectures is an open problem. In this paper, we proposed the analog backpropagation learning circuits for various memristive learning architectures, such as Deep Neural Network , Binary Neural Network , Multiple Neural Network , Hierarchical Temporal Memory and Long-Short Term Memory . The circuit design and verification is done using TSMC 180nm CMOS process models, and TiO2 based memristor models. The application level validations of the system are done using XOR problem, MNIST character and Yale face image databases",0.0625],["time-fractional phase field models admit energy dissipation law of integral type","On energy dissipation theory and numerical stability for time-fractional phase field equations","summarize: For the time-fractional phase field models, the corresponding energy dissipation law has not been settled on both the continuous level and the discrete level. In this work, we shall address this open issue. More precisely, we prove for the first time that the time-fractional phase field models indeed admit an energy dissipation law of an integral type. In the discrete level, we propose a class of finite difference schemes that can inherit the theoretical energy stability. Our discussion covers the time-fractional gradient systems, including the time-fractional Allen-Cahn equation, the time-fractional Cahn-Hilliard equation, and the time-fractional molecular beam epitaxy models. Numerical examples are presented to confirm the theoretical results. Moreover, a numerical study of the coarsening rate of random initial states depending on the fractional parameter ",0.4150457801],["equivalence analysis shows that our previous solver can be converted to deal with the","Fast Symbolic 3D Registration Solution","summarize: 3D registration has always been performed invoking singular value decomposition or eigenvalue decomposition in real engineering practices. However, these numerical algorithms suffer from uncertainty of convergence in many cases. A novel fast symbolic solution is proposed in this paper by following our recent publication in this journal. The equivalence analysis shows that our previous solver can be converted to deal with the 3D registration problem. Rather, the computation procedure is studied for further simplification of computing without complex-number support. Experimental results show that the proposed solver does not loose accuracy and robustness but improves the execution speed to a large extent by almost %50 to %80, on both personal computer and embedded processor.",0.0],["the interaction is given by a joint unitary operator acting on the system and the anci","Ancilla models for quantum operations: For what unitaries does the ancilla state have to be physical?","summarize: Any evolution described by a completely positive trace-preserving linear map can be imagined as arising from the interaction of the evolving system with an initially uncorrelated ancilla. The interaction is given by a joint unitary operator, acting on the system and the ancilla. Here we study the properties such a unitary operator must have in order to force the choice of a physical- that is, positive-state for the ancilla if the end result is to be a physical-that is, completely positive-evolution of the system.",0.4137931034],["water refraction poses significant challenges on depth determination. a proposed approach was developed to","Shallow Water Bathymetry Mapping from UAV Imagery based on Machine Learning","summarize: The determination of accurate bathymetric information is a key element for near offshore activities, hydrological studies such as coastal engineering applications, sedimentary processes, hydrographic surveying as well as archaeological mapping and biological research. UAV imagery processed with Structure from Motion and Multi View Stereo techniques can provide a low-cost alternative to established shallow seabed mapping techniques offering as well the important visual information. Nevertheless, water refraction poses significant challenges on depth determination. Till now, this problem has been addressed through customized image-based refraction correction algorithms or by modifying the collinearity equation. In this paper, in order to overcome the water refraction errors, we employ machine learning tools that are able to learn the systematic underestimation of the estimated depths. In the proposed approach, based on known depth observations from bathymetric LiDAR surveys, an SVR model was developed able to estimate more accurately the real depths of point clouds derived from SfM-MVS procedures. Experimental results over two test sites along with the performed quantitative validation indicated the high potential of the developed approach.",0.375],["local decomposition methods are used to predict steady turbulent aerodynamic fields. the local de","Surrogate Modeling of Aerodynamic Simulations for Multiple Operating Conditions Using Machine Learning","summarize: This article presents an original methodology for the prediction of steady turbulent aerodynamic fields. Due to the important computational cost of high-fidelity aerodynamic simulations, a surrogate model is employed to cope with the significant variations of several inflow conditions. Specifically, the Local Decomposition Method presented in this paper has been derived to capture nonlinear behaviors resulting from the presence of continuous and discontinuous signals. A combination of unsupervised and supervised learning algorithms is coupled with a physical criterion. It decomposes automatically the input parameter space, from a limited number of high-fidelity simulations, into subspaces. These latter correspond to different flow regimes. A measure of entropy identifies the subspace with the expected strongest non-linear behavior allowing to perform an active resampling on this low-dimensional structure. Local reduced-order models are built on each subspace using Proper Orthogonal Decomposition coupled with a multivariate interpolation tool. The methodology is assessed on the turbulent two-dimensional flow around the RAE2822 transonic airfoil. It exhibits a significant improvement in term of prediction accuracy for the Local Decomposition Method compared with the classical method of surrogate modeling for cases with different flow regimes.",0.0588235294],["spin-wave directional coupler comprises two laterally parallel nano-scale dipolarly-","Reconfigurable nano-scale spin-wave directional coupler","summarize: A spin-wave directional coupler comprised of two laterally parallel nano-scale dipolarly-coupled SW waveguides is proposed and studied using micromagnetic simulations and analytical theory. The energy of a SW excited in one of the waveguides in the course of propagation is periodically transferred to the other waveguide and back, and the spatial half-period of this transfer is defined as the coupling length. The coupling length is determined by the dipolar coupling between the waveguides, and the fraction of the SW energy transferred to the other waveguide at the device output can be varied with the SW frequency, bias magnetic field, and relative orientation of the waveguide's static magnetizations. The proposed design of a directional coupler can be used in digital computing-oriented magnonics as a connector of magnonic conduits without a direct contact, or in the analog microwave signal processing as a reconfigurable nano-scale power divider and\/or frequency separator.",0.4444444444],["in this paper we study the effect that the external management of a limited resource has on the","On the effects of a common-pool resource on cooperation among firms with linear technologies","summarize: In this paper we study the effect that the external management of a limited resource such as carbon dioxide or water quotas has on the behaviour of firms in a given sector. To do this, we choose a model in which all firms have the same technology and this is lineal. In the analysis of the problem games in partition function form arise in a natural way. It is proved, under certain conditions, that stable allocations exist in both cases with certainty and uncertainty.",0.4],["medical concept coding is to map a variable length text to medical concepts. we use","An Encoder-Decoder Model for ICD-10 Coding of Death Certificates","summarize: Information extraction from textual documents such as hospital records and healthrelated user discussions has become a topic of intense interest. The task of medical concept coding is to map a variable length text to medical concepts and corresponding classification codes in some external system or ontology. In this work, we utilize recurrent neural networks to automatically assign ICD-10 codes to fragments of death certificates written in English. We develop end-to-end neural architectures directly tailored to the task, including basic encoder-decoder architecture for statistical translation. In order to incorporate prior knowledge, we concatenate cosine similarities vector among the text and dictionary entry to the encoded state. Being applied to a standard benchmark from CLEF eHealth 2017 challenge, our model achieved F-measure of 85.01% on a full test set with significant improvement as compared to the average score of 62.2% for all official participants approaches.",0.08],["algorithmic grouping of participants and eye-tracking metrics derived from recorded eye-t","Visual Multi-Metric Grouping of Eye-Tracking Data","summarize: We present an algorithmic and visual grouping of participants and eye-tracking metrics derived from recorded eye-tracking data. Our method utilizes two well-established visualization concepts. First, parallel coordinates are used to provide an overview of the used metrics, their interactions, and similarities, which helps select suitable metrics that describe characteristics of the eye-tracking data. Furthermore, parallel coordinates plots enable an analyst to test the effects of creating a combination of a subset of metrics resulting in a newly derived eye-tracking metric. Second, a similarity matrix visualization is used to visually represent the affine combination of metrics utilizing an algorithmic grouping of subjects that leads to distinct visual groups of similar behavior. To keep the diagrams of the matrix visualization simple and understandable, we visually encode our eye-tracking data into the cells of a similarity matrix of participants. The algorithmic grouping is performed with a clustering based on the affine combination of metrics, which is also the basis for the similarity value computation of the similarity matrix. To illustrate the usefulness of our visualization, we applied it to an eye-tracking data set involving the reading behavior of metro maps of up to 40 participants. Finally, we discuss limitations and scalability issues of the approach focusing on visual and perceptual issues.",0.0833333333],["reversible computing models settings in which all processes can be reversed. it is","Reversible effects as inverse arrows","summarize: Reversible computing models settings in which all processes can be reversed. Applications include low-power computing, quantum computing, and robotics. It is unclear how to represent side-effects in this setting, because conventional methods need not respect reversibility. We model reversible effects by adapting Hughes' arrows to dagger arrows and inverse arrows. This captures several fundamental reversible effects, including serialization and mutable store computations. Whereas arrows are monoids in the category of profunctors, dagger arrows are involutive monoids in the category of profunctors, and inverse arrows satisfy certain additional properties. These semantics inform the design of functional reversible programs supporting side-effects.",0.0666666667],["we tune the wave dispersion and the level of nonlinearity by modifying the","Transition from Weak Wave Turbulence to Soliton-Gas","summarize: We report an experimental investigation of the effect of finite depth on the statistical properties of wave turbulence at the surface of water in the gravity-capillary range. We tune the wave dispersion and the level of nonlinearity by modifying the depth of water and the forcing respectively. We use space-time resolved profilometry to reconstruct the deformed surface of water. When decreasing the water depth, we observe a drastic transition between weak turbulence at the weakest forcing and a solitonic regime at stronger forcing. We characterize the transition between both states by studying their Fourier Spectra. We also study the efficiency of energy transfer in the weak turbulence regime. We report a loss of efficiency of angular transfer as the dispersion of the wave is reduced until the system bifurcates into the solitonic regime.",0.0],["supernova type Ia was first solved, multiply imaged 4 years ago. i","Two-component mass models of the lensing galaxy in the quadruply imaged supernova iPTF16geu","summarize: The first resolved, multiply imaged supernova Type Ia, iPTF16geu, was observed 4 years ago, five decades after such systems were first envisioned. Because of the unique properties of the source, these systems hold a lot of promise for the study of galaxy structure and cosmological parameters. However, this very first example presented modelers with a few puzzles. It was expected that to explain image fluxes a contribution from microlensing by stars would be required, but to accommodate the magnitude of microlensing, the density slope of the elliptical power law lens model had to be quite shallow, ",0.3680177659],["layered semiconductors are a new class of layered semiconductors. they feature","Raman spectroscopy of GaSe and InSe post-transition metal chalcogenides layers","summarize: III-VI post-transition metal chalcogenides are a new class of layered semiconductors, which feature a strong variation of size and type of their band gaps as a function of number of layers . Here, we investigate exfoliated layers of InSe and GaSe ranging from bulk crystals down to monolayer, encapsulated in hexagonal boron nitride, using Raman spectroscopy. We present the N-dependence of both intralayer vibrations within each atomic layer, as well as of the interlayer shear and layer breathing modes. A linear chain model can be used to describe the evolution of the peak positions as a function of N, consistent with first principles calculations.",0.35],["a simple theory for the green photoluminescence of ZnO quantum dots allows us","Quantum-Size Effects in the Visible Photoluminescence of Colloidal ZnO Quantum Dots: A Theoretical Analysis","summarize: In this work we develop a simple theory for the green photoluminescence of ZnO quantum dots that allows us to understand and rationalize several experimental findings on fundamental grounds. We calculate the spectrum of light emitted in the radiative recombination of a conduction band electron with a deeply trapped hole and find that the experimental behavior of this emission band with particle size can be understood in terms of quantum size effects of the electronic states and their overlap with the deep hole.We focus the comparison of our results on detailed experiments performed for colloidal ZnO nanoparticles in ethanol and find that the experimental evolution of the luminescent signal with particle sizeat room temperature can be better reproduced by assuming the deep hole to be localized at the surface of the nanoparticles. However, the experimental behavior of the intensity and decay time of the signal with temperature can be rationalized in terms of holes predominantly trapped near the center of the nanoparticles at low temperatures being transferred to surface defects at room temperature. Furthermore, the calculated values of the radiative lifetimes are comparable to the experimental values of the decay time of the visible emission signal.We also study the visible emission band as a function of the number of electrons in the conduction band of the nanoparticle, finding a pronounced dependence of the radiative lifetime but a weak dependence of energetic position of the maximum intensity.",0.3472354045],["Miles et al. argue that the lockdown costed more than the benefit","Counting the costs of COVID-19: why future treatment option values matter","summarize: I critique a recent analysis of COVID-19 lockdown costs and benefits, focussing on the United Kingdom . Miles et al. argue that the March-June UK lockdown was more costly than the benefit of lives saved, evaluated using the NICE threshold of 30000 for a quality-adjusted life year and that the costs of a lockdown for 13 weeks from mid-June would be vastly greater than any plausible estimate of the benefits, even if easing produced a second infection wave causing over 7000 deaths weekly by mid-September. I note here two key problems that significantly affect their estimates and cast doubt on their conclusions. Firstly, their calculations arbitrarily cut off after 13 weeks, without costing the epidemic end state. That is, they assume indifference between mid-September states of 13 or 7500 weekly deaths and corresponding infection rates. This seems indefensible unless one assumes that there is little chance of any effective vaccine or improved medical or social interventions for the foreseeable future, notwithstanding temporary lockdowns, COVID-19 will very likely propagate until herd immunity. Even under these assumptions it is very questionable. Secondly, they ignore the costs of serious illness, possible long-term lowering of life quality and expectancy for survivors. These are uncertain, but plausibly at least as large as the costs in deaths. In summary, policy on tackling COVID-19 cannot be rationally made without estimating probabilities of future medical interventions and long-term illness costs. More work on modelling these uncertainties is urgently needed.",0.2142857143],["genus genus genus genus genus genus","Algorithms to enumerate superspecial Howe curves of genus 4","summarize: A Howe curve is a curve of genus ",0.1010884433],["a group of experts met in Switzerland to discuss where our discipline stands today. a","Supporting Requirements Engineering Research that Industry Needs: The Naming the Pain in Requirements Engineering Initiative","summarize: In light of the 40th jubilee of Requirements Engineering , roughly 40 experts met in Switzerland to discuss where our discipline stands today. As of today, the common view is, indisputably, that RE as a discipline is stable and respected, as pointed out by Sarah Gregory when covering the seminar in her column to which articles like this one are invited to present ongoing research. However, it is also evident that after 40 years of promising research, conducting research that industry needs is still an ongoing challenge. Research that industry needs means research that solves industrial problems practitioners face; but do we really understand those problems? Here, I want to recapitulate on this research challenge and outline an initiative, the Naming the Pain in Requirements Engineering Initiative, that aims at tackling this problem.",0.48],["a new framework for multi-organ segmentation is used to address these challenges. the","Abdominal multi-organ segmentation with organ-attention networks and statistical fusion","summarize: Accurate and robust segmentation of abdominal organs on CT is essential for many clinical applications such as computer-aided diagnosis and computer-aided surgery. But this task is challenging due to the weak boundaries of organs, the complexity of the background, and the variable sizes of different organs. To address these challenges, we introduce a novel framework for multi-organ segmentation by using organ-attention networks with reverse connections which are applied to 2D views, of the 3D CT volume, and output estimates which are combined by statistical fusion exploiting structural similarity. OAN is a two-stage deep convolutional network, where deep network features from the first stage are combined with the original image, in a second stage, to reduce the complex background and enhance the discriminative information for the target organs. RCs are added to the first stage to give the lower layers semantic information thereby enabling them to adapt to the sizes of different organs. Our networks are trained on 2D views enabling us to use holistic information and allowing efficient computation. To compensate for the limited cross-sectional information of the original 3D volumetric CT, multi-sectional images are reconstructed from the three different 2D view directions. Then we combine the segmentation results from the different views using statistical fusion, with a novel term relating the structural similarity of the 2D views to the original 3D structure. To train the network and evaluate results, 13 structures were manually annotated by four human raters and confirmed by a senior expert on 236 normal cases. We tested our algorithm and computed Dice-Sorensen similarity coefficients and surface distances for evaluating our estimates of the 13 structures. Our experiments show that the proposed approach outperforms 2D- and 3D-patch based state-of-the-art methods.",0.4736842105],["a simulated dataset and large-scale chest x-ray dataset are used to model","Longitudinal detection of radiological abnormalities with time-modulated LSTM","summarize: Convolutional neural networks have been successfully employed in recent years for the detection of radiological abnormalities in medical images such as plain x-rays. To date, most studies use CNNs on individual examinations in isolation and discard previously available clinical information. In this study we set out to explore whether Long-Short-Term-Memory networks can be used to improve classification performance when modelling the entire sequence of radiographs that may be available for a given patient, including their reports. A limitation of traditional LSTMs, though, is that they implicitly assume equally-spaced observations, whereas the radiological exams are event-based, and therefore irregularly sampled. Using both a simulated dataset and a large-scale chest x-ray dataset, we demonstrate that a simple modification of the LSTM architecture, which explicitly takes into account the time lag between consecutive observations, can boost classification performance. Our empirical results demonstrate improved detection of commonly reported abnormalities on chest x-rays such as cardiomegaly, consolidation, pleural effusion and hiatus hernia.",0.25],["the re-ranking method uses representations of demographic groups computed using a label","Using Image Fairness Representations in Diversity-Based Re-ranking for Recommendations","summarize: The trade-off between relevance and fairness in personalized recommendations has been explored in recent works, with the goal of minimizing learned discrimination towards certain demographics while still producing relevant results. We present a fairness-aware variation of the Maximal Marginal Relevance re-ranking method which uses representations of demographic groups computed using a labeled dataset. This method is intended to incorporate fairness with respect to these demographic groups. We perform an experiment on a stock photo dataset and examine the trade-off between relevance and fairness against a well known baseline, MMR, by using human judgment to examine the results of the re-ranking when using different fractions of a labeled dataset, and by performing a quantitative analysis on the ranked results of a set of query images. We show that our proposed method can incorporate fairness in the ranked results while obtaining higher precision than the baseline, while our case study shows that even a limited amount of labeled data can be used to compute the representations to obtain fairness. This method can be used as a post-processing step for recommender systems and search.",0.3125],["dominant methods employ neural networks for encoding the input sentence. graph neural network","Investigating Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network","summarize: Targeted sentiment classification predicts the sentiment polarity on given target mentions in input texts. Dominant methods employ neural networks for encoding the input sentence and extracting relations between target mentions and their contexts. Recently, graph neural network has been investigated for integrating dependency syntax for the task, achieving the state-of-the-art results. However, existing methods do not consider dependency label information, which can be intuitively useful. To solve the problem, we investigate a novel relational graph attention network that integrates typed syntactic dependency information. Results on standard benchmarks show that our method can effectively leverage label information for improving targeted sentiment classification performances. Our final model significantly outperforms state-of-the-art syntax-based approaches.",0.0625],["the associated cost matrices of these transportation problems are of special structure. we present","Some Aspects on Solving Transportation Problem","summarize: In this paper, we consider a class of transportation problems which arises in sample surveys and other areas of statistics. The associated cost matrices of these transportation problems are of special structure. We observe that the optimality of North West corner solution holds for the general problem where cost component is replaced by a convex function. We revisit assignment problem and present a weighted version of K",0.0],["AFMT is a packet scheduling algorithm to achieve adaptive traffic splitting. it implements","An Adaptive Flow-Aware Packet Scheduling Algorithm for Multipath Tunnelling","summarize: This paper proposes AFMT, a packet scheduling algorithm to achieve adaptive flow-aware multipath tunnelling. AFMT has two unique properties. Firstly, it implements robust adaptive traffic splitting for the subtunnels. Secondly, it detects and schedules bursts of packets cohesively, a scheme that already enabled traffic splitting for load balancing with little to no packet reordering. Several NS-3 experiments over different network topologies show that AFMT successfully deals with changing path characteristics due to background traffic while increasing throughput and reliability.",0.2380952381],["cloud is unable to bring down network latencies to meet response time requirements.","EdgeLens: Deep Learning based Object Detection in Integrated IoT, Fog and Cloud Computing Environments","summarize: Data-intensive applications are growing at an increasing rate and there is a growing need to solve scalability and high-performance issues in them. By the advent of Cloud computing paradigm, it became possible to harness remote resources to build and deploy these applications. In recent years, new set of applications and services based on Internet of Things paradigm, require to process large amount of data in very less time. Among them surveillance and object detection have gained prime importance, but cloud is unable to bring down the network latencies to meet the response time requirements. This problem is solved by Fog computing which harnesses resources in the edge of the network along with remote cloud resources as required. However, there is still a lack of frameworks that are successfully able to integrate sophisticated software and applications, especially deep learning, with fog and cloud computing environments. In this work, we propose a framework to deploy deep learning-based applications in fog-cloud environments to harness edge and cloud resources to provide better service quality for such applications. Our proposed framework, called EdgeLens, adapts to the application or user requirements to provide high accuracy or low latency modes of services. We also tested the performance of the software in terms of accuracy, response time, jitter, network bandwidth and power consumption and show how EdgeLens adapts to different service requirements.",0.0],["indefinite number of passively receiving wireless units can synchronize to a single master clock","Scalable and Passive Wireless Network Clock Synchronization","summarize: Clock synchronization is ubiquitous in wireless systems for communication, sensing and control. In this paper we design a scalable system in which an indefinite number of passively receiving wireless units can synchronize to a single master clock at the level of discrete clock ticks. Accurate synchronization requires an estimate of the node positions. If such information is available the framework developed here takes position uncertainties into account. In the absence of such information we propose a mechanism which enables simultaneous synchronization and positioning. Furthermore we derive the Cramer-Rao bounds for the system which show that it enables synchronization accuracy at sub-nanosecond levels. Finally, we develop and evaluate an online estimation method which is statistically efficient.",0.2352941176],["magnetic material applying a temperature gradient generates a voltage perpendicular to both the","Anomalous Nernst effect beyond the magnetization scaling relation in the ferromagnetic Heusler compound Co","summarize: Applying a temperature gradient in a magnetic material generates a voltage that is perpendicular to both the heat flow and the magnetization. This is the anomalous Nernst effect which was thought to be proportional to the value of the magnetization for a long time. However, more generally, the ANE has been predicted to originate from a net Berry curvature of all bands near the Fermi level. Subsequently, a large anomalous Nernst thermopower has recently been observed in topological materials with no net magnetization but large net Berry curvature around E",0.3928319728],["in this paper we study the effect that the external management of a limited resource has on the","On the effects of a common-pool resource on cooperation among firms with linear technologies","summarize: In this paper we study the effect that the external management of a limited resource such as carbon dioxide or water quotas has on the behaviour of firms in a given sector. To do this, we choose a model in which all firms have the same technology and this is lineal. In the analysis of the problem games in partition function form arise in a natural way. It is proved, under certain conditions, that stable allocations exist in both cases with certainty and uncertainty.",0.4],["customer lifetime value prediction system deployed at ASOS.com. system provides daily estimates of","Customer Lifetime Value Prediction Using Embeddings","summarize: We describe the Customer LifeTime Value prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We detail the system deployed at ASOS and show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features.",0.0],["proposed safe policy maximizes probability of disturbances remaining in desired set. a system remains","A dynamic game approach to distributionally robust safety specifications for stochastic systems","summarize: This paper presents a new safety specification method that is robust against errors in the probability distribution of disturbances. Our proposed distributionally robust safe policy maximizes the probability of a system remaining in a desired set for all times, subject to the worst possible disturbance distribution in an ambiguity set. We propose a dynamic game formulation of constructing such policies and identify conditions under which a non-randomized Markov policy is optimal. Based on this existence result, we develop a practical design approach to safety-oriented stochastic controllers with limited information about disturbance distributions. This control method can be used to minimize another cost function while ensuring safety in a probabilistic way. However, an associated Bellman equation involves infinite-dimensional minimax optimization problems since the disturbance distribution may have a continuous density. To resolve computational issues, we propose a duality-based reformulation method that converts the infinite-dimensional minimax problem into a semi-infinite program that can be solved using existing convergent algorithms. We prove that there is no duality gap, and that this approach thus preserves optimality. The results of numerical tests confirm that the proposed method is robust against distributional errors in disturbances, while a standard stochastic safety specification tool is not.",0.3913043478],["a cylindrical specimen and a cylindrical container are analyzed for three boundary conditions. the","Analytical Solution of a Gas Release Problem Considering Permeation with Time-Dependent Boundary Conditions","summarize: In this paper the determination of material properties such as Sieverts' constant and diffusivity via so-called gas release experiments is discussed. In order to simulate the time-dependent hydrogen fluxes and concentration profiles efficiently, we make use of an analytical method, namely we provide an analytical solution for the corresponding diffusion equations on a cylindrical specimen and a cylindrical container for three boundary conditions. These conditions occur in three phases -- loading phase, evacuation phase and gas release phase. In the loading phase the specimen is charged with hydrogen assuring a constant partial pressure of hydrogen. Then the gas will be quickly removed by a vacuum pump in the second phase, and finally in the third time interval, the hydrogen is released from the specimen to the gaseous phase, where the pressure increase will be measured by an equipment which is attached to the cylindrical container. The investigated diffusion equation in each phase is a simple homogeneous equation, but due to the complex time-dependent boundary conditions which include the Sieverts' constant and the pressure, we transform the homogeneous equations to the non-homogeneous ones with a zero Dirichlet boundary condition. Compared with the time consuming numerical methods our analytical approach has an advantage that the flux of desorbed hydrogen can be explicitly given and therefore can be evaluated efficiently. Our analytical solution also assures that the time-dependent boundary conditions are exactly satisfied and furthermore that the interaction between specimen and container is correctly taken into account.",0.4117647059],["gradient discretisation method investigated optimal control problems. gradient schemes are defined for the optimality system","The gradient discretisation method for optimal control problems, with super-convergence for non-conforming finite elements and mixed-hybrid mimetic finite differences","summarize: In this paper, optimal control problems governed by diffusion equations with Dirichlet and Neumann boundary conditions are investigated in the framework of the gradient discretisation method. Gradient schemes are defined for the optimality system of the control problem. Error estimates for state, adjoint and control variables are derived. Superconvergence results for gradient schemes under realistic regularity assumptions on the exact solution is discussed. These super-convergence results are shown to apply to non-conforming ",0.3404125948],["Lie algebras were introduced by Kaplan as a class of real Lie algebras","On complex H-type Lie algebras","summarize: H-type Lie algebras were introduced by Kaplan as a class of real Lie algebras generalizing the familiar Heisenberg Lie algebra ",0.2413793103],["in this paper, we present a novel approach to perform deep neural networks layer-wise weight","Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks","summarize: In this paper, we present a novel approach to perform deep neural networks layer-wise weight initialization using Linear Discriminant Analysis . Typically, the weights of a deep neural network are initialized with: random values, greedy layer-wise pre-training or by re-using the layers from another network . Hence, many training epochs are needed before meaningful weights are learned, or a rather similar dataset is required for seeding a fine-tuning of transfer learning. In this paper, we describe how to turn an LDA into either a neural layer or a classification layer. We analyze the initialization technique on historical documents. First, we show that an LDA-based initialization is quick and leads to a very stable initialization. Furthermore, for the task of layout analysis at pixel level, we investigate the effectiveness of LDA-based initialization and show that it outperforms state-of-the-art random weight initialization methods.",0.2857142857],["the unique information measure quantifies a deviation from the blackwell order. the quantity is","Unique Information and Secret Key Decompositions","summarize: The unique information is an information measure that quantifies a deviation from the Blackwell order. We have recently shown that this quantity is an upper bound on the one-way secret key rate. In this paper, we prove a triangle inequality for the ",0.0769230769],["parallel processes are actually interleaved according to some interleaving strategy.","Process algebra with strategic interleaving","summarize: In process algebras such as ACP , parallel processes are considered to be interleaved in an arbitrary way. In the case of multi-threading as found in contemporary programming languages, parallel processes are actually interleaved according to some interleaving strategy. An interleaving strategy is what is called a process-scheduling policy in the field of operating systems. In many systems, for instance hardware\/software systems, we have to do with both parallel processes that may best be considered to be interleaved in an arbitrary way and parallel processes that may best be considered to be interleaved according to some interleaving strategy. Therefore, we extend ACP in this paper with the latter form of interleaving. The established properties of the extension concerned include an elimination property, a conservative extension property, and a unique expansion property.",0.1],["federated learning is a synchronous learning strategy on the clients. it proposes","Communication-Efficient Federated Deep Learning with Asynchronous Model Update and Temporally Weighted Aggregation","summarize: Federated learning obtains a central model on the server by aggregating models trained locally on clients. As a result, federated learning does not require clients to upload their data to the server, thereby preserving the data privacy of the clients. One challenge in federated learning is to reduce the client-server communication since the end devices typically have very limited communication bandwidth. This paper presents an enhanced federated learning technique by proposing a synchronous learning strategy on the clients and a temporally weighted aggregation of the local models on the server. In the asynchronous learning strategy, different layers of the deep neural networks are categorized into shallow and deeps layers and the parameters of the deep layers are updated less frequently than those of the shallow layers. Furthermore, a temporally weighted aggregation strategy is introduced on the server to make use of the previously trained local models, thereby enhancing the accuracy and convergence of the central model. The proposed algorithm is empirically on two datasets with different deep neural networks. Our results demonstrate that the proposed asynchronous federated deep learning outperforms the baseline algorithm both in terms of communication cost and model accuracy.",0.4736842105],["TAGS increased the average False Accept Rate from 4% to 26%.","Treadmill Assisted Gait Spoofing : An Emerging Threat to wearable Sensor-based Gait Authentication","summarize: In this work, we examine the impact of Treadmill Assisted Gait Spoofing on Wearable Sensor-based Gait Authentication . We consider more realistic implementation and deployment scenarios than the previous study, which focused only on the accelerometer sensor and a fixed set of features. Specifically, we consider the situations in which the implementation of WSGait could be using one or more sensors embedded into modern smartphones. Besides, it could be using different sets of features or different classification algorithms, or both. Despite the use of a variety of sensors, feature sets , and six different classification algorithms, TAGS was able to increase the average False Accept Rate from 4% to 26%. Such a considerable increase in the average FAR, especially under the stringent implementation and deployment scenarios considered in this study, calls for a further investigation into the design of evaluations of WSGait before its deployment for public use.",0.1515914397],["the Wasserstein distance gives a natural measure of the distance between two distributions. it","Kernel Wasserstein Distance","summarize: The Wasserstein distance is a powerful metric based on the theory of optimal transport. It gives a natural measure of the distance between two distributions with a wide range of applications. In contrast to a number of the common divergences on distributions such as Kullback-Leibler or Jensen-Shannon, it is continuous, and thus ideal for analyzing corrupted data. To date, however, no kernel methods for dealing with nonlinear data have been proposed via the Wasserstein distance. In this work, we develop a novel method to compute the L2-Wasserstein distance in a kernel space implemented using the kernel trick. The latter is a general method in machine learning employed to handle data in a nonlinear manner. We evaluate the proposed approach in identifying computerized tomography slices with dental artifacts in head and neck cancer, performing unsupervised hierarchical clustering on the resulting Wasserstein distance matrix that is computed on imaging texture features extracted from each CT slice. Our experiments show that the kernel approach outperforms classical non-kernel approaches in identifying CT slices with artifacts.",0.125],["we describe a sequence of effective divisors on the Hurwitz space.","Syzygy divisors on Hurwitz spaces","summarize: We describe a sequence of effective divisors on the Hurwitz space ",0.4166666667],["binary fluid mixtures are examples of complex fluids whose microstructure and flow are strongly coupled","Theories of Binary Fluid Mixtures: From Phase-Separation Kinetics to Active Emulsions","summarize: Binary fluid mixtures are examples of complex fluids whose microstructure and flow are strongly coupled. For pairs of simple fluids, the microstructure consists of droplets or bicontinuous demixed domains and the physics is controlled by the interfaces between these domains. At continuum level, the structure is defined by a composition field whose gradients which are steep near interfaces drive its diffusive current. These gradients also cause thermodynamic stresses which can drive fluid flow. Fluid flow in turn advects the composition field, while thermal noise creates additional random fluxes that allow the system to explore its configuration space and move towards the Boltzmann distribution. This article introduces continuum models of binary fluids, first covering some well-studied areas such as the thermodynamics and kinetics of phase separation, and emulsion stability. We then address cases where one of the fluid components has anisotropic structure at mesoscopic scales creating nematic liquid-crystalline order; this can be described through an additional tensor order parameter field. We conclude by outlining a thriving area of current research, namely active emulsions, in which one of the binary components consists of living or synthetic material that is continuously converting chemical energy into mechanical work.",0.0555555556],["a new diversity preserving method is created based on the new concept of chromos","Spectrum-Diverse Neuroevolution with Unified Neural Models","summarize: Learning algorithms are being increasingly adopted in various applications. However, further expansion will require methods that work more automatically. To enable this level of automation, a more powerful solution representation is needed. However, by increasing the representation complexity a second problem arises. The search space becomes huge and therefore an associated scalable and efficient searching algorithm is also required. To solve both problems, first a powerful representation is proposed that unifies most of the neural networks features from the literature into one representation. Secondly, a new diversity preserving method called Spectrum Diversity is created based on the new concept of chromosome spectrum that creates a spectrum out of the characteristics and frequency of alleles in a chromosome. The combination of Spectrum Diversity with a unified neuron representation enables the algorithm to either surpass or equal NeuroEvolution of Augmenting Topologies on all of the five classes of problems tested. Ablation tests justifies the good results, showing the importance of added new features in the unified neuron representation. Part of the success is attributed to the novelty-focused evolution and good scalability with chromosome size provided by Spectrum Diversity. Thus, this study sheds light on a new representation and diversity preserving mechanism that should impact algorithms and applications to come. To download the code please access the following https:\/\/github.com\/zweifel\/Physis-Shard.",0.1052631579],["inductive invariants belong to an abstract domain. we consider inductive invariants","Decidability and Synthesis of Abstract Inductive Invariants","summarize: Decidability and synthesis of inductive invariants ranging in a given domain play an important role in many software and hardware verification systems. We consider here inductive invariants belonging to an abstract domain ",0.1176470588],["ourmodel is an ensemble of frame-level models withtest-time augmentation. we","BERT for Large-scale Video Segment Classification with Test-time Augmentation","summarize: This paper presents our approach to the third YouTube-8M video understanding competition that challenges par-ticipants to localize video-level labels at scale to the pre-cise time in the video where the label actually occurs. Ourmodel is an ensemble of frame-level models such as GatedNetVLAD and NeXtVLAD and various BERT models withtest-time augmentation. We explore multiple ways to ag-gregate BERT outputs as video representation and variousways to combine visual and audio information. We proposetest-time augmentation as shifting video frames to one leftor right unit, which adds variety to the predictions and em-pirically shows improvement in evaluation metrics. We firstpre-train the model on the 4M training video-level data, andthen fine-tune the model on 237K annotated video segment-level data. We achieve MAP@100K 0.7871 on private test-ing video segment data, which is ranked 9th over 283 teams.",0.0],["online professional network platforms becoming popular on the web. people are turning to these platforms to create","JobComposer: Career Path Optimization via Multicriteria Utility Learning","summarize: With online professional network platforms becoming popular on the web, people are now turning to these platforms to create and share their professional profiles, to connect with others who share similar professional aspirations and to explore new career opportunities. These platforms however do not offer a long-term roadmap to guide career progression and improve workforce employability. The career trajectories of OPN users can serve as a reference but they are not always optimal. A career plan can also be devised through consultation with career coaches, whose knowledge may however be limited to a few industries. To address the above limitations, we present a novel data-driven approach dubbed JobComposer to automate career path planning and optimization. Its key premise is that the observed career trajectories in OPNs may not necessarily be optimal, and can be improved by learning to maximize the sum of payoffs attainable by following a career path. At its heart, JobComposer features a decomposition-based multicriteria utility learning procedure to achieve the best tradeoff among different payoff criteria in career path planning. Extensive studies using a city state-based OPN dataset demonstrate that JobComposer returns career paths better than other baseline methods and the actual career paths.",0.0833333333],["regularizing OT problems with entropy leads to faster computations and better differentiation using","Regularized Optimal Transport is Ground Cost Adversarial","summarize: Regularizing the optimal transport problem has proven crucial for OT theory to impact the field of machine learning. For instance, it is known that regularizing OT problems with entropy leads to faster computations and better differentiation using the Sinkhorn algorithm, as well as better sample complexity bounds than classic OT. In this work we depart from this practical perspective and propose a new interpretation of regularization as a robust mechanism, and show using Fenchel duality that any convex regularization of OT can be interpreted as ground cost adversarial. This incidentally gives access to a robust dissimilarity measure on the ground space, which can in turn be used in other applications. We propose algorithms to compute this robust cost, and illustrate the interest of this approach empirically.",0.0],["lithography uses 193 nm wavelength exposure and 300-mm large silicon wafer","Development of transmon qubits solely from optical lithography on 300mm wafers","summarize: Qubit information processors are increasing in footprint but currently rely on e-beam lithography for patterning the required Josephson junctions . Advanced optical lithography is an alternative patterning method, and we report on the development of transmon qubits patterned solely with optical lithography. The lithography uses 193 nm wavelength exposure and 300-mm large silicon wafers. Qubits and arrays of evaluation JJs were patterned with process control which resulted in narrow feature distributions: a standard deviation of 0:78% for a 220 nm linewidth pattern realized across over half the width of the wafers. Room temperature evaluation found a 2.8-3.6% standard deviation in JJ resistance in completed chips. The qubits used aluminum and titanium nitride films on silicon substrates without substantial silicon etching. T1 times of the qubits were extracted at 26 - 27 microseconds, indicating a low level of material-based qubit defects. This study shows that large wafer optical lithography on silicon is adequate for high-quality transmon qubits, and shows a promising path for improving many-qubit processors.",0.1818181818],["involution switches two sets of statistics known as the rises and the contacts. this is","The Rise-Contact involution on Tamari intervals","summarize: We describe an involution on Tamari intervals and m-Tamari intervals. This involution switches two sets of statistics known as the rises and the contacts and so proves an open conjecture from Pr\\'eville-Ratelle on intervals of the m-Tamari lattice.",0.1],["spherical nuclei, cylindrical phase, and crust thickness are analyzed","Probing crustal structures from neutron star compactness","summarize: With various sets of the parameters that characterize the equation of state of nuclear matter, we systematically examine the thickness of a neutron star crust and of the pasta phases contained therein. Then, with respect to the thickness of the phase of spherical nuclei, the thickness of the cylindrical phase, and the crust thickness, we successfully derive fitting formulas that express the ratio of each thickness to the star's radius as a function of the star's compactness, the incompressibility of symmetric nuclear matter, and the density dependence of the symmetry energy. In particular, we find that the thickness of the phase of spherical nuclei has such a strong dependence on the stellar compactness as the crust thickness, but both of them show a much weaker dependence on the EOS parameters. Thus, via determination of the compactness, the thickness of the phase of spherical nuclei as well as the crust thickness can be constrained reasonably, even if the EOS parameters remain to be well-determined.",0.1111111111],["multivariate singular spectrum analysis proposed to provide detailed information about phase synchronization in networks of","Mixed measurements and the detection of phase synchronization in networks","summarize: Multivariate singular spectrum analysis , with a varimax rotation of eigenvectors, was recently proposed to provide detailed information about phase synchronization in networks of nonlinear oscillators without any a priori need for phase estimation. The discriminatory power of M-SSA is often enhanced by using only the time series of the variable that provides the best observability of the node dynamics. In practice, however, diverse factors could prevent one to have access to this variable in some nodes and other variables should be used, resulting in a mixed set of variables. In the present work, the impact of this mixed measurement approach on the M-SSA is numerically investigated in networks of R\\ossler systems and cord oscillators. The results are threefold. First, a node measured by a poor variable, in terms of observability, becomes virtually invisible to the technique. Second, a side effect of using a poor variable is that the characterization of phase synchronization clustering of the \\, nodes is hindered by a small amount. This suggests that, given a network, synchronization analysis with M-SSA could be more reliable by not measuring those nodes that are accessible only through poor variables. Third, global phase synchronization could be detected even using only poor variables, given enough of them are measured. These insights could be useful in defining measurement strategies for both experimental design and real world applications for use with M-SSA.",0.2941176471],["inference method is based on a Markov chain Monte Carlo algorithm. the","Exact Bayesian inference in spatio-temporal Cox processes driven by multivariate Gaussian processes","summarize: In this paper we present a novel inference methodology to perform Bayesian inference for spatiotemporal Cox processes where the intensity function depends on a multivariate Gaussian process. Dynamic Gaussian processes are introduced to allow for evolution of the intensity function over discrete time. The novelty of the method lies on the fact that no discretisation error is involved despite the non-tractability of the likelihood function and infinite dimensionality of the problem. The method is based on a Markov chain Monte Carlo algorithm that samples from the joint posterior distribution of the parameters and latent variables of the model. The models are defined in a general and flexible way but they are amenable to direct sampling from the relevant distributions, due to careful characterisation of its components. The models also allow for the inclusion of regression covariates and\/or temporal components to explain the variability of the intensity function. These components may be subject to relevant interaction with space and\/or time. Real and simulated examples illustrate the methodology, followed by concluding remarks.",0.3888888889],["this article proposes a new approach to modeling high-dimensional time series by treating a","Modeling High-Dimensional Time Series: A Factor Model with Dynamically Dependent Factors and Diverging Eigenvalues","summarize: This article proposes a new approach to modeling high-dimensional time series by treating a ",0.5769230769],["Kadeishvili originally restricted himself to transferring a dg algebra structure to","A closer look at Kadeishvili's theorem","summarize: We give a proof of the Homotopy Transfer Theorem following Kadeishvili's original strategy. Although Kadeishvili originally restricted himself to transferring a dg algebra structure to an ",0.1666666667],["a joint article with W. Raji gave a proof of various linear relations between products","Fake proofs for identities involving products of Eisenstein series","summarize: In the workshop of the July 2016 Building Bridges 3 conference in Sarajevo, I presented the results from a joint article with W. Raji . That article gave a proof of various linear relations between products of two Eisenstein series on ",0.2],["we build the so-called cluster formula and cluster formula. we build the cluster formula and","Some conjectures on generalized cluster algebras via the cluster formula and ","summarize: In the theory of generalized cluster algebras, we build the so-called cluster formula and ",0.3714285714],["the verification of beacon signatures in Vehicular Communication systems ensure awareness among neighboring vehicles","DoS-resilient Cooperative Beacon Verification for Vehicular Communication Systems","summarize: Authenticated safety beacons in Vehicular Communication systems ensure awareness among neighboring vehicles. However, the verification of beacon signatures introduces significant processing overhead for resource-constrained vehicular On-Board Units . Even worse in dense neighborhood or when a clogging Denial of Service attack is mounted. The OBU would fail to verify for all received beacons. This could significantly delay the verifications of authentic beacons or even affect the awareness of neighboring vehicle status. In this paper, we propose an efficient cooperative beacon verification scheme leveraging efficient symmetric key based authentication on top of pseudonymous authentication , providing efficient discovery of authentic beacons among a pool of received authentic and fictitious beacons, and can significantly decrease waiting times of beacons in queue before their validations. We show with simulation results that our scheme can guarantee low waiting times for received beacons even in high neighbor density situations and under DoS attacks, under which a traditional scheme would not be workable.",0.1333333333],["the concept of water-based photonics is introduced. it suggests liquid states of matter","Water Optical Nonlinearity: Explaining Anomalously Large Electro-Optic Coefficients in Poled Silica Fibres","summarize: An explanation is offered for the large anomalous electro-optic effect reported by Fujiwara in 1994. It is based on the large e.o. coefficient of ordered water at an interface measured in recent years. The concept of water-based photonics is introduced, suggesting that liquid states of matter can allow ready shaping and exploitation of many processes in ways not previously considered.",0.0666666667],["celluDose is a stochastic simulation-trained feedback control prototype","Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance","summarize: The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a 100% success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes.",0.1839397206],["the cobra maneuver requires the aircraft to fly at extremely high angle of attacks. the feedback","Learning Pugachev's Cobra Maneuver for Tail-sitter UAVs Using Acceleration Model","summarize: The Pugachev's cobra maneuver is a dramatic and demanding maneuver requiring the aircraft to fly at extremely high Angle of Attacks where stalling occurs. This paper considers this maneuver on tail-sitter UAVs. We present a simple yet very effective feedback-iterative learning position control structure to regulate the altitude error and lateral displacement during the maneuver. Both the feedback controller and the iterative learning controller are based on the aircraft acceleration model, which is directly measurable by the onboard accelerometer. Moreover, the acceleration model leads to an extremely simple dynamic model that does not require any model identification in designing the position controller, greatly simplifying the implementation of the iterative learning control. Real-world outdoor flight experiments on the Hong Hu UAV, an aerobatic yet efficient quadrotor tail-sitter UAV of small-size, are provided to show the effectiveness of the proposed controller.",0.0434782609],["matter behaves as a nearly inviscid fluid that efficiently translates initial spatial anis","Creating small circular, elliptical, and triangular droplets of quark-gluon plasma","summarize: The experimental study of the collisions of heavy nuclei at relativistic energies has established the properties of the quark-gluon plasma , a state of hot, dense nuclear matter in which quarks and gluons are not bound into hadrons. In this state, matter behaves as a nearly inviscid fluid that efficiently translates initial spatial anisotropies into correlated momentum anisotropies among the produced particles, producing a common velocity field pattern known as collective flow. In recent years, comparable momentum anisotropies have been measured in small-system proton-proton and proton-nucleus collisions, despite expectations that the volume and lifetime of the medium produced would be too small to form a QGP. Here, we report on the observation of elliptic and triangular flow patterns of charged particles produced in proton-gold , deuteron-gold , and helium-gold collisions at a nucleon-nucleon center-of-mass energy ",0.4583333333],["the discrete versions of the well known Borg type theorem are considered discrete","A note on discrete Borg-type theorems","summarize: We consider the discrete versions of the well known Borg theorem and use simple linear algebraic techniques to obtain new versions of the discrete Borg type theorems. To be precise, we prove that the periodic potential of a discrete Schrodinger operator is almost a constant if and only if the possible spectral gaps of the operator are of small width. This result is further extended to more general settings and the connection to the well known Ten Martini problem is also discussed.",0.3684210526],["a method for grasping novel objects is to generalise the learned experience.","DGCM-Net: Dense Geometrical Correspondence Matching Network for Incremental Experience-based Robotic Grasping","summarize: This article presents a method for grasping novel objects by learning from experience. Successful attempts are remembered and then used to guide future grasps such that more reliable grasping is achieved over time. To generalise the learned experience to unseen objects, we introduce the dense geometric correspondence matching network . This applies metric learning to encode objects with similar geometry nearby in feature space. Retrieving relevant experience for an unseen object is thus a nearest neighbour search with the encoded feature maps. DGCM-Net also reconstructs 3D-3D correspondences using the view-dependent normalised object coordinate space to transform grasp configurations from retrieved samples to unseen objects. In comparison to baseline methods, our approach achieves an equivalent grasp success rate. However, the baselines are significantly improved when fusing the knowledge from experience with their grasp proposal strategy. Offline experiments with a grasping dataset highlight the capability to generalise within and between object classes as well as to improve success rate over time from increasing experience. Lastly, by learning task-relevant grasps, our approach can prioritise grasps that enable the functional use of objects.",0.3125],["the regional level-set approach is a high-resolution transport formulation. it is","High-resolution transport of regional level sets for evolving complex interface networks","summarize: In this paper we describe a high-resolution transport formulation of the regional level-set approach for an improved prediction of the evolution of complex interface networks. The novelty of this method is twofold: construction of local level sets and reconstruction of a global regional level sets, locally transporting the interface network by employing high-order spatial discretization schemes for improved representation of complex topologies. Various numerical test cases of multi-region flow problems, including triple-point advection, single vortex flow, mean curvature flow, normal driven flow and dry foam dynamics, show that the method is accurate and suitable for a wide range of complex interface-network evolutions. Its overall computational cost is comparable to the Semi-Lagrangian regional level-set method while the prediction accuracy is significantly improved. The approach thus offers a \\textbf alternative to previous interface-network level-set method.",0.2777777778],["new paradigm is based on big data for small tasks paradigm. a single artificial intelligence","Dark, Beyond Deep: A Paradigm Shift to Cognitive AI with Humanlike Common Sense","summarize: Recent progress in deep learning is essentially based on a big data for small tasks paradigm, under which massive amounts of data are used to train a classifier for a single narrow task. In this paper, we call for a shift that flips this paradigm upside down. Specifically, we propose a small data for big tasks paradigm, wherein a single artificial intelligence system is challenged to develop common sense, enabling it to solve a wide range of tasks with little training data. We illustrate the potential power of this new paradigm by reviewing models of common sense that synthesize recent breakthroughs in both machine and human vision. We identify functionality, physics, intent, causality, and utility as the five core domains of cognitive AI with humanlike common sense. When taken as a unified concept, FPICU is concerned with the questions of why and how, beyond the dominant what and where framework for understanding vision. They are invisible in terms of pixels but nevertheless drive the creation, maintenance, and development of visual scenes. We therefore coin them the dark matter of vision. Just as our universe cannot be understood by merely studying observable matter, we argue that vision cannot be understood without studying FPICU. We demonstrate the power of this perspective to develop cognitive AI systems with humanlike common sense by showing how to observe and apply FPICU with little training data to solve a wide range of challenging tasks, including tool use, planning, utility inference, and social learning. In summary, we argue that the next generation of AI must embrace dark humanlike common sense for solving novel tasks.",0.1785714286],["online professional network platforms becoming popular on the web. people are turning to these platforms to create","JobComposer: Career Path Optimization via Multicriteria Utility Learning","summarize: With online professional network platforms becoming popular on the web, people are now turning to these platforms to create and share their professional profiles, to connect with others who share similar professional aspirations and to explore new career opportunities. These platforms however do not offer a long-term roadmap to guide career progression and improve workforce employability. The career trajectories of OPN users can serve as a reference but they are not always optimal. A career plan can also be devised through consultation with career coaches, whose knowledge may however be limited to a few industries. To address the above limitations, we present a novel data-driven approach dubbed JobComposer to automate career path planning and optimization. Its key premise is that the observed career trajectories in OPNs may not necessarily be optimal, and can be improved by learning to maximize the sum of payoffs attainable by following a career path. At its heart, JobComposer features a decomposition-based multicriteria utility learning procedure to achieve the best tradeoff among different payoff criteria in career path planning. Extensive studies using a city state-based OPN dataset demonstrate that JobComposer returns career paths better than other baseline methods and the actual career paths.",0.0833333333],["method employs two language models, one generic. the other specific to the target domain","Simple Unsupervised Summarization by Contextual Matching","summarize: We propose an unsupervised method for sentence summarization using only language modeling. The approach employs two language models, one that is generic , and the other that is specific to the target domain. We show that by using a product-of-experts criteria these are enough for maintaining continuous contextual matching while maintaining output fluency. Experiments on both abstractive and extractive sentence summarization data sets show promising results of our method without being exposed to any paired data.",0.0],["a new study is underway to weave ethics into advancing ML research. a","Theories of Parenting and their Application to Artificial Intelligence","summarize: As machine learning systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.",0.3461538462],["fluid model developed for multicomponent plamas in chemical non-equilibrium.","Consistent transport properties in multicomponent two-temperature magnetized plasmas:","summarize: A fluid model is developed for multicomponent two-temperature magnetized plasmas in chemical non-equilibrium from the partially- to fully-ionized collisional regimes. We focus on transport phenomena aiming at representing the chromosphere of the Sun. Graille et al. have derived an asymptotic fluid model for multicomponent plamas from kinetic theory, yielding a rigorous description of the dissipative effects. The governing equations and consistent transport properties are obtained using a multiscale Chapman-Enskog perturbative solution to the Boltzmann equation based on a non-dimensional analysis. The mass disparity between the electrons and heavy particles is accounted for, as well as the influence of the electromagnetic field. We couple this model to the Maxwell equations for the electromagnetic field and derive the generalized Ohm's law for multicomponent plasmas. The model inherits a well-identified mathematical structure leading to an extended range of validity for the Sun chromosphere conditions. We compute consistent transport properties by means of a spectral Galerkin method using the Laguerre-Sonine polynomial approximation. Two non-vanishing polynomial terms are used when deriving the transport systems for electrons, whereas only one term is retained for heavy particles. In a simplified framework where the plasma is fully ionized, we compare the transport properties for the Sun chromosphere to conventional expressions for magnetized plasmas due to Braginskii, showing a good agreement between both results. For more general partially ionized conditions, representative of the Sun chromosphere, we compute the muticomponent transport properties corresponding to the species diffusion velocities, heavy-particle and electron heat fluxes, and viscous stress tensor of the model, for a Helium-Hydrogen mixture in local thermodynamic equilibrium. The model is assessed for the 3D radiative magnetohydrodynamic simulation of a pore, in the highly turbulent upper layer of the solar convective zone. The resistive term is found to dominate mainly the dynamics of the electric field at the pore location. The battery term for heavy particles appears to be higher at the pore location and at some intergranulation boundaries.",0.2222222222],["the associated cost matrices of these transportation problems are of special structure. we present","Some Aspects on Solving Transportation Problem","summarize: In this paper, we consider a class of transportation problems which arises in sample surveys and other areas of statistics. The associated cost matrices of these transportation problems are of special structure. We observe that the optimality of North West corner solution holds for the general problem where cost component is replaced by a convex function. We revisit assignment problem and present a weighted version of K",0.0],["a graph Laplacian graph shows the optimal control problem. the goal is to","Topology design for stochastically-forced consensus networks","summarize: We study an optimal control problem aimed at achieving a desired tradeoff between the network coherence and communication requirements in the distributed controller. Our objective is to add a certain number of edges to an undirected network, with a known graph Laplacian, in order to optimally enhance closed-loop performance. To promote controller sparsity, we introduce ",0.125],["experience replay is one of the most commonly used approaches to improve reinforcement learning algorithms. we propose","Experience Replay Using Transition Sequences","summarize: Experience replay is one of the most commonly used approaches to improve the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to select and replay sequences of transitions in order to accelerate the learning of a reinforcement learning agent in an off-policy setting. In addition to selecting appropriate sequences, we also artificially construct transition sequences using information gathered from previous agent-environment interactions. These sequences, when replayed, allow value function information to trickle down to larger sections of the state\/state-action space, thereby making the most of the agent's experience. We demonstrate our approach on modified versions of standard reinforcement learning tasks such as the mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. Further, we briefly discuss some of the possible extensions to this work, as well as applications and situations where this approach could be particularly useful.",0.0],["proximal algorithm for minimizing objective functions. it includes a nonsm","A proximal minimization algorithm for structured nonconvex and nonsmooth problems","summarize: We propose a proximal algorithm for minimizing objective functions consisting of three summands: the composition of a nonsmooth function with a linear operator, another nonsmooth function, each of the nonsmooth summands depending on an independent block variable, and a smooth function which couples the two block variables. The algorithm is a full splitting method, which means that the nonsmooth functions are processed via their proximal operators, the smooth function via gradient steps, and the linear operator via matrix times vector multiplication. We provide sufficient conditions for the boundedness of the generated sequence and prove that any cluster point of the latter is a KKT point of the minimization problem. In the setting of the Kurdyka-\\Lojasiewicz property we show global convergence, and derive convergence rates for the iterates in terms of the \\Lojasiewicz exponent.",0.6153846154],["heart disease is one of the most common diseases in middle-aged citizens. the coronary","Coronary Artery Disease Diagnosis; Ranking the Significant Features Using Random Trees Model","summarize: Heart disease is one of the most common diseases in middle-aged citizens. Among the vast number of heart diseases, the coronary artery disease is considered as a common cardiovascular disease with a high death rate. The most popular tool for diagnosing CAD is the use of medical imaging, e.g., angiography. However, angiography is known for being costly and also associated with a number of side effects. Hence, the purpose of this study is to increase the accuracy of coronary heart disease diagnosis through selecting significant predictive features in order of their ranking. In this study, we propose an integrated method using machine learning. The machine learning methods of random trees , decision tree of C5.0, support vector machine , decision tree of Chi-squared automatic interaction detection are used in this study. The proposed method shows promising results and the study confirms that RTs model outperforms other models.",0.2631578947],["a quantum dot is modeled using an infinite potential well and a two-dimensional","Energy levels in a single-electron quantum dot with hydrostatic pressure","summarize: In this article we present a study of the effects of hydrostatic pressure on the energy levels of a quantum dot with an electron. A quantum dot is modeled using an infinite potential well and a two-dimensional harmonic oscillator and solved through the formalism of second quantization. A scheme for the implementation of a quantum NOT gate controlled with hydrostatic pressure is proposed.",0.3333333333],["the maximum clique problem is a well-known NP-hard problem. the","Solving the Maximum Clique Problem with Symmetric Rank-One Nonnegative Matrix Approximation","summarize: Finding complete subgraphs in a graph, that is, cliques, is a key problem and has many real-world applications, e.g., finding communities in social networks, clustering gene expression data, modeling ecological niches in food webs, and describing chemicals in a substance. The problem of finding the largest clique in a graph is a well-known NP-hard problem and is called the maximum clique problem . In this paper, we formulate a very convenient continuous characterization of the MCP based on the symmetric rank-one nonnegative approximation of a given matrix, and build a one-to-one correspondence between stationary points of our formulation and cliques of a given graph. In particular, we show that the local minima of the continuous problem corresponds to the maximal cliques of the given graph. We also propose a new and efficient clique finding algorithm based on our continuous formulation and test it on various synthetic and real data sets to show that the new algorithm outperforms other existing algorithms based on the Motzkin-Straus formulation, and can compete with a sophisticated combinatorial heuristic.",0.2827616931],["a variety of pseudo-Voigt functions have been proposed as a closed","Notes: An assessment of some closed-form expressions for the Voigt function III: Combinations of the Lorentz and Gauss functions","summarize: A variety of pseudo-Voigt functions, i.e. a linear combination of the Lorentz and Gauss function , have been proposed as a closed-form approximation for the convolution of the Lorentz and Gauss function known as the Voigt function. First, a compact review of several approximations using a consistent notation is presented. The comparison with accurate reference values indicates relative errors as large as some percent.",0.3051947881],["model of a single stem is used to maximize the captured sunlight. the shape of each","Competition Models for Plant Stems","summarize: The models introduced in this paper describe a uniform distribution of plant stems competing for sunlight. The shape of each stem, and the density of leaves, are designed in order to maximize the captured sunlight, subject to a cost for transporting water and nutrients from the root to all the leaves. Given the intensity of light, depending on the height above ground, we first solve the optimization problem determining the best possible shape for a single stem. We then study a competitive equilibrium among a large number of similar plants, where the shape of each stem is optimal given the shade produced by all others. Uniqueness of equilibria is proved by analyzing the two-point boundary value problem for a system of ODEs derived from the necessary conditions for optimality.",0.0416666667],["nonconforming P1 finite element discretizations are thought to be less sensitive to the","Nonconforming P1 elements on distorted triangulations: Lower bounds for the discrete energy norm error","summarize: Compared to conforming P1 finite elements, nonconforming P1 finite element discretizations are thought to be less sensitive to the appearance of distorted triangulations. E.g., optimal-order discrete ",0.3086536929],["overfitting mixture models provide a solid and straightforward approach. the framework is used to","Overfitting Bayesian Mixtures of Factor Analyzers with an Unknown Number of Components","summarize: Recent advances on overfitting Bayesian mixture models provide a solid and straightforward approach for inferring the underlying number of clusters and model parameters in heterogeneous datasets. The applicability of such a framework in clustering correlated high dimensional data is demonstrated. For this purpose an overfitting mixture of factor analyzers is introduced, assuming that the number of factors is fixed. A Markov chain Monte Carlo sampler combined with a prior parallel tempering scheme is used to estimate the posterior distribution of model parameters. The optimal number of factors is estimated using information criteria. Identifiability issues related to the label switching problem are dealt by post-processing the simulated MCMC sample by relabelling algorithms. The method is benchmarked against state-of-the-art software for maximum likelihood estimation of mixtures of factor analyzers using an extensive simulation study. Finally, the applicability of the method is illustrated in publicly available data.",0.3],["a new study has shown that the problem is a more general setting of unknown change propag","Multi-Sensor Sequential Change Detection with Unknown Change Propagation Pattern","summarize: The problem of detecting changes with multiple sensors has received significant attention in the literature. In many practical applications such as critical infrastructure monitoring and modeling of disease spread, a useful change propagation model is one where change eventually happens at all sensors, but where not all sensors witness change at the same time instant. While prior work considered the case of known change propagation dynamics, this paper studies a more general setting of unknown change propagation pattern . A Bayesian formulation of the problem in both centralized and decentralized settings is studied with the goal of detecting the first time instant at which any sensor witnesses a change. Using the dynamic programming framework, the optimal solution structure is derived and in the rare change regime, several more practical change detection algorithms are proposed. Under certain conditions, the first-order asymptotic optimality of a proposed algorithm called multichart test is shown as the false alarm probability vanishes. To further reduce the computational complexity, change detection algorithms are proposed based on online estimation of the unknown change propagation pattern. Numerical studies illustrate that the proposed detection techniques offer near-optimal performance. Further, in the decentralized setting, it is shown that if an event-triggered sampling scheme called level-crossing sampling with hysteresis is used for sampling and transmission of local statistics, the detection performance can be significantly improved using the same amount of communication resources compared to the conventional uniform-in-time sampling scheme.",0.4137931034],["atomistic molecular dynamics simulation shows universal features can emerge. we compare the distribution","Distinguishing dynamical features of water inside protein hydration layer: Distribution reveals what is hidden behind the average","summarize: Since the pioneering works of Pethig, Grant and Wuthrich on protein hydration layer, many studies have been devoted to find out if there are any general and universal characteristic features that can distinguish water molecules inside the protein hydration layer from bulk. Given that the surface itself varies from protein to protein, and that each surface facing the water is heterogeneous, search for universal features has been elusive. Here, we perform atomistic molecular dynamics simulation in order to propose and demonstrate that such defining characteristics can emerge if we look not at average properties but the distribution of relaxation times. We present results of calculations of distributions of residence times and rotational relaxation times for four different protein-water systems, and compare them with the same quantities in the bulk. The distributions in the hydration layer is unusually broad and log-normal in nature, due to the simultaneous presence of peptide backbones that form weak hydrogen bonds, hydrophobic amino acid side chains that form no hydrogen bond and charged polar groups that form strong hydrogen bond with the surrounding water molecules. The broad distribution is responsible for the non-exponential dielectric response and also agrees with large specific heat of the hydration water. Our calculations reveal that while the average time constant is just about 2-3 times larger than that of bulk water, it provides a poor representation of the real behaviour. In particular, the average leads to the erroneous conclusion that water in the hydration layer is bulk-like. However, the observed and calculated lower value of static dielectric constant of hydration layer remained difficult to reconcile with the broad distribution observed in dynamical properties. We offer a plausible explanation of these unique properties.",0.1130986893],["a graph of the graph shows the sex of the sex of the","Lower bounding the Folkman numbers ","summarize: For a graph ",0.1351351351],["neural networks have shown significant success in a wide range of AI tasks. large-scale","Differentially Private Model Publishing for Deep Learning","summarize: Deep learning techniques based on neural networks have shown significant success in a wide range of AI tasks. Large-scale training datasets are one of the critical factors for their success. However, when the training datasets are crowdsourced from individuals and contain sensitive information, the model parameters may encode private information and bear the risks of privacy leakage. The recent growing trend of the sharing and publishing of pre-trained models further aggravates such privacy risks. To tackle this problem, we propose a differentially private approach for training neural networks. Our approach includes several new techniques for optimizing both privacy loss and model accuracy. We employ a generalization of differential privacy called concentrated differential privacy, with both a formal and refined privacy loss analysis on two different data batching methods. We implement a dynamic privacy budget allocator over the course of training to improve model accuracy. Extensive experiments demonstrate that our approach effectively improves privacy loss accounting, training efficiency and model quality under a given privacy budget.",0.1904761905],["superquadrics are a volumetric model that defines various 3D shape primitives","Recovery of Superquadrics from Range Images using Deep Learning: A Preliminary Study","summarize: It has been a longstanding goal in computer vision to describe the 3D physical space in terms of parameterized volumetric models that would allow autonomous machines to understand and interact with their surroundings. Such models are typically motivated by human visual perception and aim to represents all elements of the physical word ranging from individual objects to complex scenes using a small set of parameters. One of the de facto stadards to approach this problem are superquadrics - volumetric models that define various 3D shape primitives and can be fitted to actual 3D data . However, existing solutions to superquadric recovery involve costly iterative fitting procedures, which limit the applicability of such techniques in practice. To alleviate this problem, we explore in this paper the possibility to recover superquadrics from range images without time consuming iterative parameter estimation techniques by using contemporary deep-learning models, more specifically, convolutional neural networks . We pose the superquadric recovery problem as a regression task and develop a CNN regressor that is able to estimate the parameters of a superquadric model from a given range image. We train the regressor on a large set of synthetic range images, each containing a single superquadric shape and evaluate the learned model in comparaitve experiments with the current state-of-the-art. Additionally, we also present a qualitative analysis involving a dataset of real-world objects. The results of our experiments show that the proposed regressor not only outperforms the existing state-of-the-art, but also ensures a 270x faster execution time.",0.2853439738],["the physics at the extreme IR---cosmology---might provide tests of the","Dark Energy, ","summarize: We point out that the physics at the extreme IR---cosmology---might provide tests of the physics of the extreme UV---the Weak Gravity Conjecture. The current discrepancies in the determination of ",0.0],["we propose three approximate likelihoods that are computationally tractable. they are based on","Langevin diffusions on the torus: estimation and applications","summarize: We introduce stochastic models for continuous-time evolution of angles and develop their estimation. We focus on studying Langevin diffusions with stationary distributions equal to well-known distributions from directional statistics, since such diffusions can be regarded as toroidal analogues of the Ornstein-Uhlenbeck process. Their likelihood function is a product of transition densities with no analytical expression, but that can be calculated by solving the Fokker-Planck equation numerically through adequate schemes. We propose three approximate likelihoods that are computationally tractable: a likelihood based on the stationary distribution; toroidal adaptations of the Euler and Shoji-Ozaki pseudo-likelihoods; a likelihood based on a specific approximation to the transition density of the wrapped normal process. A simulation study compares, in dimensions one and two, the approximate transition densities to the exact ones, and investigates the empirical performance of the approximate likelihoods. Finally, two diffusions are used to model the evolution of the backbone angles of the protein G during a molecular dynamics simulation. The software package sdetorus implements the estimation methods and applications presented in the paper.",0.125],["the ferromagnetic ground state of the Co2FeAl is energetically","Phase stability and the effect of lattice distortions on electronic properties and half-metallic ferromagnetism of Co2FeAl Heusler alloy: An ab initio study","summarize: Density functional theory calculations within the generalized gradient approximation are employed to study the ground state of Co2FeAl. Various magnetic configurations are considered to find out its most stable phase. The ferromagnetic ground state of the Co2FeAl is energetically observed with an optimized lattice constant of 5.70 . Thereafter, the system was subjected under uniform and non-uniform strains to see their effects on spin polarization and half-metallicity. The effect of spin orbit coupling is considered in the present study. Half-metallicity is only retained under uniform strains started from 0 to +4%, and dropped rapidly from 90% to 16% for the negative strains started from -1% to -6%. We find that the present system is much sensitive under tetragonal distortions as half-metallicity is preserved only for the cubic case. The main reason for the loss of half-metallicity is due to the shift of the bands with respect to the Fermi level. We also discuss the influence of these results on spintronics devices.",0.1072168559],["the BAMSDN framework dynamically allocates resources for a MPLS network using a","A SDN\/OpenFlow Framework for Dynamic Resource Allocation based on Bandwidth Allocation Model","summarize: The communication network context in actual systems like 5G, cloud and IoT , presents an ever-increasing number of users, applications, and services that are highly distributed with distinct and heterogeneous communications requirements. Resource allocation in this context requires dynamic, efficient, and customized solutions and Bandwidth Allocation Models are an alternative to support this new trend. This paper proposes the BAMSDN framework that dynamically allocates resources for a MPLS network using a SDN \/OpenFlow strategy with BAM. The framework adopts an innovative implementation approach for BAM systems by controlling the MPLS network using SDN with OpenFlow. Experimental results suggest that using SDN\/OpenFlow with BAM for bandwidth allocation does have effective advantages for MPLS networks requiring flexible resource sharing among applications and facilitates the migration path to a SDN\/OpenFlow network.",0.5416666667],["conventional deep brain stimulation of basal ganglia uses regular electrical pulses to treat Parkinson","Effects of electrical and optogenetic deep brain stimulation on synchronized oscillatory activity in Parkinsonian basal ganglia","summarize: Conventional deep brain stimulation of basal ganglia uses high-frequency regular electrical pulses to treat Parkinsonian motor symptoms and has a series of limitations. Relatively new and not yet clinically tested optogenetic stimulation is an effective experimental stimulation technique to affect pathological network dynamics. We compared the effects of electrical and optogenetic stimulation of the basal ganglia on the pathological parkinsonian rhythmic neural activity. We studied the network response to electrical stimulation and excitatory and inhibitory optogenetic stimulations. Different stimulations exhibit different interactions with pathological activity in the network. We studied these interactions for different network and stimulation parameter values. Optogenetic stimulation was found to be more efficient than electrical stimulation in suppressing pathological rhythmicity. Our findings indicate that optogenetic control of neural synchrony may be more efficacious than electrical control because of the different ways of how stimulations interact with network dynamics.",0.5572786498],["quantum information measures can be written as an optimization of the quantum relative entropy between sets","Efficient optimization of the quantum relative entropy","summarize: Many quantum information measures can be written as an optimization of the quantum relative entropy between sets of states. For example, the relative entropy of entanglement of a state is the minimum relative entropy to the set of separable states. The various capacities of quantum channels can also be written in this way. We propose a unified framework to numerically compute these quantities using off-the-shelf semidefinite programming solvers, exploiting the approximation method proposed in . As a notable application, this method allows us to provide numerical counterexamples for a proposed lower bound on the quantum conditional mutual information in terms of the relative entropy of recovery.",0.347826087],["the Wasserstein metric or earth mover's distance is a useful tool in statistics","DOTmark - A Benchmark for Discrete Optimal Transport","summarize: The Wasserstein metric or earth mover's distance is a useful tool in statistics, machine learning and computer science with many applications to biological or medical imaging, among others. Especially in the light of increasingly complex data, the computation of these distances via optimal transport is often the limiting factor. Inspired by this challenge, a variety of new approaches to optimal transport has been proposed in recent years and along with these new methods comes the need for a meaningful comparison. In this paper, we introduce a benchmark for discrete optimal transport, called DOTmark, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy. Along with the DOTmark we present a survey and a performance test for a cross section of established methods ranging from more traditional algorithms, such as the transportation simplex, to recently developed approaches, such as the shielding neighborhood method, and including also a comparison with commercial solvers.",0.3],["we prove that it is a good thing.","Counting Polygon Triangulations is Hard","summarize: We prove that it is ",0.3333333333],["timbre transfer is a technique used to manipulate timbre to match another instrument","TimbreTron: A WaveNet)) Pipeline for Musical Timbre Transfer","summarize: In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies image domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.",0.1363636364],["a 2.5-dimensional charge-conservative electromagnetic particle-in-cell algorithm is optimized","Axisymmetric Charge-Conservative Electromagnetic Particle Simulation Algorithm on Unstructured Grids: Application to Microwave Vacuum Electronic Devices","summarize: We present a 2.5-dimensional charge-conservative electromagnetic particle-in-cell algorithm optimized for the analysis of vacuum electronic devices with cylindrical symmetry . We explore the axisymmetry present in the device geometry, fields, and sources to reduce the dimensionality of the problem from 3D to 2D. Further, we explore `transformation optics' principles to map the original problem in polar coordinates to an equivalent problem on Cartesian coordinates with an effective inhomogeneous medium introduced. The resulting problem in the meridian plane is discretized using an unstructured 2D mesh considering TE-polarized fields and properly scaled charges. EM field and source variables are expressed as differential forms of various degrees, and discretized using Whitney forms. Using leapfrog time integration, we obtain a mixed finite-element time-domain scheme for the full-discrete Maxwell's equations. We achieve a local and explicit time-update for the field equations by employing the sparse approximate inverse algorithm. Interpolating field values to particles' positions for solving Newton-Lorentz equations of motion is also done via Whitney forms. Particles are advanced using the Boris algorithm with a relativistic correction. In the scatter step, we apply a radial scaling factor on top of a charge-conserving scatter scheme tailored for 2-dimensional unstructured grids. As validation examples, we demonstrate simulations that investigate the physical performance of VEDs designed to harness particle bunching effects arising from the coherent Cerenkov electron beam interactions within micromachined slow-wave structures.",0.2382068684],["magnetic flux emergence occurs in complex multipolar regions. surface pattern of magnetic structures alone can","Magnetic Helicity from Multipolar Regions on the Solar Surface","summarize: The emergence of dipolar magnetic features on the solar surface is an idealization. Most of the magnetic flux emergence occurs in complex multipolar regions. Here, we show that the surface pattern of magnetic structures alone can reveal the sign of the underlying magnetic helicity in the nearly force-free coronal regions above. The sign of the magnetic helicity can be predicted to good accuracy by considering the three-dimensional position vectors of three spots on the sphere ordered by their relative strengths at the surface and compute from them the skew product. This product, which is a pseudoscalar, is shown to be a good proxy for the sign of the coronal magnetic helicity.",0.0],["in this paper, we study an energy efficiency maximization problem in uplink for D2D","Energy Efficient Power Allocation for Device-to-Device Communications Underlaid Cellular Networks Using Stochastic Geometry","summarize: In this paper, we study an energy efficiency maximization problem in uplink for D2D communications underlaid with cellular networks on multiple bands. Utilizing stochastic geometry, we derive closed-form expressions for the average sum rate, successful transmission probability, and energy efficiency of cellular and D2D users. Then, we formulate an optimization problem to jointly maximize the energy efficiency of D2D and cellular users and obtain optimum transmission power of both D2D and cellular users. In the optimization problem, we guarantee the QoS of users by taking into account the success transmission probability on each link. To solve the problem, first we convert the problem into canonical convex form. Afterwards, we solve the problem in two phases, energy efficiency maximization of devices and energy efficiency maximization of cellular users. In the first phase, we maximize the energy efficiency of D2D users and feed the solution to the second phase where we maximize the energy efficiency of cellular users. Simulation results reveal that significant energy efficiency can be attained e.g., 10% energy efficiency improvement compared to fix transmission power in high density scenario.",0.2222222222],["researchers have used recommender systems to generate effective recommendations. SPARP models the personality and","Socially-Aware Conference Participant Recommendation with Personality Traits","summarize: As a result of the importance of academic collaboration at smart conferences, various researchers have utilized recommender systems to generate effective recommendations for participants. Recent research has shown that the personality traits of users can be used as innovative entities for effective recommendations. Nevertheless, subjective perceptions involving the personality of participants at smart conferences are quite rare and haven't gained much attention. Inspired by the personality and social characteristics of users, we present an algorithm called Socially and Personality Aware Recommendation of Participants . Our recommendation methodology hybridizes the computations of similar interpersonal relationships and personality traits among participants. SPARP models the personality and social characteristic profiles of participants at a smart conference. By combining the above recommendation entities, SPARP then recommends participants to each other for effective collaborations. We evaluate SPARP using a relevant dataset. Experimental results confirm that SPARP is reliable and outperforms other state-of-the-art methods.",0.0],["the attractive tail of the intermolecular interaction affects very weakly the structural properties of","Role of attractive forces in the relaxation dynamics of supercooled liquids","summarize: The attractive tail of the intermolecular interaction affects very weakly the structural properties of liquids, while it affects dramatically their dynamical ones. Via the numerical simulations of model systems not prone to crystallization, both in three and in two spatial dimensions, here we demonstrate that the non-perturbative dynamical effects of the attractive forces are tantamount to a rescaling of the activation energy by the glass transition temperature ",0.3636363636],["horizonless objects have been proposed to arise in models inspired by quantum gravity. they may","The stochastic gravitational-wave background in the absence of horizons","summarize: Gravitational-wave astronomy has the potential to explore one of the deepest and most puzzling aspects of Einstein's theory: the existence of black holes. A plethora of ultracompact, horizonless objects have been proposed to arise in models inspired by quantum gravity. These objects may solve Hawking's information-loss paradox and the singularity problem associated with black holes, while mimicking almost all of their classical properties. They are, however, generically unstable on relatively short timescales. Here, we show that this ergoregion instability leads to a strong stochastic background of gravitational waves, at a level detectable by current and future gravitational-wave detectors. The absence of such background in the first observation run of Advanced LIGO already imposes the most stringent limits to date on black-hole alternatives, showing that certain models of quantum-dressed stellar black holes can be at most a small percentage of the total population. The future LISA mission will allow for similar constraints on supermassive black-hole mimickers.",0.125],["Miles et al. argue that the lockdown costed more than the benefit","Counting the costs of COVID-19: why future treatment option values matter","summarize: I critique a recent analysis of COVID-19 lockdown costs and benefits, focussing on the United Kingdom . Miles et al. argue that the March-June UK lockdown was more costly than the benefit of lives saved, evaluated using the NICE threshold of 30000 for a quality-adjusted life year and that the costs of a lockdown for 13 weeks from mid-June would be vastly greater than any plausible estimate of the benefits, even if easing produced a second infection wave causing over 7000 deaths weekly by mid-September. I note here two key problems that significantly affect their estimates and cast doubt on their conclusions. Firstly, their calculations arbitrarily cut off after 13 weeks, without costing the epidemic end state. That is, they assume indifference between mid-September states of 13 or 7500 weekly deaths and corresponding infection rates. This seems indefensible unless one assumes that there is little chance of any effective vaccine or improved medical or social interventions for the foreseeable future, notwithstanding temporary lockdowns, COVID-19 will very likely propagate until herd immunity. Even under these assumptions it is very questionable. Secondly, they ignore the costs of serious illness, possible long-term lowering of life quality and expectancy for survivors. These are uncertain, but plausibly at least as large as the costs in deaths. In summary, policy on tackling COVID-19 cannot be rationally made without estimating probabilities of future medical interventions and long-term illness costs. More work on modelling these uncertainties is urgently needed.",0.2142857143],["quantitative structure-activity relationship has proved invaluable tool in medicinal chemistry. predictive models are","On the Virtues of Automated QSAR The New Kid on the Block","summarize: Quantitative Structure-Activity Relationship has proved an invaluable tool in medicinal chemistry. Data availability at unprecedented levels through various databases have collaborated to a resurgence in the interest for QSAR. In this context, rapid generation of quality predictive models is highly desirable for hit identification and lead optimization. We showcase the application of an automated QSAR approach, which randomly selects multiple training\/test sets and utilizes machine-learning algorithms to generate predictive models. Results demonstrate that AutoQSAR produces models of improved or similar quality to those generated by practitioners in the field but in just a fraction of the time. Despite the potential of the concept to the benefit of the community, the AutoQSAR opportunity has been largely undervalued.",0.0],["the Wiener-Hopf equation for this case is derived. it involves two","Diffraction by a quarter-plane. Analytical continuation of spectral functions","summarize: The problem of diffraction by a Dirichlet quarter-plane in a 3D space is studied. The Wiener-Hopf equation for this case is derived and involves two unknown functions depending on two complex variables. The aim of the present work is to build an analytical continuation of these functions onto a well-described Riemann manifold and to study their behaviour and singularities on this manifold. In order to do so, integral formulae for analytical continuation of the spectral functions are derived and used. It is shown that the Wiener-Hopf problem can be reformulated using the concept of additive crossing of branch lines introduced in the paper. Both the integral formulae and the additive crossing reformulation are novel and represent the main results of this work.",0.0],["the von Neumann entropy of a graph has recently found applications in complex networks","On the Von Neumann Entropy of Graphs","summarize: The von Neumann entropy of a graph is a spectral complexity measure that has recently found applications in complex networks analysis and pattern recognition. Two variants of the von Neumann entropy exist based on the graph Laplacian and normalized graph Laplacian, respectively. Due to its computational complexity, previous works have proposed to approximate the von Neumann entropy, effectively reducing it to the computation of simple node degree statistics. Unfortunately, a number of issues surrounding the von Neumann entropy remain unsolved to date, including the interpretation of this spectral measure in terms of structural patterns, understanding the relation between its two variants, and evaluating the quality of the corresponding approximations. In this paper we aim to answer these questions by first analysing and comparing the quadratic approximations of the two variants and then performing an extensive set of experiments on both synthetic and real-world graphs. We find that 1) the two entropies lead to the emergence of similar structures, but with some significant differences; 2) the correlation between them ranges from weakly positive to strongly negative, depending on the topology of the underlying graph; 3) the quadratic approximations fail to capture the presence of non-trivial structural patterns that seem to influence the value of the exact entropies; 4) the quality of the approximations, as well as which variant of the von Neumann entropy is better approximated, depends on the topology of the underlying graph.",0.2631578947],["quadratic M-convex functions are a generalization of valuated mat","The quadratic M-convexity testing problem","summarize: M-convex functions, which are a generalization of valuated matroids, play a central role in discrete convex analysis. Quadratic M-convex functions constitute a basic and important subclass of M-convex functions, which has a close relationship with phylogenetics as well as valued constraint satisfaction problems. In this paper, we consider the quadratic M-convexity testing problem , which is the problem of deciding whether a given quadratic function on ",0.2352941176],["quantum computers speed up classical Markov chain algorithms. quantum algorithms are faster than classical Markov","Adaptive Quantum Simulated Annealing for Bayesian Inference and Estimating Partition Functions","summarize: Markov chain Monte Carlo algorithms have important applications in counting problems and in machine learning problems, settings that involve estimating quantities that are difficult to compute exactly. How much can quantum computers speed up classical Markov chain algorithms? In this work we consider the problem of speeding up simulated annealing algorithms, where the stationary distributions of the Markov chains are Gibbs distributions at temperatures specified according to an annealing schedule. We construct a quantum algorithm that both adaptively constructs an annealing schedule and quantum samples at each temperature. Our adaptive annealing schedule roughly matches the length of the best classical adaptive annealing schedules and improves on nonadaptive temperature schedules by roughly a quadratic factor. Our dependence on the Markov chain gap matches other quantum algorithms and is quadratically better than what classical Markov chains achieve. Our algorithm is the first to combine both of these quadratic improvements. Like other quantum walk algorithms, it also improves on classical algorithms by producing qsamples instead of classical samples. This means preparing quantum states whose amplitudes are the square roots of the target probability distribution. In constructing the annealing schedule we make use of amplitude estimation, and we introduce a method for making amplitude estimation nondestructive at almost no additional cost, a result that may have independent interest. Finally we demonstrate how this quantum simulated annealing algorithm can be applied to the problems of estimating partition functions and Bayesian inference.",0.0],["data is captured and shared from personal devices, transactional operations, sensors, social media and other","Towards a new Framework linking knowledge management systems and organizational agility : an empirical study","summarize: The amount of data has exploded over the last ten years. Data is captured and shared from personal devices, transactional operations, sensors, social media and other sources. Firms should, thus, be able to explore the new opportunities and rapidly seize them by developing the corresponding capabilities. In our work, we focus on two emerging dynamic capabilities: Absorptive capacity and organizational agility. We propose a new theoretical Framework based on the previous literature linking the use of knowledge management systems and organizational agility by highlighting the mediating role of absorptive capacity. In addition, we carried out an empirical study based on a survey to support and validate the proposed Framework. The main findings of this study are presented.",0.1176470588],["time-of-flight momentum microscopy setup is combined with a 0.5","Efficient orbital imaging based on ultrafast momentum microscopy and sparsity-driven phase retrieval","summarize: We present energy-resolved photoelectron momentum maps for orbital tomography that have been collected with a novel and efficient time-of-flight momentum microscopy setup. This setup is combined with a 0.5 MHz table-top femtosecond extreme-ultraviolet light source, which enables unprecedented speed in data collection and paves the way towards time-resolved orbital imaging experiments in the future. Moreover, we take a significant step forward in the data analysis procedure for orbital imaging, and present a sparsity-driven approach to the required phase retrieval problem, which uses only the number of non-zero pixels in the orbital. Here, no knowledge of the object support is required, and the sparsity number can easily be determined from the measured data. Used in the relaxed averaged alternating reflections algorithm, this sparsity constraint enables fast and reliable phase retrieval for our experimental as well as noise-free and noisy simulated photoelectron momentum map data.",0.2388437702],["our proof is based on two ingredients. the first is the factorization theory of motion","Kempe's Universality Theorem for Rational Space Curves","summarize: We prove that every bounded rational space curve of degree d and circularity c can be drawn by a linkage with 9\/2 d - 6c + 1 revolute joints. Our proof is based on two ingredients. The first one is the factorization theory of motion polynomials. The second one is the construction of a motion polynomial of minimum degree with given orbit. Our proof also gives the explicity construction of the linkage.",0.0416666667],["a MCTS variant may only encounter states with an explicit, extrinsic","Preference-Based Monte Carlo Tree Search","summarize: Monte Carlo tree search is a popular choice for solving sequential anytime problems. However, it depends on a numeric feedback signal, which can be difficult to define. Real-time MCTS is a variant which may only rarely encounter states with an explicit, extrinsic reward. To deal with such cases, the experimenter has to supply an additional numeric feedback signal in the form of a heuristic, which intrinsically guides the agent. Recent work has shown evidence that in different areas the underlying structure is ordinal and not numerical. Hence erroneous and biased heuristics are inevitable, especially in such domains. In this paper, we propose a MCTS variant which only depends on qualitative feedback, and therefore opens up new applications for MCTS. We also find indications that translating absolute into ordinal feedback may be beneficial. Using a puzzle domain, we show that our preference-based MCTS variant, wich only receives qualitative feedback, is able to reach a performance level comparable to a regular MCTS baseline, which obtains quantitative feedback.",0.1764705882],["reversible data hiding in encrypted image is calculated and analyzed. the prediction errors","Reversible Data Hiding in Encrypted Images using Local Difference of Neighboring Pixels","summarize: This paper presents a reversible data hiding in encrypted image , which divides image into non-overlapping blocks. In each block, central pixel of the block is considered as leader pixel and others as follower ones. The prediction errors between the intensity of follower pixels and leader ones are calculated and analyzed to determine a feature for block embedding capacity. This feature indicates the amount of data that can be embedded in a block. Using this pre-process for whole blocks, we vacate rooms before the encryption of the original image to achieve high embedding capacity. Also, using the features of all blocks, embedded data is extracted and the original image is perfectly reconstructed at the decoding phase. In effect, comparing to existent RDHEI algorithms, embedding capacity is significantly increased in the proposed algorithm. Experimental results confirm that the proposed algorithm outperforms state of the art ones.",0.1428571429],["emerging technologies that make use of new materials are at the forefront in the race for the physical implementation","Experimental realisation of tunable ferroelectric\/superconductor N\/STO 1D photonic crystals in the whole visible spectrum","summarize: Emergent technologies that make use of novel materials and quantum properties of light states are at the forefront in the race for the physical implementation, encoding and transmission of information. Photonic crystals enter this paradigm with optical materials that allow the control of light propagation and can be used for optical communication, and photonics and electronics integration making use of materials ranging from semiconductors, to metals, metamaterials, and topological insulators, to mention but a few. In particular, here we show how designer superconductor materials integrated into PCs fabrication allow for an extraordinary reduction of electromagnetic waves damping and possibilitate their optimal propagation and tuning through the structure, below critical superconductor temperature. We experimentally demonstrate, for the first time, a successful integration of ferroelectric and superconductor materials into a one-dimensional PC composed of N\/STO bilayers that work in the whole visible spectrum, and below critical superconductor temperature . Theoretical calculations support, for different number of bilayers N, the effectiveness of the produced 1D PCs and pave the way for novel optoelectronics integration and information processing in the visible spectrum at low temperature, while preserving their electric and optical properties.",0.2],["numerical simulations of Nambu-Goto cosmic strings show that the loop distribution relax","Cosmic string loop production functions","summarize: Numerical simulations of Nambu-Goto cosmic strings in an expanding universe show that the loop distribution relaxes to an universal configuration, the so-called scaling regime, which is of power law shape on large scales. Precise estimations of the power law exponent are, however, still matter of debate while numerical simulations do not incorporate all the radiation and backreaction effects expected to affect the network dynamics at small scales. By using a Boltzmann approach, we show that the steepness of the loop production function with respect to loops size is associated with drastic changes in the cosmological loop distribution. For a scale factor varying as a~t^nu, we find that sub-critical loop production functions, having a Polchinski-Rocha exponent chi = \/2, are shown to be IR-physics dependent and this generically prevents the loop distribution to relax towards scaling. In the latter situation, we discuss the additional regularisations needed for convergence and show that, although a scaling regime can still be reached, the shape of the cosmological loop distribution is modified compared to the naive expectation. Finally, we discuss the implications of our findings.",0.0833333333],["the genotype assignment problem arises in a variety of contexts, including wildlife forensic","Mycorrhiza: Genotype Assignment usingPhylogenetic Networks","summarize: Motivation The genotype assignment problem consists of predicting, from the genotype of an individual, which of a known set of populations it originated from. The problem arises in a variety of contexts, including wildlife forensics, invasive species detection and biodiversity monitoring. Existing approaches perform well under ideal conditions but are sensitive to a variety of common violations of the assumptions they rely on. Results In this article, we introduce Mycorrhiza, a machine learning approach for the genotype assignment problem. Our algorithm makes use of phylogenetic networks to engineer features that encode the evolutionary relationships among samples. Those features are then used as input to a Random Forests classifier. The classification accuracy was assessed on multiple published empirical SNP, microsatellite or consensus sequence datasets with wide ranges of size, geographical distribution and population structure and on simulated datasets. It compared favorably against widely used assessment tests or mixture analysis methods such as STRUCTURE and Admixture, and against another machine-learning based approach using principal component analysis for dimensionality reduction. Mycorrhiza yields particularly significant gains on datasets with a large average fixation index or deviation from the Hardy-Weinberg equilibrium. Moreover, the phylogenetic network approach estimates mixture proportions with good accuracy.",0.1111111111],["the workshop aims at discussing the lessons learned from making formal methods for verification and analysis of real","Proceedings Third Workshop on Models for Formal Analysis of Real Systems and Sixth International Workshop on Verification and Program Transformation","summarize: This volume contains the joint proceedings of MARS 2018, the third workshop on Models for Formal Analysis of Real Systems, and VPT 2018, the sixth international workshop on Verification and Program Transformation, held together on April 20, 2018 in Thessaloniki, Greece, as part of ETAPS 2018, the European Joint Conferences on Theory and Practice of Software. MARS emphasises modelling over verification. It aims at discussing the lessons learned from making formal methods for the verification and analysis of realistic systems. Examples are: Which formalism is chosen, and why? Which abstractions have to be made and why? How are important characteristics of the system modelled? Were there any complications while modelling the system? Which measures were taken to guarantee the accuracy of the model? We invited papers that present full models of real systems, which may lay the basis for future comparison and analysis. An aim of the workshop is to present different modelling approaches and discuss pros and cons for each of them. Alternative formal descriptions of the systems presented at this workshop are encouraged, which should foster the development of improved specification formalisms. VPT aims to provide a forum where people from the areas of program transformation and program verification can fruitfully exchange ideas and gain a deeper understanding of the interactions between those two fields. These interactions have been beneficial in both directions. On the one hand, methods and tools developed in the field of program transformation, such as partial deduction, partial evaluation, fold\/unfold transformations, and supercompilation, are applied with success to verification, in particular to the verification of infinite state and parameterized systems. On the other hand, methods developed in program verification, such as model checking, abstract interpretation, SAT and SMT solving, and automated theorem proving, are used to enhance program transformation techniques, thereby making these techniques more powerful and useful in practice.",0.2440470864],["the proposed codes are zero-error and can be decoded segment-by-se","Coding for Segmented Edit Channels","summarize: This paper considers insertion and deletion channels with the additional assumption that the channel input sequence is implicitly divided into segments such that at most one edit can occur within a segment. No segment markers are available in the received sequence. We propose code constructions for the segmented deletion, segmented insertion, and segmented insertion-deletion channels based on subsets of Varshamov-Tenengolts codes chosen with pre-determined prefixes and\/or suffixes. The proposed codes, constructed for any finite alphabet, are zero-error and can be decoded segment-by-segment. We also derive an upper bound on the rate of any zero-error code for the segmented edit channel, in terms of the segment length. This upper bound shows that the rate scaling of the proposed codes as the segment length increases is the same as that of the maximal code.",0.0],["a long elastic filament confined to a spherical container is confined","Spontaneous Domain Formation in Spherically-Confined Elastic Filaments","summarize: Although the free energy of a genome packing into a virus is dominated by DNA-DNA interactions, ordering of the DNA inside the capsid is elasticity-driven, suggesting general solutions with DNA organized into spool-like domains. Using analytical calculations and computer simulations of a long elastic filament confined to a spherical container, we show that the ground state is not a single spool as assumed hitherto, but an ordering mosaic of multiple homogeneously-ordered domains. At low densities, we observe concentric spools, while at higher densities, other morphologies emerge, which resemble topological links. We discuss our results in the context of metallic wires, viral DNA, and flexible polymers.",0.5217391304],["Let us know what you think about it!","Conditional nonlinear expectations","summarize: Let ",0.0],["symmetry breaking operators for real reductive groups have been classed. we illustrate","Conformal symmetry breaking on differential forms and some applications","summarize: Rapid progress has been made recently on symmetry breaking operators for real reductive groups. Based on Program A-C for branching problems , we illustrate a scheme of the classification of symmetry breaking operators by an example of conformal representations on differential forms on the model space ",0.25],["Frequently Asked Question based QA is usually a practical and effective solution.","FAQ-based Question Answering via Knowledge Anchors","summarize: Question answering aims to understand questions and find appropriate answers. In real-world QA systems, Frequently Asked Question based QA is usually a practical and effective solution, especially for some complicated questions . Recent years have witnessed the great successes of knowledge graphs in KBQA systems, while there are still few works focusing on making full use of KGs in FAQ-based QA. In this paper, we propose a novel Knowledge Anchor based Question Answering framework for FAQ-based QA to better understand questions and retrieve more appropriate answers. More specifically, KAQA mainly consists of three modules: knowledge graph construction, query anchoring and query-document matching. We consider entities and triples of KGs in texts as knowledge anchors to precisely capture the core semantics, which brings in higher precision and better interpretability. The multi-channel matching strategy also enables most sentence matching models to be flexibly plugged in our KAQA framework to fit different real-world computation limitations. In experiments, we evaluate our models on both offline and online query-document matching tasks on a real-world FAQ-based QA system in WeChat Search, with detailed analysis, ablation tests and case studies. The significant improvements confirm the effectiveness and robustness of the KAQA framework in real-world FAQ-based QA.",0.2352941176],["law enforcement agencies face challenges in terms of privacy and consent. this paper reviews the state-","The Challenges of Investigating Cryptocurrencies and Blockchain Related Crime","summarize: We increasingly live in a world where there is a balance between the rights to privacy and the requirements for consent, and the rights of society to protect itself. Within this world, there is an ever-increasing requirement to protect the identities involved within financial transactions, but this makes things increasingly difficult for law enforcement agencies, especially in terms of financial fraud and money laundering. This paper reviews the state-of-the-art in terms of the methods of privacy that are being used within cryptocurrency transactions, and in the challenges that law enforcement face.",0.1875],["tensor multiplication of corresponding processes in a linear logic game category is represented","An Optimal Itinerary Generation in a Configuration Space of Large Intellectual Agent Groups with Linear Logic","summarize: A group of intelligent agents which fulfill a set of tasks in parallel is represented first by the tensor multiplication of corresponding processes in a linear logic game category. An optimal itinerary in the configuration space of the group states is defined as a play with maximal total reward in the category. New moments also are: the reward is represented as a degree of certainty of an agent goal, and the system goals are chosen by the greatest value corresponding to these processes in the system goal lattice.",0.3760686274],["we consider full feedback from the loads, bandit feedback, and two intermediate types of feedback","Setpoint Tracking with Partially Observed Loads","summarize: We use online convex optimization for setpoint tracking with uncertain, flexible loads. We consider full feedback from the loads, bandit feedback, and two intermediate types of feedback: partial bandit where a subset of the loads are individually observed and the rest are observed in aggregate, and Bernoulli feedback where in each round the aggregator receives either full or bandit feedback according to a known probability. We give sublinear regret bounds in all cases. We numerically evaluate our algorithms on examples with thermostatically controlled loads and electric vehicles.",0.0],["the Hessian discretisation method provides a unified convergence analysis framework. some examples","Improved ","summarize: The Hessian discretisation method for fourth order linear elliptic equations provides a unified convergence analysis framework based on three properties namely coercivity, consistency, and limit-conformity. Some examples that fit in this approach include conforming and nonconforming finite element methods, finite volume methods and methods based on gradient recovery operators. A generic error estimate has been established in ",0.0],["Halo model is a physically intuitive method for modelling the non-linear power spectrum","Non-linear matter power spectrum without screening dynamics modelling in ","summarize: Halo model is a physically intuitive method for modelling the non-linear power spectrum, especially for the alternatives to the standard ",0.4117647059],["a fundamental problem of fluid dynamics requires complex physical or numerical experiments. computational resources are required","Performance of parallel-in-time integration for Rayleigh B\\'enard Convection","summarize: Rayleigh-B\\'enard convection is a fundamental problem of fluid dynamics, with many applications to geophysical, astrophysical, and industrial flows. Understanding RBC at parameter regimes of interest requires complex physical or numerical experiments. Numerical simulations require large amounts of computational resources; in order to more efficiently use the large numbers of processors now available in large high performance computing clusters, novel parallelisation strategies are required. To this end, we investigate the performance of the parallel-in-time algorithm Parareal when used in numerical simulations of RBC. We present the first parallel-in-time speedups for RBC simulations at finite Prandtl number. We also investigate the problem of convergence of Parareal with respect to to statistical numerical quantities, such as the Nusselt number, and discuss the importance of reliable online stopping criteria in these cases.",0.3333333333],["the data was combined with information available in DOAJ, CrossRef, OpenDOAR,","Evidence of Open Access of scientific publications in Google Scholar: a large-scale analysis","summarize: This article uses Google Scholar as a source of data to analyse Open Access levels across all countries and fields of research. All articles and reviews with a DOI and published in 2009 or 2014 and covered by the three main citation indexes in the Web of Science were selected for study. The links to freely available versions of these documents displayed in GS were collected. To differentiate between more reliable forms of access and less reliable ones, the data extracted from GS was combined with information available in DOAJ, CrossRef, OpenDOAR, and ROAR. This allowed us to distinguish the percentage of documents in our sample that are made OA by the publisher from those available as Green OA , and those available from other sources . The data shows an overall free availability of 54.6%, with important differences at the country and subject category levels. The data extracted from GS yielded very similar results to those found by other studies that analysed similar samples of documents, but employed different methods to find evidence of OA, thus suggesting a relative consistency among methods.",0.0641348399],["smart world concept and smart world concept addressed in the fourth industrial revolution. new challenges in distributed","Smart systems, the fourth industrial revolution and new challenges in distributed computing","summarize: Smart systems and the smart world concept are addressed in the framework of the fourth industrial revolution. New challenges in distributed autonomous robots and computing are considered. An illustration of a new kind of smart and reconfigurable distributed modular robot system is given. A prototype is also presented as well as the associated distributed algorithm.",0.5185185185],["color sequences can be realized from a fixed point set. we first study the case","Colored ray configurations","summarize: We study the cyclic color sequences induced at infinity by colored rays with apices being a given balanced finite bichromatic point set. We first study the case in which the rays are required to be pairwise disjoint. We derive a lower bound on the number of color sequences that can be realized from any such fixed point set and examine color sequences that can be realized regardless of the point set, exhibiting negative examples as well. We also provide a tight upper bound on the number of configurations that can be realized from a point set, and point sets for which there are asymptotically less configurations than that number. In addition, we provide algorithms to decide whether a color sequence is realizable from a given point set in a line or in general position. We address afterwards the variant of the problem where the rays are allowed to intersect. We prove that for some configurations and point sets, the number of ray crossings must be ",0.1111111111],["we crowdsource the largest known dataset of labeled network traffic from smart home devices from within","IoT Inspector: Crowdsourcing Labeled Network Traffic from Smart Home Devices at Scale","summarize: The proliferation of smart home devices has created new opportunities for empirical research in ubiquitous computing, ranging from security and privacy to personal health. Yet, data from smart home deployments are hard to come by, and existing empirical studies of smart home devices typically involve only a small number of devices in lab settings. To contribute to data-driven smart home research, we crowdsource the largest known dataset of labeled network traffic from smart home devices from within real-world home networks. To do so, we developed and released IoT Inspector, an open-source tool that allows users to observe the traffic from smart home devices on their own home networks. Since April 2019, 4,322 users have installed IoT Inspector, allowing us to collect labeled network traffic from 44,956 smart home devices across 13 categories and 53 vendors. We demonstrate how this data enables new research into smart homes through two case studies focused on security and privacy. First, we find that many device vendors use outdated TLS versions and advertise weak ciphers. Second, we discover about 350 distinct third-party advertiser and tracking domains on smart TVs. We also highlight other research areas, such as network management and healthcare, that can take advantage of IoT Inspector's dataset. To facilitate future reproducible research in smart homes, we will release the IoT Inspector data to the public.",0.1111111111],["image forensic plays a crucial role in both criminal investigations and civil litigation. there are","A Survey of Machine Learning Techniques in Adversarial Image Forensics","summarize: Image forensic plays a crucial role in both criminal investigations and civil litigation . Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial examples, with real-world consequences . Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.",0.32],["two experiments on the impact of post hoc explanations by example and error rates on peoples","Play MNIST For Me! User Studies on the Effects of Post-Hoc, Example-Based Explanations & Error Rates on Debugging a Deep Learning, Black-Box Classifier","summarize: This paper reports two experiments on the impact of post hoc explanations by example and error rates on peoples perceptions of a black box classifier. Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct. They also show that as error rates increase above 4%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. The implications of these results for XAI are discussed.",0.2582594106],["proposed architecture can alleviate global traffic variations by dynamically grouping small cells into self-organized","Load Balancing for Ultra-Dense Networks: A Deep Reinforcement Learning Based Approach","summarize: In this paper, we propose a deep reinforcement learning based mobility load balancing algorithm along with a two-layer architecture to solve the large-scale load balancing problem for ultra-dense networks . Our contribution is three-fold. First, this work proposes a two-layer architecture to solve the large-scale load balancing problem in a self-organized manner. The proposed architecture can alleviate the global traffic variations by dynamically grouping small cells into self-organized clusters according to their historical loads, and further adapt to local traffic variations through intra-cluster load balancing afterwards. Second, for the intra-cluster load balancing, this paper proposes an off-policy DRL-based MLB algorithm to autonomously learn the optimal MLB policy under an asynchronous parallel learning framework, without any prior knowledge assumed over the underlying UDN environments. Moreover, the algorithm enables joint exploration with multiple behavior policies, such that the traditional MLB methods can be used to guide the learning process thereby improving the learning efficiency and stability. Third, this work proposes an offline-evaluation based safeguard mechanism to ensure that the online system can always operate with the optimal and well-trained MLB policy, which not only stabilizes the online performance but also enables the exploration beyond current policies to make full use of machine learning in a safe way. Empirical results verify that the proposed framework outperforms the existing MLB methods in general UDN environments featured with irregular network topologies, coupled interferences, and random user movements, in terms of the load balancing performance.",0.0],["motive energy played a crucial role in human civilization. since ancient times, motive energy played","History of Prime Movers and Future Implications","summarize: Motive and electrical energy has played a crucial role in human civilization. Since Ancient times, motive energy played a primary role in agricultural and industrial production as well as transportation. At that time, motive energy was provided by work of humans and draft animals. Later, work of water and wind power was harnessed. During the 19",0.0714285714],["RFID systems are among the major infrastructures of the Internet of Things. the proposed methods are","ISO\/EPC Addressing Methods to Support Supply Chain in the Internet of Things","summarize: RFID systems are among the major infrastructures of the Internet of Things, which follow ISO and EPC standards. In addition, ISO standard constitutes the main layers of supply chain, and many RFID systems benefit from ISO standard for different purposes. In this paper, we tried to introduce addressing systems based on ISO standards, through which the range of things connected to the Internet of Things will grow. Our proposed methods are addressing methods which can be applied to both ISO and EPC standards. The proposed methods are simple, hierarchical, and low cost implementation. In addition, the presented methods enhance interoperability among RFIDs, and also enjoys a high scalability, since it well covers all of EPC schemes and ISO supply chain standards. Further, by benefiting from a new algorithm for long EPCs known as selection algorithm, they can significantly facilitate and accelerate the operation of address mapping.",0.2307692308],["a new study is underway to weave ethics into advancing ML research. a","Theories of Parenting and their Application to Artificial Intelligence","summarize: As machine learning systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.",0.3461538462],["we study the effective theory of soft photons in slowly varying electromagnetic background fields at one","Photon propagation in slowly varying electromagnetic fields","summarize: We study the effective theory of soft photons in slowly varying electromagnetic background fields at one-loop order in QED. This is of relevance for the study of all-optical signatures of quantum vacuum nonlinearity in realistic electromagnetic background fields as provided by high-intensity lasers. The central result derived in this article is a new analytical expression for the photon polarization tensor in two linearly polarized counter-propagating pulsed Gaussian laser beams. As we treat the peak field strengths of both laser beams as free parameters this field configuration can be considered as interpolating between the limiting cases of a purely right- or left-moving laser beam and the standing-wave type scenario with two counter-propagating beams of equal strength.",0.3684210526],["the computational bottleneck is because of the similarity between a read and candidate locations in that","GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment in DNA Short Read Mapping","summarize: Motivation: High throughput DNA sequencing technologies generate an excessive number of small DNA segments -- called short reads -- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and candidate locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: it is implemented using quadratic-time dynamic programming algorithms, and the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment operations. Results: We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays , which can perform pre-alignment much faster than software. GateKeeper can be integrated with any mapper that performs sequence alignment for verification. When implemented on a single FPGA chip, GateKeeper maintains high accuracy while providing up to 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance , respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability: https:\/\/github.com\/BilkentCompGen\/GateKeeper",0.2413793103],["the method subdivides a large problem in two smaller ones. the solution's","Analysis of Divide & Conquer strategies for the 0-1 Minimization Knapsack Problem","summarize: We introduce and asses several Divide \\& Conquer heuristic strategies aimed to solve large instances of the 0-1 Minimization Knapsack Problem. The method subdivides a large problem in two smaller ones , to lower down the global computational complexity of the original problem, at the expense of a moderate loss of quality in the solution. Theoretical mathematical results are presented in order to guarantee an algorithmically successful application of the method and to suggest the potential strategies for its implementation. In contrast, due to the lack of theoretical results, the solution's quality deterioration is measured empirically by means of Monte Carlo simulations for several types and values of the chosen strategies. Finally, introducing parameters of efficiency we suggest the best strategies depending on the data input.",0.375],["the so-called is a powerful tool in qualitative studies of one dimensional parabolic","The Zero Number Diminishing Property under General Boundary Conditions","summarize: The so-called is a powerful tool in qualitative studies of one dimensional parabolic equations, which says that, under the zero- or non-zero-Dirichlet boundary conditions, the number of zeroes of the solution ",0.2105263158],["a naively deep neural network can classify dynamic objects regarding their shape estimates","Extended Target Tracking and Classification Using Neural Networks","summarize: Extended target\/object tracking problem involves tracking objects which potentially generate multiple measurements at a single sensor scan. State-of-the-art ETT algorithms can efficiently exploit the available information in these measurements such that they can track the dynamic behaviour of objects and learn their shapes simultaneously. Once the shape estimate of an object is formed, it can naturally be utilized by high-level tasks such as classification of the object type. In this work, we propose to use a naively deep neural network, which consists of one input, two hidden and one output layers, to classify dynamic objects regarding their shape estimates. The proposed method shows superior performance in comparison to a Bayesian classifier for simulation experiments.",0.2857142857],["graphene is a promising material for applications in spintronics requiring long distance","Optospintronics in graphene via proximity coupling","summarize: The observation of micron size spin relaxation makes graphene a promising material for applications in spintronics requiring long distance spin communication. However, spin dependent scatterings at the contact\/graphene interfaces affect the spin injection efficiencies and hence prevent the material from achieving its full potential. While this major issue could be eliminated by nondestructive direct optical spin injection schemes, graphenes intrinsically low spin orbit coupling strength and optical absorption place an obstacle in their realization. We overcome this challenge by creating sharp artificial interfaces between graphene and WSe2 monolayers. Application of a circularly polarized light activates the spin polarized charge carriers in the WSe2 layer due to its spin coupled valley selective absorption. These carriers diffuse into the superjacent graphene layer, transport over a 3.5 um distance, and are finally detected electrically using BN\/Co contacts in a non local geometry. Polarization dependent measurements confirm the spin origin of the non local signal.",0.3043478261],["graph drawings are based on the notions of thickness and antithickness. the","Thickness and Antithickness of Graphs","summarize: This paper studies questions about duality between crossings and non-crossings in graph drawings via the notions of thickness and antithickness. The thickness of a graph ",0.1875],["the detection of the first electromagnetic counterpart to the binary neutron star merger remnant GW1708","Probing the magnetic field in the GW170817 outflow using H.E.S.S. observations","summarize: The detection of the first electromagnetic counterpart to the binary neutron star merger remnant GW170817 established the connection between short ",0.3333333333],["proposed technique is used to maximise probability of detection of the RS. null-space","Coexistence of MIMO Radar and FD MIMO Cellular Systems with QoS Considerations","summarize: In this work, the feasibility of spectrum sharing between a multiple-input multiple-output radar system and a MIMO cellular system , comprising of a full duplex base station serving multiple downlink and uplink users at the same time and frequency is investigated. While a joint transceiver design technique at the CS's BS and users is proposed to maximise the probability of detection of the MIMO RS, subject to constraints of quality of service of users and transmit power at the CS, null-space based waveform projection is used to mitigate the interference from RS towards CS. In particular, the proposed technique optimises the performance of PoD of RS by maximising its lower bound, which is obtained by exploiting the monotonically increasing relationship of PoD and its non-centrality parameter. Numerical results show the utility of the proposed spectrum sharing framework, but with certain trade-offs in performance corresponding to RS's transmit power, RS's PoD, CS's residual self interference power at the FD BS and QoS of users.",0.1875],["inpainting based algorithm approaches problem in three steps. feature selection and matching,","Learning Energy Based Inpainting for Optical Flow","summarize: Modern optical flow methods are often composed of a cascade of many independent steps or formulated as a black box neural network that is hard to interpret and analyze. In this work we seek for a plain, interpretable, but learnable solution. We propose a novel inpainting based algorithm that approaches the problem in three steps: feature selection and matching, selection of supporting points and energy based inpainting. To facilitate the inference we propose an optimization layer that allows to backpropagate through 10K iterations of a first-order method without any numerical or memory problems. Compared to recent state-of-the-art networks, our modular CNN is very lightweight and competitive with other, more involved, inpainting based methods.",0.1875],["the homogenization of periodic elastic composites is addressed through the reformulation of the local equation","Geometric variational principles for computational homogenization","summarize: The homogenization of periodic elastic composites is addressed through the reformulation of the local equations of the mechanical problem in a geometric functional setting. This relies on the definition of Hilbert spaces of kinematically and statically admissible tensor fields, whose orthogonality and duality properties are recalled. These are endowed with specific energetic scalar products that make use of a reference and uniform elasticity tensor. The corresponding strain and stress Green's operators are introduced and interpreted as orthogonal projection operators in the admissibility spaces. In this context and as an alternative to classical minimum energy principles, two geometric variational principles are investigated with the introduction of functionals that aim at measuring the discrepancy of arbitrary test fields to the kinematic, static or material admissibility conditions of the problem. By relaxing the corresponding local equations, this study aims in particular at laying the groundwork for the homogenization of composites whose constitutive properties are only partially known or uncertain. The local fields in the composite and their macroscopic responses are computed through the minimization of the proposed geometric functionals. To do so, their gradients are computed using the Green's operators and gradient-based optimization schemes are discussed. A FFT-based implementation of these schemes is proposed and they are assessed numerically on a canonical example for which analytical solutions are available.",0.0434782609],["late M dwarf system TRAPPIST-1 has seven known transiting planets. the late","TRAPPIST-1 Habitable Atmosphere Intercomparison . Motivations and protocol version 1.0","summarize: Upcoming telescopes such as the James Webb Space Telescope , or the Extremely Large Telescope , may soon be able to characterize, through transmission, emission or reflection spectroscopy, the atmospheres of rocky exoplanets orbiting nearby M dwarfs. One of the most promising candidates is the late M dwarf system TRAPPIST-1 which has seven known transiting planets for which Transit Timing Variation measurements suggest that they are terrestrial in nature, with a possible enrichment in volatiles. Among these seven planets, TRAPPIST-1e seems to be the most promising candidate to have habitable surface conditions, receiving ~66 % of the Earth's incident radiation, and thus needing only modest greenhouse gas inventories to raise surface temperatures to allow surface liquid water to exist. TRAPPIST-1e is therefore one of the prime targets for JWST atmospheric characterization. In this context, the modeling of its potential atmosphere is an essential step prior to observation. Global Climate Models offer the most detailed way to simulate planetary atmospheres. However, intrinsic differences exist between GCMs which can lead to different climate prediction and thus observability of gas and\/or cloud features in transmission and thermal emission spectra. Such differences should preferably be known prior to observations. In this paper we present a protocol to inter-compare planetary GCMs. Four testing cases are considered for TRAPPIST-1e but the methodology is applicable to other rocky exoplanets in the Habitable Zone. The four test cases included two land planets composed with a modern Earth and pure CO2 atmospheres, respectively, and two aqua planets with the same atmospheric compositions. Currently, there are four participating models , however this protocol is intended to let other teams participate as well.",0.1428571429],["we consider merges where the gap between cars is smaller than the size of the ego vehicle","Interactive Decision Making for Autonomous Vehicles in Dense Traffic","summarize: Dense urban traffic environments can produce situations where accurate prediction and dynamic models are insufficient for successful autonomous vehicle motion planning. We investigate how an autonomous agent can safely negotiate with other traffic participants, enabling the agent to handle potential deadlocks. Specifically we consider merges where the gap between cars is smaller than the size of the ego vehicle. We propose a game theoretic framework capable of generating and responding to interactive behaviors. Our main contribution is to show how game-tree decision making can be executed by an autonomous vehicle, including approximations and reasoning that make the tree-search computationally tractable. Additionally, to test our model we develop a stochastic rule-based traffic agent capable of generating interactive behaviors that can be used as a benchmark for simulating traffic participants in a crowded merge setting.",0.0416666667],["novel density-based clustering algorithm is proposed in this paper. it focuses on the","Clustering by the way of atomic fission","summarize: Cluster analysis which focuses on the grouping and categorization of similar elements is widely used in various fields of research. Inspired by the phenomenon of atomic fission, a novel density-based clustering algorithm is proposed in this paper, called fission clustering . It focuses on mining the dense families of a dataset and utilizes the information of the distance matrix to fissure clustering dataset into subsets. When we face the dataset which has a few points surround the dense families of clusters, K-nearest neighbors local density indicator is applied to distinguish and remove the points of sparse areas so as to obtain a dense subset that is constituted by the dense families of clusters. A number of frequently-used datasets were used to test the performance of this clustering approach, and to compare the results with those of algorithms. The proposed algorithm is found to outperform other algorithms in speed and accuracy.",0.2352941176],["PENTACLE is a parallelized hybrid hybrid. it is a parallelized","PENTACLE: Parallelized Particle-Particle Particle-Tree Code for Planet Formation","summarize: We have newly developed a Parallelized Particle-Particle Particle-tree code for Planet formation, PENTACLE, which is a parallelized hybrid ",0.5],["the Schrodinger equation violates local causality. it causes instantan","Nonlocality and local causality in the Schr\\odinger Equation with time-dependent boundary conditions","summarize: We investigate the nonlocal dynamics of a single particle placed in an infinite well with moving walls. It is shown that in this situation, the Schr\\odinger equation violates local causality by causing instantaneous changes in the probability current everywhere inside the well. This violation is formalized by designing a gedanken faster-than-light communication device which uses an ensemble of long narrow cavities and weak measurements to resolve the weak value of the momentum far away from the movable wall. Our system is free from the usual features causing nonphysical violations of local causality when using the SE, such as instantaneous changes in potentials or states involving arbitrarily high energies or velocities. We explore in detail several possible artifacts that could account for the failure of the SE to respect local causality for systems involving time-dependent boundary conditions.",0.3582656553],["compositional nonparametric method is proposed in this paper. model is expressed as a","On the Statistical Efficiency of Compositional Nonparametric Prediction","summarize: In this paper, we propose a compositional nonparametric method in which a model is expressed as a labeled binary tree of ",0.3181818182],["entanglement verification can be used to detect the presence of intruders.","Entanglement Verification in Quantum Networks with Tampered Nodes","summarize: In this paper, we consider the problem of entanglement verification across the quantum memories of any two nodes of a quantum network. Its solution can be a means for detecting the presence of intruders that have taken full control of a node, either to make a denial-of-service attack or to reprogram the node. Looking for strategies that only require local operations and classical communication , we propose two entanglement verification protocols characterized by increasing robustness and efficiency.",0.0],["VAR-modeling was used for describing signals from connected with working memory brain zones","Application of statistical analysis to working memory problem","summarize: This article is devoted to EEG studying of connectivity cortical areas involved in keeping vision information in working memory. VAR-modeling was used for describing signals got from connected with working memory brain zones. Brain connections were estimated by based in Granger Causality Partial Directed Coherence and then compared by Wilcoxon signed-rank test. In paper connection intensity dependence on executing task was found.",0.1538461538],["manual prioritisation of suspicious files is proposed to reduce manual analysis effort required. supervised","Methodology for the Automated Metadata-Based Classification of Incriminating Digital Forensic Artefacts","summarize: The ever increasing volume of data in digital forensic investigation is one of the most discussed challenges in the field. Usually, most of the file artefacts on seized devices are not pertinent to the investigation. Manually retrieving suspicious files relevant to the investigation is akin to finding a needle in a haystack. In this paper, a methodology for the automatic prioritisation of suspicious file artefacts is proposed to reduce the manual analysis effort required. This methodology is designed to work in a human-in-the-loop fashion. In other words, it predicts\/recommends that an artefact is likely to be suspicious rather than giving the final analysis result. A supervised machine learning approach is employed, which leverages the recorded results of previously processed cases. The process of features extraction, dataset generation, training and evaluation are presented in this paper. In addition, a toolkit for data extraction from disk images is outlined, which enables this method to be integrated with the conventional investigation process and work in an automated fashion.",0.1052631579],["a simple theory for the green photoluminescence of ZnO quantum dots allows us","Quantum-Size Effects in the Visible Photoluminescence of Colloidal ZnO Quantum Dots: A Theoretical Analysis","summarize: In this work we develop a simple theory for the green photoluminescence of ZnO quantum dots that allows us to understand and rationalize several experimental findings on fundamental grounds. We calculate the spectrum of light emitted in the radiative recombination of a conduction band electron with a deeply trapped hole and find that the experimental behavior of this emission band with particle size can be understood in terms of quantum size effects of the electronic states and their overlap with the deep hole.We focus the comparison of our results on detailed experiments performed for colloidal ZnO nanoparticles in ethanol and find that the experimental evolution of the luminescent signal with particle sizeat room temperature can be better reproduced by assuming the deep hole to be localized at the surface of the nanoparticles. However, the experimental behavior of the intensity and decay time of the signal with temperature can be rationalized in terms of holes predominantly trapped near the center of the nanoparticles at low temperatures being transferred to surface defects at room temperature. Furthermore, the calculated values of the radiative lifetimes are comparable to the experimental values of the decay time of the visible emission signal.We also study the visible emission band as a function of the number of electrons in the conduction band of the nanoparticle, finding a pronounced dependence of the radiative lifetime but a weak dependence of energetic position of the maximum intensity.",0.3472354045],["the method is adapted to the particular challenges of the eddy current problem. it","Parallel-In-Time Simulation of Eddy Current Problems Using Parareal","summarize: In this contribution the usage of the Parareal method is proposed for the time-parallel solution of the eddy current problem. The method is adapted to the particular challenges of the problem that are related to the differential algebraic character due to non-conducting regions. It is shown how the necessary modification can be automatically incorporated by using a suitable time stepping method. The paper closes with a first demonstration of a simulation of a realistic four-pole induction machine model using Parareal.",0.05],["generative models are deep generative latent variable models. the learned generative model capture","Characterizing and Avoiding Problematic Global Optima of Variational Autoencoders","summarize: Variational Auto-encoders are deep generative latent variable models consisting of two components: a generative model that captures a data distribution p by transforming a distribution p over latent space, and an inference model that infers likely latent codes for each data point . Recent work shows that traditional training methods tend to yield solutions that violate modeling desiderata: the learned generative model captures the observed data distribution but does so while ignoring the latent codes, resulting in codes that do not represent the data ; Kim et al. ); the aggregate of the learned latent codes does not match the prior p. This mismatch means that the learned generative model will be unable to generate realistic data with samples from p; Tomczak and Welling ). In this paper, we demonstrate that both issues stem from the fact that the global optima of the VAE training objective often correspond to undesirable solutions. Our analysis builds on two observations: the generative model is unidentifiable - there exist many generative models that explain the data equally well, each with different properties and bias in the VAE objective - the VAE objective may prefer generative models that explain the data poorly but have posteriors that are easy to approximate. We present a novel inference method, LiBI, mitigating the problems identified in our analysis. On synthetic datasets, we show that LiBI can learn generative models that capture the data distribution and inference models that better satisfy modeling assumptions when traditional methods struggle to do so.",0.0],["crowd counting aims to predict the number of people and generate density map. the proposed D","Multi-Scale Context Aggregation Network with Attention-Guided for Crowd Counting","summarize: Crowd counting aims to predict the number of people and generate the density map in the image. There are many challenges, including varying head scales, the diversity of crowd distribution across images and cluttered backgrounds. In this paper, we propose a multi-scale context aggregation network based on single-column encoder-decoder architecture for crowd counting, which consists of an encoder based on a dense context-aware module and a hierarchical attention-guided decoder. To handle the issue of scale variation, we construct the DCAM to aggregate multi-scale contextual information by densely connecting the dilated convolution with varying receptive fields. The proposed DCAM can capture rich contextual information of crowd areas due to its long-range receptive fields and dense scale sampling. Moreover, to suppress the background noise and generate a high-quality density map, we adopt a hierarchical attention-guided mechanism in the decoder. This helps to integrate more useful spatial information from shallow feature maps of the encoder by introducing multiple supervision based on semantic attention module . Extensive experiments demonstrate that the proposed approach achieves better performance than other similar state-of-the-art methods on three challenging benchmark datasets for crowd counting. The code is available at https:\/\/github.com\/KingMV\/MSCANet",0.0],["unbiasedness concept is demonstrated with three examples of classifiers. unbiasedness could","Fisher consistency for prior probability shift","summarize: We introduce Fisher consistency in the sense of unbiasedness as a desirable property for estimators of class prior probabilities. Lack of Fisher consistency could be used as a criterion to dismiss estimators that are unlikely to deliver precise estimates in test datasets under prior probability and more general dataset shift. The usefulness of this unbiasedness concept is demonstrated with three examples of classifiers used for quantification: Adjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted Classify & Count and EM-algorithm are Fisher consistent. A counter-example shows that CDE-Iterate is not Fisher consistent and, therefore, cannot be trusted to deliver reliable estimates of class probabilities.",0.0769230769],["this paper provides a new way to improve the efficiency of the REINFORCE training process","Posterior-regularized REINFORCE for Instance Selection in Distant Supervision","summarize: This paper provides a new way to improve the efficiency of the REINFORCE training process. We apply it to the task of instance selection in distant supervision. Modeling the instance selection in one bag as a sequential decision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization to integrate some domain-specific rules in instance selection using REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classifier trained on cleaned distant supervision dataset as well as the efficiency of the REINFORCE training.",0.2],["additive manufacturing process often produces a shape that closely conforms to the intended design to be manufactured","Automatic Support Removal for Additive Manufacturing Post Processing","summarize: An additive manufacturing process often produces a shape that closely conforms to the intended design to be manufactured. It sometimes contains additional support structure , which has to be removed in post-processing. We describe an approach to automatically generate process plans for support removal using a multi-axis machining instrument. The goal is to fracture the contact regions between each support component and the part, and to do it in the most cost-effective order while avoiding collisions with evolving near-net shape, including the remaining support components. A recursive algorithm identifies a maximal collection of support components whose connection regions to the part are accessible as well as the orientations at which they can be removed at a given round. For every such region, the accessible orientations appear as a 'fiber' in the collision-free space of the evolving near-net shape and the tool assembly. To order the removal of accessible supports, the algorithm constructs a search graph whose edges are weighted by the Riemannian distance between the fibers. The least expensive process plan is obtained by solving a traveling salesman problem over the search graph. The sequence of configurations obtained by solving TSP is used as the input to a motion planner that finds collision free paths to visit all accessible features. The resulting part without the support structure can then be finished using traditional machining to produce the intended design. The effectiveness of the method is demonstrated through benchmark examples in 3D.",0.2307692308],["iGEN-NET is a generative network and segmentation network. focuses","Predictive and Generative Neural Networks for Object Functionality","summarize: Humans can predict the functionality of an object even without any surroundings, since their knowledge and experience would allow them to hallucinate the interaction or usage scenarios involving the object. We develop predictive and generative deep convolutional neural networks to replicate this feat. Specifically, our work focuses on functionalities of man-made 3D objects characterized by human-object or object-object interactions. Our networks are trained on a database of scene contexts, called interaction contexts, each consisting of a central object and one or more surrounding objects, that represent object functionalities. Given a 3D object in isolation, our functional similarity network , a variation of the triplet network, is trained to predict the functionality of the object by inferring functionality-revealing interaction contexts. fSIM-NET is complemented by a generative network and a segmentation network . iGEN-NET takes a single voxelized 3D object with a functionality label and synthesizes a voxelized surround, i.e., the interaction context which visually demonstrates the corresponding functionality. iSEG-NET further separates the interacting objects into different groups according to their interaction types.",0.3571428571],["simulation is increasingly being used for generating large labelled datasets. the entire data generation","AutoSimulate: Learning Synthetic Data Generation","summarize: Simulation is increasingly being used for generating large labelled datasets in many machine learning problems. Recent methods have focused on adjusting simulator parameters with the goal of maximising accuracy on a validation task, usually relying on REINFORCE-like gradient estimators. However these approaches are very expensive as they treat the entire data generation, model training, and validation pipeline as a black-box and require multiple costly objective evaluations at each iteration. We propose an efficient alternative for optimal synthetic data generation, based on a novel differentiable approximation of the objective. This allows us to optimize the simulator, which may be non-differentiable, requiring only one objective evaluation at each iteration with a little overhead. We demonstrate on a state-of-the-art photorealistic renderer that the proposed method finds the optimal data distribution faster , with significantly reduced training data generation and better accuracy on real-world test datasets than previous methods.",0.0666666667],["a universal distributional calculus is developed for regulated volumes of metrics that are singular along","Renormalized Volume","summarize: We develop a universal distributional calculus for regulated volumes of metrics that are singular along hypersurfaces. When the hypersurface is a conformal infinity we give simple integrated distribution expressions for the divergences and anomaly of the regulated volume functional valid for any choice of regulator. For closed hypersurfaces or conformally compact geometries, methods from a previously developed boundary calculus for conformally compact manifolds can be applied to give explicit holographic formulae for the divergences and anomaly expressed as hypersurface integrals over local quantities . The resulting anomaly does not depend on any particular choice of regulator, while the regulator dependence of the divergences is precisely captured by these formulae. Conformal hypersurface invariants can be studied by demanding that the singular metric obey, smoothly and formally to a suitable order, a Yamabe type problem with boundary data along the conformal infinity. We prove that the volume anomaly for these singular Yamabe solutions is a conformally invariant integral of a local Q-curvature that generalizes the Branson Q-curvature by including data of the embedding. In each dimension this canonically defines a higher dimensional generalization of the Willmore energy\/rigid string action. Recently Graham proved that the first variation of the volume anomaly recovers the density obstructing smooth solutions to this singular Yamabe problem; we give a new proof of this result employing our boundary calculus. Physical applications of our results include studies of quantum corrections to entanglement entropies.",0.0416666667],["theoretical evidence of the existence of 12 inequivalent Dirac cones at","Twelve Inequivalent Dirac Cones in Two-Dimensional ZrB2","summarize: Theoretical evidence of the existence of 12 inequivalent Dirac cones at the vicinity of the Fermi energy in monolayered ZrB",0.0714285714],["we study inverse problems of reconstructing static and dynamic discrete structures from tomographic data","On the Reconstruction of Static and Dynamic Discrete Structures","summarize: We study inverse problems of reconstructing static and dynamic discrete structures from tomographic data . The main emphasis is on recent mathematical developments and new applications, which emerge in scientific areas such as physics and materials science, but also in inner mathematical fields such as number theory, optimization, and imaging. Along with a concise introduction to the field of discrete tomography, we give pointers to related aspects of computerized tomography in order to contrast the worlds of continuous and discrete inverse problems.",0.1428571429],["mainstream works fuse feature maps of input images with deep neural networks. but RNNs","Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images","summarize: Recovering the 3D shape of an object from single or multiple images with deep neural networks has been attracting increasing attention in the past few years. Mainstream works use recurrent neural networks to sequentially fuse feature maps of input images. However, RNN-based approaches are unable to produce consistent reconstruction results when given the same input images with different orders. Moreover, RNNs may forget important features from early input images due to long-term memory loss. To address these issues, we propose a novel framework for single-view and multi-view 3D object reconstruction, named Pix2Vox++. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. A multi-scale context-aware fusion module is then introduced to adaptively select high-quality reconstructions for different parts from all coarse 3D volumes to obtain a fused 3D volume. To further correct the wrongly recovered parts in the fused 3D volume, a refiner is adopted to generate the final output. Experimental results on the ShapeNet, Pix3D, and Things3D benchmarks show that Pix2Vox++ performs favorably against state-of-the-art methods in terms of both accuracy and efficiency.",0.0],["FPUT operators have asymptotically no unstable eigenvalues except","Stability of high-energy solitary waves in Fermi-Pasta-Ulam-Tsingou chains","summarize: The dynamical stability of solitary lattice waves in non-integrable FPUT chains is a longstanding open problem and has been solved so far only in a certain asymptotic regime, namely by Friesecke and Pego for the KdV limit, in which the waves propagate with near sonic speed, have large wave length, and carry low energy. In this paper we derive a similar result in a complementary asymptotic regime related to fast and strongly localized waves with high energy. In particular, we show that the spectrum of the linearized FPUT operator contains asymptotically no unstable eigenvalues except for the neutral ones that stem from the shift symmetry and the spatial discreteness. This ensures that high-energy waves are linearly stable in some orbital sense, and the corresponding nonlinear stability is granted by the general, non-asymptotic part of the seminal Friesecke-Pego result and the extension by Mizumachi. Our analytical work splits into two principal parts. First we refine two-scale techniques that relate high-energy wave to a nonlinear asymptotic shape ODE and provide accurate approximation formulas. In this way we establish the existence, local uniqueness, smooth parameter dependence, and exponential localization of fast lattice waves for a wide class of interaction potentials with algebraic singularity. Afterwards we study the crucial eigenvalue problem in exponentially weighted spaces, so that there is no unstable essential spectrum. Our key argument is that all proper eigenfunctions can asymptotically be linked to the unique bounded and normalized solution of the linearized shape ODE, and this finally enables us to disprove the existence of unstable eigenfunctions in the symplectic complement of the neutral ones.",0.0],["the filters are designed in the frequency domain via derivative constraints at dc. the filters","Digital filters with vanishing moments for shape analysis","summarize: Shape- and scale-selective digital-filters, with steerable finite\/infinite impulse responses and non-recursive\/recursive realizations, that are separable in both spatial dimensions and adequately isotropic, are derived. The filters are conveniently designed in the frequency domain via derivative constraints at dc, which guarantees orthogonality and monomial selectivity in the pixel domain , unlike more commonly used FIR filters derived from Gaussian functions. A two-stage low-pass\/high-pass architecture, for blur\/derivative operations, is recommended. Expressions for the coefficients of a low-order IIR blur filter with repeated poles are provided, as a function of scale; discrete Butterworth , and colored Savitzky-Golay , blurs are also examined. Parallel software implementations on central processing units and graphics processing units , for scale-selective blob-detection in aerial surveillance imagery, are analyzed. It is shown that recursive IIR filters are significantly faster than non-recursive FIR filters when detecting large objects at coarse scales, i.e. using filters with long impulse responses; however, the margin of outperformance decreases as the degree of parallelization increases.",0.1153846154],["a significant increase in X-ray flux by 1.5 orders of magnitude was observed following the","New changing look case in NGC 1566","summarize: We present a study of optical, UV and X-ray light curves of the nearby changing look active galactic nucleus in the galaxy NGC 1566 obtained with the Neil Gehrels Swift Observatory and the MASTER Global Robotic Network over the period 2007 - 2018. We also report on our optical spectroscopy at the South African Astronomical Observatory with the 1.9-m telescope on the night 2018 August 2-3. A substantial increase in X-ray flux by 1.5 orders of magnitude was observed following the brightening in the UV and optical bands during the last year. After a maximum was reached at the beginning of 2018 July the fluxes in all bands decreased with some fluctuations. The amplitude of the flux variability is strongest in the X-ray band and decreases with increasing wavelength. Low-resolution spectra reveal a dramatic strengthening of the broad emission as well as high-ionization 6374 A lines. These lines were not detected so strongly in the past published spectra. The change in the type of the optical spectrum was accompanied by a significant change in the X-ray spectrum. All these facts confirm NGC 1566 to be a changing look Seyfert galaxy.",0.1818181818],["this paper advocates the use of the distributed compressed sensing paradigm. we consider networks with signal","On the Energy Self-Sustainability of IoT via Distributed Compressed Sensing","summarize: This paper advocates the use of the distributed compressed sensing paradigm to deploy energy harvesting Internet of Thing devices for energy self-sustainability. We consider networks with signal\/energy models that capture the fact that both the collected signals and the harvested energy of different devices can exhibit correlation. We provide theoretical analysis on the performance of both the classical compressive sensing approach and the proposed distributed CS -based approach to data acquisition for EH IoT. Moreover, we perform an in-depth comparison of the proposed DCS-based approach against the distributed source coding system. These performance characterizations and comparisons embody the effect of various system phenomena and parameters including signal correlation, EH correlation, network size, and energy availability level. Our results unveil that, the proposed approach offers significant increase in data gathering capability with respect to the CS-based approach, and offers a substantial reduction of the mean-squared error distortion with respect to the DSC system.",0.1666666667],["RPDEs are a common form of a RPDE. we discuss","Stochastic partial differential equations: a rough path view","summarize: We discuss regular and weak solutions to rough partial differential equations , thereby providing a wise view on important classes of SPDEs. In contrast to many previous works on RPDEs, our definition gives honest meaning to RPDEs as integral equation, based on which we are able to obtain existence, uniqueness and stability results. The case of weak rough forward equations, may be seen as robustification of the Zakai equation in the rough path sense. Feynman-Kac representation for RPDEs, in formal analogy to similar classical results in SPDE theory, play an important role.",0.4],["topological insulators are materials that have a gapped bulk energy spectrum. they","Quantum interference of topological states of light","summarize: Topological insulators are materials that have a gapped bulk energy spectrum, but contain protected in-gap states appearing at their surface. These states exhibit remarkable properties such as unidirectional propagation and robustness to noise that offer an opportunity to improve the performance and scalability of quantum technologies. For quantum applications, it is essential that the topological states are indistinguishable. Here we report high-visibility quantum interference of single photon topological states in an integrated photonic circuit. Two topological boundary-states, initially at opposite edges of a coupled waveguide array, are brought into proximity, where they interfere and undergo a beamsplitter operation. We observe ",0.2],["the two layers are represented by current sheets embedded in three dielectrics. the dynamics of","The role of the time delay in the reflection and transmission of ultrashort electromagnetic pulses on a system of parallel current sheets","summarize: The reflection and transmission of a few-cycle laser pulse impinging on two parallel thin metal layers have been analyzed. The two layers, with a thickness much smaller than the skin depth of the incoming radiation field, are represented by current sheets embedded in three dielectrics, all with different index of refraction. The dynamics of the surface currents and the scattered radiation field are described by the coupled system of Maxwell-Lorentz equations. When applying the plane wave modeling assumptions, these reduce to a hybrid system of two delay differential equations for the electron motion in the layers and a recurrence relation for the scattered field. The solution is given as the limit of a singularly perturbed system and the effects of the time delay on the dynamics is analyzed.",0.295100746],["new methodology involves essentially the introduction of a time window which works as a temporal","Towards Evidence of Long-Range Correlations in Shallow Seismic Activities","summarize: In this work, we introduce a new methodology to construct a network of epicenters that avoids problems found in well-established methodologies when they are applied to global catalogs of earthquakes located in shallow zones. The new methodology involves essentially the introduction of a time window which works as a temporal filter. Our approach is more generic and for small regions the results coincide with previous findings. The network constructed with that model has small-world properties and the distribution of node connectivity follows a non-traditional function, namely a q-exponential, where scale-free properties are present. The vertices with larger connectivity in the network correspond to the areas with very intense seismic activities in the period considered. These new results strengthen the hypothesis of long spatial and temporal correlations between earthquakes.",0.3913043478],["fore-aft asymmetry favors front neighbors changes qualitatively the phase diagram","Fore-aft asymmetric flocking","summarize: We show that fore-aft asymmetry, a generic feature of living organisms and some active matter systems, can have a strong influence on the collective properties of even the simplest flocking models. Specifically, an arbitrarily weak asymmetry favoring front neighbors changes qualitatively the phase diagram of the Vicsek model. A region where many sharp traveling band solutions coexist is present at low noise strength, below the Toner-Tu liquid, at odds with the phase-separation scenario well describing the usual isotropic model. Inside this region, a `banded liquid' phase with algebraic density distribution coexists with band solutions. Linear stability analysis at the hydrodynamic level suggests that these results are generic and not specific to the Vicsek model.",0.0],["reversible data hiding in encrypted image is calculated and analyzed. the prediction errors","Reversible Data Hiding in Encrypted Images using Local Difference of Neighboring Pixels","summarize: This paper presents a reversible data hiding in encrypted image , which divides image into non-overlapping blocks. In each block, central pixel of the block is considered as leader pixel and others as follower ones. The prediction errors between the intensity of follower pixels and leader ones are calculated and analyzed to determine a feature for block embedding capacity. This feature indicates the amount of data that can be embedded in a block. Using this pre-process for whole blocks, we vacate rooms before the encryption of the original image to achieve high embedding capacity. Also, using the features of all blocks, embedded data is extracted and the original image is perfectly reconstructed at the decoding phase. In effect, comparing to existent RDHEI algorithms, embedding capacity is significantly increased in the proposed algorithm. Experimental results confirm that the proposed algorithm outperforms state of the art ones.",0.1428571429],["naive solutions for polynomially solvable problems have been made in recent","On problems equivalent to -convolution","summarize: In recent years, significant progress has been made in explaining the apparent hardness of improving upon the naive solutions for many fundamental polynomially solvable problems. This progress has come in the form of conditional lower bounds -- reductions from a problem assumed to be hard. The hard problems include 3SUM, All-Pairs Shortest Path, SAT, Orthogonal Vectors, and others. In the ",0.0909090909],["tool is used to debug the output and attention weights of neural machine translation systems.","Debugging Neural Machine Translations","summarize: In this paper, we describe a tool for debugging the output and attention weights of neural machine translation systems and for improved estimations of confidence about the output based on the attention. The purpose of the tool is to help researchers and developers find weak and faulty example translations that their NMT systems produce without the need for reference translations. Our tool also includes an option to directly compare translation outputs from two different NMT engines or experiments. In addition, we present a demo website of our tool with examples of good and bad translations: http:\/\/attention.lielakeda.lv",0.0],["this work introduces the class of generalized linear-quadratic functions. we consider","Epiconvergence, the Moreau envelope and generalized linear-quadratic functions","summarize: This work introduces the class of generalized linear-quadratic functions, constructed using maximally monotone symmetric linear relations. Calculus rules and properties of the Moreau envelope for this class of functions are developed. In finite dimensions, on a metric space defined by Moreau envelopes, we consider the epigraphical limit of a sequence of quadratic functions and categorize the results. We explore the question of when a quadratic function is a Moreau envelope of a generalized linear-quadratic function; characterizations involving nonexpansiveness and Lipschitz continuity are established. This work generalizes some results by Hiriart-Urruty and by Rockafellar and Wets.",0.2727272727],["pairwise completely positive matrices are a set of completely positive matrices","Pairwise Completely Positive Matrices and Conjugate Local Diagonal Unitary Invariant Quantum States","summarize: We introduce a generalization of the set of completely positive matrices that we call pairwise completely positive matrices. These are pairs of matrices that share a joint decomposition so that one of them is necessarily positive semidefinite while the other one is necessarily entrywise non-negative. We explore basic properties of these matrix pairs and develop several testable necessary and sufficient conditions that help determine whether or not a pair is PCP. We then establish a connection with quantum entanglement by showing that determining whether or not a pair of matrices is pairwise completely positive is equivalent to determining whether or not a certain type of quantum state, called a conjugate local diagonal unitary invariant state, is separable. Many of the most important quantum states in entanglement theory are of this type, including isotropic states, mixed Dicke states , and maximally correlated states. As a specific application of our results, we show that a wide family of states that have absolutely positive partial transpose are in fact separable.",0.2174049324],["immersed boundary method uses elements of lubrication theory to resolve thin fluid layers between immer","Lubricated Immersed Boundary Method in Two Dimensions","summarize: Many biological examples of fluid-structure interaction, including the transit of red blood cells through the narrow slits in the spleen and the intracellular trafficking of vesicles into dendritic spines, involve the near-contact of elastic structures separated by thin layers of fluid. Motivated by such problems, we introduce an immersed boundary method that uses elements of lubrication theory to resolve thin fluid layers between immersed boundaries. We demonstrate 2nd-order accurate convergence for simple two-dimensional flows with known exact solutions to showcase the increased accuracy of this method compared to the standard immersed boundary method. Motivated by the phenomenon of wall-induced migration, we apply the lubricated immersed boundary method to simulate an elastic vesicle near a wall in shear flow. We also simulate the dynamics of a vesicle traveling through a narrow channel and observe the ability of the lubricated method to capture the vesicle motion on relatively coarse fluid grids.",0.0],["a fetal sheep astrocyte culture re-exposed to LPS in","Alpha7 nicotinic acetylcholine receptor signaling modulates ovine fetal brain astrocytes transcriptome in response to endotoxin","summarize: Neuroinflammation in utero may result in lifelong neurological disabilities. Astrocytes play a pivotal role, but the mechanisms are poorly understood. No early postnatal treatment strategies exist to enhance neuroprotective potential of astrocytes. We hypothesized that agonism on alpha7 nicotinic acetylcholine receptor in fetal astrocytes will augment their neuroprotective transcriptome profile, while the antagonistic stimulation of alpha7nAChR will achieve the opposite. Using an in vivo - in vitro model of developmental programming of neuroinflammation induced by lipopolysaccharide , we validated this hypothesis in primary fetal sheep astrocytes cultures re-exposed to LPS in presence of a selective alpha7nAChR agonist or antagonist. Our RNAseq findings show that a pro-inflammatory astrocyte transcriptome phenotype acquired in vitro by LPS stimulation is reversed with alpha7nAChR agonistic stimulation. Conversely, antagonistic alpha7nAChR stimulation potentiates the pro-inflammatory astrocytic transcriptome phenotype. Furthermore, we conduct a secondary transcriptome analysis against the identical alpha7nAChR experiments in fetal sheep primary microglia cultures and discuss the implications for neurodevelopment.",0.3267199848],["shuttle buses have been a popular means to move commuters sharing similar origins and destinations","A Visual Analytics Approach to Scheduling Customized Shuttle Buses via Perceiving Passengers' Travel Demands","summarize: Shuttle buses have been a popular means to move commuters sharing similar origins and destinations during periods of high travel demand. However, planning and deploying reasonable, customized service bus systems becomes challenging when the commute demand is rather dynamic. It is difficult, if not impossible to form a reliable, unbiased estimation of user needs in such a case using traditional modeling methods. We propose a visual analytics approach to facilitating assessment of actual, varying travel demands and planning of night customized shuttle systems. A preliminary case study verifies the efficacy of our approach.",0.4090909091],["sensorimotor controls are mainly based on information gathered from vision, proprio","Towards the Enhancement of Body Standing Balance Recovery by Means of a Wireless Audio-Biofeedback System","summarize: Human maintain their body balance by sensorimotor controls mainly based on information gathered from vision, proprioception and vestibular systems. When there is a lack of information, caused by pathologies, diseases or aging, the subject may fall. In this context, we developed a system to augment information gathering, providing the subject with warning audio-feedback signals related to his\/her equilibrium. The system comprises an inertial measurement unit , a data processing unit, a headphone audio device and a software application. The IMU is a low-weight, small-size wireless instrument that, body-back located between the L2 and L5 lumbar vertebrae, measures the subject's trunk kinematics. The application drives the data processing unit to feeding the headphone with electric signals related to the kinematic measures. Consequently, the user is audio-alerted, via headphone, of his\/her own equilibrium, hearing a pleasant sound when in a stable equilibrium, or an increasing bothering sound when in an increasing unstable condition. Tests were conducted on a group of six older subjects and a group of four young subjects to underline difference in effectiveness of the system, if any, related to the age of the users. For each subject, standing balance tests were performed in normal or altered conditions, such as, open or closed eyes, and on a solid or foam surface The system was evaluated in terms of usability, reliability, and effectiveness in improving the subject's balance in all conditions. As a result, the system successfully helped the subjects in reducing the body swaying within 10.65%-65.90%, differences depending on subjects' age and test conditions.",0.0],["the multimessenger discovery of the merger of two neutron stars on august 17, 2017 is in","Discovery of the neutron stars merger GW170817\/GRB170817A and Binary Stellar Evolution","summarize: The Multimessenger discovery of the merger of two neutron stars on August 17, 2017, GW170817 \/ GRB170817A, accompanied by a gamma-ray burst and an optical kilonova, is a triumph of the ideas about the evolution of the baryon component in the Universe. Despite the current uniqueness of this observation, the obtained variety of experimental data makes it possible right now to draw important theoretical conclusions about the origin of the double neutron star, their merger, and the subsequent flare-up of the electromagnetic radiation. We present that the discovery of the merger at a distance of 40 Mpc is in full agreement with the very first calculations of the Scenario Machine . In modern terms, the predicted rate is ~ 10 000 Gpc-3.",0.5],["given an initial convex polygon with convex polygon with convex poly","A universal result for consecutive random subdivision of polygons","summarize: We consider consecutive random subdivision of polygons described as follows. Given an initial convex polygon with ",0.1739130435],["a neutron star crust is shown to yielding response to smooth, unbalanced","Global Crustal Dynamics of Magnetars in Relation to their Bright X-ray Outbursts","summarize: This paper considers the yielding response of a neutron star crust to smooth, unbalanced Maxwell stresses imposed at the core-crust boundary, and the coupling of the dynamic crust to the external magnetic field. Stress buildup and yielding in a magnetar crust is a global phenomenon: an elastic distortion radiating from one plastically deforming zone is shown to dramatically increase the creep rate in distant zones. Runaway creep to dynamical rates is shown to be possible, being enhanced by in situ heating and suppressed by thermal conduction and shearing of an embedded magnetic field. A global and time-dependent model of elastic, plastic, magnetic, and thermal evolution is developed. Fault-like structures develop naturally, and a range of outburst timescales is observed. Transient events with time profiles similar to giant magnetar flares result from runaway creep that starts in localized sub-km-sized patches and spreads across the crust. A one-dimensional model of stress relaxation in the vertically stratified crust shows that a modest increase in applied stress allows embedded magnetic shear to escape the star over ",0.3529411765],["vision-based localization has demonstrated superior performance to other localization methods. a scheme","Cooperative Vision-based Localization Networks with Communication Constraints","summarize: Accurate location information is indispensable for the emerging applications of \\ac, such as automatic driving and formation control. In the real scenario, vision-based localization has demonstrated superior performance to other localization methods for its stability and flexibility. In this paper, a scheme of cooperative vision-based localization with communication constraints is proposed. Vehicles collect images of the environment and distance measurements between each other. Then vehicles transmit the coordinates of feature points and distances with constrained bits to the edge to estimate their positions. The \\ac for absolute localization is first obtained, based on which we derive the relative \\ac through subspace projection. Furthermore, we formulate the corresponding bit allocation problem for relative localization. Finally, a \\ac algorithm is developed by considering the influence of photographing, distance measurements and quantization noises. Compared with conventional bit allocation methods, numerical results demonstrate the localization performance gain of our proposed algorithm with higher computational efficiency.",0.2727272727],["theorem describes the convergence of an infinite array of variants of SGD.","SGD: General Analysis and Improved Rates","summarize: We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form mini-batches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.",0.1],["tight continuity bounds obtained for quantum mutual information and relative entropy of entang","Lower bounds on distances between a given quantum channel and certain classes of channels","summarize: The tight, in a sense, lower estimates of diamond-norm distance from a given quantum channel to the sets of degradable, antidegradable and entanglement-breaking channels are obtained via the tight continuity bounds for quantum mutual information and for relative entropy of entanglement in finite-dimensional case. As an auxiliary result there are established lower bounds of trace-norm distance from a given bipartite state to the set of all separable states.",0.2645603082],["a synthesis technique of heat treatment is developed to grow 2D graphene oxide sheet","Facile synthesis of 2D graphene oxide sheet enveloping ultrafine 1D LiMn2O4 as interconnected framework to enhance cathodic property for Li-ion battery","summarize: Cubic spinel lithium manganese oxide has been able to attract a great deal of attention over the years as a promising cathode material for large scale lithium ion batteries. Here a facile hydrothermal route followed by solid state reaction is developed using as grown ultrafine alpha-MnO2 nanorods to prepare one dimensional LiMn2O4 with 10-50nm diameters. To enhance the cathodic property of these nanorods, a unique synthesis technique of heat treatment is developed to grow 2D graphene oxide sheet enveloping 1D LiMn2O4 as interconnected framework. This nanocomposite 3D porous cathode exhibits a high specific charge capacity of 130mAh\/g at 0.05C rate and Coulombic efficiency of about 98% after 100 cycles in the potential window of 3.5 to 4.3V versus Li\/Li+ with promising initial charge capacity retention of about 87%, and outstanding structural stability even after 100 cycles. Enhancement in the lithiation and delithiation processes leading to improved performance is likely to have its origin in the 2D conducting graphene oxide sheets. It allows for decreasing the Mn dissolution, improve the electron conductivity and reduce the Li-ion path diffusion inside the favourable morphology and crystallinity of the ultrafine 1D LiMn2O4 nanorods, giving rise to a promising cathode nanocomposite.",0.4043537731],["NP-completed.","Turing Kernelization for Finding Long Paths and Cycles in Restricted Graph Classes","summarize: The NP-complete ",0.0],["rowland et al. analysed the C51 algorithm in terms of the C","Distributional reinforcement learning with linear function approximation","summarize: Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. 's analysis of the C51 algorithm in terms of the Cram\\'er distance, but their results only apply to the tabular setting and ignore C51's use of a softmax to produce normalized distributions. In this paper we adapt the Cram\\'er distance to deal with arbitrary vectors. From it we derive a new distributional algorithm which is fully Cram\\'er-based and can be combined to linear function approximation, with formal guarantees in the context of policy evaluation. In allowing the model's prediction to be any real vector, we lose the probabilistic interpretation behind the method, but otherwise maintain the appealing properties of distributional approaches. To the best of our knowledge, ours is the first proof of convergence of a distributional algorithm combined with function approximation. Perhaps surprisingly, our results provide evidence that Cram\\'er-based distributional methods may perform worse than directly approximating the value function.",0.1176470588],["proposed scheme of ion beams in the CERN accelerator complex. it is","High-luminosity Large Hadron Collider with laser-cooled isoscalar ion beams","summarize: This paper presents one of the case studies of the Gamma Factory initiative -- a proposal of a new operation scheme of ion beams in the CERN accelerator complex. Its goal is to extend the scope and precision of the LHC-based research by complementing the proton-proton collision programme with the high-luminosity nucleus-nucleus one. Its numerous physics highlights include studies of the exclusive Higgs-boson production in photon-photon collisions and precision measurements of the electroweak parameters. There are two principal ways to increase the LHC luminosity which do not require an upgrade of the CERN injectors: modification of the beam-collision optics and reduction of the transverse emittance of the colliding beams. The former scheme is employed by the ongoing high-luminosity project. The latter one, applicable only to ion beams, is proposed in this paper. It is based on laser cooling of bunches of partially stripped ions at the SPS flat-top energy. For isoscalar calcium beams, which fulfil the present beam-operation constrains and which are particularly attractive for the EW physics, the transverse beam emittance can be reduced by a factor of ",0.4166666667],["in this paper we show the existence of the generalized Eberlein decomposition for","On the Fourier Analysis of Measures with Meyer Set Support","summarize: In this paper we show the existence of the generalized Eberlein decomposition for Fourier transformable measures with Meyer set support. We prove that each of the three components is also Fourier transformable and has Meyer set support. We obtain that each of the pure point, absolutely continuous and singular continuous components of the Fourier transform is a strong almost periodic measure, and hence is either trivial or has relatively dense support. We next prove that the Fourier transform of a measure with Meyer set support is norm almost periodic, and hence so is each of the pure point, absolutely continuous and singular continuous components. We show that a measure with Meyer set support is Fourier transformable if and only if it is a linear combination of positive definite measures, which can be chosen with Meyer set support, solving a particular case of an open problem. We complete the paper by discussing some applications to the diffraction of weighted Dirac combs with Meyer set support.",0.1875],["a fusion power plant is based on a realistic design. a","Spatial heterogeneity of W transmutation in a fusion device","summarize: Accurately quantifying the transmutation rate of tungsten under neutron irradiation is a necessary requirement in the assessment of its performance as an armour material in a fusion power plant. The usual approach of calculating average responses, assuming large, homogenised material volumes, is insufficient to capture the full complexity of the transmutation picture in the context of a realistic fusion power plant design, particularly for rhenium production from W. Combined neutron transport and inventory simulations for representative models of a fusion power plant show that the production rate of Re is strongly influenced by the local spatial environment. Localised variation in neutron moderation due to structural steel and coolant, particularly water, can dramatically increase Re production because of the huge cross sections of giant resolved resonances in the neutron-capture reaction of \\W at low neutron energies. Calculations using cross section data corrected for temperature effects suggest that temperature may have a relatively lesser influence on transmutation rates.",0.6428571429],["the challenge culture is a deeply rooted online phenomenon. authorities around the world have","Online Suicide Games: A Form of Digital Self-harm or A Myth?","summarize: Online suicide games are claimed to involve a series of challenges, ending in suicide. A whole succession of these such as the Blue Whale Challenge, Momo, the Fire Fairy and Doki Doki have appeared in recent years. The challenge culture is a deeply rooted online phenomenon, whether the challenge is dangerous or not, while social media particularly motivates youngsters to take part because of their desire for attention. Although there is no evidence that the suicide games are real, authorities around the world have reacted by releasing warnings and creating information campaigns to warn youngsters and parents. We interviewed teachers, child protection experts and NGOs, conducted a systematic review of historical news reports from 2015-2019 and searched police and other authority websites to identify relevant warning releases. We then synthesized the existing knowledge on the suicide games phenomenon. A key finding of our work is that media, social media and warning releases by authorities are mainly just serving to spread the challenge culture and exaggerate fears regarding online risk.",0.15],["exact solutions with the initial conditions are presented in the cubic duffing equation.","Exact Solutions to Cubic Duffing Equation by Leaf Functions Under Free Vibration","summarize: Exact solutions with the initial conditions are presented in the cubic duffing equation. These exact solutions are expressed in terms of the leaf function and the trigonometric function. The leaf functions: ",0.0588235294],["a special class of standard gaussian Autoregressive Hilbertian processes of","Classical and bayesian componentwise predictors for non-compact correlated ARH processes","summarize: A special class of standard Gaussian Autoregressive Hilbertian processes of order one processes), with bounded linear autocorrelation operator, which does not satisfy the usual Hilbert-Schmidt assumption, is considered. To compensate the slow decay of the diagonal coefficients of the autocorrelation operator, a faster decay velocity of the eigenvalues of the trace autocovariance operator of the innovation process is assumed. As usual, the eigenvectors of the autocovariance operator of the ARH process are considered for projection, since, here, they are assumed to be known. Diagonal componentwise classical and bayesian estimation of the autocorrelation operator is studied for prediction. The asymptotic efficiency and equivalence of both estimators is proved, as well as of their associated componentwise ARH plugin predictors. A simulation study is undertaken to illustrate the theoretical results derived.",0.4210526316],["tech and automobile companies investing huge amounts of capital in research and development of SAVs.","Coverage based testing for V&V and Safety Assurance of Self-driving Autonomous Vehicles: A Systematic Literature Review","summarize: Self-driving Autonomous Vehicles are gaining more interest each passing day by the industry as well as the general public. Tech and automobile companies are investing huge amounts of capital in research and development of SAVs to make sure they have a head start in the SAV market in the future. One of the major hurdles in the way of SAVs making it to the public roads is the lack of confidence of public in the safety aspect of SAVs. In order to assure safety and provide confidence to the public in the safety of SAVs, researchers around the world have used coverage-based testing for Verification and Validation and safety assurance of SAVs. The objective of this paper is to investigate the coverage criteria proposed and coverage maximizing techniques used by researchers in the last decade up till now, to assure safety of SAVs. We conduct a Systematic Literature Review for this investigation in our paper. We present a classification of existing research based on the coverage criteria used. Several research gaps and research directions are also provided in this SLR to enable further research in this domain. This paper provides a body of knowledge in the domain of safety assurance of SAVs. We believe the results of this SLR will be helpful in the progression of V&V and safety assurance of SAVs.",0.26728771],["a new framework should be designed for VRU safety applications. the framework should be designed","Implementation and Evaluation of a Cooperative Vehicle-to-Pedestrian Safety Application","summarize: While the development of Vehicle-to-Vehicle safety applications based on Dedicated Short-Range Communications has been extensively undergoing standardization for more than a decade, such applications are extremely missing for Vulnerable Road Users . Nonexistence of collaborative systems between VRUs and vehicles was the main reason for this lack of attention. Recent developments in Wi-Fi Direct and DSRC-enabled smartphones are changing this perspective. Leveraging the existing V2V platforms, we propose a new framework using a DSRC-enabled smartphone to extend safety benefits to VRUs. The interoperability of applications between vehicles and portable DSRC enabled devices is achieved through the SAE J2735 Personal Safety Message . However, considering the fact that VRU movement dynamics, response times, and crash scenarios are fundamentally different from vehicles, a specific framework should be designed for VRU safety applications to study their performance. In this article, we first propose an end-to-end Vehicle-to-Pedestrian framework to provide situational awareness and hazard detection based on the most common and injury-prone crash scenarios. The details of our VRU safety module, including target classification and collision detection algorithms, are explained next. Furthermore, we propose and evaluate a mitigating solution for congestion and power consumption issues in such systems. Finally, the whole system is implemented and analyzed for realistic crash scenarios.",0.2142857143],["neural networks are generally designed as a stack of differentiable layers. they are designed","Why should we add early exits to neural networks?","summarize: Deep neural networks are generally designed as a stack of differentiable layers, in which a prediction is obtained only after running the full stack. Recently, some contributions have proposed techniques to endow the networks with early exits, allowing to obtain predictions at intermediate points of the stack. These multi-output networks have a number of advantages, including: significant reductions of the inference time, reduced tendency to overfitting and vanishing gradients, and capability of being distributed over multi-tier computation platforms. In addition, they connect to the wider themes of biological plausibility and layered cognitive reasoning. In this paper, we provide a comprehensive introduction to this family of neural networks, by describing in a unified fashion the way these architectures can be designed, trained, and actually deployed in time-constrained scenarios. We also describe in-depth their application scenarios in 5G and Fog computing environments, as long as some of the open research questions connected to them.",0.1923076923],["MCTS is one of the most widely used methods for planning. it is used to","Static and Dynamic Values of Computation in MCTS","summarize: Monte-Carlo Tree Search is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the myopic limitations of existing computation-value-based methods in two senses: we are able to account for the impact of non-immediate computations on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art.",0.1052631579],["this paper considers the distributed optimization problem over a network. the objective is to optimize","Accelerated Distributed Nesterov Gradient Descent","summarize: This paper considers the distributed optimization problem over a network, where the objective is to optimize a global function formed by a sum of local functions, using only local computation and communication. We develop an Accelerated Distributed Nesterov Gradient Descent method. When the objective function is convex and ",0.1428571429],["the seidel spectrum of a graph is determined by distance. it is known that","Complete multipartite graphs that are determined, up to switching, by their Seidel spectrum","summarize: It is known that complete multipartite graphs are determined by their distance spectrum but not by their adjacency spectrum. The Seidel spectrum of a graph ",0.5238095238],["we consider continuous-time stochastic optimal control problems. the problem is a bi","Optimal Control of Conditional Value-at-Risk in Continuous Time","summarize: We consider continuous-time stochastic optimal control problems featuring Conditional Value-at-Risk in the objective. The major difficulty in these problems arises from time-inconsistency, which prevents us from directly using dynamic programming. To resolve this challenge, we convert to an equivalent bilevel optimization problem in which the inner optimization problem is standard stochastic control. Furthermore, we provide conditions under which the outer objective function is convex and differentiable. We compute the outer objective's value via a Hamilton-Jacobi-Bellman equation and its gradient via the viscosity solution of a linear parabolic equation, which allows us to perform gradient descent. The significance of this result is that we provide an efficient dynamic programming-based algorithm for optimal control of CVaR without lifting the state-space. To broaden the applicability of the proposed algorithm, we propose convergent approximation schemes in cases where our key assumptions do not hold and characterize relevant suboptimality bounds. In addition, we extend our method to a more general class of risk metrics, which includes mean-variance and median-deviation. We also demonstrate a concrete application to portfolio optimization under CVaR constraints. Our results contribute an efficient framework for solving time-inconsistent CVaR-based sequential optimization.",0.2666666667],["the method includes a mapping module, which incrementally creates a topological map of","Incremental Semantic Mapping with Unsupervised On-line Learning","summarize: This paper introduces an incremental semantic mapping approach, with on-line unsupervised learning, based on Self-Organizing Maps for robotic agents. The method includes a mapping module, which incrementally creates a topological map of the environment, enriched with objects recognized around each topological node, and a module of places categorization, endowed with an incremental unsupervised learning SOM with on-line training. The proposed approach was tested in experiments with real-world data, in which it demonstrates promising capabilities of incremental acquisition of topological maps enriched with semantic information, and for clustering together similar places based on this information. The approach was also able to continue learning from newly visited environments without degrading the information previously learned.",0.3076923077],["conformal immersions are a must-see for conformal immersions.","Dirac Tori","summarize: We consider conformal immersions ",0.0588235294],["the density matrix and the HFX matrix are both stored in the sparse format","The dynamic parallel distribution algorithm for hybrid density-functional calculations in HONPAS package","summarize: This work presents a dynamic parallel distribution scheme for the Hartree-Fock exchange~ calculations based on the real-space NAO2GTO framework. The most time-consuming electron repulsion integrals~ calculation is perfectly load-balanced with 2-level master-worker dynamic parallel scheme, the density matrix and the HFX matrix are both stored in the sparse format, the network communication time is minimized via only communicating the index of the batched ERIs and the final sparse matrix form of the HFX matrix. The performance of this dynamic scalable distributed algorithm has been demonstrated by several examples of large scale hybrid density-functional calculations on Tianhe-2 supercomputers, including both molecular and solid states systems with multiple dimensions, and illustrates good scalability.",0.0909090909],["a new system is proposed to increase the number of review-response pairs","App-Aware Response Synthesis for User Reviews","summarize: Responding to user reviews promptly and satisfactorily improves application ratings, which is key to application popularity and success. The proliferation of such reviews makes it virtually impossible for developers to keep up with responding manually. To address this challenge, recent work has shown the possibility of automatic response generation. However, because the training review-response pairs are aggregated from many different apps, it remains challenging for such models to generate app-specific responses, which, on the other hand, are often desirable as apps have different features and concerns. Solving the challenge by simply building a model per app may be insufficient because individual apps have limited review-response pairs, and such pairs typically lack the relevant information needed to respond to a new review. To enable app-specific response generation, this work proposes AARSynth: an app-aware response synthesis system. The key idea behind AARSynth is to augment the seq2seq model with information specific to a given app. Given a new user review, it first retrieves the top-K most relevant app reviews and the most relevant snippet from the app description. The retrieved information and the new user review are then fed into a fused machine learning model that integrates the seq2seq model with a machine reading comprehension model. The latter helps digest the retrieved reviews and app description. Finally, the fused model generates a response that is customized to the given app. We evaluated AARSynth using a large corpus of reviews and responses from Google Play. The results show that AARSynth outperforms the state-of-the-art system by 22.2% on BLEU-4 score. Furthermore, our human study shows that AARSynth produces a statistically significant improvement in response quality compared to the state-of-the-art system.",0.2142857143],["a model is proposed based on artificial Neural networks. the model is based","A hybrid neural network model based on improved PSO and SA for bankruptcy prediction","summarize: Predicting firm's failure is one of the most interesting subjects for investors and decision makers. In this paper, a bankruptcy prediction model is proposed based on Artificial Neural networks . Taking into consideration that the choice of variables to discriminate between bankrupt and non-bankrupt firms influences significantly the model's accuracy and considering the problem of local minima, we propose a hybrid ANN based on variables selection techniques. Moreover, we evolve the convergence of Particle Swarm Optimization by proposing a training algorithm based on an improved PSO and Simulated Annealing. A comparative performance study is reported, and the proposed hybrid model shows a high performance and convergence in the context of missing data.",0.3472354045],["nuclear magnetic resonance experiment aims to premagnetize liquid samples. liquid samples of","Fast switching coil system for sample premagnetization in an unshielded ultra-low-field Nuclear Magnetic Resonance experiment","summarize: We present a system developed to premagnetize liquid samples in an ultra-low-field Nuclear Magnetic Resonance experiment. Liquid samples of a few milliliter are exposed to a magnetic field of about 70~mT, which is abruptely switched off, so to leave a transverse microtesla field where nuclei start precessing. An accurate characterization of the transients and intermediate field level enables a reliable operation of the detection system, which is based on an optical magnetometer.",0.0486750489],["x1 family is the standard paradigm of orbital motion in galactic bars. the","The orbital content of bars: The origin of 'non-x1-tree', bar-supporting orbits","summarize: Recently, many orbital studies in barred galaxy potentials have revealed the existence of orbits which are not trapped around x1 tree orbits, but could be potentially appropriate building blocks for bars. These findings question the uniqueness of the x1 family as the standard paradigm of orbital motion in galactic bars. The main goal of this paper is to investigate the role that such orbits could play in shaping the morphology of bars. We trace the morphological patterns appearing in the face-on and edge-on views of the non-periodic orbits presented in these studies and we show that they are introduced in the system by second type bifurcations of x1. For this purpose, we use a typical 3D Ferrers bar model and follow the radial and vertical bifurcations of the x1 family considered as being multi periodic, with multi = 2, 3, 5. The variation of the stability indices of x1 in the multi = 2, 3 cases give us also the four- and six- periodic orbits, respectively. We tabulate these orbits including all information necessary to assess their role as appropriate building blocks. We discuss their stability and their extent, as well as their size and morphological evolution, as a function of energy. We conclude that even the most important of the multi-periodic orbits presented in Tables 2 to 5 are less appropriate building blocks for bars than the families of the x1 tree at the same energy.",0.3333333333],["ternary TiSiN exhibits a very interesting family of binary transition metal","Substrate Mediated Synthesis of Ti-Si-N Nano-and-Micro Structures for Optoelectronic Applications","summarize: Being one of the strongest materials, ternary TiSiN exhibits a very interesting family of binary transition metal nitride and silicide systems. A novel technique to fabricate morphologically fascinating nano and micro structures of TiSiN is reported here. The referred TiSiN films, majorly constituted with cubic TiN phase, are enriched with crystalline nanoparticles, micro-flowers and faceted micro-crystals which possess attractive functionalities towards plasmon mediated optoelectronic applications. Reactivity of titanium to silicon nitride based dielectric topping on the substrate at high temperature plays the key role in nitride formation for the demonstrated protocol. The optoelectronic response for these morphologically enriched composite films indicates an influential role of photo-induced surface plasmon polaritons on their dc transport properties. A plasmonically tuned resistive switching, controlled by the surface morphology in association with the film thickness, is observed under light illumination. Using Drudes modified frequency dependent bulk electron scattering rates and surface mediated SPPs-electron scattering rates, a generic model is proposed for addressing unambiguously the increased device resistance in response to light. The featured synthesis process opens a new direction towards the growth of transition metal nitrides while the proposed model serves as a basic platform to understand photo-induced electron scattering mechanisms in metal.",0.375],["open domain keyphrase extraction dataset contains nearly one hundred thousand web documents. BLING-","Open Domain Web Keyphrase Extraction Beyond Language Modeling","summarize: This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.",0.0],["plasmonic slot waveguides are made from an anisotropic metamaterial core","Spatial nonlinearity in anisotropic metamaterial plasmonic slot waveguides","summarize: We study the main nonlinear solutions of plasmonic slot waveguides made from an anisotropic metamaterial core with a positive Kerr-type nonlinearity surrounded by two semi-infinite metal regions. First, we demonstrate that for a highly anisotropic diagonal elliptical core, the bifurcation threshold of the asymmetric mode is reduced from GW\/m threshold for the isotropic case to 50 MW\/m one indicating a strong enhancement of the spatial nonlinear effects, and that the slope of the dispersion curve of the asymmetric mode stays positive, at least near the bifurcation, suggesting a stable mode. Second, we show that for the hyperbolic case there is no physically meaningful asymmetric mode, and that the sign of the effective nonlinearity can become negative.",0.5454545455],["present paper constructs two classes of non-weight modules. non-weight modules are","Two classes of non-weight modules over the twisted Heisenberg-Virasoro algebra","summarize: In the present paper, we construct two classes of non-weight modules ",0.4],["study uses questionnaire surveys with sparse samples and non-individual level statistical data.","CD-CNN: A Partially Supervised Cross-Domain Deep Learning Model for Urban Resident Recognition","summarize: Driven by the wave of urbanization in recent decades, the research topic about migrant behavior analysis draws great attention from both academia and the government. Nevertheless, subject to the cost of data collection and the lack of modeling methods, most of existing studies use only questionnaire surveys with sparse samples and non-individual level statistical data to achieve coarse-grained studies of migrant behaviors. In this paper, a partially supervised cross-domain deep learning model named CD-CNN is proposed for migrant\/native recognition using mobile phone signaling data as behavioral features and questionnaire survey data as incomplete labels. Specifically, CD-CNN features in decomposing the mobile data into location domain and communication domain, and adopts a joint learning framework that combines two convolutional neural networks with a feature balancing scheme. Moreover, CD-CNN employs a three-step algorithm for training, in which the co-training step is of great value to partially supervised cross-domain learning. Comparative experiments on the city Wuxi demonstrate the high predictive power of CD-CNN. Two interesting applications further highlight the ability of CD-CNN for in-depth migrant behavioral analysis.",0.0],["graph product multilayer networks are a family of multilayer networks. the networks can be","Graph Product Multilayer Networks: Spectral Properties and Applications","summarize: This paper aims to establish theoretical foundations of graph product multilayer networks , a family of multilayer networks that can be obtained as a graph product of two or more factor networks. Cartesian, direct , and strong product operators are considered, and then generalized. We first describe mathematical relationships between GPMNs and their factor networks regarding their degree\/strength, adjacency, and Laplacian spectra, and then show that those relationships can still hold for nonsimple and generalized GPMNs. Applications of GPMNs are discussed in three areas: predicting epidemic thresholds, modeling propagation in nontrivial space and time, and analyzing higher-order properties of self-similar networks. Directions of future research are also discussed.",0.1785714286],["identifying who is a child in the audience can be a challenging task.","Age Group Classification with Speech and Metadata Multimodality Fusion","summarize: Children comprise a significant proportion of TV viewers and it is worthwhile to customize the experience for them. However, identifying who is a child in the audience can be a challenging task. Identifying gender and age from audio commands is a well-studied problem but is still very challenging to get good accuracy when the utterances are typically only a couple of seconds long. We present initial studies of a novel method which combines utterances with user metadata. In particular, we develop an ensemble of different machine learning techniques on different subsets of data to improve child detection. Our initial results show a 9.2\\% absolute improvement over the baseline, leading to a state-of-the-art performance.",0.48],["parametric timed pattern matching has strong connections with real-time systems. a log","Online Parametric Timed Pattern Matching with Automata-Based Skipping","summarize: Timed pattern matching has strong connections with monitoring real-time systems. Given a log and a specification containing timing parameters , parametric timed pattern matching aims at exhibiting for which start and end dates, as well as which parameter valuations, a specification holds on that log. This problem is notably close to robustness. We propose here a new framework for parametric timed pattern matching. Not only we dramatically improve the efficiency when compared to a previous method based on parametric timed model checking, but we further propose optimizations based on skipping. Our algorithm is suitable for online monitoring, and experiments show that it is fast enough to be applied at runtime.",0.4444444444],["Let us know what you think about it!","Minimal graded Lie algebras and representations of quadratic algebras","summarize: Let ",0.0],["numerical simulations of Nambu-Goto cosmic strings show that the loop distribution relax","Cosmic string loop production functions","summarize: Numerical simulations of Nambu-Goto cosmic strings in an expanding universe show that the loop distribution relaxes to an universal configuration, the so-called scaling regime, which is of power law shape on large scales. Precise estimations of the power law exponent are, however, still matter of debate while numerical simulations do not incorporate all the radiation and backreaction effects expected to affect the network dynamics at small scales. By using a Boltzmann approach, we show that the steepness of the loop production function with respect to loops size is associated with drastic changes in the cosmological loop distribution. For a scale factor varying as a~t^nu, we find that sub-critical loop production functions, having a Polchinski-Rocha exponent chi = \/2, are shown to be IR-physics dependent and this generically prevents the loop distribution to relax towards scaling. In the latter situation, we discuss the additional regularisations needed for convergence and show that, although a scaling regime can still be reached, the shape of the cosmological loop distribution is modified compared to the naive expectation. Finally, we discuss the implications of our findings.",0.0833333333],["VPA recognize a robust and algorithmically tractable fragment of context-free languages.","Minimization of Visibly Pushdown Automata Using Partial Max-SAT","summarize: We consider the problem of state-space reduction for nondeterministic weakly-hierarchical visibly pushdown automata . VPA recognize a robust and algorithmically tractable fragment of context-free languages that is natural for modeling programs. We define an equivalence relation that is sufficient for language-preserving quotienting of VPA. Our definition allows to merge states that have different behavior, as long as they show the same behavior for reachable equivalent stacks. We encode the existence of such a relation as a Boolean partial maximum satisfiability problem and present an algorithm that quickly finds satisfying assignments. These assignments are sub-optimal solutions to the PMax-SAT problem but can still lead to a significant reduction of states. We integrated our method in the automata-based software verifier Ultimate Automizer and show performance improvements on benchmarks from the software verification competition SV-COMP.",0.3684210526],["task 2 is a task on semantic similarity between clinical cases and discussions. for task","Qwant Research @DEFT 2019: Document matching and information retrieval using clinical cases","summarize: This paper reports on Qwant Research contribution to tasks 2 and 3 of the DEFT 2019's challenge, focusing on French clinical cases analysis. Task 2 is a task on semantic similarity between clinical cases and discussions. For this task, we propose an approach based on language models and evaluate the impact on the results of different preprocessings and matching techniques. For task 3, we have developed an information extraction system yielding very encouraging results accuracy-wise. We have experimented two different approaches, one based on the exclusive use of neural networks, the other based on a linguistic analysis.",0.4516129032],["unbiased Monte Carlo estimator is called unbiased. they call it unbiased","Computing the variance of a conditional expectation via non-nested Monte Carlo","summarize: Computing the variance of a conditional expectation has often been of importance in uncertainty quantification. Sun et al. has introduced an unbiased nested Monte Carlo estimator, which they call ",0.1764705882],["in this paper, we consider the problem of subspace clustering in presence of contiguous","Neither Global Nor Local: A Hierarchical Robust Subspace Clustering For Image Data","summarize: In this paper, we consider the problem of subspace clustering in presence of contiguous noise, occlusion and disguise. We argue that self-expressive representation of data in current state-of-the-art approaches is severely sensitive to occlusions and complex real-world noises. To alleviate this problem, we propose a hierarchical framework that brings robustness of local patches-based representations and discriminant property of global representations together. This approach consists of 1) a top-down stage, in which the input data is subject to repeated division to smaller patches and 2) a bottom-up stage, in which the low rank embedding of local patches in field of view of a corresponding patch in upper level are merged on a Grassmann manifold. This summarized information provides two key information for the corresponding patch on the upper level: cannot-links and recommended-links. This information is employed for computing a self-expressive representation of each patch at upper levels using a weighted sparse group lasso optimization problem. Numerical results on several real data sets confirm the efficiency of our approach.",0.15],["forensics can identify images that are captured by the same camera device. this knowledge can","Device-based Image Matching with Similarity Learning by Convolutional Neural Networks that Exploit the Underlying Camera Sensor Pattern Noise","summarize: One of the challenging problems in digital image forensics is the capability to identify images that are captured by the same camera device. This knowledge can help forensic experts in gathering intelligence about suspects by analyzing digital images. In this paper, we propose a two-part network to quantify the likelihood that a given pair of images have the same source camera, and we evaluated it on the benchmark Dresden data set containing 1851 images from 31 different cameras. To the best of our knowledge, we are the first ones addressing the challenge of device-based image matching. Though the proposed approach is not yet forensics ready, our experiments show that this direction is worth pursuing, achieving at this moment 85 percent accuracy. This ongoing work is part of the EU-funded project 4NSEEK concerned with forensics against child sexual abuse.",0.1444818976],["color sequences can be realized from a fixed point set. we first study the case","Colored ray configurations","summarize: We study the cyclic color sequences induced at infinity by colored rays with apices being a given balanced finite bichromatic point set. We first study the case in which the rays are required to be pairwise disjoint. We derive a lower bound on the number of color sequences that can be realized from any such fixed point set and examine color sequences that can be realized regardless of the point set, exhibiting negative examples as well. We also provide a tight upper bound on the number of configurations that can be realized from a point set, and point sets for which there are asymptotically less configurations than that number. In addition, we provide algorithms to decide whether a color sequence is realizable from a given point set in a line or in general position. We address afterwards the variant of the problem where the rays are allowed to intersect. We prove that for some configurations and point sets, the number of ray crossings must be ",0.1111111111],["we review the geometry of the space of quantum states.","Stratified Manifold of Quantum States, actions of the complex special linear group","summarize: We review the geometry of the space of quantum states ",0.3508846085],["the authors propose a multiple-microphone audio source separation algorithm. the algorithm is","Convolutive Audio Source Separation using Robust ICA and an intelligent evolving permutation ambiguity solution","summarize: Audio source separation is the task of isolating sound sources that are active simultaneously in a room captured by a set of microphones. Convolutive audio source separation of equal number of sources and microphones has a number of shortcomings including the complexity of frequency-domain ICA, the permutation ambiguity and the problem's scalabity with increasing number of sensors. In this paper, the authors propose a multiple-microphone audio source separation algorithm based on a previous work of Mitianoudis and Davies . Complex FastICA is substituted by Robust ICA increasing robustness and performance. Permutation ambiguity is solved using two methodologies. The first is using the Likelihood Ration Jump solution, which is now modified to decrease computational complexity in the case of multiple microphones. The application of the MuSIC algorithm, as a preprocessing step to the previous solution, forms a second methodology with promising results.",0.2308586522],["quantum adiabatic algorithm of Farhi emph can find pareto-","Multiobjective Optimization in a Quantum Adiabatic Computer","summarize: In this work we present a quantum algorithm for multiobjective combinatorial optimization. We show how to map a convex combination of objective functions onto a Hamiltonian and then use that Hamiltonian to prove that the quantum adiabatic algorithm of Farhi \\emph can find Pareto-optimal solutions in finite time provided certain convex combinations of objectives are used and the underlying multiobjective problem meets certain restrictions.",0.0],["spin-orbit coupling and centrosymmetric crystal structure are key bottleneck to si spin","Large spin-Hall effect in Si at room temperature","summarize: Silicon's weak intrinsic spin-orbit coupling and centrosymmetric crystal structure are a critical bottleneck to the development of Si spintronics, because they lead to an insignificant spin-Hall effect and inverse spin-Hall effect . Here, we undertake current, magnetic field, crystallography dependent magnetoresistance and magneto thermal transport measurements to study the spin transport behavior in freestanding Si thin films. We observe a large spin-Hall magnetoresistance in both p-Si and n-Si at room temperature and it is an order of magnitude larger than that of Pt. One explanation of the unexpectedly large and efficient spin-Hall effect is spin-phonon coupling instead of spin-orbit coupling. The macroscopic origin of the spin-phonon coupling can be large strain gradients that can exist in the freestanding Si films. This discovery in a light, earth abundant and centrosymmetric material opens a new path of strain engineering to achieve spin dependent properties in technologically highly-developed materials.",0.0769230769],["feature extraction on convolutional neural networks is unfeasible. the aim","Feature discriminativity estimation in CNNs for transfer learning","summarize: The purpose of feature extraction on convolutional neural networks is to reuse deep representations learnt for a pre-trained model to solve a new, potentially unrelated problem. However, raw feature extraction from all layers is unfeasible given the massive size of these networks. Recently, a supervised method using complexity reduction was proposed, resulting in significant improvements in performance for transfer learning tasks. This approach first computes the discriminative power of features, and then discretises them using thresholds computed for the task. In this paper, we analyse the behaviour of these thresholds, with the purpose of finding a methodology for their estimation. After a comprehensive study, we find a very strong correlation between problem size and threshold value, with coefficient of determination above 90%. These results allow us to propose a unified model for threshold estimation, with potential application to transfer learning tasks.",0.1538461538],["LET was related to the RBE per energy with two polyline fitting functions. the","Estimation of linear energy transfer distribution for broad-beam carbon-ion radiotherapy at the National Institute of Radiological Sciences, Japan","summarize: Carbon-ion radiotherapy is generally evaluated with the dose weighted by relative biological effectiveness , while the radiation quality varying in the body of each patient is ignored for lack of such distribution. In this study, we attempted to develop a method to estimate linear energy transfer for a treatment planning system that only handled physical and RBE-weighted doses. The LET taken from a database of clinical broad beams was related to the RBE per energy with two polyline fitting functions for spread-out Bragg peak and for entrance depths, which would be selected by RBE threshold per energy per modulation. The LET estimation was consistent with the original calculation typically within a few keV\/m except for the overkill at the distal end of SOBP. The CIRT treatments can thus be related to the knowledge obtained in radiobiology experiments that used LET to represent radiation quality.",0.2348366541],["heterointerface of spinel\/perovskite heterointerface.","Microscopic origin of the mobility enhancement at a spinel\/perovskite oxide heterointerface revealed by photoemission spectroscopy","summarize: The spinel\/perovskite heterointerface ",0.0426185741],["c","Detection of Gas Molecule using C","summarize: C",0.006737947],["parallel processes are actually interleaved according to some interleaving strategy.","Process algebra with strategic interleaving","summarize: In process algebras such as ACP , parallel processes are considered to be interleaved in an arbitrary way. In the case of multi-threading as found in contemporary programming languages, parallel processes are actually interleaved according to some interleaving strategy. An interleaving strategy is what is called a process-scheduling policy in the field of operating systems. In many systems, for instance hardware\/software systems, we have to do with both parallel processes that may best be considered to be interleaved in an arbitrary way and parallel processes that may best be considered to be interleaved according to some interleaving strategy. Therefore, we extend ACP in this paper with the latter form of interleaving. The established properties of the extension concerned include an elimination property, a conservative extension property, and a unique expansion property.",0.1],["we propose a statistical non-linear model based on the photomultiplier tube","Statistical Non-linear Model, Achievable Rates and Signal Detection for Photon-level Photomultiplier Receiver","summarize: We characterize the practical receiver in a wide range of signal intensity for optical wireless communication, from discrete pulse regime to continuous waveform regime. We first propose a statistical non-linear model based on the photomultiplier tube multi-stage amplification and Poisson channel, and then derive the optimal and tractable suboptimal duty cycle with peak-power and average-power constraints for on-off key modulation in linear regime. Subsequently, a threshold-based classifier is proposed to distinguish the PMT working regimes based on the non-linear model. Moreover, we derive the approximate performance of mean power detection with infinite sampling rate and finite over-sampling rate in the linear regime based on small dead time and central-limit theorem. We also fomulate a signal model in the non-linar regime. Furthermore, the performance of mean power detection and photon counting detection with maximum likelihood detection for different sampling rates is evaluated from both theoretical and numerical perspectives. We can conclude that the sample interval equivalent to dead time is a good choice, and lower sampling rate would significantly degrade the performance.",0.3994815634],["an andor 1K.","EMPOL: an EMCCD based optical imaging polarimeter","summarize: An Andor 1K ",0.0658992845],["model averaged tail area confidence interval proposed by Turek and Fletcher, 2012","Upper bounds on the minimum coverage probability of model averaged tail area confidence intervals in regression","summarize: Frequentist model averaging has been proposed as a method for incorporating model uncertainty into confidence interval construction. Such proposals have been of particular interest in the environmental and ecological statistics communities. A promising method of this type is the model averaged tail area confidence interval put forward by Turek and Fletcher, 2012. The performance of this interval depends greatly on the data-based model weights on which it is based. A computationally convenient formula for the coverage probability of this interval is provided by Kabaila, Welsh and Abeysekera, 2016, in the simple scenario of two nested linear regression models. We consider the more complicated scenario that there are many linear regression models obtained as follows. For each of a specified set of components of the regression parameter vector, we either set the component to zero or let it vary freely. We provide an easily-computed upper bound on the minimum coverage probability of the MATA confidence interval. This upper bound provides evidence against the use of a model weight based on the Bayesian Information Criterion .",0.3582656553],["ion mobility results and collision cross section values are a source of experimental variation.","Optimizing Native Ion Mobility Q-TOF in Helium and Nitrogen for Very Fragile Noncovalent Interactions","summarize: The meaningful comparison of ion mobility results and of collision cross section values on different platforms is a prerequisite for using CCS for identification or structural assignment. The amount of internal energy imparted to the ions prior to the ion mobility cell is a source of experimental variation. Here we investigated the effects of virtually all tuning parameters of the Agilent 6560 IM-Q-TOF on the arrival time distributions of Ubiquitin7+, and found conditions in which the native state prevails. We will discuss the effects of solvent evaporation conditions in the source, in the entire pre-IM DC voltage gradient, and with the funnel RF amplitudes, and will also report on ubiquitin7+ conformations in different solvents, including native supercharging conditions. Collision-induced unfolding can be conveniently provoked in two distinct regions: behind the source capillary and in the trapping funnel . The softness of the instrumental conditions were then optimized with the benchmark DNA G-quadruplex 5-, for which ion activation results in ammonia loss. To reduce the ion internal energy and obtain the intact 3-NH4+ complex, we reduced the post-IM voltage gradient, but this resulted in a lower IM resolving power due to increased diffusion behind the drift tube. The article thus describes the various trade-offs between ion activation, ion transmission, and ion mobility performance for native MS of very fragile structures.",0.3043478261],["a completely automated public Turing test to tell computers and humans apart, CA","Efficient and Secure Flash-based Gaming CAPTCH","summarize: With the growth of connectivity to smart grids, new applications, and the changing interaction between customer and energy clouds, clouds are more vulnerable to denial-of-service attacks. Efficient detection methods are required to authenticate, detect and control attackers. Completely Automated Public Turing test to tell Computers and Humans Apart, CAPTCHA, is one efficient tool to thwart denial of service attacks. The server presents the user with a client puzzle to solve in order to gain access to the service or website. The puzzle should be hard enough for computers, but easy for humans to solve. Several methods have been suggested including the popular image-based, as well as video-based, and text-based CAPTCHAs. In this paper, we present a new Flash-based gaming CAPTCHA to differentiate bots from humans. We propose a drag and drop client puzzle where the user will play a simple game to answer a visual question. Our method turns out to be convenient, easy for users and challenging for bots. Additionally, it has gaming aspect, which makes it interesting to users of all age groups.",0.3],["a frequent transmission of collective perception messages could improve the perception capabilities of CAVs.","Generation of Cooperative Perception Messages for Connected and Automated Vehicles","summarize: Connected and Automated Vehicles utilize a variety of onboard sensors to sense their surrounding environment. CAVs can improve their perception capabilities if vehicles exchange information about what they sense using V2X communications. This is known as cooperative or collective perception . A frequent transmission of collective perception messages could improve the perception capabilities of CAVs. However, this improvement can be compromised if vehicles generate too many messages and saturate the communications channel. An important aspect is then when vehicles should generate the perception messages. ETSI has proposed the first set of message generation rules for collective perception. These rules define when vehicles should generate collective perception messages and what should be their content. We show that the current rules generate a high number of collective perception messages with information about a small number of detected objects. This results in an inefficient use of the communication channel that reduces the effectiveness of collective perception. We address this challenge and propose an improved algorithm that modifies the generation of collective perception messages. We demonstrate that the proposed solution improves the reliability of V2X communication and the perception of CAVs.",0.3181818182],["biases can pose a threat of being manifested as unethical predictions","Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective Learning","summarize: Human society had a long history of suffering from cognitive biases leading to social prejudices and mass injustice. The prevalent existence of cognitive biases in large volumes of historical data can pose a threat of being manifested as unethical and seemingly inhuman predictions as outputs of AI systems trained on such data. To alleviate this problem, we propose a bias-aware multi-objective learning framework that given a set of identity attributes and a subset of sensitive categories of the possible classes of prediction outputs, learns to reduce the frequency of predicting certain combinations of them, e.g. predicting stereotypes such as `most blacks use abusive language', or `fear is a virtue of women'. Our experiments conducted on an emotion prediction task with balanced class priors shows that a set of baseline bias-agnostic models exhibit cognitive biases with respect to gender, such as women are prone to be afraid whereas men are more prone to be angry. In contrast, our proposed bias-aware multi-objective learning methodology is shown to reduce such biases in the predictied emotions.",0.3333333333],["Let us know what you think about it!","Adams operations on classical compact Lie groups","summarize: Let ",0.0],["we explore a specific modified gravity model: mimetic dark matter. we present an","Can Dark Matter be Geometry? A Case Study with Mimetic Dark Matter","summarize: We investigate the possibility of dark matter being a pure geometrical effect, rather than a particle or a compact object, by exploring a specific modified gravity model: mimetic dark matter. We present an alternative formulation of the theory, closer to the standard cosmological perturbation theory framework. We make manifest the presence of arbitrary parameters and extra functions, both at background level and at first order in perturbation theory. We present the full set of independent equations of motion for this model, and we discuss the amount of tuning needed to match predictions of the theory to actual data. By using the matter power spectrum and cosmic microwave background angular power spectra as benchmark observables, we explicitly show that since there is no natural mechanism to generate adiabatic initial conditions in this specific model, extra fine-tuning is required. We modify the publicly available Boltzmann code \\texttt to make accurate predictions for the observables in mimetic dark matter. Our modified version of \\texttt is available on GitHub. We have used mimetic dark matter as an illustration of how much one is allowed to change the initial conditions before contradicting observations when modifying the laws of gravity as described by General Relativity but we point out that modifying gravity without providing a natural mechanism to generate adiabatic initial conditions will always lead to highly fine-tuned models.",0.3157894737],["adaptive model learning algorithm employs barrier certificates for systems with possibly nonstationary agent dynamics.","Barrier-Certified Adaptive Reinforcement Learning with Applications to Brushbot Navigation","summarize: This paper presents a safe learning framework that employs an adaptive model learning algorithm together with barrier certificates for systems with possibly nonstationary agent dynamics. To extract the dynamic structure of the model, we use a sparse optimization technique. We use the learned model in combination with control barrier certificates which constrain policies in order to maintain safety, which refers to avoiding particular undesirable regions of the state space. Under certain conditions, recovery of safety in the sense of Lyapunov stability after violations of safety due to the nonstationarity is guaranteed. In addition, we reformulate an action-value function approximation to make any kernel-based nonlinear function estimation method applicable to our adaptive learning framework. Lastly, solutions to the barrier-certified policy optimization are guaranteed to be globally optimal, ensuring the greedy policy improvement under mild conditions. The resulting framework is validated via simulations of a quadrotor, which has previously been used under stationarity assumptions in the safe learnings literature, and is then tested on a real robot, the brushbot, whose dynamics is unknown, highly complex and nonstationary.",0.1428571429],["master node has access to a database of files in every shuffling iteration","On the Fundamental Limits of Coded Data Shuffling for Distributed Machine Learning","summarize: We consider the data shuffling problem in a distributed learning system, in which a master node is connected to a set of worker nodes, via a shared link, in order to communicate a set of files to the worker nodes. The master node has access to a database of files. In every shuffling iteration, each worker node processes a new subset of files, and has excess storage to partially cache the remaining files, assuming the cached files are uncoded. The caches of the worker nodes are updated every iteration, and they should be designed to satisfy any possible unknown permutation of the files in subsequent iterations. For this problem, we characterize the exact load-memory trade-off for worst-case shuffling by deriving the minimum communication load for a given storage capacity per worker node. As a byproduct, the exact load-memory trade-off for any shuffling is characterized when the number of files is equal to the number of worker nodes. We propose a novel deterministic coded shuffling scheme, which improves the state of the art, by exploiting the cache memories to create coded functions that can be decoded by several worker nodes. Then, we prove the optimality of our proposed scheme by deriving a matching lower bound and showing that the placement phase of the proposed coded shuffling scheme is optimal over all shuffles.",0.4285714286],["nonparametric methods play a central role in modern empirical work. they provide inference","On the Effect of Bias Estimation on Coverage Accuracy in Nonparametric Inference","summarize: Nonparametric methods play a central role in modern empirical work. While they provide inference procedures that are more robust to parametric misspecification bias, they may be quite sensitive to tuning parameter choices. We study the effects of bias correction on confidence interval coverage in the context of kernel density and local polynomial regression estimation, and prove that bias correction can be preferred to undersmoothing for minimizing coverage error and increasing robustness to tuning parameter choice. This is achieved using a novel, yet simple, Studentization, which leads to a new way of constructing kernel-based bias-corrected confidence intervals. In addition, for practical cases, we derive coverage error optimal bandwidths and discuss easy-to-implement bandwidth selectors. For interior points, we show that the MSE-optimal bandwidth for the original point estimator delivers the fastest coverage error decay rate after bias correction when second-order kernels are employed, but is otherwise suboptimal because it is too large. Finally, for odd-degree local polynomial regression, we show that, as with point estimation, coverage error adapts to boundary points automatically when appropriate Studentization is used; however, the MSE-optimal bandwidth for the original point estimator is suboptimal. All the results are established using valid Edgeworth expansions and illustrated with simulated data. Our findings have important consequences for empirical work as they indicate that bias-corrected confidence intervals, coupled with appropriate standard errors, have smaller coverage error and are less sensitive to tuning parameter choices in practically relevant cases where additional smoothness is available.",0.3684210526],["gamification is a tool to guide targeted behavioural changes. it is a","Enhancing an eco-driving gamification platform through wearable and vehicle sensor data integration","summarize: As road transportation has been identified as a major contributor of environmental pollution, motivating individuals to adopt a more eco-friendly driving style could have a substantial ecological as well as financial benefit. With gamification being an effective tool towards guiding targeted behavioural changes, the development of realistic frameworks delivering a high end user experience, becomes a topic of active research. This paper presents a series of enhancements introduced to an eco-driving gamification platform by the integration of additional wearable and vehicle-oriented sensing data sources, leading to a much more realistic evaluation of the context of a driving session.",0.5862068966],["we propose a kernel classifier based on the optimal scoring framework. we provide theoretical","Sparse Feature Selection in Kernel Discriminant Analysis via Optimal Scoring","summarize: We consider the two-group classification problem and propose a kernel classifier based on the optimal scoring framework. Unlike previous approaches, we provide theoretical guarantees on the expected risk consistency of the method. We also allow for feature selection by imposing structured sparsity using weighted kernels. We propose fully-automated methods for selection of all tuning parameters, and in particular adapt kernel shrinkage ideas for ridge parameter selection. Numerical studies demonstrate the superior classification performance of the proposed approach compared to existing nonparametric classifiers.",0.3181818182],["the first order node model is a model of traffic flow behavior on individual roads.","On node models for high-dimensional road networks","summarize: Macroscopic traffic models are necessary for simulation and study of traffic's complex macro-scale dynamics, and are often used by practitioners for road network planning, integrated corridor management, and other applications. These models have two parts: a link model, which describes traffic flow behavior on individual roads, and a node model, which describes behavior at road junctions. As the road networks under study become larger and more complex --- nowadays often including arterial networks --- the node model becomes more important. This paper focuses on the first order node model and has two main contributions. First, we formalize the multi-commodity flow distribution at a junction as an optimization problem with all the necessary constraints. Most interesting here is the formalization of input flow priorities. Then, we discuss a very common conservation of turning fractions or first-in-first-out constraint, and how it often produces unrealistic spillback. This spillback occurs when, at a diverge, a queue develops for a movement that only a few lanes service, but FIFO requires that all lanes experience spillback from this queue. As we show, avoiding this unrealistic spillback while retaining FIFO in the node model requires complicated network topologies. Our second contribution is a partial FIFO mechanism that avoids this unrealistic spillback, and a node model and solution algorithm that incorporates this mechanism. The partial FIFO mechanism is parameterized through intervals that describe how individual movements influence each other, can be intuitively described from physical lane geometry and turning movement rules, and allows tuning to describe a link as having anything between full FIFO and no FIFO. Excepting the FIFO constraint, the present node model also fits within the well-established general class of first-order node models for multi-commodity flows.",0.2857142857],["dependency parsers reach very high overall accuracy. some dependency relations are much harder than others","Improving a Strong Neural Parser with Conjunction-Specific Features","summarize: While dependency parsers reach very high overall accuracy, some dependency relations are much harder than others. In particular, dependency parsers perform poorly in coordination construction . We extend a state-of-the-art dependency parser with conjunction-specific features, focusing on the similarity between the conjuncts head words. Training the extended parser yields an improvement in conj attachment as well as in overall dependency parsing accuracy on the Stanford dependency conversion of the Penn TreeBank.",0.0],["TAGS increased the average False Accept Rate from 4% to 26%.","Treadmill Assisted Gait Spoofing : An Emerging Threat to wearable Sensor-based Gait Authentication","summarize: In this work, we examine the impact of Treadmill Assisted Gait Spoofing on Wearable Sensor-based Gait Authentication . We consider more realistic implementation and deployment scenarios than the previous study, which focused only on the accelerometer sensor and a fixed set of features. Specifically, we consider the situations in which the implementation of WSGait could be using one or more sensors embedded into modern smartphones. Besides, it could be using different sets of features or different classification algorithms, or both. Despite the use of a variety of sensors, feature sets , and six different classification algorithms, TAGS was able to increase the average False Accept Rate from 4% to 26%. Such a considerable increase in the average FAR, especially under the stringent implementation and deployment scenarios considered in this study, calls for a further investigation into the design of evaluations of WSGait before its deployment for public use.",0.1515914397],["color sequences can be realized from a fixed point set. we first study the case","Colored ray configurations","summarize: We study the cyclic color sequences induced at infinity by colored rays with apices being a given balanced finite bichromatic point set. We first study the case in which the rays are required to be pairwise disjoint. We derive a lower bound on the number of color sequences that can be realized from any such fixed point set and examine color sequences that can be realized regardless of the point set, exhibiting negative examples as well. We also provide a tight upper bound on the number of configurations that can be realized from a point set, and point sets for which there are asymptotically less configurations than that number. In addition, we provide algorithms to decide whether a color sequence is realizable from a given point set in a line or in general position. We address afterwards the variant of the problem where the rays are allowed to intersect. We prove that for some configurations and point sets, the number of ray crossings must be ",0.1111111111],["Let us know what you think about it!","On a Conjecture of Lemmermeyer","summarize: Let ",0.0],["this paper uses some families of special numbers and polynomials with their generating functions","Combinatorial applications of the special numbers and polynomials","summarize: In this paper, by using some families of special numbers and polynomials with their generating functions, we give various properties of these numbers and polynomials. These numbers are related to the well-known numbers and polynomials, which are the Euler numbers, the Stirling numbers of the second kind, the central factorial numbers and the array polynomials. We also discuss some combinatorial interpretations of these numbers related to the rook polynomials and numbers. Furthermore, we give computation formulas for these numbers and polynomials.",0.3571428571],["erroneous result on achievability part of the R'en","Corrections to Wyner's Common Information under R\\'enyi Divergence Measures","summarize: In this correspondence, we correct an erroneous result on the achievability part of the R\\'enyi common information with order ",0.1961104228],["neural models are often able to focus on a few frequent words with sentiment polarities","Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis","summarize: In aspect-level sentiment classification , it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active\/misleading influence on the correct\/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural ASC models. Source code and trained models are available at https:\/\/github.com\/DeepLearnXMU\/PSSAttention.",0.1578947368],["this paper studies the dynamic generator model for spatial-temporal processes. the model follows","Learning Dynamic Generator Model by Alternating Back-Propagation Through Time","summarize: This paper studies the dynamic generator model for spatial-temporal processes such as dynamic textures and action sequences in video data. In this model, each time frame of the video sequence is generated by a generator model, which is a non-linear transformation of a latent state vector, where the non-linear transformation is parametrized by a top-down neural network. The sequence of latent state vectors follows a non-linear auto-regressive model, where the state vector of the next frame is a non-linear transformation of the state vector of the current frame as well as an independent noise vector that provides randomness in the transition. The non-linear transformation of this transition model can be parametrized by a feedforward neural network. We show that this model can be learned by an alternating back-propagation through time algorithm that iteratively samples the noise vectors and updates the parameters in the transition model and the generator model. We show that our training method can learn realistic models for dynamic textures and action patterns.",0.0],["state-of-the-art approaches perform semantic segmentation or refine object bounding boxes obtained","Instance Segmentation of Biomedical Images with an Object-aware Embedding Learned with Local Constraints","summarize: Automatic instance segmentation is a problem that occurs in many biomedical applications. State-of-the-art approaches either perform semantic segmentation or refine object bounding boxes obtained from detection methods. Both suffer from crowded objects to varying degrees, merging adjacent objects or suppressing a valid object. In this work, we assign an embedding vector to each pixel through a deep neural network. The network is trained to output embedding vectors of similar directions for pixels from the same object, while adjacent objects are orthogonal in the embedding space, which effectively avoids the fusion of objects in a crowd. Our method yields state-of-the-art results even with a light-weighted backbone network on a cell segmentation and a leaf segmentation data set . The code and model weights are public available.",0.0],["dynamic landscape models are proposed for prisoner's Dilemma and Snowdrift games","Dynamic landscape models of coevolutionary games","summarize: Players of coevolutionary games may update not only their strategies but also their networks of interaction. Based on interpreting the payoff of players as fitness, dynamic landscape models are proposed. The modeling procedure is carried out for Prisoner's Dilemma and Snowdrift games that both use either birth--death or death--birth strategy updating. The main focus is on using dynamic fitness landscapes as a mathematical model of coevolutionary game dynamics. Hence, an alternative tool for analyzing coevolutionary games becomes available, and landscape measures such as modality, ruggedness and information content can be computed and analyzed. In addition, fixation properties of the games and quantifiers characterizing the interaction networks are calculated numerically. Relations are established between landscape properties expressed by landscape measures and quantifiers of coevolutionary game dynamics such as fixation probabilities, fixation times and network properties.",0.3333333333],["ferromagnetic-resonance driven spin pumping and spin-Hall effect","Spin transport parameters of NbN thin films characterised by spin pumping experiments","summarize: We present measurements of ferromagnetic-resonance - driven spin pumping and inverse spin-Hall effect in NbN\/Y3Fe5O12 bilayers. A clear enhancement of the Gilbert damping constant of the thin-film YIG was observed due to the presence of the NbN spin sink. By varying the NbN thickness and employing spin-diffusion theory, we have estimated the room temperature values of the spin diffusion length and the spin Hall angle in NbN to be 14 nm and -1.1 10-2, respectively. Furthermore, we have determined the spin-mixing conductance of the NbN\/YIG interface to be 10 nm-2. The experimental quantification of these spin transport parameters is an important step towards the development of superconducting spintronic devices involving NbN thin films.",0.1223854149],["Given a fusion system, the system is a fusion system.","Cohomology of infinite groups realizing fusion systems","summarize: Given a fusion system ",0.3333333333],["new methods for model assessment have been proposed for scaling leave-one-out cross-validation","Leave-One-Out Cross-Validation for Bayesian Model Comparison in Large Data","summarize: Recently, new methods for model assessment, based on subsampling and posterior approximations, have been proposed for scaling leave-one-out cross-validation to large datasets. Although these methods work well for estimating predictive performance for individual models, they are less powerful in model comparison. We propose an efficient method for estimating differences in predictive performance by combining fast approximate LOO surrogates with exact LOO subsampling using the difference estimator and supply proofs with regards to scaling characteristics. The resulting approach can be orders of magnitude more efficient than previous approaches, as well as being better suited to model comparison.",0.1428571429],["a new primal-dual mixed finite element method is introduced. the method","A Conforming Primal-Dual Mixed Formulation for the 2D Multiscale Porous Media Flow Problem","summarize: In this paper a new primal-dual mixed finite element method is introduced, aimed to model multiscale problems with several geometric subregions in the domain of interest. In each of these regions porous media fluid flow takes place, but governed by physical parameters at a different scale; additionally, a fluid exchange through contact interfaces occurs between neighboring regions. The well-posedness of the primal-dual mixed finite element formulation on bounded simply connected polygonal domains of the plane is presented. Next, the convergence of the discrete solution to the exact solution of the problem is discussed, together with the convergence rate analysis. Finally, the numerical examples illustrate the method's capabilities to handle multiscale problems and interface discontinuities as well as experimental rates of convergence.",0.2779176394],["this paper combines high order finite element methods with parallel preconditioners. the algorithms","Parallel preconditioners for high order discretizations arising from full system modeling for brain microwave imaging","summarize: This paper combines the use of high order finite element methods with parallel preconditioners of domain decomposition type for solving electromagnetic problems arising from brain microwave imaging. The numerical algorithms involved in such complex imaging systems are computationally expensive since they require solving the direct problem of Maxwell's equations several times. Moreover, wave propagation problems in the high frequency regime are challenging because a sufficiently high number of unknowns is required to accurately represent the solution. In order to use these algorithms in practice for brain stroke diagnosis, running time should be reasonable. The method presented in this paper, coupling high order finite elements and parallel preconditioners, makes it possible to reduce the overall computational cost and simulation time while maintaining accuracy.",0.1978624429],["proposed majorization minimization methods to distributed PGO. proposed methods rely a","Majorization Minimization Methods to Distributed Pose Graph Optimization with Convergence Guarantees","summarize: In this paper, we consider the problem of distributed pose graph optimization that has extensive applications in multi-robot simultaneous localization and mapping . We propose majorization minimization methods to distributed PGO and show that our proposed methods are guaranteed to converge to first-order critical points under mild conditions. Furthermore, since our proposed methods rely a proximal operator of distributed PGO, the convergence rate can be significantly accelerated with Nesterov's method, and more importantly, the acceleration induces no compromise of theoretical guarantees. In addition, we also present accelerated majorization minimization methods to the distributed chordal initialization that have a quadratic convergence, which can be used to compute an initial guess for distributed PGO. The efficacy of this work is validated through applications on a number of 2D and 3D SLAM datasets and comparisons with existing state-of-the-art methods, which indicates that our proposed methods have faster convergence and result in better solutions to distributed PGO.",0.2777777778],["fore-aft asymmetry favors front neighbors changes qualitatively the phase diagram","Fore-aft asymmetric flocking","summarize: We show that fore-aft asymmetry, a generic feature of living organisms and some active matter systems, can have a strong influence on the collective properties of even the simplest flocking models. Specifically, an arbitrarily weak asymmetry favoring front neighbors changes qualitatively the phase diagram of the Vicsek model. A region where many sharp traveling band solutions coexist is present at low noise strength, below the Toner-Tu liquid, at odds with the phase-separation scenario well describing the usual isotropic model. Inside this region, a `banded liquid' phase with algebraic density distribution coexists with band solutions. Linear stability analysis at the hydrodynamic level suggests that these results are generic and not specific to the Vicsek model.",0.0],["we propose a possible solution to these challenges: secure federated learning. we propose","Federated Machine Learning: Concept and Applications","summarize: Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.",0.2],["we introduce the notion of connection thickness in a Cayley graph. it was well","Connectedness of spheres in Cayley graphs","summarize: We introduce the notion of connection thickness of spheres in a Cayley graph, related to dead-ends and their retreat depth. It was well-known that connection thickness is bounded for finitely presented one-ended groups. We compute that for natural generating sets of lamplighter groups on a line or on a tree, connection thickness is linear or logarithmic respectively. We show that it depends strongly on the generating set. We give an example where the metric induced at the thickness of connection gives diameter of order ",0.3157894737],["the study of variations in solar activity is important for understanding the underlying mechanism of solar activity","Will Solar Cycles 25 and 26 Be Weaker than Cycle 24 ?","summarize: The study of variations in solar activity is important for understanding the underlying mechanism of solar activity and for predicting the level of activity in view of the activity impact on space weather and global climate. Here we have used the amplitudes of Solar Cycles 1-24 to predict the relative amplitudes of the solar cycles during the rising phase of the upcoming Gleissberg cycle. We fitted a cosine function to the amplitudes and times of the solar cycles after subtracting a linear fit of the amplitudes. The best cosine fit shows overall properties of Gleissberg cycles, but with large uncertainties. We obtain a pattern of the rising phase of the upcoming Gleissberg cycle, but there is considerable ambiguity. Using the epochs of violations of the Gnevyshev-Ohl rule and the `tentative inverse G-O rule' of solar cycles during the period 1610-2015, and also using the epochs where the orbital angular momentum of the Sun is steeply decreased during the period 1600-2099, we infer that Solar Cycle 25 will be weaker than Cycle 24. Cycles 25 and 26 will have almost same strength, and their epochs are at the minimum between the current and upcoming Gleissberg cycles. In addition, Cycle 27 is expected to be stronger than Cycle 26 and weaker than Cycle 28, and Cycle 29 is expected to be stronger than both Cycles 28 and 30. The maximum of Cycle 29 is expected to represent the next Gleissberg maximum. Our analysis also suggests a much lower value for the maximum amplitude of the upcoming Cycle 25.",0.0],["a novel deep fusion algorithm is based on the representations from an end-to","Dense Feature Aggregation and Pruning for RGBT Tracking","summarize: How to perform effective information fusion of different modalities is a core factor in boosting the performance of RGBT tracking. This paper presents a novel deep fusion algorithm based on the representations from an end-to-end trained convolutional neural network. To deploy the complementarity of features of all layers, we propose a recursive strategy to densely aggregate these features that yield robust representations of target objects in each modality. In different modalities, we propose to prune the densely aggregated features of all modalities in a collaborative way. In a specific, we employ the operations of global average pooling and weighted random selection to perform channel scoring and selection, which could remove redundant and noisy features to achieve more robust feature representation. Experimental results on two RGBT tracking benchmark datasets suggest that our tracker achieves clear state-of-the-art against other RGB and RGBT tracking methods.",0.3157894737],["the first international workshop on deep learning and music, joint with IJCNN, Anchorage,","Proceedings of the First International Workshop on Deep Learning and Music","summarize: Proceedings of the First International Workshop on Deep Learning and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017",0.2857142857],["dragged meniscus occurs for striped prestructures with two orientations.","Dip-coating with prestructured substrates: transfer of simple liquids and Langmuir-Blodgett monolayers","summarize: When a plate is withdrawn from a liquid bath, either a static meniscus forms in the transition region between the bath and the substrate or a liquid film of finite thickness is transferred onto the moving substrate. If the substrate is inhomogeneous, e.g., has a prestructure consisting of stripes of different wettabilities, the meniscus can be deformed or show a complex dynamic behavior. Here we study the free surface shape and dynamics of a dragged meniscus occurring for striped prestructures with two orientations, parallel and perpendicular to the transfer direction. A thin film model is employed that accounts for capillarity through a Laplace pressure and for the spatially varying wettability through a Derjaguin pressure. Numerical continuation is used to obtain steady free surface profiles and corresponding bifurcation diagrams in the case of substrates with different homogeneous wettabilities. Direct numerical simulations are employed in the case of the various striped prestructures. The final part illustrates the importance of our findings for particular applications that involve complex liquids by modeling a Langmuir-Blodgett transfer experiment. There, one transfers a monolayer of an insoluble surfactant that covers the surface of the bath onto the moving substrate. The resulting pattern formation phenomena can be crucially influenced by the hydrodynamics of the liquid meniscus that itself depends on the prestructure on the substrate. In particular, we show how prestructure stripes parallel to the transfer direction lead to the formation of bent stripes in the surfactant coverage after transfer and present similar experimental results.",0.0889708225],["SSDs have gained tremendous attention in computing and storage systems. the cost per capacity of SSD","ReCA: an Efficient Reconfigurable Cache Architecture for Storage Systems with Online Workload Characterization","summarize: In recent years, SSDs have gained tremendous attention in computing and storage systems due to significant performance improvement over HDDs. The cost per capacity of SSDs, however, prevents them from entirely replacing HDDs in such systems. One approach to effectively take advantage of SSDs is to use them as a caching layer to store performance critical data blocks to reduce the number of accesses to disk subsystem. Due to characteristics of Flash-based SSDs such as limited write endurance and long latency on write operations, employing caching algorithms at the Operating System level necessitates to take such characteristics into consideration. Previous caching techniques are optimized towards only one type of application, which affects both generality and applicability. In addition, they are not adaptive when the workload pattern changes over time. This paper presents an efficient Reconfigurable Cache Architecture for storage systems using a comprehensive workload characterization to find an optimal cache configuration for I\/O intensive applications. For this purpose, we first investigate various types of I\/O workloads and classify them into five major classes. Based on this characterization, an optimal cache configuration is presented for each class of workloads. Then, using the main features of each class, we continuously monitor the characteristics of an application during system runtime and the cache organization is reconfigured if the application changes from one class to another class of workloads. The cache reconfiguration is done online and workload classes can be extended to emerging I\/O workloads in order to maintain its efficiency with the characteristics of I\/O requests. Experimental results obtained by implementing ReCA in a server running Linux show that the proposed architecture improves performance and lifetime up to 24\\% and 33\\%, respectively.",0.0526315789],["the experiments were performed in a rectangular bubble column heated from one side wall and cooled from","Experimental investigation of heat transport in inhomogeneous bubbly flow","summarize: In this work we study the heat transport in inhomogeneous bubbly flow. The experiments were performed in a rectangular bubble column heated from one side wall and cooled from the other, with millimetric bubbles introduced through one half of the injection section . We characterise the global heat transport while varying two parameters: the gas volume fraction ",0.2083333333],["the reduced trees will have at least 15k-9 taxa where k is the TBR","New reduction rules for the tree bisection and reconnection distance","summarize: Recently it was shown that, if the subtree and chain reduction rules have been applied exhaustively to two unrooted phylogenetic trees, the reduced trees will have at most 15k-9 taxa where k is the TBR distance between the two trees, and that this bound is tight. Here we propose five new reduction rules and show that these further reduce the bound to 11k-9. The new rules combine the ``unrooted generator'' approach introduced in with a careful analysis of agreement forests to identify situations when chains of length 3 can be further shortened without reducing the TBR distance, and situations when small subtrees can be identified whose deletion is guaranteed to reduce the TBR distance by 1. To the best of our knowledge these are the first reduction rules that strictly enhance the reductive power of the subtree and chain reduction rules.",0.1764705882],["spectrum sharing is one of the promising solutions to meet the spectrum demand in 5G networks.","Spectrum Matching in Licensed Spectrum Sharing","summarize: Spectrum sharing is one of the promising solutions to meet the spectrum demand in 5G networks that results from the emerging services like machine to machine and vehicle to infrastructure communication. The idea is to allow a set of entities access the spectrum whenever and wherever it is unused by the licensed users. In the proposed framework, different spectrum provider networks with surplus spectrum available may rank the operators requiring the spectrum, called spectrum users hereafter, differently in terms of their preference to lease spectrum, based for example on target business market considerations of the SUs. Similarly, SUs rank SPs depending on a number of criteria, for example based on coverage and availability in a service area. Ideally, both SPs and SUs prefer to provide\/get spectrum to\/from the operator of their first choice, but this is not necessarily always possible due to conflicting preferences. We apply matching theory algorithms with the aim to resolve the conflicting preferences of the SPs and SUs and quantify the effect of the proposed matching theory approach on establishing preferred provider-user network pairs. We discuss both one-to-one and many-to-one spectrum sharing scenarios and evaluate the performance using Monte Carlo simulations. The results show that comprehensive gains in terms of preferred matching of the provider-user network pairs can be achieved by applying matching theory for spectrum sharing as compared to uncoordinated spectrum allocation of the available spectrum to the SUs.",0.1304347826],["cold pools can influence the initiation of new convective cells. but how exactly","Tracking the Gust Fronts of Convective Cold Pools","summarize: It is increasingly acknowledged that cold pools can influence the initiation of new convective cells. Yet, the full complexity of convective organization through cold pool interaction is poorly understood. This lack of understanding may partially be due to the intricacy of the dynamical pattern formed by precipitation cells and their cold pools. Additionally, how exactly cold pools interact is insufficiently known. To better understand this dynamics, we develop a tracking algorithm for cold pool gust fronts. Rather than tracking thermodynamic anomalies, which do not generally coincide with the gust front boundaries, our approach tracks the dynamical cold pool outflow. Our algorithm first determines the locus of the precipitation event. Second, relative to this origin and for each azimuthal bin, the steepest gradient in the near-surface horizontal radial velocity ",0.1538461538],["silicon-on-insulator devices offer a whole new generation of system-on-chip","Coupling ","summarize: Silicon nitride planar waveguide platform combined with silicon-on-insulator devices offer a whole new generation of system-on-chip applications. Therefore, efficient coupling of an ",0.0],["we propose a kernel classifier based on the optimal scoring framework. we provide theoretical","Sparse Feature Selection in Kernel Discriminant Analysis via Optimal Scoring","summarize: We consider the two-group classification problem and propose a kernel classifier based on the optimal scoring framework. Unlike previous approaches, we provide theoretical guarantees on the expected risk consistency of the method. We also allow for feature selection by imposing structured sparsity using weighted kernels. We propose fully-automated methods for selection of all tuning parameters, and in particular adapt kernel shrinkage ideas for ridge parameter selection. Numerical studies demonstrate the superior classification performance of the proposed approach compared to existing nonparametric classifiers.",0.3181818182],["fewer steps lead to better systems, says a paper. a guiding principle","Revisiting Efficient Multi-Step Nonlinearity Compensation with Machine Learning: An Experimental Demonstration","summarize: Efficient nonlinearity compensation in fiber-optic communication systems is considered a key element to go beyond the capacity crunch''. One guiding principle for previous work on the design of practical nonlinearity compensation schemes is that fewer steps lead to better systems. In this paper, we challenge this assumption and show how to carefully design multi-step approaches that provide better performance--complexity trade-offs than their few-step counterparts. We consider the recently proposed learned digital backpropagation approach, where the linear steps in the split-step method are re-interpreted as general linear functions, similar to the weight matrices in a deep neural network. Our main contribution lies in an experimental demonstration of this approach for a 25 Gbaud single-channel optical transmission system. It is shown how LDBP can be integrated into a coherent receiver DSP chain and successfully trained in the presence of various hardware impairments. Our results show that LDBP with limited complexity can achieve better performance than standard DBP by using very short, but jointly optimized, finite-impulse response filters in each step. This paper also provides an overview of recently proposed extensions of LDBP and we comment on potentially interesting avenues for future work.",0.5],["the paper presents the description of several key RAN enablers. the five-generation","RAN Enablers for 5G Radio Resource Management","summarize: This paper presents the description of several key RAN enablers for the radio resource management framework of the fifth generation radio access network , referred to as building blocks of the 5G RRM. In particular, the following key RAN enablers are discussed: i) interference management techniques for dense and dynamic deployments, focusing on cell-edge performance enhancement; ii) dynamic traffic steering mechanisms that aim to attain the optimum mapping of 5G services to any available resources when and where needed by considering the peculiarities of different air interface variants ; iii) resource management strategies that deal with network slices; and iv) tight interworking between novel 5G AIVs and evolved legacy AIVs such as Long-term Evolution . Evaluation results for each of these key RAN enablers are also presented.",0.0555555556],["a significant increase in X-ray flux by 1.5 orders of magnitude was observed following the","New changing look case in NGC 1566","summarize: We present a study of optical, UV and X-ray light curves of the nearby changing look active galactic nucleus in the galaxy NGC 1566 obtained with the Neil Gehrels Swift Observatory and the MASTER Global Robotic Network over the period 2007 - 2018. We also report on our optical spectroscopy at the South African Astronomical Observatory with the 1.9-m telescope on the night 2018 August 2-3. A substantial increase in X-ray flux by 1.5 orders of magnitude was observed following the brightening in the UV and optical bands during the last year. After a maximum was reached at the beginning of 2018 July the fluxes in all bands decreased with some fluctuations. The amplitude of the flux variability is strongest in the X-ray band and decreases with increasing wavelength. Low-resolution spectra reveal a dramatic strengthening of the broad emission as well as high-ionization 6374 A lines. These lines were not detected so strongly in the past published spectra. The change in the type of the optical spectrum was accompanied by a significant change in the X-ray spectrum. All these facts confirm NGC 1566 to be a changing look Seyfert galaxy.",0.1818181818],["visualization applications are yet to benefit from the capacity of VEs. we aim to conduct","Cost-benefit Analysis of Visualization in Virtual Environments","summarize: Visualization and virtual environments have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice are yet to benefit from the capacity of VEs. In this paper, we examine this perplexity from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems , to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.",0.1176470588],["a decomposition of the average treatment effect in terms of direct and indirect effects is guaranteed","Causal mediation analysis for stochastic interventions","summarize: Mediation analysis in causal inference has traditionally focused on binary exposures and deterministic interventions, and a decomposition of the average treatment effect in terms of direct and indirect effects. In this paper we present an analogous decomposition of the \\textit, defined through stochastic interventions on the exposure. Population intervention effects provide a generalized framework in which a variety of interesting causal contrasts can be defined, including effects for continuous and categorical exposures. We show that identification of direct and indirect effects for the population intervention effect requires weaker assumptions than its average treatment effect counterpart, under the assumption of no mediator-outcome confounders affected by exposure. In particular, identification of direct effects is guaranteed in experiments that randomize the exposure and the mediator. We discuss various estimators of the direct and indirect effects, including substitution, re-weighted, and efficient estimators based on flexible regression techniques, allowing for multivariate mediators. Our efficient estimator is asymptotically linear under a condition requiring ",0.2962962963],["we present a strikingly simple proof that two rules are sufficient to automate gradient descent","Adaptive Gradient Descent without Descent","summarize: We present a strikingly simple proof that two rules are sufficient to automate gradient descent: 1) don't increase the stepsize too fast and 2) don't overstep the local curvature. No need for functional values, no line search, no information about the function except for the gradients. By following these rules, you get a method adaptive to the local geometry, with convergence guarantees depending only on the smoothness in a neighborhood of a solution. Given that the problem is convex, our method converges even if the global smoothness constant is infinity. As an illustration, it can minimize arbitrary continuously twice-differentiable convex function. We examine its performance on a range of convex and nonconvex problems, including logistic regression and matrix factorization.",0.0952380952],["the standard approach to MCMC involves constructing discrete-time reversible Mar","Limit theorems for the Zig-Zag process","summarize: Markov chain Monte Carlo methods provide an essential tool in statistics for sampling from complex probability distributions. While the standard approach to MCMC involves constructing discrete-time reversible Markov chains whose transition kernel is obtained via the Metropolis- Hastings algorithm, there has been recent interest in alternative schemes based on piecewise deterministic Markov processes . One such approach is based on the Zig-Zag process, introduced in Bierkens and Roberts , which proved to provide a highly scalable sampling scheme for sampling in the big data regime ). In this paper we study the performance of the Zig-Zag sampler, focusing on the one-dimensional case. In particular, we identify conditions under which a Central limit theorem holds and characterize the asymptotic variance. Moreover, we study the influence of the switching rate on the diffusivity of the Zig-Zag process by identifying a diffusion limit as the switching rate tends to infinity. Based on our results we compare the performance of the Zig-Zag sampler to existing Monte Carlo methods, both analytically and through simulations.",0.1],["Mubayi's Conjecture states that if Mubayi's Conject","On Mubayi's Conjecture and conditionally intersecting sets","summarize: Mubayi's Conjecture states that if ",0.4],["we consider the transmit and receive antenna array gain of massive MIMO systems. we look at","Limits of Transmit and Receive Array Gain in Massive MIMO","summarize: In this paper, we consider the transmit and receive antenna array gain of massive MIMO systems. In particular, we look at their dependence on the number of antennas in the array, and the antenna spacing for uniform linear and uniform circular arrays. It is known that the transmit array gain saturates at a certain antenna spacing, but the receive array gain had not been considered. With our physically consistent analysis based on the Multiport Communication Theory, we show that the receive array gain does not saturate, but that there is a peak at a certain antenna spacing when there is no decoupling network at the receiver. As implementing a decoupling network for massive MIMO would be almost impossible, this is a reasonable assumption. Furthermore, we analyze how the array gain changes depending on the antenna spacing and the size of the antenna array and derive design recommendations.",0.1666666667],["the popularity of these devices encourages malware developer to penetrate the market with malicious apps to anno","An investigation of the classifiers to detect android malicious apps","summarize: Android devices are growing exponentially and are connected through the internet accessing billion of online websites. The popularity of these devices encourages malware developer to penetrate the market with malicious apps to annoy and disrupt the victim. Although, for the detection of malicious apps different approaches are discussed. However, proposed approaches are not suffice to detect the advanced malware to limit\/prevent the damages. In this, very few approaches are based on opcode occurrence to classify the malicious apps. Therefore, this paper investigates the five classifiers using opcodes occurrence as the prominent features for the detection of malicious apps. For the analysis, we use WEKA tool and found that FT detection accuracy is best among the investigated classifiers. However, true positives rate i.e. malware detection rate is highest by RF and fluctuate least with the different number of prominent features compared to other studied classifiers. The analysis shows that overall accuracy is majorly affected by the false positives of the classifier.",0.3043478261],["the 4 SCAO systems of LBT are being upgraded. the system will push the current","Bringing SOUL on sky","summarize: The SOUL project is upgrading the 4 SCAO systems of LBT, pushing the current guide star limits of about 2 magnitudes fainter thanks to Electron Multiplied CCD detector. This improvement will open the NGS SCAO correction to a wider number of scientific cases from high contrast imaging in the visible to extra-galactic source in the NIR. The SOUL systems are today the unique case where pyramid WFS, adaptive secondary and EMCCD are used together. This makes SOUL a pathfinder for most of the ELT SCAO systems like the one of GMT, MICADO and HARMONI of E-ELT, where the same key technologies will be employed. Today we have 3 SOUL systems installed on the telescope in commissioning phase. The 4th system will be installed in a few months. We will present here the results achieved during daytime testing and commissioning nights up to the present date.",0.0],["boundary conditions are a common or special boundary universality class. boundary conditions are e","Why boundary conditions do not generally determine the universality class for boundary critical behavior","summarize: Interacting field theories for systems with a free surface frequently exhibit distinct universality classes of boundary critical behaviors depending on gross surface properties. The boundary condition satisfied by the continuum field theory on some scale may or may not be decisive for the universality class that applies. In many recent papers on boundary field theories it is taken for granted that Dirichlet or Neumann boundary conditions decide whether the ordinary or special boundary universality class is observed. While true in a certain sense for the Dirichlet boundary condition, this is not the case for the Neumann boundary condition. Building on results that have been worked out in the 1980s, but have not always been appropriately appreciated in the literature, the subtle role of boundary conditions and their scale dependence is elucidated and the question of whether or not they determine the observed boundary universality class is discussed.",0.6388888889],["a novel cache-enabled heterogeneous network is used to cache multimedia contents","Cache-enabled HetNets With Millimeter Wave Small Cells","summarize: In this paper, we consider a novel cache-enabled heterogeneous network , where macro base stations with traditional sub-6 GHz are overlaid by dense millimeter wave pico BSs. These two-tier BSs, which are modeled as two independent homogeneous Poisson Point Processes, cache multimedia contents following the popularity rank. High-capacity backhauls are utilized between macro BSs and the core server. In contrast to the simplified flat-top antenna pattern analyzed in previous articles, we employ an actual antenna model with the uniform linear array at all mmWave BSs. To evaluate the performance of our system, we introduce two distinctive user association strategies: 1) maximum received power scheme; and 2) maximum rate scheme. With the aid of these two schemes, we deduce new theoretical equations for success probabilities and area spectral efficiencies . Considering a special case with practical path loss laws, several closed-form expressions for coverage probabilities are derived to gain several insights. Monte Carlo simulations are presented to verify the analytical conclusions. We show that: 1) the proposed HetNet is an interference-limited system and it outperforms the traditional HetNets in terms of the success probability; 2) there exists an optimal pre-decided rate threshold that contributes to the maximum ASE; and 3) Max-Rate achieves higher success probability and ASE than Max-RP but it needs the extra information of the interference effect.",0.25],["Molecular motors walk along filaments until they detach stochastically.","Force-dependent unbinding rate of molecular motors from stationary optical trap data","summarize: Molecular motors walk along filaments until they detach stochastically with a force-dependent unbinding rate. Here, we show that this unbinding rate can be obtained from the analysis of experimental data of molecular motors moving in stationary optical traps. Two complementary methods are presented, based on the analysis of the distribution for the unbinding forces and of the motor's force traces. In the first method, analytically derived force distributions for slip bonds, slip-ideal bonds, and catch bonds are used to fit the cumulative distributions of the unbinding forces. The second method is based on the statistical analysis of the observed force traces. We validate both methods with stochastic simulations and apply them to experimental data for kinesin-1.",0.0889708225],["a graph representation will accelerate breadth-first search based on sparse-mat","SlimSell: A Vectorizable Graph Representation for Breadth-First Search","summarize: Vectorization and GPUs will profoundly change graph processing. Traditional graph algorithms tuned for 32- or 64-bit based memory accesses will be inefficient on architectures with 512-bit wide instruction units that are already present in the Intel Knights Landing manycore CPU. Anticipating this shift, we propose SlimSell: a vectorizable graph representation to accelerate Breadth-First Search based on sparse-matrix dense-vector products. SlimSell extends and combines the state-of-the-art SIMD-friendly Sell-C-sigma matrix storage format with tropical, real, boolean, and sel-max semiring operations. The resulting design reduces the necessary storage and thus pressure on the memory subsystem. We augment SlimSell with the SlimWork and SlimChunk schemes that reduce the amount of work and improve load balance, further accelerating BFS. We evaluate all the schemes on Intel Haswell multicore CPUs, the state-of-the-art Intel Xeon Phi KNL manycore CPUs, and NVIDIA Tesla GPUs. Our experiments indicate which semiring offers highest speedups for BFS and illustrate that SlimSell accelerates a tuned Graph500 BFS code by up to 33%. This work shows that vectorization can secure high-performance in BFS based on SpMV products; the proposed principles and designs can be extended to other graph algorithms.",0.3],["this paper presents a framework for norm-based capacity control for norm-based capacity control","Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units","summarize: This paper presents a general framework for norm-based capacity control for ",0.1379310345],["a research field has primarily focused on either knowledge bases or free text as a source","Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds","summarize: Question Answering , as a research field, has primarily focused on either knowledge bases or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.",0.5666666667],["artificial swimmer system exhibits chemotaxis and negative auto-chemotaxis","Chemotaxis and auto-chemotaxis of self-propelling artificial droplet swimmers","summarize: Chemotaxis and auto-chemotaxis play an important role in many essential biological processes. We present a self-propelling artificial swimmer system which exhibits chemotaxis as well as negative auto-chemotaxis. Oil droplets in an aqueous surfactant solution are driven by interfacial Marangoni flows induced by micellar solubilization of the oil phase. We demonstrate that chemotaxis along micellar surfactant gradients can guide these swimmers through a microfluidic maze. Similarly, a depletion of empty micelles in the wake of a droplet swimmer causes negative autochemotaxis and thereby trail avoidance. We have studied autochemotaxis quantitatively in a microfluidic device of bifurcating channels: Branch choices of consecutive swimmers are anticorrelated, an effect decaying over time due to trail dispersion. We have modeled this process by a simple one-dimensional diffusion process and stochastic Langevin dynamics. Our results are consistent with a linear surfactant gradient force and diffusion constants appropriate for micellar diffusion, and provide a measure of autochemotactic feedback strength versus stochastic forces.",0.5555555556],["shared mobility-on-demand services are expanding rapidly in cities around the world. despite","K-Prototype Segmentation Analysis on Large-scale Ridesourcing Trip Data","summarize: Shared mobility-on-demand services are expanding rapidly in cities around the world. As a prominent example, app-based ridesourcing is becoming an integral part of many urban transportation ecosystems. Despite the centrality, limited public availability of detailed temporal and spatial data on ridesourcing trips has limited research on how new services interact with traditional mobility options and how they impact travel in cities. Improving data-sharing agreements are opening unprecedented opportunities for research in this area. This study examines emerging patterns of mobility using recently released City of Chicago public ridesourcing data. The detailed spatio-temporal ridesourcing data are matched with weather, transit, and taxi data to gain a deeper understanding of ridesourcings role in Chicagos mobility system. The goal is to investigate the systematic variations in patronage of ride-hailing. K-prototypes is utilized to detect user segments owing to its ability to accept mixed variable data types. An extension of the K-means algorithm, its output is a classification of the data into several clusters called prototypes. Six ridesourcing prototypes are identified and discussed based on significant differences in relation to adverse weather conditions, competition with alternative modes, location and timing of use, and tendency for ridesplitting. The paper discusses implications of the identified clusters related to affordability, equity and competition with transit.",0.0714285714],["this is the third of the series of articles on the large-scale.","Large-","summarize: This is the third of the series of articles on the large-",0.0],["a polarized nanodiamond absorbs energy form laser beams and causes","Optical Levitation of Nanodiamonds by Doughnut Beams in Vacuum","summarize: Optically levitated nanodiamonds with nitrogen-vacancy centers promise a high-quality hybrid spin-optomechanical system. However, the trapped nanodiamond absorbs energy form laser beams and causes thermal damage in vacuum. We propose to solve the problem by trapping a composite particle at the center of strongly focused doughnut-shaped laser beams. Systematical study on the trapping stability, heat absorption, and oscillation frequency concludes that the azimuthally polarized Gaussian beam and the linearly polarized Laguerre-Gaussian beam ",0.3333333333],["astronomers have been interested in analyzing 1599 gamma-ray burs","Multivariate ","summarize: Determining the kinds of gamma-ray bursts has been of interest to astronomers for many years. We analyzed 1599 GRBs from the Burst and Transient Source Experiment 4Br catalogue using ",0.0],["adaptive discontinuous Galerkin methods are used to solve fractional diffusion equations. the","Discontinuous Galerkin methods and their adaptivity for the tempered fractional diffusion equations","summarize: This paper focuses on the adaptive discontinuous Galerkin methods for the tempered fractional diffusion equations. The DG schemes with interior penalty for the diffusion term and numerical flux for the convection term are used to solve the equations, and the detailed stability and convergence analyses are provided. Based on the derived posteriori error estimates, the local error indicator is designed. The theoretical results and the effectiveness of the adaptive DG methods are respectively verified and displayed by the extensive numerical experiments. The strategy of designing adaptive schemes presented in this paper works for the general PDEs with fractional operators.",0.4166666667],["the Pitman--Yor process provides a simple and mathematically tractable generalization","A simple proof of Pitman-Yor's Chinese restaurant process from its stick-breaking representation","summarize: For a long time, the Dirichlet process has been the gold standard discrete random measure in Bayesian nonparametrics. The Pitman--Yor process provides a simple and mathematically tractable generalization, allowing for a very flexible control of the clustering behaviour. Two commonly used representations of the Pitman--Yor process are the stick-breaking process and the Chinese restaurant process. The former is a constructive representation of the process which turns out very handy for practical implementation, while the latter describes the partition distribution induced. However, the usual proof of the connection between them is indirect and involves measure theory. We provide here an elementary proof of Pitman--Yor's Chinese Restaurant process from its stick-breaking representation.",0.2865557636],["the von Neumann entropy of a graph has recently found applications in complex networks","On the Von Neumann Entropy of Graphs","summarize: The von Neumann entropy of a graph is a spectral complexity measure that has recently found applications in complex networks analysis and pattern recognition. Two variants of the von Neumann entropy exist based on the graph Laplacian and normalized graph Laplacian, respectively. Due to its computational complexity, previous works have proposed to approximate the von Neumann entropy, effectively reducing it to the computation of simple node degree statistics. Unfortunately, a number of issues surrounding the von Neumann entropy remain unsolved to date, including the interpretation of this spectral measure in terms of structural patterns, understanding the relation between its two variants, and evaluating the quality of the corresponding approximations. In this paper we aim to answer these questions by first analysing and comparing the quadratic approximations of the two variants and then performing an extensive set of experiments on both synthetic and real-world graphs. We find that 1) the two entropies lead to the emergence of similar structures, but with some significant differences; 2) the correlation between them ranges from weakly positive to strongly negative, depending on the topology of the underlying graph; 3) the quadratic approximations fail to capture the presence of non-trivial structural patterns that seem to influence the value of the exact entropies; 4) the quality of the approximations, as well as which variant of the von Neumann entropy is better approximated, depends on the topology of the underlying graph.",0.2631578947],["supplementary material is given as Parts I and II. Part II establishes results","Inequalities for the fundamental Robin eigenvalue of the Laplacian for box-shaped domains","summarize: This document consists of two papers, both submitted, and supplementary material. The submitted papers are here given as Parts I and II. Part I establishes results, used in Part II, 'on functions and inverses, both positive, decreasing and convex'. Part II uses results from Part I to extablish 'inequalities for the fundamental Robin eigenvalue for the Laplacian on N-dimensional boxes'",0.0476190476],["deep models are state-of-the-art for many vision tasks including video action recognition and","Excitation Backprop for RNNs","summarize: Deep models are state-of-the-art for many vision tasks including video action recognition and video captioning. Models are trained to caption or classify activity in videos, but little is known about the evidence used to make such decisions. Grounding decisions made by deep networks has been studied in spatial visual content, giving more insight into model predictions for images. However, such studies are relatively lacking for models of spatiotemporal visual content - videos. In this work, we devise a formulation that simultaneously grounds evidence in space and time, in a single pass, using top-down saliency. We visualize the spatiotemporal cues that contribute to a deep model's classification\/captioning output using the model's internal representation. Based on these spatiotemporal cues, we are able to localize segments within a video that correspond with a specific action, or phrase from a caption, without explicitly optimizing\/training for these tasks.",0.0769230769],["lipograms are a literary composition which omits one or more letters.","Protein Lipograms","summarize: Linguistic analysis of protein sequences is an underexploited technique. Here, we capitalize on the concept of the lipogram to characterize sequences at the proteome levels. A lipogram is a literary composition which omits one or more letters. A protein lipogram likewise omits one or more types of amino acid. In this article, we establish a usable terminology for the decomposition of a sequence collection in terms of the lipogram. Next, we characterize Uniref50 using a lipogram decomposition. At the global level, protein lipograms exhibit power-law properties. A clear correlation with metabolic cost is seen. Finally, we use the lipogram construction to differentiate proteomes between the four branches of the tree-of-life: archaea, bacteria, eukaryotes and viruses. We conclude from this pilot study that the lipogram demonstrates considerable potential as an additional tool for sequence analysis and proteome classification.",0.0666666667],["in vivo wireless body area networks are shaping the future of healthcare. it is necessary","Anatomical Region-Specific In Vivo Wireless Communication Channel Characterization","summarize: In vivo wireless body area networks and their associated technologies are shaping the future of healthcare by providing continuous health monitoring and noninvasive surgical capabilities, in addition to remote diagnostic and treatment of diseases. To fully exploit the potential of such devices, it is necessary to characterize the communication channel which will help to build reliable and high-performance communication systems. This paper presents an in vivo wireless communication channel characterization for male torso both numerically and experimentally considering various organs at 915 MHz and 2.4 GHz. A statistical path loss model is introduced, and the anatomical region-specific parameters are provided. It is found that the mean PL in dB scale exhibits a linear decaying characteristic rather than an exponential decaying profile inside the body, and the power decay rate is approximately twice at 2.4 GHz as compared to 915 MHz. Moreover, the variance of shadowing increases significantly as the in vivo antenna is placed deeper inside the body since the main scatterers are present in the vicinity of the antenna. Multipath propagation characteristics are also investigated to facilitate proper waveform designs in the future wireless healthcare systems, and a root-mean-square delay spread of 2.76 ns is observed at 5 cm depth. Results show that the in vivo channel exhibit different characteristics than the classical communication channels, and location dependency is very critical for accurate, reliable, and energy-efficient link budget calculations.",0.0],["multi-level summarizer supervised method to construct abstractive summaries.","Interpretable Multi-Headed Attention for Abstractive Summarization at Controllable Lengths","summarize: Abstractive summarization at controllable lengths is a challenging task in natural language processing. It is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand. At the same time, when it comes to trusting machine-generated summaries, explaining how a summary was constructed in human-understandable terms may be critical. We propose Multi-level Summarizer , a supervised method to construct abstractive summaries of a text document at controllable lengths. The key enabler of our method is an interpretable multi-headed attention mechanism that computes attention distribution over an input document using an array of timestep independent semantic kernels. Each kernel optimizes a human-interpretable syntactic or semantic property. Exhaustive experiments on two low-resource datasets in the English language show that MLS outperforms strong baselines by up to 14.70% in the METEOR score. Human evaluation of the summaries also suggests that they capture the key concepts of the document at various length-budgets.",0.0],["study aims to compare different designs for the world men's handball championships.","A simulation comparison of tournament designs for the World Men's Handball Championships","summarize: The study aims to compare different designs for the World Men's Handball Championships. This event, organised in every two years, has adopted four hybrid formats consisting of knockout and round-robin stages in recent decades, including a change of design between the two recent championships in 2017 and 2019. They are evaluated under two extremal seeding policies with respect to various outcome measures through Monte-Carlo simulations. We find that the ability to give the first four positions to the strongest teams, as well as the expected quality and outcome uncertainty of the final is not necessarily a monotonic function of the number of matches played: the most frugal format is the second best with respect to these outcome measures, making it a good compromise in an unavoidable trade-off. A possible error is identified in a particular design. The relative performance of the formats is independent of the seeding rules and the competitive balance of the teams. The recent reform is demonstrated to have increased the probability of winning for the top teams. Our results have useful implications for the organisers of hybrid tournaments.",0.3333333333],["entropy solutions are uniformly bounded with respect to space and time variables","Large time behavior of entropy solutions to one-dimensional unipolar hydrodynamic model for semiconductor devices","summarize: We are concerned with the global existence and large time behavior of entropy solutions to the one dimensional unipolar hydrodynamic model for semiconductors in the form of Euler-Poisson equations in a bounded interval. In this paper, we first prove the global existence of entropy solution by vanishing viscosity and compensated compactness framework. In particular, the solutions are uniformly bounded with respect to space and time variables by introducing modified Riemann invariants and the theory of invariant region. Based on the uniform estimates of density, we further show that the entropy solution converges to the corresponding unique stationary solution exponentially in time. No any smallness condition is assumed on the initial data and doping profile. Moreover, the novelty in this paper is about the unform bound with respect to time for the weak solutions of the isentropic Euler-Possion system.",0.282160575],["a new primal-dual mixed finite element method is introduced. the method","A Conforming Primal-Dual Mixed Formulation for the 2D Multiscale Porous Media Flow Problem","summarize: In this paper a new primal-dual mixed finite element method is introduced, aimed to model multiscale problems with several geometric subregions in the domain of interest. In each of these regions porous media fluid flow takes place, but governed by physical parameters at a different scale; additionally, a fluid exchange through contact interfaces occurs between neighboring regions. The well-posedness of the primal-dual mixed finite element formulation on bounded simply connected polygonal domains of the plane is presented. Next, the convergence of the discrete solution to the exact solution of the problem is discussed, together with the convergence rate analysis. Finally, the numerical examples illustrate the method's capabilities to handle multiscale problems and interface discontinuities as well as experimental rates of convergence.",0.2779176394],["smart world concept and smart world concept addressed in the fourth industrial revolution. new challenges in distributed","Smart systems, the fourth industrial revolution and new challenges in distributed computing","summarize: Smart systems and the smart world concept are addressed in the framework of the fourth industrial revolution. New challenges in distributed autonomous robots and computing are considered. An illustration of a new kind of smart and reconfigurable distributed modular robot system is given. A prototype is also presented as well as the associated distributed algorithm.",0.5185185185],["proposed determinant approximation allows us to construct a voltage stability index.","Hierarchical and Distributed Monitoring of Voltage Stability in Distribution Networks","summarize: We consider the problem of quantifying and assessing the steady-state voltage stability in radial distribution networks. Our approach to the voltage stability problem is based on a local, approximate, and yet highly accurate characterization of the determinant of the Jacobian of the power flow equations parameterized according to the branch-flow model. The proposed determinant approximation allows us to construct a voltage stability index that can be computed in a fully distributed or in a hierarchical fashion, resulting in a scalable approach to the assessment of steady-state voltage stability. Finally, we provide upper bounds for the approximation error and we numerically validate the quality and the robustness of the proposed approximation with the IEEE 123-bus test feeder.",0.3529411765],["moTe2 involves attractive polymorphic TMD crystals. they can exist in","Mechanical responses of two-dimensional MoTe2; pristine 2H, 1T and 1T' and 1T'\/2H heterostructure","summarize: Transition metal dichalcogenides are currently among the most interesting two-dimensional materials due to their outstanding properties. MoTe2 involves attractive polymorphic TMD crystals which can exist in three different 2D atomic lattices of 2H, 1T and 1T', with diverse properties, like semiconducting and metallic electronic characters. Using the polymorphic heteroepitaxy, most recently coplanar semiconductor\/metal few-layer MoTe2 heterostructures were experimentally synthesized, highly promising to build circuit components for next generation nanoelectronics. Motivated by the recent experimental advances, we conducted first-principles calculations to explore the mechanical properties of single-layer MoTe2 structures. We first studied the mechanical responses of pristine and single-layer 2H-, 1T- and 1T'-MoTe2. In these cases we particularly analyzed the possibility of engineering of the electronic properties of these attractive 2D structures using the biaxial or uniaxial tensile loadings. Finally, the mechanical-failure responses of 1T'\/2H-MoTe2 heterostructure were explored, which confirms the remarkable strength of this novel 2D system.",0.067347111],["a zero-dimensional ideal I in a polynomial ring is a","Computing and Using Minimal Polynomials","summarize: Given a zero-dimensional ideal I in a polynomial ring, many computations start by finding univariate polynomials in I. Searching for a univariate polynomial in I is a particular case of considering the minimal polynomial of an element in P\/I. It is well known that minimal polynomials may be computed via elimination, therefore this is considered to be a resolved problem. But being the key of so many computations, it is worth investigating its meaning, its optimization, its applications . We present efficient algorithms for computing the minimal polynomial of an element of P\/I. For the specific case where the coefficients are in Q, we show how to use modular methods to obtain a guaranteed result. We also present some applications of minimal polynomials, namely algorithms for computing radicals and primary decompositions of zero-dimensional ideals, and also for testing radicality and maximality.",0.4230769231],["lines restricted to invariant submanifolds generally gives rise to nonlinear dynamics","Nonlinear Dynamics from Linear Quantum Evolutions","summarize: Linear dynamics restricted to invariant submanifolds generally gives rise to nonlinear dynamics. Submanifolds in the quantum framework may emerge for several reasons: one could be interested in specific properties possessed by a given family of states, either as a consequence of experimental constraints or inside an approximation scheme. In this work we investigate such issues in connection with a one parameter group ",0.0],["a central potential is dominated by a central potential. the system is driven into","Isotropic-Nematic Phase Transitions in Gravitational Systems","summarize: We examine dense self-gravitating stellar systems dominated by a central potential, such as nuclear star clusters hosting a central supermassive black hole. Different dynamical properties of these systems evolve on vastly different timescales. In particular, the orbital-plane orientations are typically driven into internal thermodynamic equilibrium by vector resonant relaxation before the orbital eccentricities or semimajor axes relax. We show that the statistical mechanics of such systems exhibit a striking resemblance to liquid crystals, with analogous ordered-nematic and disordered-isotropic phases. The ordered phase consists of bodies orbiting in a disk in both directions, with the disk thickness depending on temperature, while the disordered phase corresponds to a nearly isotropic distribution of the orbit normals. We show that below a critical value of the total angular momentum, the system undergoes a first-order phase transition between the ordered and disordered phases. At the critical point the phase transition becomes second-order while for higher angular momenta there is a smooth crossover. We also find metastable equilibria containing two identical disks with mutual inclinations between ",0.375],["the purpose of the present work is to answer the question how the surface preparation influences the oxid","Effect of surface mechanical treatment on the oxidation behavior of FeAl-model alloy","summarize: Fe based alloys are commonly used in almost every sector of human life. For different reasons, the surfaces of the real parts are prepared using different methods, e.g., mirror-like polishing, grit-blasting, etc. The purpose of the present work is to answer the question how the surface preparation influences the oxidation behavior of Fe-based alloys. To answer this question, a high purity model alloy, Fe 5 wt Al, was isothermally oxidized in a thermogravimetrical furnace. The post-exposure analysis included SEM\/EDS and XRD. The surface roughness was determined by a contact and laser profilometer. The obtained results demonstrate that the mechanical surface preparation influences oxidation kinetics as well as the microstructure of the oxide scale formed on the alloy at both studied temperatures. Namely, polishing and grinding caused local formation of Fe-rich nodules and sub-layer of protective Al2O3. In contrast, gritblasting leads to the formation of a thick outer Fe-oxide and internal aluminum nitridation. A significant increase in the oxidation rate of the material after gritblasting was attributed to grain refinement in the near-surface region, resulting in an increase in easy diffusion paths, namely grain boundaries.",0.2105263158],["a loxodrome on the surface of the globe is related to the straight line","On some information geometric structures concerning Mercator projections","summarize: Some information geometric structures concerning the Mercator projections are studied. It is known that a loxodrome on the surface of the globe is related to the straight line on a Mercator map by the Mercator projection. It is not well known that an affine connection with torsion plays a fundamental role to describe an auto-parallel path on the surface. Based on these information geometric structures, Gauss distribution is reconsidered from the view point of the affine connection with a torsion. Some relations with deformed functions are also pointed out.",0.1739130435],["proposed protocol measures the weight for an edge the measuring the mobility patterns of the nodes and channel","An Efficient Routing Protocol for Secured Communication in Cognitive Radio Sensor Networks","summarize: This paper introduces an efficient reactive routing protocol considering the mobility and the reliability of a node in Cognitive Radio Sensor Networks . The proposed protocol accommodates the dynamic behavior of the spectrum availability and selects a stable transmission path from a source node to the destination. Outlined as a weighted graph problem, the proposed protocol measures the weight for an edge the measuring the mobility patterns of the nodes and channel availability. Furthermore, the mobility pattern of a node is defined in the proposed routing protocol from the viewpoint of distance, speed, direction, and node's reliability. Besides, the spectrum awareness in the proposed protocol is measured over the number of shared common channels and the channel quality. It is anticipated that the proposed protocol shows efficient routing performance by selecting stable and secured paths from source to destination. Simulation is carried out to assess the performance of the protocol where it is witnessed that the proposed routing protocol outperforms existing ones.",0.03125],["boron nitride has a thermal conductivity of 751 W\/","High thermal conductivity of high-quality monolayer boron nitride and its thermal expansion","summarize: Heat management becomes more and more critical, especially in miniaturized modern devices, so the exploration of highly thermally conductive materials with electrical insulation and favorable mechanical properties is of great importance. Here, we report that high-quality monolayer boron nitride has a thermal conductivity of 751 W\/mK at room temperature. Though smaller than that of graphene, this value is larger than that of cubic boron nitride and only second to those of diamond and lately discovered cubic boron arsenide . Monolayer BN has the second largest \\k per unit weight among all semiconductors and insulators, just behind diamond, if density is considered. The \\k of atomically thin BN decreases with increased thickness. Our large-scale molecular dynamic simulations using Green-Kubo formalism accurately reproduce this trend, and the density functional theory calculations reveal the main scattering mechanism. The thermal expansion coefficients of monolayer to trilayer BN at 300-400 K are also experimentally measured, and the results are comparable to atomistic ab initio DFT calculations in a wider range of temperatures. Thanks to its wide bandgap, high thermal conductivity, outstanding strength, good flexibility, and excellent thermal and chemical stability, atomically thin BN is a strong candidate for heat dissipation applications, especially in the next generation of flexible electronic devices.",0.5211136804],["a cluster-based histogram is called equal intensity k-means space partition","Concept Drift Detection via Equal Intensity k-means Space Partitioning","summarize: Data stream poses additional challenges to statistical classification tasks because distributions of the training and target samples may differ as time passes. Such distribution change in streaming data is called concept drift. Numerous histogram-based distribution change detection methods have been proposed to detect drift. Most histograms are developed on grid-based or tree-based space partitioning algorithms which makes the space partitions arbitrary, unexplainable, and may cause drift blind-spots. There is a need to improve the drift detection accuracy for histogram-based methods with the unsupervised setting. To address this problem, we propose a cluster-based histogram, called equal intensity k-means space partitioning . In addition, a heuristic method to improve the sensitivity of drift detection is introduced. The fundamental idea of improving the sensitivity is to minimize the risk of creating partitions in distribution offset regions. Pearson's chi-square test is used as the statistical hypothesis test so that the test statistics remain independent of the sample distribution. The number of bins and their shapes, which strongly influence the ability to detect drift, are determined dynamically from the sample based on an asymptotic constraint in the chi-square test. Accordingly, three algorithms are developed to implement concept drift detection, including a greedy centroids initialization algorithm, a cluster amplify-shrink algorithm, and a drift detection algorithm. For drift adaptation, we recommend retraining the learner if a drift is detected. The results of experiments on synthetic and real-world datasets demonstrate the advantages of EI-kMeans and show its efficacy in detecting concept drift.",0.3333333333],["Hitomi carries two Hard X-ray Telescopes that can focus","Inorbit Performance of the Hard X-ray Telescope on board the Hitomi satellite","summarize: Hitomi carries two Hard X-ray Telescopes that can focus X-rays up to 80 keV. Combined with the Hard X-ray Imagers that detect the focused X-rays, imaging spectroscopy in the high-energy band from 5 keV to 80 keV is made possible. We studied characteristics of HXTs after the launch such as the encircled energy function and the effective area using the data of a Crab observation. The half power diameters in the 5--80 keV band evaluated from the EEFs are 1.59 arcmin for HXT-1 and 1.65 arcmin for HXT-2. Those are consistent with the HPDs measured with ground experiments when uncertainties are taken into account. We can conclude that there is no significant change in the characteristics of the HXTs before and after the launch. The off-axis angle of the aim point from the optical axis is evaluated to be less than 0.5 arcmin for both HXT-1 and HXT-2. The best-fit parameters for the Crab spectrum obtained with the HXT-HXI system are consistent with the canonical values.",0.2388437702],["modified gravitational theory by Hajdukovic predicts, among other things, anomal","A comment on Can observations inside the Solar System reveal the gravitational properties of the quantum vacuum? by D.S. Hajdukovic","summarize: The modified gravitational theory by Hajdukovic , based on the idea that quantum vacuum contains virtual gravitational dipoles, predicts, among other things, anomalous secular precessions of the planets of the Solar System as large as ",0.1103638324],["the popularity of these devices encourages malware developer to penetrate the market with malicious apps to anno","An investigation of the classifiers to detect android malicious apps","summarize: Android devices are growing exponentially and are connected through the internet accessing billion of online websites. The popularity of these devices encourages malware developer to penetrate the market with malicious apps to annoy and disrupt the victim. Although, for the detection of malicious apps different approaches are discussed. However, proposed approaches are not suffice to detect the advanced malware to limit\/prevent the damages. In this, very few approaches are based on opcode occurrence to classify the malicious apps. Therefore, this paper investigates the five classifiers using opcodes occurrence as the prominent features for the detection of malicious apps. For the analysis, we use WEKA tool and found that FT detection accuracy is best among the investigated classifiers. However, true positives rate i.e. malware detection rate is highest by RF and fluctuate least with the different number of prominent features compared to other studied classifiers. The analysis shows that overall accuracy is majorly affected by the false positives of the classifier.",0.3043478261],["a vacancy under compression is fissioned into a pair of dislocation","Stress driven fractionalization of vacancies in regular packings of elastic particles","summarize: Elucidating the interplay of defect and stress at the microscopic level is a fundamental physical problem that has strong connection with materials science. Here, based on the two-dimensional crystal model, we show that the instability mode of vacancies with varying size and morphology conforms to a common scenario. A vacancy under compression is fissioned into a pair of dislocations that glide and vanish at the boundary. This neat process is triggered by the local shear stress around the vacancy. The remarkable fractionalization of vacancies creates rich modes of interaction between vacancies and other topological defects, and provides a new dimension for mechanical engineering of defects in extensive crystalline structures.",0.5652173913],["the Hessian discretisation method provides a unified convergence analysis framework. some examples","Improved ","summarize: The Hessian discretisation method for fourth order linear elliptic equations provides a unified convergence analysis framework based on three properties namely coercivity, consistency, and limit-conformity. Some examples that fit in this approach include conforming and nonconforming finite element methods, finite volume methods and methods based on gradient recovery operators. A generic error estimate has been established in ",0.0],["cell-free massive multi-input network is downlink of multiple-input networks","Energy Efficiency in Cell-Free Massive MIMO with Zero-Forcing Precoding Design","summarize: We consider the downlink of a cell-free massive multiple-input multiple-output network where numerous distributed access points serve a smaller number of users under time division duplex operation. An important issue in deploying cell-free networks is high power consumption, which is proportional to the number of APs. This issue has raised the question as to their suitability for green communications in terms of the total energy efficiency . To tackle this, we develop a novel low-complexity power control technique with zero-forcing precoding design to maximize the energy efficiency of cell-free massive MIMO taking into account the backhaul power consumption and the imperfect channel state information.",0.0],["the extended jet structures of radio galaxies represent an ideal acceleration site for high energy co","High Energy Cosmic Rays from Fanaroff-Riley Radio Galaxies","summarize: The extended jet structures of radio galaxies represent an ideal acceleration site for High Energy Cosmic Rays and a recent model showed that the HECR data can be explained by these sources, if the arrival directions of HECRs at energies ",0.125],["the Connectionist Temporal Classification has achieved great success in sequence to sequence analysis tasks.","A Hardware-Oriented and Memory-Efficient Method for CTC Decoding","summarize: The Connectionist Temporal Classification has achieved great success in sequence to sequence analysis tasks such as automatic speech recognition and scene text recognition . These applications can use the CTC objective function to train the recurrent neural networks , and decode the outputs of RNNs during inference. While hardware architectures for RNNs have been studied, hardware-based CTCdecoders are desired for high-speed CTC-based inference systems. This paper, for the first time, provides a low-complexity and memory-efficient approach to build a CTC-decoder based on the beam search decoding. Firstly, we improve the beam search decoding algorithm to save the storage space. Secondly, we compress a dictionary and use it as the language model. Meanwhile searching this dictionary is trivial. Finally, a fixed-point CTC-decoder for an English ASR and an STR task using the proposed method is implemented with C++ language. It is shown that the proposed method has little precision loss compared with its floating-point counterpart. Our experiments demonstrate the compression ratio of the storage required by the proposed beam search decoding algorithm are 29.49 and 17.95 .",0.0625],["we study the solutions of a generalized Allen-Cahn equation deduced","Stability of the stationary solutions of the Allen-Cahn equation with non-constant stiffness","summarize: We study the solutions of a generalized Allen-Cahn equation deduced from a Landau energy functional, endowed with a non-constant higher order stiffness. We assume the stiffness to be a positive function of the field and we discuss the stability of the stationary solutions proving both linear and local non-linear stability.",0.5668135983],["threshold functions for homogeneous random intersection graphs have to be modified and extended.","The coupling method for inhomogeneous random intersection graphs","summarize: We present new results concerning threshold functions for a wide family of random intersection graphs. To this end we apply the coupling method used for establishing threshold functions for homogeneous random intersection graphs introduced by Karo\\'nski, Scheinerman, and Singer--Cohen. In the case of inhomogeneous random intersection graphs the method has to be considerably modified and extended. By means of the altered method we are able to establish threshold functions for a general random intersection graph for such properties as ",0.4285714286],["this paper examines the degree to which connectivity and automation can reduce the overall fuel consumption of on","Reducing Road Vehicle Fuel Consumption by Exploiting Connectivity and Automation: A Literature Survey","summarize: This paper examines the degree to which connectivity and automation can potentially reduce the overall fuel consumption of on-road vehicles. The paper begins with a simulation study highlighting the tradeoff between: the fuel that a vehicle can save through speed trajectory shaping, versus the additional inter-vehicle spacing needed for this trajectory shaping to be feasible. This study shows that connectivity and automation are essential, rather than merely useful, for substantial reductions in the fuel consumed by fixed on-road vehicle powertrain\/chassis configurations in traffic. Motivated by this insight, we survey the literature on the fuel savings achievable through different connected\/automated vehicle technologies. This includes optimal vehicle routing, eco-arrival\/departure at intersections, platooning, speed trajectory optimization, predictive driveline disengagement, predictive gear shifting, and predictive powertrain accessory control. This survey shows that the ability to shape vehicle speed trajectories collaboratively plays a dominant role in reducing urban\/suburban fuel consumption, while platooning plays a dominant role in influencing the attainable fuel savings on the highway. Moreover, the survey shows that the degree to which connectivity\/automation can reduce on-road vehicle fuel consumption, in both urban\/suburban and highway settings, depends critically on the integration of powertrain- and chassis-level control.",0.24],["a base station needs to mitigate interference to users associated with other coexisting networks in the","Unsupervised frequency clustering algorithm for null space estimation in wideband spectrum sharing networks","summarize: In spectrum sharing networks, a base station needs to mitigate the interference to users associated with other coexisting network in the same band. The BS can achieve this by transmitting its downlink signal in the null space of channels to such users. However, under a wideband scenario, the BS needs to estimate null space matrices using the received signal from such non-cooperative users in each frequency bin where the users are active. To reduce the computational complexity of this operation, we propose a frequency clustering algorithm that exploits the channel correlations among adjacent frequency bins. The proposed algorithm forms clusters of frequency bins with correlated channel vectors without prior knowledge of the channels and obtains a single null space matrix for each cluster. We show that the number of matrices and the number of eigenvalue decompositions required to obtain the null space significantly reduce using the proposed clustering algorithm.",0.3461538462],["spectrum sharing is one of the promising solutions to meet the spectrum demand in 5G networks.","Spectrum Matching in Licensed Spectrum Sharing","summarize: Spectrum sharing is one of the promising solutions to meet the spectrum demand in 5G networks that results from the emerging services like machine to machine and vehicle to infrastructure communication. The idea is to allow a set of entities access the spectrum whenever and wherever it is unused by the licensed users. In the proposed framework, different spectrum provider networks with surplus spectrum available may rank the operators requiring the spectrum, called spectrum users hereafter, differently in terms of their preference to lease spectrum, based for example on target business market considerations of the SUs. Similarly, SUs rank SPs depending on a number of criteria, for example based on coverage and availability in a service area. Ideally, both SPs and SUs prefer to provide\/get spectrum to\/from the operator of their first choice, but this is not necessarily always possible due to conflicting preferences. We apply matching theory algorithms with the aim to resolve the conflicting preferences of the SPs and SUs and quantify the effect of the proposed matching theory approach on establishing preferred provider-user network pairs. We discuss both one-to-one and many-to-one spectrum sharing scenarios and evaluate the performance using Monte Carlo simulations. The results show that comprehensive gains in terms of preferred matching of the provider-user network pairs can be achieved by applying matching theory for spectrum sharing as compared to uncoordinated spectrum allocation of the available spectrum to the SUs.",0.1304347826],["simple cases of acoustic black holes were studied in the references.","New examples of Hawking radiation from acoustic black holes","summarize: Rotating acoustic metrics may have black holes inside the ergosphere. Simple cases of acoustic black holes were studied in the references , . In the present paper we study the Hawking radiation for more complicated cases of acoustic black holes including black holes that have corner points.",0.4545454545],["in this article we study the irreducibility of polynomials of the form.","Irreducibility criterion for certain trinomials","summarize: In this article we study the irreducibility of polynomials of the form ",0.0625],["the Colombian Andes is a cosmopolitan region with a potential","Low Dimensional Embedding of Climate Data for Radio Astronomical Site Testing in the Colombian Andes","summarize: We set out to evaluate the potential of the Colombian Andes for millimeter-wave astronomical observations. Previous studies for astronomical site testing in this region have suggested that nighttime humidity and cloud cover conditions make most sites unsuitable for professional visible-light observations. Millimeter observations can be done during the day, but require that the precipitable water vapor column above a site stays below ",0.4380499209],["global solutions often rely on monotonicity properties. but this often comes at the cost","Mixed Monotonic Programming for Fast Global Optimization","summarize: While globally optimal solutions to many convex programs can be computed efficiently in polynomial time, this is, in general, not possible for nonconvex optimization problems. Therefore, locally optimal approaches or other efficient suboptimal heuristics are usually applied for practical implementations. However, there is also a strong interest in computing globally optimal solutions of nonconvex problems in offline simulations in order to benchmark the faster suboptimal algorithms. Global solutions often rely on monotonicity properties. A common approach is to reformulate problems into a canonical monotonic optimization problem where the monotonicity becomes evident, but this often comes at the cost of nested optimizations, increased numbers of variables, and\/or slow convergence. The framework of mixed monotonic programming proposed in this paper avoids such performance-deteriorating reformulations by revealing hidden monotonicity properties directly in the original problem formulation. By means of a wide range of application examples from the area of signal processing for communications , we demonstrate that the novel MMP approach leads to tremendous complexity reductions compared to state-of-the-art methods for global optimization. However, the framework is not limited to optimizing communication systems, and we expect that similar speed-ups can be obtained for optimization problems from other areas of research as well.",0.2105263158],["TAGS increased the average False Accept Rate from 4% to 26%.","Treadmill Assisted Gait Spoofing : An Emerging Threat to wearable Sensor-based Gait Authentication","summarize: In this work, we examine the impact of Treadmill Assisted Gait Spoofing on Wearable Sensor-based Gait Authentication . We consider more realistic implementation and deployment scenarios than the previous study, which focused only on the accelerometer sensor and a fixed set of features. Specifically, we consider the situations in which the implementation of WSGait could be using one or more sensors embedded into modern smartphones. Besides, it could be using different sets of features or different classification algorithms, or both. Despite the use of a variety of sensors, feature sets , and six different classification algorithms, TAGS was able to increase the average False Accept Rate from 4% to 26%. Such a considerable increase in the average FAR, especially under the stringent implementation and deployment scenarios considered in this study, calls for a further investigation into the design of evaluations of WSGait before its deployment for public use.",0.1515914397],["a network model which relies solely on bounding boxes can predict future traject","Simple means Faster: Real-Time Human Motion Forecasting in Monocular First Person Videos on CPU","summarize: We present a simple, fast, and light-weight RNN based framework for forecasting future locations of humans in first person monocular videos. The primary motivation for this work was to design a network which could accurately predict future trajectories at a very high rate on a CPU. Typical applications of such a system would be a social robot or a visual assistance system for all, as both cannot afford to have high compute power to avoid getting heavier, less power efficient, and costlier. In contrast to many previous methods which rely on multiple type of cues such as camera ego-motion or 2D pose of the human, we show that a carefully designed network model which relies solely on bounding boxes can not only perform better but also predicts trajectories at a very high rate while being quite low in size of approximately 17 MB. Specifically, we demonstrate that having an auto-encoder in the encoding phase of the past information and a regularizing layer in the end boosts the accuracy of predictions with negligible overhead. We experiment with three first person video datasets: CityWalks, FPL and JAAD. Our simple method trained on CityWalks surpasses the prediction accuracy of state-of-the-art method while being 9.6x faster on a CPU . We also demonstrate that our model can transfer zero-shot or after just 15% fine-tuning to other similar datasets and perform on par with the state-of-the-art methods on such datasets . To the best of our knowledge, we are the first to accurately forecast trajectories at a very high prediction rate of 78 trajectories per second on CPU.",0.2469229543],["a class of receivers which demultiplex an optical field into a set of ortho","Approaching Quantum Limited Super-Resolution Imaging without Prior Knowledge of the Object Location","summarize: A recently identified class of receivers which demultiplex an optical field into a set of orthogonal spatial modes prior to detection can surpass canonical diffraction limits on spatial resolution for simple incoherent imaging tasks. However, these mode-sorting receivers tend to exhibit high sensitivity to contextual nuisance parameters , raising questions on their viability in realistic imaging scenarios where little or no prior information about the scene is available. We propose a multi-stage passive imaging strategy which segments the total recording time between different physical measurements to build up the required prior information for near quantum-optimal imaging performance at sub-Rayleigh length scales. We show via Monte Carlo simulations that an adaptive two-stage scheme which dynamically allocates the total recording time between a traditional direct detection measurement and a binary mode-sorting receiver outperforms idealized direct detection alone for simple estimation tasks when no prior knowledge of the object centroid is available, achieving one to two orders of magnitude improvement in mean squared error. Our scheme can be generalized for more sophisticated imaging tasks with multiple parameters and minimal prior information.",0.4583333333],["spectrally-efficient network is a major research interest for providing secure networks.","Decoding Orders and Power Allocation for Untrusted NOMA: A Secrecy Perspective","summarize: The amalgamation of non-orthogonal multiple access and physical layer security is a significant research interest for providing spectrally-efficient secure fifth-generation networks. Observing the secrecy issue among multiplexed NOMA users, which is stemmed from successive interference cancellation based decoding at receivers, we focus on safeguarding untrusted NOMA. Considering the problem of each user's privacy from each other, the appropriate secure decoding order and power allocation for users are investigated. Specifically, a decoding order strategy is proposed which is efficient in providing positive secrecy at all NOMA users. An algorithm is also provided through which all the feasible secure decoding orders in accordance with the proposed decoding order strategy can be obtained. Further, in order to maximize the sum secrecy rate of the system, the joint solution of decoding order and PA is obtained numerically. Also, a sub-optimal decoding order solution is proposed. Lastly, numerical results present useful insights on the impact of key system parameters and demonstrate that average secrecy rate performance gain of about 27 dB is obtained by the jointly optimized solution over different relevant schemes.",0.2],["most bug assignment approaches use text classification and information retrieval techniques. the textual contents of","Using Categorical Features in Mining Bug Tracking Systems to Assign Bug Reports","summarize: Most bug assignment approaches utilize text classification and information retrieval techniques. These approaches use the textual contents of bug reports to build recommendation models. The textual contents of bug reports are usually of high dimension and noisy source of information. These approaches suffer from low accuracy and high computational needs. In this paper, we investigate whether using categorical fields of bug reports, such as component to which the bug belongs, are appropriate to represent bug reports instead of textual description. We build a classification model by utilizing the categorical features, as a representation, for the bug report. The experimental evaluation is conducted using three projects namely NetBeans, Freedesktop, and Firefox. We compared this approach with two machine learning based bug assignment approaches. The evaluation shows that using the textual contents of bug reports is important. In addition, it shows that the categorical features can improve the classification accuracy.",0.0],["theorem is a new type of the darbo fixed point theore","A New Type of Darbo's Fixed Point Theorem Defined by The Sequences of Functions","summarize: In this paper, we introduce a new type of Darbo's fixed point theorem by using concept of function sequences with shifting distance property. Afterward, we investigate existence of fixed point under this the theorem. Also we are going to give interesting example held the conditions of sequences of functions",0.1015067182],["a range of models have been proposed to explain and predict popularity. but there is","Will This Video Go Viral? Explaining and Predicting the Popularity of Youtube Videos","summarize: What makes content go viral? Which videos become popular and why others don't? Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. HIPie -- an interactive visualization system -- is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and predict future popularity, to identify viral videos, and to estimate response to online promotion.",0.35],["new infrastructures will be at a greater risk if not dealt with at an early age","Need for Critical Cyber Defence, Security Strategy and Privacy Policy in Bangladesh - Hype or Reality?","summarize: Cyber security is one of the burning issues in modern world. Increased IT infrastructure has given rise to enormous chances of security breach. Bangladesh being a relatively new member of cyber security arena has its own demand and appeal. Digitalization is happening in Bangladesh for last few years at an appreciable rate. People are being connected to the worldwide web community with their smart devices. These devices have their own vulnerability issues as well as the data shared over the internet has a very good chances of getting breached. Common vulnerability issues like infecting the device with malware, Trojan, virus are on the rise. Moreover, a lack of proper cyber security policy and strategy might make the existing situation at the vulnerable edge of tipping point. Hence the upcoming new infrastructures will be at a greater risk if the issues are not dealt with at an early age. In this paper common vulnerability issues including their recent attacks on cyber space of Bangladesh, cyber security strategy and need for data privacy policy is discussed and analysed briefly.",0.3928571429],["analysis estimator is a direct study of the theory. we then extend the theory to","Oracle inequalities for square root analysis estimators with application to total variation penalties","summarize: Through the direct study of the analysis estimator we derive oracle inequalities with fast and slow rates by adapting the arguments involving projections by Dalalyan, Hebiri and Lederer . We then extend the theory to the square root analysis estimator. Finally, we focus on total variation regularized estimators on graphs and obtain constant-friendly rates, which, up to log-terms, match previous results obtained by entropy calculations. We also obtain an oracle inequality for the total variation regularized estimator over the cycle graph.",0.3],["explainability has been a goal for Artificial Intelligence systems since their conception. the","Explanation Ontology: A Model of Explanations for User-Centered AI","summarize: Explainability has been a goal for Artificial Intelligence systems since their conception, with the need for explainability growing as more complex AI models are increasingly used in critical, high-stakes settings such as healthcare. Explanations have often added to an AI system in a non-principled, post-hoc manner. With greater adoption of these systems and emphasis on user-centric explainability, there is a need for a structured representation that treats explainability as a primary consideration, mapping end user needs to specific explanation types and the system's AI capabilities. We design an explanation ontology to model both the role of explanations, accounting for the system and user attributes in the process, and the range of different literature-derived explanation types. We indicate how the ontology can support user requirements for explanations in the domain of healthcare. We evaluate our ontology with a set of competency questions geared towards a system designer who might use our ontology to decide which explanation types to include, given a combination of users' needs and a system's capabilities, both in system design settings and in real-time operations. Through the use of this ontology, system designers will be able to make informed choices on which explanations AI systems can and should provide.",0.2631578947],["the geometric Langlands Correspondence cite demonstrates that the construction of a","Quantization of Fields and Automorphic Representations","summarize: In this paper we use the quantization of fields based on Geometric Langlands Correspondence \\cite to realize the automorphic representations of some concrete series of groups: for the affine Heisenberg groups it is reduced to the construction of the affine Kac-Moody representation by the Weyl relations in Fock spaces. For the solvable and nilpotent groups following the construction we show that it is the result of applying the constructions of irreducible unitary representation via the geometric quantization and the construction of positive energy representations and finally, for the semi-simple or reductive Lie groups, using the Geometric Langlands Correspondence, we show that a repeated application of the construction give all the automorphic representations of reductive Lie groups: first we show that every representation of the fundamental group of Riemann surface into the dual Langlands groups ",0.2941176471],["the first projection method is a generalization of the classical primal-dual method","Projection based model order reduction methods for the estimation of vector-valued variables of interest","summarize: We propose and compare goal-oriented projection based model order reduction methods for the estimation of vector-valued functionals of the solution of parameter-dependent equations. The first projection method is a generalization of the classical primal-dual method to the case of vector-valued variables of interest. We highlight the role played by three reduced spaces: the approximation space and the test space associated to the primal variable, and the approximation space associated to the dual variable. Then we propose a Petrov-Galerkin projection method based on a saddle point problem involving an approximation space for the primal variable and an approximation space for an auxiliary variable. A goal-oriented choice of the latter space, defined as the sum of two spaces, allows us to improve the approximation of the variable of interest compared to a primal-dual method using the same reduced spaces. Then, for both approaches, we derive computable error estimates for the approximations of the variable of interest and we propose greedy algorithms for the goal-oriented construction of reduced spaces. The performance of the algorithms are illustrated on numerical examples and compared to standard algorithms.",0.3847644204],["a method is proposed for obtaining diffuse field measurements in untreated environments. a","A Novel Method for Obtaining Diffuse Field Measurements for Microphone Calibration","summarize: We propose a straightforward and cost-effective method to perform diffuse soundfield measurements for calibrating the magnitude response of a microphone array. Typically, such calibration is performed in a diffuse soundfield created in reverberation chambers, an expensive and time-consuming process. A method is proposed for obtaining diffuse field measurements in untreated environments. First, a closed-form expression for the spatial correlation of a wideband signal in a diffuse field is derived. Next, we describe a practical procedure for obtaining the diffuse field response of a microphone array in the presence of a non-diffuse soundfield by the introduction of random perturbations in the microphone location. Experimental spatial correlation data obtained is compared with the theoretical model, confirming that it is possible to obtain diffuse field measurements in untreated environments with relatively few loudspeakers. A 30 second test signal played from 4-8 loudspeakers is shown to be sufficient in obtaining a diffuse field measurement using the proposed method. An Eigenmike is then successfully calibrated at two different geographical locations.",0.4782608696],["we consider relations between the size, treewidth, and local crossing number of graphs embedded","Structure of Graphs with Locally Restricted Crossings","summarize: We consider relations between the size, treewidth, and local crossing number of graphs embedded on topological surfaces. We show that an ",0.0666666667],["foreign exchange broker considers trading in a currency triplet. the illiquid pair","Trading Foreign Exchange Triplets","summarize: We develop the optimal trading strategy for a foreign exchange broker who must liquidate a large position in an illiquid currency pair. To maximize revenues, the broker considers trading in a currency triplet which consists of the illiquid pair and two other liquid currency pairs. The liquid pairs in the triplet are chosen so that one of the pairs is redundant. The broker is risk-neutral and accounts for model ambiguity in the FX rates to make her strategy robust to model misspecification. When the broker is ambiguity neutral the trading strategy in each pair is independent of the inventory in the other two pairs in the triplet. We employ simulations to illustrate how the robust strategies perform. For a range of ambiguity aversion parameters, we find the mean Profit and Loss of the strategy increases and the standard deviation of the P&L decreases as ambiguity aversion increases.",0.1875],["new algorithm for finding video CNN architectures that capture rich spatio-temporal information in videos","Evolving Space-Time Neural Architectures for Videos","summarize: We present a new method for finding video CNN architectures that capture rich spatio-temporal information in videos. Previous work, taking advantage of 3D convolutions, obtained promising results by manually designing video CNN architectures. We here develop a novel evolutionary search algorithm that automatically explores models with different types and combinations of layers to jointly learn interactions between spatial and temporal aspects of video representations. We demonstrate the generality of this algorithm by applying it to two meta-architectures, obtaining new architectures superior to manually designed architectures. Further, we propose a new component, the iTGM layer, which more efficiently utilizes its parameters to allow learning of space-time interactions over longer time horizons. The iTGM layer is often preferred by the evolutionary algorithm and allows building cost-efficient networks. The proposed approach discovers new and diverse video architectures that were previously unknown. More importantly they are both more accurate and faster than prior models, and outperform the state-of-the-art results on multiple datasets we test, including HMDB, Kinetics, and Moments in Time. We will open source the code and models, to encourage future model development.",0.1052631579],["reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals. it","Brain-inspired photonic signal processor for periodic pattern generation and chaotic system emulation","summarize: Reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals. Its hardware implementations have received much attention because of their simplicity and remarkable performance on a series of benchmark tasks. In previous experiments the output was uncoupled from the system and in most cases simply computed offline on a post-processing computer. However, numerical investigations have shown that feeding the output back into the reservoir would open the possibility of long-horizon time series forecasting. Here we present a photonic reservoir computer with output feedback, and demonstrate its capacity to generate periodic time series and to emulate chaotic systems. We study in detail the effect of experimental noise on system performance. In the case of chaotic systems, this leads us to introduce several metrics, based on standard signal processing techniques, to evaluate the quality of the emulation. Our work significantly enlarges the range of tasks that can be solved by hardware reservoir computers, and therefore the range of applications they could potentially tackle. It also raises novel questions in nonlinear dynamics and chaos theory.",0.2941176471],["a scale bridging strategy has been developed to determine the flow stress of al-Cu","Strengthening of Al-Cu alloys by Guinier-Preston zones: predictions from atomistic simulations","summarize: A scale bridging strategy based in molecular statics and molecular dynamics simulations in combination with transition state theory has been developed to determine the flow stress of Al-Cu alloy containing Guinier-Preston zones. The athermal contribution to the flow stress was determined from the Taylor model, while the thermal contribution was obtained from the obstacle strength and the free energy barrier. These two magnitudes were obtained by means of molecular statics and molecular dynamics simulations of the interaction of edge dislocations with Guinier-Preston zones in two different orientations. The predictions of the model were compared with experimental data and were in reasonable agreement, showing the potential of atomistic simulations in combination with transition state theory to predict the flow stress of metallic alloys strengthened with precipitates.",0.3333333333],["in this paper, we present some new ideas on pressure to exploit the energy hidden in pressure","New ","summarize: In this paper, by invoking the appropriate decomposition of pressure to exploit the energy hidden in pressure, we present some new ",0.0],["two-qubit quantum gates play an essential role in quantum computing. the entang","Performance Assessment of Resonantly Driven Silicon Two-Qubit Quantum Gate","summarize: Two-qubit quantum gates play an essential role in quantum computing, whose operation critically depends on the entanglement between two qubits. Resonantly driven controlled-NOT gates based on silicon double quantum dots are studied theoretically. The physical mechanisms for effective gate modulation of the exchange coupling between two qubits are elucidated. Scaling behaviors of the singlet-triplet energy split, gate-switching speed, and gate fidelity are investigated as a function of the quantum dot spacing and modulation gate voltage. It is shown that the entanglement strength and gate-switching speed exponentially depend on the quantum dot spacing. A small spacing of ~10nm can promise a CNOT gate delay of <1 ns and reliable gate switching in the presence of decoherence. The results show promising performance potential of the resonantly driven two-qubit quantum gates based on aggressively scaled silicon DQDs.",0.1666666667],["Lie algebras were introduced by Kaplan as a class of real Lie algebras","On complex H-type Lie algebras","summarize: H-type Lie algebras were introduced by Kaplan as a class of real Lie algebras generalizing the familiar Heisenberg Lie algebra ",0.2413793103],["the proposed algorithm is arguably one of the fastest-emerging metaheuristics","Semi-steady-state Jaya Algorithm","summarize: The Jaya algorithm is arguably one of the fastest-emerging metaheuristics amongst the newest members of the evolutionary computation family. The present paper proposes a new, improved Jaya algorithm by modifying the update strategies of the best and the worst members in the population. Simulation results on a twelve-function benchmark test-suite as well as a real-world problem of practical importance show that the proposed strategy produces results that are better and faster in the majority of cases. Statistical tests of significance are used to validate the performance improvement.",0.0],["two-way relay is potentially an effective approach to spectrum sharing and aggregation.","On the DoF of Two-way ","summarize: Two-way relay is potentially an effective approach to spectrum sharing and aggregation by allowing simultaneous bidirectional transmissions between source-destinations pairs. This paper studies the two-way ",0.0],["we analyze the short cosmic ray intensity increase on June 22, 2015. we find that the","Cosmic ray short burst observed with the Global Muon Detector Network on June 22, 2015","summarize: We analyze the short cosmic ray intensity increase on June 22, 2015 utilizing a global network of muon detectors and derive the global anisotropy of cosmic ray intensity and the density with 10-minute time resolution. We find that the CRB was caused by a local density maximum and an enhanced anisotropy of cosmic rays both of which appeared in association with Earth's crossing of the heliospheric current sheet . This enhanced anisotropy was normal to the HCS and consistent with a diamagnetic drift arising from the spatial gradient of cosmic ray density, which indicates that cosmic rays were drifting along the HCS from the north of Earth. We also find a significant anisotropy along the HCS, lasting a few hours after the HCS crossing, indicating that cosmic rays penetrated into the inner heliosphere along the HCS. Based on the latest geomagnetic field model, we quantitatively evaluate the reduction of the geomagnetic cut-off rigidity and the variation of the asymptotic viewing direction of cosmic rays due to a major geomagnetic storm which occurred during the CRB and conclude that the CRB is not caused by the geomagnetic storm, but by a rapid change in the cosmic ray anisotropy and density outside the magnetosphere.",0.35],["complexity of many of these systems poses accountability challenges. data provenance methods show much promise as","Decision Provenance: Harnessing data flow for accountable systems","summarize: Demand is growing for more accountability regarding the technological systems that increasingly occupy our world. However, the complexity of many of these systems - often systems-of-systems - poses accountability challenges. A key reason for this is because the details and nature of the information flows that interconnect and drive systems, which often occur across technical and organisational boundaries, tend to be invisible or opaque. This paper argues that data provenance methods show much promise as a technical means for increasing the transparency of these interconnected systems. Specifically, given the concerns regarding ever-increasing levels of automated and algorithmic decision-making, and so-called 'algorithmic systems' in general, we propose decision provenance as a concept showing much promise. Decision provenance entails using provenance methods to provide information exposing decision pipelines: chains of inputs to, the nature of, and the flow-on effects from the decisions and actions taken throughout systems. This paper introduces the concept of decision provenance, and takes an interdisciplinary exploration into its potential for assisting accountability in algorithmic systems. We argue that decision provenance can help facilitate oversight, audit, compliance, risk mitigation, and user empowerment, and we also indicate the implementation considerations and areas for research necessary for realising its vision. More generally, we make the case that considerations of data flow, and systems more broadly, are important to discussions of accountability, and complement the considerable attention already given to algorithmic specifics.",0.1111111111],["commutative algebras in braided tensor categories do not admit faithful","Hopf algebra actions in tensor categories","summarize: We prove that commutative algebras in braided tensor categories do not admit faithful Hopf algebra actions unless they come from group actions. We also show that a group action allows us to see the algebra as the regular algebra in the representation category of the acting group.",0.3],["the availability of large-scale facial databases has led to the generation of extremely realistic fake facial content","GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection","summarize: The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks , have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. In this study, we focus on the synthesis of entire facial images, which is a specific type of facial manipulation. The main contributions of this study are four-fold: i) a novel strategy to remove GAN fingerprints from synthetic fake images based on autoencoders is described, in order to spoof facial manipulation detection systems while keeping the visual quality of the resulting images; ii) an in-depth analysis of the recent literature in facial manipulation detection; iii) a complete experimental assessment of this type of facial manipulation, considering the state-of-the-art fake detection systems , remarking how challenging is this task in unconstrained scenarios; and finally iv) we announce a novel public database, named iFakeFaceDB, yielding from the application of our proposed GAN-fingerprint Removal approach to already very realistic synthetic fake images. The results obtained in our empirical evaluation show that additional efforts are required to develop robust facial manipulation detection systems against unseen conditions and spoof techniques, such as the one proposed in this study.",0.347826087],["the second international workshop on safe control of autonomous vehicles took place on the 10th of","Proceedings 2nd International Workshop on Safe Control of Autonomous Vehicles","summarize: These are the proceedings of the Second International Workshop on Safe Control of Autonomous Vehicles, which took place on the 10th of April 2018 in Porto, Portugal as an affiliated workshop of CSPWeek. The task of this workshop is to identify open research problems, discuss recent achievements, bring together researchers in, e.g., control theory, adaptive systems, machine self-organization and autonomy, mobile intelligent robotics, transportation, traffic control, machine learning, software verification, and dependability and security engineering.",0.3333333333],["optimisation technique employs Monte-Carlo Basin-Hopping. s","A global optimisation study of the low-lying isomers of the alumina octomer ","summarize: We employ the Monte-Carlo Basin-Hopping global optimisation technique with inter- atomic pair potentials to generate low-energy candidates of stoichiometric alumina octomers ",0.1730017911],["crystallographic T-duality is a crystallographic concept inspired by the appearance of","Crystallographic T-duality","summarize: We introduce the notion of crystallographic T-duality, inspired by the appearance of ",0.1904761905],["chiral DWs are stabilized in these systems due to the spin-orbit","Collective Coordinate Models of Domain Wall Motion in Perpendicularly Magnetized Systems under the Spin Hall Effect and Longitudinal Fields","summarize: Recent studies on heterostructures of ultrathin ferromagnets sandwiched between a heavy metal layer and an oxide have highlighted the importance of spin-orbit coupling and broken inversion symmetry in domain wall motion. Specifically, chiral DWs are stabilized in these systems due to the Dzyaloshinskii-Moriya interaction . SOC can also lead to enhanced current induced DW motion, with the spin Hall effect suggested as the dominant mechanism for this observation. The efficiency of SHE driven DW motion depends on the internal magnetic structure of the DW, which could be controlled using externally applied longitudinal in-plane fields. In this work, micromagnetic simulations and collective coordinate models are used to study current-driven DW motion under longitudinal in-plane fields in perpendicularly magnetized samples with strong DMI. Several extended collective coordinate models are developed to reproduce the micromagnetic results. While these extended models show improvements over traditional models of this kind, there are still discrepancies between them and micromagnetic simulations which require further work.",0.1115134803],["the burst radio emission analysis was carried out on the basis of an improved methodology for","Alternative Models of Zebra Patterns in the Event on June 21, 2011","summarize: The analysis of the spectral characteristics of the burst radio emission on June 21, 2011 was carried out on the basis of an improved methodology for determining harmonic numbers for the corresponding stripes of the zebra structure. By using the parameters of the zebra structure in the time frequency spectrum and basing on the double plasma resonance model, the magnetic field and its dynamics, electron density, and the time variation of the distance between the stripes with harmonics s = 55 and 56 and adjacent stripes near the frequency 183 MHz have been determined in the burst generation region. The relationships between the scale characteristics of the field and the density along and across the axis of the power tube and their dependence on time have been also determined. The field obtained turned out to be so small that, firstly, it fails to explain the dynamic features of the spectrum based on MHD waves, and secondly, it results in large values for plasma betta . Other possible difficulties of the generation mechanism of bursts with zebra pattern based on the double plasma resonance are also noted. Another possible mechanism, with whistlers explains qualitatively the main observational characteristics of this zebra. The magnetic field required in this case is about 4.5 G, and the plasma betta is 0.14, which fully corresponds to the coronal conditions.",0.2],["cross-diffusion system modeling information herding of individuals is analyzed in a","A meeting point of entropy and bifurcations in cross-diffusion herding","summarize: A cross-diffusion system modeling the information herding of individuals is analyzed in a bounded domain with no-flux boundary conditions. The variables are the species' density and an influence function which modifies the information state of the individuals. The cross-diffusion term may stabilize or destabilize the system. Furthermore, it allows for a formal gradient-flow or entropy structure. Exploiting this structure, the global-in-time existence of weak solutions and the exponential decay to the constant steady state is proved in certain parameter regimes. This approach does not extend to all parameters. We investigate local bifurcations from homogeneous steady states analytically to determine whether this defines the validity boundary. This analysis shows that generically there is a gap in the parameter regime between the entropy approach validity and the first local bifurcation. Next, we use numerical continuation methods to track the bifurcating non-homogeneous steady states globally and to determine non-trivial stationary solutions related to herding behaviour. In summary, we find that the main boundaries in the parameter regime are given by the first local bifurcation point, the degeneracy of the diffusion matrix and a certain entropy decay validity condition. We study several parameter limits analytically as well as numerically, with a focus on the role of changing a linear damping parameter as well as a parameter controlling the cross-diffusion. We suggest that our paradigm of comparing bifurcation-generated obstructions to the parameter validity of global-functional methods could also be of relevance for many other models beyond the one studied here.",0.4736842105],["the resistance of ballistic helical edge channels in typical quantum spin-Hall experiments is","On the accuracy of conductance quantization in spin-Hall insulators","summarize: In contrast to the case of ordinary quantum Hall effect, the resistance of ballistic helical edge channels in typical quantum spin-Hall experiments is non-vanishing, additive and poorly quantized. Here we present a simple argument connecting this qualitative difference with a spin relaxation in the current\/voltage leads in an experimentally relevant multi-terminal bar geometry. Both the finite lead resistance and the spin relaxation contribute to a non-vanishing four-terminal edge resistance, explaining poor quantization quality. We show that corrections to the four-terminal and two-terminal resistances in the limit of strong spin relaxation are opposite in sign, making a measurement of the spin relaxation resistance feasible, and estimate the magnitude of the effect in HgTe-based quantum wells.",0.3125],["polytropic spheres with properly fixed polytropic index are fixed.","Polytropic spheres containing regions of trapped null geodesics","summarize: We demonstrate that in the framework of standard general relativity polytropic spheres with properly fixed polytropic index ",0.0769230769],["reversible data hiding in encrypted image is calculated and analyzed. the prediction errors","Reversible Data Hiding in Encrypted Images using Local Difference of Neighboring Pixels","summarize: This paper presents a reversible data hiding in encrypted image , which divides image into non-overlapping blocks. In each block, central pixel of the block is considered as leader pixel and others as follower ones. The prediction errors between the intensity of follower pixels and leader ones are calculated and analyzed to determine a feature for block embedding capacity. This feature indicates the amount of data that can be embedded in a block. Using this pre-process for whole blocks, we vacate rooms before the encryption of the original image to achieve high embedding capacity. Also, using the features of all blocks, embedded data is extracted and the original image is perfectly reconstructed at the decoding phase. In effect, comparing to existent RDHEI algorithms, embedding capacity is significantly increased in the proposed algorithm. Experimental results confirm that the proposed algorithm outperforms state of the art ones.",0.1428571429],["asynchronous decentralized parallel stochastic Gradient Descent can work with much","A Highly Efficient Distributed Deep Learning System For Automatic Speech Recognition","summarize: Modern Automatic Speech Recognition systems rely on distributed deep learning to for quick training completion. To enable efficient distributed training, it is imperative that the training algorithms can converge with a large mini-batch size. In this work, we discovered that Asynchronous Decentralized Parallel Stochastic Gradient Descent can work with much larger batch size than commonly used Synchronous SGD algorithm. On commonly used public SWB-300 and SWB-2000 ASR datasets, ADPSGD can converge with a batch size 3X as large as the one used in SSGD, thus enable training at a much larger scale. Further, we proposed a Hierarchical-ADPSGD system in which learners on the same computing node construct a super learner via a fast allreduce implementation, and super learners deploy ADPSGD algorithm among themselves. On a 64 Nvidia V100 GPU cluster connected via a 100Gb\/s Ethernet network, our system is able to train SWB-2000 to reach a 7.6% WER on the Hub5-2000 Switchboard test-set and a 13.2% WER on the Call-home test-set in 5.2 hours. To the best of our knowledge, this is the fastest ASR training system that attains this level of model accuracy for SWB-2000 task to be ever reported in the literature.",0.0],["Meeker and Hong provided an extensive discussion of big data and reliability. they described engineering","Big Data and Reliability Applications: The Complexity Dimension","summarize: Big data features not only large volumes of data but also data with complicated structures. Complexity imposes unique challenges in big data analytics. Meeker and Hong provided an extensive discussion of the opportunities and challenges in big data and reliability, and described engineering systems that can generate big data that can be used in reliability analysis. Meeker and Hong focused on large scale system operating and environment data , and provided examples on how to link such data as covariates to traditional reliability responses such as time to failure, time to recurrence of events, and degradation measurements. This paper intends to extend that discussion by focusing on how to use data with complicated structures to do reliability analysis. Such data types include high-dimensional sensor data, functional curve data, and image streams. We first provide a review of recent development in those directions, and then we provide a discussion on how analytical methods can be developed to tackle the challenging aspects that arise from the complexity feature of big data in reliability applications. The use of modern statistical methods such as variable selection, functional data analysis, scalar-on-image regression, spatio-temporal data models, and machine learning techniques will also be discussed.",0.1578947368],["accretion affects the gravitational-wave phase at negative post-Newto","Gravitational-wave detection and parameter estimation for accreting black-hole binaries and their electromagnetic counterpart","summarize: We study the impact of gas accretion on the orbital evolution of black-hole binaries initially at large separation in the band of the planned Laser Interferometer Space Antenna . We focus on two sources: ~stellar-origin black-hole binaries~ that can migrate from the LISA band to the band of ground-based gravitational-wave observatories within weeks\/months; and intermediate-mass black-hole binaries~ in the LISA band only. Because of the large number of observable gravitational-wave cycles, the phase evolution of these systems needs to be modeled to great accuracy to avoid biasing the estimation of the source parameters. Accretion affects the gravitational-wave phase at negative post-Newtonian order, and is therefore dominant for binaries at large separations. If accretion takes place at the Eddington or at super-Eddington rate, it will leave a detectable imprint on the dynamics of SOBHBs. In optimistic astrophysical scenarios, a multiwavelength strategy with LISA and a ground-based interferometer can detect about ",0.1605784286],["pairwise completely positive matrices are a set of completely positive matrices","Pairwise Completely Positive Matrices and Conjugate Local Diagonal Unitary Invariant Quantum States","summarize: We introduce a generalization of the set of completely positive matrices that we call pairwise completely positive matrices. These are pairs of matrices that share a joint decomposition so that one of them is necessarily positive semidefinite while the other one is necessarily entrywise non-negative. We explore basic properties of these matrix pairs and develop several testable necessary and sufficient conditions that help determine whether or not a pair is PCP. We then establish a connection with quantum entanglement by showing that determining whether or not a pair of matrices is pairwise completely positive is equivalent to determining whether or not a certain type of quantum state, called a conjugate local diagonal unitary invariant state, is separable. Many of the most important quantum states in entanglement theory are of this type, including isotropic states, mixed Dicke states , and maximally correlated states. As a specific application of our results, we show that a wide family of states that have absolutely positive partial transpose are in fact separable.",0.2174049324],["linear switched systems in continuous time a controllability condition implies that state feedbacks allow to","Decay rates for stabilization of linear continuous-time systems with random switching","summarize: For a class of linear switched systems in continuous time a controllability condition implies that state feedbacks allow to achieve almost sure stabilization with arbitrary exponential decay rates. This is based on the Multiplicative Ergodic Theorem applied to an associated system in discrete time. This result is related to the stabilizability problem for linear persistently excited systems.",0.5652173913],["we propose a mathematical model for the onset and progression of Alzheimer's disease.","Alzheimer's disease: a mathematical model for onset and progression","summarize: In this paper we propose a mathematical model for the onset and progression of Alzheimer's disease based on transport and diffusion equations. We regard brain neurons as a continuous medium, and structure them by their degree of malfunctioning. Two different mechanisms are assumed to be relevant for the temporal evolution of the disease: i) diffusion and agglomeration of soluble polymers of amyloid, produced by damaged neurons; ii) neuron-to-neuron prion-like transmission. We model these two processes by a system of Smoluchowski equations for the amyloid concentration, coupled to a kinetic-type transport equation for the distribution function of the degree of malfunctioning of neurons. The second equation contains an integral term describing the random onset of the disease as a jump process localised in particularly sensitive areas of the brain. Our numerical simulations are in good qualitative agreement with clinical images of the disease distribution in the brain which vary from early to advanced stages.",0.7894736842],["perovskites have been praised for their exceptional photovoltaic and optoe","Efficient indoor p-i-n hybrid perovskite solar cells using low temperature solution processed NiO as hole extraction layers","summarize: Hybrid perovskites have received tremendous attention due to their exceptional photovoltaic and optoelectronic properties. Among the two widely used perovskite solar cell device architectures of n-ip and p-i-n, the latter is interesting in terms of its simplicity of fabrication and lower energy input. However this structure mostly uses PEDOT:PSS as a hole transporting layer which can accelerate the perovskite solar cell degradation. Hence the development of stable, inorganic hole extraction layers , without compromising the simplicity of device fabrication is crucial in this fast-growing photovoltaic field. Here we demonstrate a low temperature solution - processed and ultrathin NiO nanoparticle thin films as an efficient HEL for CH3NH3PbI3 based perovskite solar cells. We measure a power conversion efficiency of 13.3 % on rigid glass substrates and 8.5 % on flexible substrates. A comparison with PEDOT:PSS based MAPbI3 solar cells shows that NiO based solar cells have higher short circuit current density and improved open circuit voltage . Apart from the photovoltaic performance under 1 Sun, the efficient hole extraction property of NiO is demonstrated for indoor lighting as well with a PCE of 23.0 % for NiO based CH3NH3PbI2.9Cl0.1 p-i-n solar cells under compact fluorescent lighting. Compared to the perovskite solar cells fabricated on PEDOT:PSS HEL, better shelf-life stability is observed for perovskite solar cells fabricated on NiO HEL. Detailed microstructural and photophysical investigations imply uniform morphology, lower recombination losses, and improved charge transfer properties for CH3NH3PbI3 grown on NiO HEL.",0.0],["nano-magnets are coupled to random external sources. their magnetization becomes a","Finite-dimensional colored fluctuation-dissipation theorem for spin systems","summarize: When nano-magnets are coupled to random external sources, their magnetization becomes a random variable, whose properties are defined by an induced probability density, that can be reconstructed from its moments, using the Langevin equation, for mapping the noise to the dynamical degrees of freedom. When the spin dynamics is discretized in time, a general fluctuation-dissipation theorem, valid for non-Markovian noise, can be established, even when zero modes are present. We discuss the subtleties that arise, when Gilbert damping is present and the mapping between noise and spin degrees of freedom is non--linear.",0.1666666667],["deep learning architectures start to achieve interesting results. we believe the proposed architecture could be improved","Deep intrinsic decomposition trained on surreal scenes yet with realistic light effects","summarize: Estimation of intrinsic images still remains a challenging task due to weaknesses of ground-truth datasets, which either are too small or present non-realistic issues. On the other hand, end-to-end deep learning architectures start to achieve interesting results that we believe could be improved if important physical hints were not ignored. In this work, we present a twofold framework: a flexible generation of images overcoming some classical dataset problems such as larger size jointly with coherent lighting appearance; and a flexible architecture tying physical properties through intrinsic losses. Our proposal is versatile, presents low computation time, and achieves state-of-the-art results.",0.0],["urban form visualizations have compressed physical urban complexity into easily comprehensible information artifact","Spatial Information and the Legibility of Urban Form: Big Data in Urban Morphology","summarize: Urban planning and morphology have relied on analytical cartography and visual communication tools for centuries to illustrate spatial patterns, propose designs, compare alternatives, and engage the public. Classic urban form visualizations - from Giambattista Nolli's ichnographic maps of Rome to Allan Jacobs's figure-ground diagrams of city streets - have compressed physical urban complexity into easily comprehensible information artifacts. Today we can enhance these traditional workflows through the Smart Cities paradigm of understanding cities via user-generated content and harvested data in an information management context. New spatial technology platforms and big data offer new lenses to understand, evaluate, monitor, and manage urban form and evolution. This paper builds on the theoretical framework of visual cultures in urban planning and morphology to introduce and situate computational data science processes for exploring urban fabric patterns and spatial order. It demonstrates these workflows with OSMnx and data from OpenStreetMap, a collaborative spatial information system and mapping platform, to examine street network patterns, orientations, and configurations in different study sites around the world, considering what these reveal about the urban fabric. The age of ubiquitous urban data and computational toolkits opens up a new era of worldwide urban form analysis from integrated quantitative and qualitative perspectives.",0.0625],["traditional approaches assign each node with an independent continuous vector. this will cause huge memory overhead","Multi-Hot Compact Network Embedding","summarize: Network embedding, as a promising way of the network representation learning, is capable of supporting various subsequent network mining and analysis tasks, and has attracted growing research interests recently. Traditional approaches assign each node with an independent continuous vector, which will cause huge memory overhead for large networks. In this paper we propose a novel multi-hot compact embedding strategy to effectively reduce memory cost by learning partially shared embeddings. The insight is that a node embedding vector is composed of several basis vectors, which can significantly reduce the number of continuous vectors while maintain similar data representation ability. Specifically, we propose a MCNE model to learn compact embeddings from pre-learned node features. A novel component named compressor is integrated into MCNE to tackle the challenge that popular back-propagation optimization cannot propagate through discrete samples. We further propose an end-to-end model MCNE",0.0],["Strang splitting is a well established tool for the numerical integration of evolution equations.","Efficient boundary corrected Strang splitting","summarize: Strang splitting is a well established tool for the numerical integration of evolution equations. It allows the application of tailored integrators for different parts of the vector field. However, it is also prone to order reduction in the case of non-trivial boundary conditions. This order reduction can be remedied by correcting the boundary values of the intermediate splitting step. In this paper, three different approaches for constructing such a correction in the case of inhomogeneous Dirichlet, Neumann, and mixed boundary conditions are presented. Numerical examples that illustrate the effectivity and benefits of these corrections are included.",0.2],["this paper proposes an original approach to better understanding the behavior of robust scatter matrix.","New insights into the statistical properties of ","summarize: This paper proposes an original approach to better understanding the behavior of robust scatter matrix ",0.1875],["gradient-free optimization methodology is developed for multiple access channels. the algorithm is based on","Fast Optimization with Zeroth-Order Feedback in Distributed, Multi-User MIMO Systems","summarize: In this paper, we develop a gradient-free optimization methodology for efficient resource allocation in Gaussian MIMO multiple access channels. Our approach combines two main ingredients: an entropic semidefinite optimization based on matrix exponential learning ; and a one-shot gradient estimator which achieves low variance through the reuse of past information. This novel algorithm, which we call gradient-free MXL algorithm with callbacks , retains the convergence speed of gradient-based methods while requiring minimal feedback per iteration",0.1764705882],["generators are able to generate a modeled distribution. the distribution always has connected","Learning disconnected manifolds: a no GANs land","summarize: Typical architectures of Generative AdversarialNetworks make use of a unimodal latent distribution transformed by a continuous generator. Consequently, the modeled distribution always has connected support which is cumbersome when learning a disconnected set of manifolds. We formalize this problem by establishing a no free lunch theorem for the disconnected manifold learning stating an upper bound on the precision of the targeted distribution. This is done by building on the necessary existence of a low-quality region where the generator continuously samples data between two disconnected modes. Finally, we derive a rejection sampling method based on the norm of generators Jacobian and show its efficiency on several generators including BigGAN.",0.2173913043],["a hypothesis has been proposed that mean motion resonances between Planet Nine and distant objects of the","Feasibility of a resonance-based Planet Nine search","summarize: It has been proposed that mean motion resonances between Planet Nine and distant objects of the scattered disk might inform the semimajor axis and instantaneous position of Planet Nine. Within the context of this hypothesis, the specific distribution of occupied MMRs largely determines the available constraints. Here we characterize the behavior of scattered Kuiper Belt objects arising in the presence of an eccentric Planet Nine , focusing on relative sizes of populations occupying particular commensurabilities. Highlighting the challenge of predicting the exact MMR of a given object, we find that the majority of resonant test particles have period ratios with Planet Nine other than those of the form ",0.36],["autonomous, vision-based flight brings up fundamental challenges in robotics. our approach combines","Deep Drone Racing: Learning Agile Flight in Dynamic Environments","summarize: Autonomous agile flight brings up fundamental challenges in robotics, such as coping with unreliable state estimation, reacting optimally to dynamically changing environments, and coupling perception and action in real time under severe resource constraints. In this paper, we consider these challenges in the context of autonomous, vision-based drone racing in dynamic environments. Our approach combines a convolutional neural network with a state-of-the-art path-planning and control system. The CNN directly maps raw images into a robust representation in the form of a waypoint and desired speed. This information is then used by the planner to generate a short, minimum-jerk trajectory segment and corresponding motor commands to reach the desired goal. We demonstrate our method in autonomous agile flight scenarios, in which a vision-based quadrotor traverses drone-racing tracks with possibly moving gates. Our method does not require any explicit map of the environment and runs fully onboard. We extensively test the precision and robustness of the approach in simulation and in the physical world. We also evaluate our method against state-of-the-art navigation approaches and professional human drone pilots.",0.2142857143],["the so-called is a powerful tool in qualitative studies of one dimensional parabolic","The Zero Number Diminishing Property under General Boundary Conditions","summarize: The so-called is a powerful tool in qualitative studies of one dimensional parabolic equations, which says that, under the zero- or non-zero-Dirichlet boundary conditions, the number of zeroes of the solution ",0.2105263158],["we study the dynamics of the fluctuations of the variance.","Dynamics of fluctuations in the Gaussian model with dissipative Langevin Dynamics","summarize: We study the dynamics of the fluctuations of the variance ",0.3016124727],["astrocytes may regulate microscopic liquid flow by osmotic effects","Astrocytic Ion Dynamics: Implications for Potassium Buffering and Liquid Flow","summarize: We review modeling of astrocyte ion dynamics with a specific focus on the implications of so-called spatial potassium buffering, where excess potassium in the extracellular space is transported away to prevent pathological neural spiking. The recently introduced Kirchoff-Nernst-Planck scheme for modeling ion dynamics in astrocytes is outlined and used to study such spatial buffering. We next describe how the ion dynamics of astrocytes may regulate microscopic liquid flow by osmotic effects and how such microscopic flow can be linked to whole-brain macroscopic flow. We thus include the key elements in a putative multiscale theory with astrocytes linking neural activity on a microscopic scale to macroscopic fluid flow.",0.0],["existing methods in the literature are based on elaborated representations of a correlation matrix","A fast Metropolis-Hastings method for generating random correlation matrices","summarize: We propose a novel Metropolis-Hastings algorithm to sample uniformly from the space of correlation matrices. Existing methods in the literature are based on elaborated representations of a correlation matrix, or on complex parametrizations of it. By contrast, our method is intuitive and simple, based the classical Cholesky factorization of a positive definite matrix and Markov chain Monte Carlo theory. We perform a detailed convergence analysis of the resulting Markov chain, and show how it benefits from fast convergence, both theoretically and empirically. Furthermore, in numerical experiments our algorithm is shown to be significantly faster than the current alternative approaches, thanks to its simple yet principled approach.",0.4],["twisted K-theory classes on D-branes have invariant meaning","Gauge enhancement of super M-branes via parametrized stable homotopy theory","summarize: A key open problem in M-theory is the mechanism of gauge enhancement, which supposedly makes M-branes exhibit the nonabelian gauge degrees of freedom that are seen perturbatively in the limit of 10d string theory. In fact, since only the twisted K-theory classes represented by nonabelian Chan-Paton gauge fields on D-branes have invariant meaning, the problem is really the lift to M-theory of the twisted K-theory classification of D-brane charges. Here we show how this problem has a solution by universal constructions in super homotopy theory, at least rationally. We recall how double dimensional reduction of super M-brane charges is described by the cyclification adjunction applied to the 4-sphere, and how M-theory degrees of freedom hidden at ADE-singularities are induced by the suspended Hopf action on the 4-sphere. Combining these, we demonstrate, at the level of rational homotopy theory, that gauge enhancement in M-theory is exhibited by lifting against the fiberwise stabilization of the unit of this cyclification adjunction on the A-type orbispace of the 4-sphere. This explains how the fundamental D6 and D8 brane cocycles can be lifted from twisted K-theory to a cohomology theory for M-brane charge, at least rationally.",0.0],["TESLA-type cavities are widely used to accelerate electrons in long bunch trains.","Detuning related coupler kick variation of a superconducting nine-cell 1.3 GHz cavity","summarize: Superconducting TESLA-type cavities are widely used to accelerate electrons in long bunch trains, such as in high repetition rate free electron lasers. The TESLA cavity is equipped with two higher order mode couplers and a fundamental power coupler , which break the axial symmetry of the cavity. The passing electrons therefore experience axially asymmetrical coupler kicks, which depend on the transverse beam position at the couplers and the rf phase. The resulting emittance dilution has been studied in detail in the literature. However, the kick induced by the FPC depends explicitly on the ratio of the forward to the backward traveling waves at the coupler, which has received little attention. The intention of this paper is to present the concept of discrete coupler kicks with a novel approach of separating the field disturbances related to the standing wave and a reflection dependent part. Particular attention is directed to the role of the penetration depth of the FPC antenna, which determines the loaded quality factor of the cavity. The developed beam transport model is compared to dedicated experiments at FLASH and European XFEL. Both the observed transverse coupling and detuning related coupler kick variations are in good agreement with the model. Finally, the expected trajectory variations due to coupler kick variations at European XFEL are investigated and results of numerical studies are presented.",0.1538461538],["ErdHs conjectures that for every positive integer integer, for every positive integer integer","King-serf duo by monochromatic paths in k-edge-coloured tournaments","summarize: An open conjecture of Erd\\Hs states that for every positive integer ",0.0],["the vulnerability of the proposed event-triggered finite-time controller in the presence of DoS","Finite-Time Stability Under Denial of Service","summarize: Finite-time stability of networked control systems under Denial of Service attacks are investigated in this paper, where the communication between the plant and the controller is compromised at some time intervals. Toward this goal, first an event-triggered mechanism based on the variation rate of the Lyapunov function is proposed such that the closed-loop system remains finite-time stable and at the same time, the amount data exchange in the network is reduced. Next, the vulnerability of the proposed event-triggered finite-time controller in the presence of DoS attacks are evaluated and sufficient conditions on the DoS duration and frequency are obtained to assure the finite-time stability of the closed-loop system in the presence of DoS attack where no assumption on the DoS attack in terms of following a certain probabilistic or a well-structured periodic model is considered. Finally, the efficiency of the proposed approach is demonstrated through a simulation study.",0.1363636364],["we present effective numerical algorithms for locally recovering unknown governing differential equations from measurement data.","Numerical Aspects for Approximating Governing Equations Using Data","summarize: We present effective numerical algorithms for locally recovering unknown governing differential equations from measurement data. We employ a set of standard basis functions, e.g., polynomials, to approximate the governing equation with high accuracy. Upon recasting the problem into a function approximation problem, we discuss several important aspects for accurate approximation. Most notably, we discuss the importance of using a large number of short bursts of trajectory data, rather than using data from a single long trajectory. Several options for the numerical algorithms to perform accurate approximation are then presented, along with an error estimate of the final equation approximation. We then present an extensive set of numerical examples of both linear and nonlinear systems to demonstrate the properties and effectiveness of our equation recovery algorithms.",0.0666666667],["quantum spin chains described by boundary-driven quantum spin chains. asymmetrical quantum spin","One-way street for the energy current: A ubiquitous phenomenon in boundary-driven quantum spin chains","summarize: Focusing on the description of nontrivial properties of the energy transport at quantum scale, we investigate asymmetrical quantum spin chains described by boundary-driven ",0.2604559154],["neural models are often able to focus on a few frequent words with sentiment polarities","Progressive Self-Supervised Attention Learning for Aspect-Level Sentiment Analysis","summarize: In aspect-level sentiment classification , it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tends to excessively focus on a few frequent words with sentiment polarities, while ignoring infrequent ones. In this paper, we propose a progressive self-supervised attention learning approach for neural ASC models, which automatically mines useful attention supervision information from a training corpus to refine attention mechanisms. Specifically, we iteratively conduct sentiment predictions on all training instances. Particularly, at each iteration, the context word with the maximum attention weight is extracted as the one with active\/misleading influence on the correct\/incorrect prediction of every instance, and then the word itself is masked for subsequent iterations. Finally, we augment the conventional training objective with a regularization term, which enables ASC models to continue equally focusing on the extracted active context words while decreasing weights of those misleading ones. Experimental results on multiple datasets show that our proposed approach yields better attention mechanisms, leading to substantial improvements over the two state-of-the-art neural ASC models. Source code and trained models are available at https:\/\/github.com\/DeepLearnXMU\/PSSAttention.",0.1578947368],["research is underway on the 6G mobile system. the research is based on a","An Abstracted Survey on 6G: Drivers, Requirements, Efforts, and Enablers","summarize: As of today, 5G mobile systems have been already widely rolled out, it is the right time for academia and industry to explore the next generation mobile communication system beyond 5G. To this end, this paper provides an abstracted survey for the 6G mobile system. We shed light on the key driving factors for 6G through predicting the growth trend of mobile traffic and mobile service subscriptions until the year of 2030, envisioning the potential use cases and applications, as well as deriving the potential use scenarios. Then, a number of key performance indicators to support the 6G use cases are identified and their target values are estimated in a quantitatively manner, which is compared with those of 5G clearly in a visualized way. An investigation of the efforts spent on 6G research in different countries and institutions until now is summarized, and a potential roadmap in terms of the definition, specification, standardization, and spectrum regulation is given. Finally, an introduction to potential key 6G technologies is provided. The principle, technical advantages, challenges, and open research issues for each identified technology are discussed.",0.2307692308],["adaptive radar detection of dim moving targets is formulated as a multiple hypothesis test. the","Adaptive Radar Detection of Dim Moving Targets in Presence of Range Migration","summarize: This paper addresses adaptive radar detection of dim moving targets. To circumvent range migration, the detection problem is formulated as a multiple hypothesis test and solved applying model order selection rules which allow to estimate the position of the target within the CPI and eventually detect it. The performance analysis proves the effectiveness of the proposed approach also in comparison to existing alternatives.",0.2916666667],["Orbifold Reduction is a new method for generating a new generation of","Orbifold Reduction and 2d Gauge Theories","summarize: We introduce Orbifold Reduction, a new method for generating ",0.3],["Graph-based methods have been quite successful in solving unsupervised and semi-supervised learning problems","A Sampling Theory Perspective of Graph-based Semi-supervised Learning","summarize: Graph-based methods have been quite successful in solving unsupervised and semi-supervised learning problems, as they provide a means to capture the underlying geometry of the dataset. It is often desirable for the constructed graph to satisfy two properties: first, data points that are similar in the feature space should be strongly connected on the graph, and second, the class label information should vary smoothly with respect to the graph, where smoothness is measured using the spectral properties of the graph Laplacian matrix. Recent works have justified some of these smoothness conditions by showing that they are strongly linked to the semi-supervised smoothness assumption and its variants. In this work, we reinforce this connection by viewing the problem from a graph sampling theoretic perspective, where class indicator functions are treated as bandlimited graph signals and label prediction as a bandlimited reconstruction problem. Our approach involves analyzing the bandwidth of class indicator signals generated from statistical data models with separable and nonseparable classes. These models are quite general and mimic the nature of most real-world datasets. Our results show that in the asymptotic limit, the bandwidth of any class indicator is also closely related to the geometry of the dataset. This allows one to theoretically justify the assumption of bandlimitedness of class indicator signals, thereby providing a sampling theoretic interpretation of graph-based semi-supervised classification.",0.2],["canonical trace and Wodzicki residue on closed manifold characterised by","Spectral ","summarize: The canonical trace and the Wodzicki residue on classical pseudodifferential operators on a closed manifold are characterised by their locality and shown to be preserved under lifting to the universal covering as a result of their local feature. As a consequence, we lift a class of spectral ",0.0],["we used the transverse Ising model to map the observed results of the quantum state onto","Deep Neural Network Detects Quantum Phase Transition","summarize: We detect the quantum phase transition of a quantum many-body system by mapping the observed results of the quantum state onto a neural network. In the present study, we utilized the simplest case of a quantum many-body system, namely a one-dimensional chain of Ising spins with the transverse Ising model. We prepared several spin configurations, which were obtained using repeated observations of the model for a particular strength of the transverse field, as input data for the neural network. Although the proposed method can be employed using experimental observations of quantum many-body systems, we tested our technique with spin configurations generated by a quantum Monte Carlo simulation without initial relaxation. The neural network successfully classified the strength of transverse field only from the spin configurations, leading to consistent estimations of the critical point of our model ",0.0],["change detection system takes as input two images of a region captured at two different times.","Broad Neural Network for Change Detection in Aerial Images","summarize: A change detection system takes as input two images of a region captured at two different times, and predicts which pixels in the region have undergone change over the time period. Since pixel-based analysis can be erroneous due to noise, illumination difference and other factors, contextual information is usually used to determine the class of a pixel . This contextual information is taken into account by considering a pixel of the difference image along with its neighborhood. With the help of ground truth information, the labeled patterns are generated. Finally, Broad Learning classifier is used to get prediction about the class of each pixel. Results show that Broad Learning can classify the data set with a significantly higher F-Score than that of Multilayer Perceptron. Performance comparison has also been made with other popular classifiers, namely Multilayer Perceptron and Random Forest.",0.2916666667],["a language is dense if the set of all words is the set of all words","On the Density of Languages Accepted by Turing Machines and Other Machine Models","summarize: A language is dense if the set of all infixes of the language is the set of all words. Here, it is shown that it is decidable whether the language accepted by a nondeterministic Turing machine with a one-way read-only input and a reversal-bounded read\/write worktape is dense. From this, it is implied that it is also decidable for one-way reversal-bounded queue automata, one-way reversal-bounded stack automata, and one-way reversal-bounded ",0.34375],["drone launches rockets at a release altitude of up to 20 feet. a","Drone Launched Short Range Rockets","summarize: A concept of drone launched short range rockets is presented. A drone or an aircraft rises DLSRR to a release altitude of up to 20 ",0.1739130435],["the attractive tail of the intermolecular interaction affects very weakly the structural properties of","Role of attractive forces in the relaxation dynamics of supercooled liquids","summarize: The attractive tail of the intermolecular interaction affects very weakly the structural properties of liquids, while it affects dramatically their dynamical ones. Via the numerical simulations of model systems not prone to crystallization, both in three and in two spatial dimensions, here we demonstrate that the non-perturbative dynamical effects of the attractive forces are tantamount to a rescaling of the activation energy by the glass transition temperature ",0.3636363636],["the team from the new technologies for the Information Society research center of the university of west boh","UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge","summarize: In this paper, we present our system developed by the team from the New Technologies for the Information Society research center of the University of West Bohemia in Pilsen, for the Second DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i\/x-vector extraction, clustering, and resegmentation. The hyperparameters for each of the subsystems were selected according to the domain classifier trained on the development set of DIHARD II. We compared our system with results from the Kaldi diarization and combined these systems. At the time of writing of this abstract, our best submission achieved a DER of 23.47% and a JER of 48.99% on the evaluation set .",0.1515151515],["we define a coarse normality condition for connected coarse spaces. we show that a","Normality conditions of structures in coarse geometry and an alternative description of coarse proximities","summarize: We introduce an alternative description of coarse proximities. We define a coarse normality condition for connected coarse spaces and show that this definition agrees with large scale normality defined in and asymptotic normality defined in . We utilize the alternative definition of coarse proximities to show that a connected coarse space naturally induces a coarse proximity if and only if the connected coarse space is coarsely normal. We conclude with showing that every connected asymptotic resemblance space induces a coarse proximity if and only if the connected asymptotic resemblance space is asymptotically normal.",0.6333333333],["golfarshchi and Khalilzadeh disprove two results.","On the paper: Numerical radius preserving linear maps on Banach algebras","summarize: We give an example of a unital commutative complex Banach algebra having a normalized state which is not a spectral state and admitting an extreme normalized state which is not multiplicative. This disproves two results by Golfarshchi and Khalilzadeh.",0.0],["the unfavored unfavored work is the unfavored work of the unfavored unfavored work.","Systematic study of unfavored ","summarize: In present work, the unfavored ",0.1333333333],["detecting and segmenting salient objects from natural scenes has attracted great interest in computer vision","Salient Object Detection: A Survey","summarize: Detecting and segmenting salient objects from natural scenes, often referred to as salient object detection, has attracted great interest in computer vision. While many models have been proposed and several applications have emerged, a deep understanding of achievements and issues remains lacking. We aim to provide a comprehensive review of recent progress in salient object detection and situate this field among other closely related areas such as generic scene segmentation, object proposal generation, and saliency for fixation prediction. Covering 228 publications, we survey i) roots, key concepts, and tasks, ii) core techniques and main modeling trends, and iii) datasets and evaluation metrics for salient object detection. We also discuss open problems such as evaluation metrics and dataset bias in model performance, and suggest future research directions.",0.0],["we develop an efficient and high order panel method with applications in airfoil design.","A general high order two-dimensional panel method","summarize: We develop an efficient and high order panel method with applications in airfoil design. Through the use of analytic work and careful considerations near singularities our approach is quadrature-free. The resulting method is examined with respect to accuracy and efficiency and we discuss the different trade-offs in approximation order and computational complexity. A reference implementation within a package for a two-dimensional fast multipole method is distributed freely.",0.3125],["ZnGeN is a spokesman for the snub","Blue-Green Emission from Epitaxial Yet Cation-Disordered ZnGeN","summarize: ZnGeN",0.5],["film production is preferred due to faster processing times and lower processing costs. films can be produced","Crack initiation of printed lines predicted with digital image correlation","summarize: Printing of metallic films has been preferred for roll-to-roll processes over vacuum technologies due to faster processing times and lower processing costs. Films can be produced by depositing inks containing suspended metallic particles within a solvent and then heating the films to both remove the solvent and sinter the particles. The resulting printed structure, electrical and mechanical behavior of the printed films has been studied to better understand their electro-mechanical response to loading and eventual brittle fracture. This study evaluated the electro-mechanical behavior of 1.25 m printed Ag films using in-situ resistance and in-situ imaging methods. Digital image correlation was utilized with confocal laser scanning microscope images to better visualize crack initiation during tensile straining. This technique showed that cracks initiated earlier in the thicker areas of the film than in lower areas because of a higher density of printing defects and the increased thickness.",0.0],["Let us know what you think about it!","Universality in Random Moment Problems","summarize: Let ",0.0],["smart surfaces consisting of smaller structures may be designed to respond uniquely through combinatorial design strategies","Smart patterned surfaces with programmable thermal emissivity and their design through combinatorial strategies","summarize: The emissivity of common materials remains constant with temperature variations, and cannot drastically change. However, it is possible to design its entire behaviour as a function of temperature, and to significantly modify the thermal emissivity of a surface through the combination of different materials and patterns. Here, we show that smart patterned surfaces consisting of smaller structures may be designed to respond uniquely through combinatorial design strategies by transforming themselves from 2D to 3D complex structures with a two-way shape memory effect. The smart surfaces can passively manipulate thermal radiation without-the use of controllers and power supplies-because their modus operandi has already been programmed and integrated into their intrinsic characteristics; the environment provides the energy required for their activation. Each motif emits thermal radiation in a certain manner, as it changes its geometry; however, the spatial distribution of these motifs causes them to interact with each other. Therefore, their combination and interaction determine the global behaviour of the surfaces, thus enabling their a priori design. The emissivity behaviour is not random; it is determined by two fundamental parameters, namely the combination of orientations in which the motifs open ) and the combination of materials on the motifs; these generate functions which fully determine the dependency of the emissivity on the temperature.",0.3333333333],["the idea we propose can be seen as an extension of the k2-tree to support","Compact and Efficient Representation of General Graph Databases","summarize: In this paper, we propose a compact data structure to store labeled attributed graphs based on the k2-tree, which is a very compact data structure designed to represent a simple directed graph. The idea we propose can be seen as an extension of the k2-tree to support property graphs. In addition to the static approach, we also propose a dynamic version of the storage representation, which allows exible schemas and insertion or deletion of data. We provide an implementation of a basic set of operations, which can be combined to form complex queries over these graphs with attributes. We evaluate the performance of our proposal with existing graph database systems and prove that our compact attributed graph representation obtains also competitive time results.",0.1666666667],["location data owners can't share their data with other businesses. this paper proposes solutions","Privacy-Preserving Aggregate Queries for Optimal Location Selection","summarize: Today, vast amounts of location data are collected by various service providers. These location data owners have a good idea of where their users are most of the time. Other businesses also want to use this information for location analytics, such as finding the optimal location for a new branch. However, location data owners cannot share their data with other businesses, mainly due to privacy and legal concerns. In this paper, we propose privacy-preserving solutions in which location-based queries can be answered by data owners without sharing their data with other businesses and without accessing sensitive information such as the customer list of the businesses that send the query. We utilize a partially homomorphic cryptosystem as the building block of the proposed protocols. We prove the security of the protocols in semi-honest threat model. We also explain how to achieve differential privacy in the proposed protocols and discuss its impact on utility. We evaluate the performance of the protocols with real and synthetic datasets and show that the proposed solutions are highly practical. The proposed solutions will facilitate an effective sharing of sensitive data between entities and joint analytics in a wide range of applications without violating their customers' privacy.",0.0],["normal forms of Hamiltonian are very important to analyze the nonlinear stability of a","Normalization of Hamiltonian and nonlinear stability of triangular equilibrium points in the photogravitational restricted three body problem with P-R drag in non-resonance case","summarize: Normal forms of Hamiltonian are very important to analyze the nonlinear stability of a dynamical system in the vicinity of invariant objects. This paper presents the normalization of Hamiltonian and the analysis of nonlinear stability of triangular equilibrium points in non-resonance case, in the photogravitational restricted three body problem under the influence of radiation pressures and P-R drags of the radiating primaries. The Hamiltonian of the system is normalized up to fourth order through Lie transform method and then to apply the Arnold-Moser theorem, Birkhoff normal form of the Hamiltonian is computed followed by nonlinear stability of the equilibrium points is examined. Similar to the case of classical problem, we have found that in the presence of assumed perturbations, there always exists one value of mass parameter within the stability range at which the discriminant ",0.4044523265],["abstractive summarization is the process of generating novel sentences based on the information extracted","Bengali Abstractive News Summarization: A Neural Attention Approach","summarize: Abstractive summarization is the process of generating novel sentences based on the information extracted from the original text document while retaining the context. Due to abstractive summarization's underlying complexities, most of the past research work has been done on the extractive summarization approach. Nevertheless, with the triumph of the sequence-to-sequence model, abstractive summarization becomes more viable. Although a significant number of notable research has been done in the English language based on abstractive summarization, only a couple of works have been done on Bengali abstractive news summarization . In this article, we presented a seq2seq based Long Short-Term Memory network model with attention at encoder-decoder. Our proposed system deploys a local attention-based model that produces a long sequence of words with lucid and human-like generated sentences with noteworthy information of the original document. We also prepared a dataset of more than 19k articles and corresponding human-written summaries collected from bangla.bdnews24.com1 which is till now the most extensive dataset for Bengali news document summarization and publicly published in Kaggle2. We evaluated our model qualitatively and quantitatively and compared it with other published results. It showed significant improvement in terms of human evaluation scores with state-of-the-art approaches for BANS.",0.1111111111],["a nonlinear algebraic equation system of 5 variables is numerically solved. the","Reduction of a nonlinear system and its numerical solution using a fractional iterative method","summarize: A nonlinear algebraic equation system of 5 variables is numerically solved, which is derived from the application of the Fourier transform to a differential equation system that allows modeling the behavior of the temperatures and the efficiencies of a hybrid solar receiver, which in simple terms is the combination of a photovoltaic system with a thermoelectric system. In addition, a way to reduce the previous system to a nonlinear system of only 2 variables is presented. Naturally, reducing algebraic equation systems of dimension N to systems of smaller dimensions has the main advantage of reducing the number of variables involved in a problem, but the analytical expressions of the systems become more complicated. However, to minimize this disadvantage, an iterative method that does not explicitly depend on the analytical complexity of the system to be solved is used. A fractional iterative method, valid for one and several variables, that uses the properties of fractional calculus, in particular the fact that the fractional derivatives of constants are not always zero, to find solutions of nonlinear systems is presented.",0.490068367],["adaptive discontinuous Galerkin methods are used to solve fractional diffusion equations. the","Discontinuous Galerkin methods and their adaptivity for the tempered fractional diffusion equations","summarize: This paper focuses on the adaptive discontinuous Galerkin methods for the tempered fractional diffusion equations. The DG schemes with interior penalty for the diffusion term and numerical flux for the convection term are used to solve the equations, and the detailed stability and convergence analyses are provided. Based on the derived posteriori error estimates, the local error indicator is designed. The theoretical results and the effectiveness of the adaptive DG methods are respectively verified and displayed by the extensive numerical experiments. The strategy of designing adaptive schemes presented in this paper works for the general PDEs with fractional operators.",0.4166666667],["this paper proposes an algorithm to minimize weighted service latency for different classes of tenants","Differentiated latency in data center networks with erasure coded files through traffic engineering","summarize: This paper proposes an algorithm to minimize weighted service latency for different classes of tenants in a data center network where erasure-coded files are stored on distributed disks\/racks and access requests are scattered across the network. Due to limited bandwidth available at both top-of-the-rack and aggregation switches and tenants in different service classes need differentiated services, network bandwidth must be apportioned among different intra- and inter-rack data flows for different service classes in line with their traffic statistics. We formulate this problem as weighted queuing and employ a class of probabilistic request scheduling policies to derive a closed-form upper-bound of service latency for erasure-coded storage with arbitrary file access patterns and service time distributions. The result enables us to propose a joint weighted latency optimization over three entangled control knobs: the bandwidth allocation at top-of-the-rack and aggregation switches for different service classes, dynamic scheduling of file requests, and the placement of encoded file chunks . The joint optimization is shown to be a mixed-integer problem. We develop an iterative algorithm which decouples and solves the joint optimization as 3 sub-problems, which are either convex or solvable via bipartite matching in polynomial time. The proposed algorithm is prototyped in an open-source, distributed file system, , and evaluated on a cloud testbed with 16 separate physical hosts in an Openstack cluster using 48-port Cisco Catalyst switches. Experiments validate our theoretical latency analysis and show significant latency reduction for diverse file access patterns. The results provide valuable insights on designing low-latency data center networks with erasure coded storage.",0.0625],["corona-virus caused diseases like acute respiratory syndrome. the k-means","Target specific mining of COVID-19 scholarly articles using one-class approach","summarize: In recent years, several research articles have been published in the field of corona-virus caused diseases like severe acute respiratory syndrome , middle east respiratory syndrome and COVID-19. In the presence of numerous research articles, extracting best-suited articles is time-consuming and manually impractical. The objective of this paper is to extract the activity and trends of corona-virus related research articles using machine learning approaches. The COVID-19 open research dataset is used for experiments, whereas several target-tasks along with explanations are defined for classification, based on domain knowledge. Clustering techniques are used to create the different clusters of available articles, and later the task assignment is performed using parallel one-class support vector machines . Experiments with original and reduced features validate the performance of the approach. It is evident that the k-means clustering algorithm, followed by parallel OCSVMs, outperforms other methods for both original and reduced feature space.",0.0],["the AVX-512 is a novel two part hybrid sort based on the Quick","A Novel Hybrid Quicksort Algorithm Vectorized using AVX-512 on Intel Skylake","summarize: The modern CPU's design, which is composed of hierarchical memory and SIMD\/vectorization capability, governs the potential for algorithms to be transformed into efficient implementations. The release of the AVX-512 changed things radically, and motivated us to search for an efficient sorting algorithm that can take advantage of it. In this paper, we describe the best strategy we have found, which is a novel two parts hybrid sort, based on the well-known Quicksort algorithm. The central partitioning operation is performed by a new algorithm, and small partitions\/arrays are sorted using a branch-free Bitonic-based sort. This study is also an illustration of how classical algorithms can be adapted and enhanced by the AVX-512 extension. We evaluate the performance of our approach on a modern Intel Xeon Skylake and assess the different layers of our implementation by sorting\/partitioning integers, double floating-point numbers, and key\/value pairs of integers. Our results demonstrate that our approach is faster than two libraries of reference: the GNU \\emph sort algorithm by a speedup factor of 4, and the Intel IPP library by a speedup factor of 1.4.",0.2941176471],["this paper proposes an original approach to better understanding the behavior of robust scatter matrix.","New insights into the statistical properties of ","summarize: This paper proposes an original approach to better understanding the behavior of robust scatter matrix ",0.1875],["Graphene is a two-dimensional layer of carbon atoms arranged in","Tunable nonlinear and active THz devices based on hybrid graphene metasurfaces","summarize: Graphene is a two-dimensional layer of carbon atoms arranged in a honeycomb lattice, whose outstanding properties makes it an excellent material for future electronic and photonic terahertz devices. In this work, we design hybrid graphene metasurfaces by using a monolayer graphene placed over a metallic grating, operating in the THz frequency range. Perfect absorption can be achieved at the resonance, where the electric field is greatly enhanced due to the coupling between the graphene and the grating plasmonic responses. The enhancement of the electric field along the graphene monolayer, as well as the large nonlinear conductivity of graphene, can dramatically boost the nonlinear response of the proposed THz device. In addition, the presented enhanced nonlinear effects can be significantly tuned by varying the doping level of graphene. The proposed structure can be used in the design of THz-frequency generators and all-optical processors.",0.4790315743],["data were collected from 34 agile teams from six software development organizations and one university in both Brazil and","Agile Ways of Working: A Team Maturity Perspective","summarize: With the agile approach to managing software development projects comes an increased dependability on well functioning teams, since many of the practices are built on teamwork. The objective of this study was to investigate if, and how, team development from a group psychological perspective is related to some work practices of agile teams. Data were collected from 34 agile teams from six software development organizations and one university in both Brazil and Sweden using the Group Development Questionnaire and the Perceptive Agile Measurement . The result indicates a strong correlation between levels of group maturity and the two agile practices \\emph and \\emph. We, therefore, conclude that agile teams at different group development stages adopt parts of team agility differently, thus confirming previous studies but with more data and by investigating concrete and applied agile practices. We thereby add evidence to the hypothesis that an agile implementation and management of agile projects need to be adapted to the group maturity levels of the agile teams.",0.0434782609],["a proof of concept is a proof of concept implementation of an Incremental CAD","Towards Incremental Cylindrical Algebraic Decomposition in Maple","summarize: Cylindrical Algebraic Decomposition is an important tool within computational real algebraic geometry, capable of solving many problems for polynomial systems over the reals. It has long been studied by the Symbolic Computation community and has found recent interest in the Satisfiability Checking community. The present report describes a proof of concept implementation of an Incremental CAD algorithm in Maple, where CADs are built and then refined as additional polynomial constraints are added. The aim is to make CAD suitable for use as a theory solver for SMT tools who search for solutions by continually reformulating logical formula and querying whether a logical solution is admissible. We describe experiments for the proof of concept, which clearly display the computational advantages compared to iterated re-computation. In addition, the project implemented this work under the recently verified Lazard projection scheme .",0.2894736842],["we generalize to the setting of 1-sided chord-arc domains. domains satisfying","Perturbations of elliptic operators in 1-sided chord-arc domains. Part II: Non-symmetric operators and Carleson measure estimates","summarize: We generalize to the setting of 1-sided chord-arc domains, that is, to domains satisfying the interior Corkscrew and Harnack Chain conditions and which have an Ahlfors regular boundary, a result of Kenig-Kirchheim-Pipher-Toro, in which Carleson measure estimates for bounded solutions of the equation ",0.2929552703],["the method is proposed to estimate the static electric polarizability of two-dimensional infinite","Evaluation of the Electric Polarizability for Planar Frequency Selective Arrays","summarize: This paper presents a method to estimate the static electric polarizability of two-dimensional infinitely periodic metal patch arrays with dielectric substrate. The main features of the proposed method is its numerical efficiency and a deep insight into the physics of the fields interacting with the structure. We provide derivation and analysis of the method, and its verification against two another commercial solver-based approaches for various structure geometries. Additionally, we suggest the guidelines for applying the method to bandwidth optimization of frequency selective structures and illustrate this with an example.",0.2],["acoustic wave equation is a scalar wave equation based on","A nonlinear method for imaging with acoustic waves via reduced order model backprojection","summarize: We introduce a novel nonlinear imaging method for the acoustic wave equation based on data-driven model order reduction. The objective is to image the discontinuities of the acoustic velocity, a coefficient of the scalar wave equation from the discretely sampled time domain data measured at an array of transducers that can act as both sources and receivers. We treat the wave equation along with transducer functionals as a dynamical system. A reduced order model for the propagator of such system can be computed so that it interpolates exactly the measured time domain data. The resulting ROM is an orthogonal projection of the propagator on the subspace of the snapshots of solutions of the acoustic wave equation. While the wavefield snapshots are unknown, the projection ROM can be computed entirely from the measured data, thus we refer to such ROM as data-driven. The image is obtained by backprojecting the ROM. Since the basis functions for the projection subspace are not known, we replace them with the ones computed for a known smooth kinematic velocity model. A crucial step of ROM construction is an implicit orthogonalization of solution snapshots. It is a nonlinear procedure that differentiates our approach from the conventional linear imaging methods . It resolves all dynamical behavior captured by the data, so the error from the imperfect knowledge of the velocity model is purely kinematic. This allows for almost complete removal of multiple reflection artifacts, while simultaneously improving the resolution in the range direction compared to conventional RTM.",0.3395416845],["holograms are recorded by directive antennas aligned along each other's bore","Early Detection of Cancerous Tissues in Human Breast utilizing Near field Microwave Holography","summarize: This work demonstrates an application of near field indirect microwave holography for the detection of malignant tissues in the human breast in an effective way. The holograms are recorded by two directive antennas aligned along each other's boresight while performing a raster scan over a 2D plane utilizing XY-linear motorized translation stage and a uniform reference wave. The whole information i.e. amplitude and phase of an object has been provided by indirect holography at microwave frequencies. The extracted phase values are used to determine the dielectric permittivity values which are further utilized for the identification and validating the positions of malignant tissues in the breast phantom. The experimental evaluations performed on the in-house designed and developed tissue mimicking 3D printed breast phantoms. The experimental results demonstrate the ability of microwave holography using directive antennas in locating and identifying the tumors up to the minimum size of 4mm and a maximum depth of 25mm in fabricated phantom. The preliminary results present the potential of the Near Field Indirect Holographic Imaging in order to develop an efficient and economical tool for breast cancer detection.",0.0],["in tracking of time-varying low-rank models, we present a method robust to both","Pursuit of Low-Rank Models of Time-Varying Matrices Robust to Sparse and Measurement Noise","summarize: In tracking of time-varying low-rank models of time-varying matrices, we present a method robust to both uniformly-distributed measurement noise and arbitrarily-distributed ``sparse'' noise. In theory, we bound the tracking error. In practice, our use of randomised coordinate descent is scalable and allows for encouraging results on changedetection net, a benchmark.",0.3888888889],["Given an arbitrary closed set A of the arbitrary set A of the arbitrary closed set","Fine properties of the curvature of arbitrary closed sets","summarize: Given an arbitrary closed set A of ",0.3888888889],["remote sensing image scene classification has been receiving remarkable attention. a systematic review of literature","Remote Sensing Image Scene Classification: Benchmark and State of the Art","summarize: Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various datasets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning datasets and methods for scene classification is still lacking. In addition, almost all existing datasets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale dataset, termed NWPU-RESISC45, which is a publicly available benchmark for REmote Sensing Image Scene Classification , created by Northwestern Polytechnical University . This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 is large-scale on the scene classes and the total image number, holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion, and has high within-class diversity and between-class similarity. The creation of this dataset will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed dataset and the results are reported as a useful baseline for future research.",0.2916666667],["we recast the Einstein-Hilbert action as a theory of purely","Hidden Simplicity of the Gravity Action","summarize: We derive new representations of the Einstein-Hilbert action in which graviton perturbation theory is immensely simplified. To accomplish this, we recast the Einstein-Hilbert action as a theory of purely cubic interactions among gravitons and a single auxiliary field. The corresponding equations of motion are the Einstein field equations rewritten as two coupled first-order differential equations. Since all Feynman diagrams are cubic, we are able to derive new off-shell recursion relations for tree-level graviton scattering amplitudes. With a judicious choice of gauge fixing, we then construct an especially compact form for the Einstein-Hilbert action in which all graviton interactions are simply proportional to the graviton kinetic term. Our results apply to graviton perturbations about an arbitrary curved background spacetime.",0.2],["advances in the ability to construct three-dimensional tissues and organoids have opened abundant new research avenue","Roles of Diffusion Dynamics and Molecular Concentration Gradients in Cellular Differentiation and Three-Dimensional Tissue Development","summarize: Recent advancements in the ability to construct three-dimensional tissues and organoids from stem cells and biomaterials have not only opened abundant new research avenues in disease modeling and regenerative medicine but also have ignited investigation into important aspects of molecular diffusion in 3D cellular architectures. This paper describes fundamental mechanics of diffusion with equations for modeling these dynamic processes under a variety of scenarios in 3D cellular tissue constructs. The effects of these diffusion processes and resultant concentration gradients are described in the context of the major molecular signaling pathways in stem cells that both mediate and are influenced by gas and nutrient concentrations, including how diffusion phenomena can affect stem cell state, cell differentiation, and metabolic states of the cell. The application of these diffusion models and pathways is of vital importance for future studies of developmental processes, disease modeling, and tissue regeneration.",0.125],["advances in the ability to construct three-dimensional tissues and organoids have opened abundant new research avenue","Roles of Diffusion Dynamics and Molecular Concentration Gradients in Cellular Differentiation and Three-Dimensional Tissue Development","summarize: Recent advancements in the ability to construct three-dimensional tissues and organoids from stem cells and biomaterials have not only opened abundant new research avenues in disease modeling and regenerative medicine but also have ignited investigation into important aspects of molecular diffusion in 3D cellular architectures. This paper describes fundamental mechanics of diffusion with equations for modeling these dynamic processes under a variety of scenarios in 3D cellular tissue constructs. The effects of these diffusion processes and resultant concentration gradients are described in the context of the major molecular signaling pathways in stem cells that both mediate and are influenced by gas and nutrient concentrations, including how diffusion phenomena can affect stem cell state, cell differentiation, and metabolic states of the cell. The application of these diffusion models and pathways is of vital importance for future studies of developmental processes, disease modeling, and tissue regeneration.",0.125],["differential equations are often studied through their modified equation. this is a differential equation,","Modified equations and the Basel problem","summarize: Discretizations of differential equations are often studied through their modified equation. This is a differential equation, usually obtained as a power series, with solutions that exactly interpolate the discretization. By comparing the St\\ormer-Verlet discretization of the harmonic oscillator with its modified equation, we obtain a relatively simple derivation of the expansion \\ which can be used to show that ",0.2],["deep neural networks are a complex and complex learning process. we aim to walk the reader","A Simplified Approach to Deep Learning for Image Segmentation","summarize: Leaping into the rapidly developing world of deep learning is an exciting and sometimes confusing adventure. All of the advice and tutorials available can be hard to organize and work through, especially when training specific models on specific datasets, different from those originally used to train the network. In this short guide, we aim to walk the reader through the techniques that we have used to successfully train two deep neural networks for pixel-wise classification, including some data management and augmentation approaches for working with image data that may be insufficiently annotated or relatively homogenous.",0.2],["resetting to the origin was assumed to take zero time or a time decoupled","Invariants of motion with stochastic resetting and space-time coupled returns","summarize: Motion under stochastic resetting serves to model a myriad of processes in physics and beyond, but in most cases studied to date resetting to the origin was assumed to take zero time or a time decoupled from the spatial position at the resetting moment. However, in our world, getting from one place to another always takes time and places that are further away take more time to be reached. We thus set off to extend the theory of stochastic resetting such that it would account for this inherent spatio-temporal coupling. We consider a particle that starts at the origin and follows a certain law of stochastic motion until it is interrupted at some random time. The particle then returns to the origin via a prescribed protocol. We study this model and surprisingly discover that the shape of the steady-state distribution which governs the stochastic motion phase does not depend on the return protocol. This shape invariance then gives rise to a simple, and generic, recipe for the computation of the full steady-state distribution. Several case studies are analyzed and a class of processes whose steady-state is completely invariant with respect to the speed of return is highlighted. For processes in this class we recover the same steady-state obtained for resetting with instantaneous returns---irrespective of whether the actual return speed is high or low. Our work significantly extends previous results on motion with stochastic resetting and is expected to find various applications in statistical, chemical, and biological physics.",0.4090909091],["dynamic pricing plays an important role in the form of an incentive for the decision of the empowered passenger","Towards Dynamic Pricing for Shared Mobility on Demand using Markov Decision Processes and Dynamic Programming","summarize: In a Shared Mobility on Demand Service , dynamic pricing plays an important role in the form of an incentive for the decision of the empowered passenger on the ride offer. Strategies for determining the dynamic tariff should be suitably designed so that the incurred demand and supply are balanced and therefore economic efficiency is achieved. In this manuscript, we formulate a discrete time Markov Decision Process to determine the probability desired by the SMoDS platform corresponding to the acceptance rate of each empowered passenger at each state of the system. We use Estimated Waiting Time as the metric for the balance between demand and supply, with the goal that EWT be regulated around a target value. We then develop a Dynamic Programming algorithm to derive the optimal policy of the MDP that regulates EWT around the target value. Computational experiments are conducted that demonstrate the regulation of EWT is effective, through various scenarios. The overall demonstration is carried out offline. The MDP formulation together with the DP algorithm can be utilized to an online determination of the dynamic tariff by integrating with our earlier works on Cumulative Prospect Theory based passenger behavioral modeling and the AltMin dynamic routing algorithm, and form the subject of future works.",0.2352941176],["the ferromagnetic ground state of the Co2FeAl is energetically","Phase stability and the effect of lattice distortions on electronic properties and half-metallic ferromagnetism of Co2FeAl Heusler alloy: An ab initio study","summarize: Density functional theory calculations within the generalized gradient approximation are employed to study the ground state of Co2FeAl. Various magnetic configurations are considered to find out its most stable phase. The ferromagnetic ground state of the Co2FeAl is energetically observed with an optimized lattice constant of 5.70 . Thereafter, the system was subjected under uniform and non-uniform strains to see their effects on spin polarization and half-metallicity. The effect of spin orbit coupling is considered in the present study. Half-metallicity is only retained under uniform strains started from 0 to +4%, and dropped rapidly from 90% to 16% for the negative strains started from -1% to -6%. We find that the present system is much sensitive under tetragonal distortions as half-metallicity is preserved only for the cubic case. The main reason for the loss of half-metallicity is due to the shift of the bands with respect to the Fermi level. We also discuss the influence of these results on spintronics devices.",0.1072168559],["a continuous-time nonlinear regression model is considered. the paper is based","On the Whittle estimator for linear random noise spectral density parameter in continuous-time nonlinear regression models","summarize: A continuous-time nonlinear regression model with L\\'evy-driven linear noise process is considered. Sufficient conditions of consistency and asymptotic normality of the Whittle estimator for the parameter of the noise spectral density are obtained in the paper.",0.436381288],["transfer Krull monoids are a recent concept. they are a wide class of","Sets of Arithmetical Invariants in Transfer Krull Monoids","summarize: Transfer Krull monoids are a recent concept including all commutative Krull domains and also, for example, wide classes of non-commutative Dedekind domains. We show that transfer Krull monoids are fully elastic . In commutative Krull monoids which have sufficiently many prime divisors in all classes of their class group, the set of catenary degrees and the set of tame degrees are intervals. Without the assumption on the distribution of prime divisors, arbitrary finite sets can be realized as sets of catenary degrees and as sets of tame degrees.",0.4],["continuous cohomology theory for topological quandles introduced. inverse limit is applied to","Continuous cohomology of topological quandles","summarize: A continuous cohomology theory for topological quandles is introduced, and compared to the algebraic theories. Extensions of topological quandles are studied with respect to continuous 2-cocycles, and used to show the differences in second cohomology groups for specific topological quandles. A method of computing the cohomology groups of the inverse limit is applied to quandles.",0.3076923077],["a partial differential equations model for ELISPOT and Fluorospot immunoa","Cell detection on image-based immunoassays","summarize: Cell detection and counting in the image-based ELISPOT and Fluorospot immunoassays is considered a bottleneck. The task has remained hard to automatize, and biomedical researchers often have to rely on results that are not accurate. Previously proposed solutions are heuristic, and data-based solutions are subject to a lack of objective ground truth data. In this paper, we analyze a partial differential equations model for ELISPOT, Fluorospot, and assays of similar design. This leads us to a mathematical observation model for the images generated by these assays. We use this model to motivate a methodology for cell detection. Finally, we provide a real-data example that suggests that this cell detection methodology and a human expert perform comparably.",0.3125],["the Connectionist Temporal Classification has achieved great success in sequence to sequence analysis tasks.","A Hardware-Oriented and Memory-Efficient Method for CTC Decoding","summarize: The Connectionist Temporal Classification has achieved great success in sequence to sequence analysis tasks such as automatic speech recognition and scene text recognition . These applications can use the CTC objective function to train the recurrent neural networks , and decode the outputs of RNNs during inference. While hardware architectures for RNNs have been studied, hardware-based CTCdecoders are desired for high-speed CTC-based inference systems. This paper, for the first time, provides a low-complexity and memory-efficient approach to build a CTC-decoder based on the beam search decoding. Firstly, we improve the beam search decoding algorithm to save the storage space. Secondly, we compress a dictionary and use it as the language model. Meanwhile searching this dictionary is trivial. Finally, a fixed-point CTC-decoder for an English ASR and an STR task using the proposed method is implemented with C++ language. It is shown that the proposed method has little precision loss compared with its floating-point counterpart. Our experiments demonstrate the compression ratio of the storage required by the proposed beam search decoding algorithm are 29.49 and 17.95 .",0.0625],["assortative mixing in networks is the tendency for nodes with the same attributes to link to","Multiscale mixing patterns in networks","summarize: Assortative mixing in networks is the tendency for nodes with the same attributes, or metadata, to link to each other. It is a property often found in social networks manifesting as a higher tendency of links occurring between people with the same age, race, or political belief. Quantifying the level of assortativity or disassortativity can shed light on the factors involved in the formation of links and contagion processes in complex networks. It is common practice to measure the level of assortativity according to the assortativity coefficient, or modularity in the case of discrete-valued metadata. This global value is the average level of assortativity across the network and may not be a representative statistic when mixing patterns are heterogeneous. For example, a social network spanning the globe may exhibit local differences in mixing patterns as a consequence of differences in cultural norms. Here, we introduce an approach to localise this global measure so that we can describe the assortativity, across multiple scales, at the node level. Consequently we are able to capture and qualitatively evaluate the distribution of mixing patterns in the network. We find that for many real-world networks the distribution of assortativity is skewed, overdispersed and multimodal. Our method provides a clearer lens through which we can more closely examine mixing patterns in networks.",0.2272727273],["algencan is a well established safeguarded Augmented Lagrangian algorithm introduced in","Complexity and performance of an Augmented Lagrangian algorithm","summarize: Algencan is a well established safeguarded Augmented Lagrangian algorithm introduced in . Complexity results that report its worst-case behavior in terms of iterations and evaluations of functions and derivatives that are necessary to obtain suitable stopping criteria are presented in this work. In addition, the computational performance of a new version of the method is presented, which shows that the updated software is a useful tool for solving large-scale constrained optimization problems.",0.4545454545],["the computational bottleneck is because of the similarity between a read and candidate locations in that","GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment in DNA Short Read Mapping","summarize: Motivation: High throughput DNA sequencing technologies generate an excessive number of small DNA segments -- called short reads -- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and candidate locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: it is implemented using quadratic-time dynamic programming algorithms, and the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment operations. Results: We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays , which can perform pre-alignment much faster than software. GateKeeper can be integrated with any mapper that performs sequence alignment for verification. When implemented on a single FPGA chip, GateKeeper maintains high accuracy while providing up to 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance , respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability: https:\/\/github.com\/BilkentCompGen\/GateKeeper",0.2413793103],["for more information","Bandwidth Selection for the Wolverton-Wagner Estimator","summarize: For ",0.0919698603],["the method subdivides a large problem in two smaller ones. the solution's","Analysis of Divide & Conquer strategies for the 0-1 Minimization Knapsack Problem","summarize: We introduce and asses several Divide \\& Conquer heuristic strategies aimed to solve large instances of the 0-1 Minimization Knapsack Problem. The method subdivides a large problem in two smaller ones , to lower down the global computational complexity of the original problem, at the expense of a moderate loss of quality in the solution. Theoretical mathematical results are presented in order to guarantee an algorithmically successful application of the method and to suggest the potential strategies for its implementation. In contrast, due to the lack of theoretical results, the solution's quality deterioration is measured empirically by means of Monte Carlo simulations for several types and values of the chosen strategies. Finally, introducing parameters of efficiency we suggest the best strategies depending on the data input.",0.375],["we show that for all time-scales the mean squared displacement of Brownian micro","Viscous-Viscoelastic Correspondence Principle for Brownian Motion","summarize: Motivated from the classical expressions of the mean squared displacement and the velocity autocorrelation function of Brownian particles suspended either in a Newtonian viscous fluid or trapped in a harmonic potential, we show that for all time-scales the mean squared displacement of Brownian microspheres with mass ",0.1538461538],["a multidisciplinary approach is presented to analyse precipitation process in a model Al-Cu","A multidisciplinary approach to study precipitation kinetics and hardening in an Al-4Cu alloy","summarize: A multidisciplinary approach is presented to analyse the precipitation process in a model Al-Cu alloy. Although this topic has been extensively studied in the past, most of the investigations are focussed either on transmission electron microscopy or on thermal analysis of the processes. The information obtained from these techniques cannot, however, provide a coherent picture of all the complex transformations that take place during decomposition of supersaturated solid solution. Thermal analysis, high resolution dilatometry, transmission electron microscopy and density functional calculations are combined to study precipitation kinetics, interfacial energies, and the effect of second phase precipitates on the mechanical strength of the alloy. Data on both the coherent and semi-coherent orientations of the \/Al interface are reported for the first time. The combination of the different characterization and modelling techniques provides a detailed picture of the precipitation phenomena that take place during aging and of the different contributions to the strength of the alloy. This strategy can be used to analyse and design more complex alloys.",0.7931034483],["oriented graphs are a partition of vertex set V into k independent sets","Oriented coloring on recursively defined digraphs","summarize: Coloring is one of the most famous problems in graph theory. The coloring problem on undirected graphs has been well studied, whereas there are very few results for coloring problems on directed graphs. An oriented k-coloring of an oriented graph G= is a partition of the vertex set V into k independent sets such that all the arcs linking two of these subsets have the same direction. The oriented chromatic number of an oriented graph G is the smallest k such that G allows an oriented k-coloring. Deciding whether an acyclic digraph allows an oriented 4-coloring is NP-hard. It follows, that finding the chromatic number of an oriented graph is an NP-hard problem. This motivates to consider the problem on oriented co-graphs. After giving several characterizations for this graph class, we show a linear time algorithm which computes an optimal oriented coloring for an oriented co-graph. We further prove how the oriented chromatic number can be computed for the disjoint union and order composition from the oriented chromatic number of the involved oriented co-graphs. It turns out that within oriented co-graphs the oriented chromatic number is equal to the length of a longest oriented path plus one. We also show that the graph isomorphism problem on oriented co-graphs can be solved in linear time.",0.1176470588],["self-assembly is the autonomous organization of components into patterns or structures.","Targeted Assembly and Synchronization of Self-Spinning Microgears","summarize: Self-assembly is the autonomous organization of components into patterns or structures: an essential ingredient of biology and a desired route to complex organization. At equilibrium, the structure is encoded through specific interactions, at an unfavorable entropic cost for the system. An alternative approach, widely used by Nature, uses energy input to bypass the entropy bottleneck and develop features otherwise impossible at equilibrium. Dissipative building blocks that inject energy locally were made available by recent advance in colloidal science but have not been used to control self-assembly. Here we show the robust formation of self-powered rotors and dynamical superstructures from active particles and harness non-equilibrium phoretic phenomena to tailor interactions and direct self-assembly. We use a photoactive component that consumes fuel, hematite, to devise phototactic microswimmers that form self-spinning microgears following spatiotemporal light patterns. The gears are coupled via their chemical clouds and constitute the elementary bricks of synchronized superstructures, which autonomously regulate their dynamics. The results are quantitatively rationalized on the basis of a stochastic description of diffusio-phoretic oscillators dynamically coupled by chemical gradients to form directional interactions. Our findings demonstrate that non-equilibrium phenomena can be harnessed to shape interactions and program hierarchical constructions. It lays the groundwork for the self-assembly of dynamical architectures and synchronized micro-machinery.",0.0833333333],["existing studies have used approximation techniques. the method improves efficiency by reducing","Efficient Network Reliability Computation in Uncertain Graphs","summarize: Network reliability is an important metric to evaluate the connectivity among given vertices in uncertain graphs. Since the network reliability problem is known as #P-complete, existing studies have used approximation techniques. In this paper, we propose a new sampling-based approach that efficiently and accurately approximates network reliability. Our approach improves efficiency by reducing the number of samples based on stratified sampling. We theoretically guarantee that our approach improves the accuracy of approximation by using lower and upper bounds of network reliability, even though it reduces the number of samples. To efficiently compute the bounds, we develop an extended BDD, called S2BDD. During constructing the S2BDD, our approach employs dynamic programming for efficiently sampling possible graphs. Our experiment with real datasets demonstrates that our approach is up to 51.2 times faster than the existing sampling-based approach with higher accuracy.",0.0],["method is based on the construction of a polynomial basis to interpolate","Chebyshev Reduced Basis Function applied to Option Valuation","summarize: We present a numerical method for the frequent pricing of financial derivatives that depends on a large number of variables. The method is based on the construction of a polynomial basis to interpolate the value function of the problem by means of a hierarchical orthogonalization process that allows to reduce the number of degrees of freedom needed to have an accurate representation of the value function. In the paper we consider, as an example, a GARCH model that depends on eight parameters and show that a very large number of contracts for different maturities and asset and parameters values can be valued in a small computational time with the proposed procedure. In particular the method is applied to the problem of model calibration. The method is easily generalizable to be used with other models or problems.",0.4736842105],["this article provides an accessible illustration of the measurement approach. we apply it to a quantum","Classicalization by phase space measurements","summarize: This article provides an accessible illustration of the measurement approach to the study of the quantum-classical transition suitable for beginning graduate students. As an example, we apply it to a quantum system with a general quadratic Hamiltonian and obtain the exact solution of the dynamics for an arbitrary measurement strength.",0.2692307692],["the TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of","TADPOLE Challenge: Accurate Alzheimer's disease prediction through crowdsourced forecasting of future data","summarize: The TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants train their models and algorithms on historical data from the Alzheimer's Disease Neuroimaging Initiative study. Participants are then required to make forecasts of three key outcomes for ADNI-3 rollover participants: clinical diagnosis, ADAS-Cog 13, and total volume of the ventricles -- which are then compared with future measurements. Strong points of the challenge are that the test data did not exist at the time of forecasting , and that it focuses on the challenging problem of cohort selection for clinical trials by identifying fast progressors. The submission phase of TADPOLE was open until 15 November 2017; since then data has been acquired until April 2019 from 219 subjects with 223 clinical visits and 150 Magnetic Resonance Imaging scans, which was used for the evaluation of the participants' predictions. Thirty-three teams participated with a total of 92 submissions. No single submission was best at predicting all three outcomes. For diagnosis prediction, the best forecast , which was based on gradient boosting, obtained a multiclass area under the receiver-operating curve of 0.931, while for ventricle prediction the best forecast , which was based on disease progression modelling and spline regression, obtained mean absolute error of 0.41% of total intracranial volume . For ADAS-Cog 13, no forecast was considerably better than the benchmark mixed effects model , provided to participants before the submission deadline. Further analysis can help understand which input features and algorithms are most suitable for Alzheimer's disease prediction and for aiding patient stratification in clinical trials.",0.2727272727],["we consider a minimal physical model where the fluid self-organizes in a con","Order Out of Chaos: Slowly Reversing Mean Flows Emerge from Turbulently Generated Internal Waves","summarize: We demonstrate via direct numerical simulations that a periodic, oscillating mean flow spontaneously develops from turbulently generated internal waves. We consider a minimal physical model where the fluid self-organizes in a convective layer adjacent to a stably stratified one. Internal waves are excited by turbulent convective motions, then nonlinearly interact to produce a mean flow reversing on timescales much longer than the waves' period. Our results demonstrate for the first time that the three-scale dynamics due to convection, waves, and mean flow is generic and hence can occur in many astrophysical and geophysical fluids. We discuss efforts to reproduce the mean flow in reduced models, where the turbulence is bypassed. We demonstrate that wave intermittency, resulting from the chaotic nature of convection, plays a key role in the mean-flow dynamics, which thus cannot be captured using only second-order statistics of the turbulent motions.",0.4428509507],["the Estonian network of payments is a unique case of study. the overlapping community","Detecting overlapping community structure: Estonian network of payments","summarize: Revealing the community structure exhibited by real networks is a fundamental phase towards a comprehensive understanding of complex systems beyond the local organization of their components. Community detection techniques help on providing insights into understanding the local organization of the components of networks. In this study we identify and investigate the overlapping community structure of an interesting and unique case of study: the Estonian network of payments. In order to perform the study, we use the Clique Percolation Method and explore statistical distribution functions of the communities, where in most cases we found scale-free properties. In this network the nodes represent Estonian companies and the links represent payments done between the companies. Our study adds to the literature of complex networks by presenting the first overlapping community detection analysis of a country's network of payments.",0.4545454545],["morphological dilation and erosion with additive structuring function. morphological erosion is","A simple expression for the map of Asplund's distances with the multiplicative Logarithmic Image Processing law","summarize: We introduce a simple expression for the map of Asplund's distances with the multiplicative Logarithmic Image Processing law. It is a difference between a morphological dilation and a morphological erosion with an additive structuring function which corresponds to a morphological gradient.",0.0846315225],["vortex rings are characterized as nonlinear Hermite-Laguerre modes","Vortex rings and vortex ring solitons in shaken Bose-Einstein condensate","summarize: In a shaken Bose-Einstein condensate, confined in a vibrating trap, there can appear different nonlinear coherent modes. Here we concentrate on two types of such coherent modes, vortex ring solitons and vortex rings. In a cylindrical trap, vortex ring solitons can be characterized as nonlinear Hermite-Laguerre modes, whose description can be done by means of optimized perturbation theory. The energy, required for creating vortex ring solitons, is larger than that needed for forming vortex rings. This is why, at a moderate excitation energy, vortex rings appear before vortex ring solitons. The generation of vortex rings is illustrated by numerical simulations for trapped ",0.1947001958],["software testing is becoming a critical part of the development cycle of embedded devices. the goal","Side-Channel Aware Fuzzing","summarize: Software testing is becoming a critical part of the development cycle of embedded devices, enabling vulnerability detection. A well-studied approach of software testing is fuzz-testing , during which mutated input is sent to an input-processing software while its behavior is monitored. The goal is to identify faulty states in the program, triggered by malformed inputs. Even though this technique is widely performed, fuzzing cannot be applied to embedded devices to its full extent. Due to the lack of adequately powerful I\/O capabilities or an operating system the feedback needed for fuzzing cannot be acquired. In this paper we present and evaluate a new approach to extract feedback for fuzzing on embedded devices using information the power consumption leaks. Side-channel aware fuzzing is a threefold process that is initiated by sending an input to a target device and measuring its power consumption. First, we extract features from the power traces of the target device using machine learning algorithms. Subsequently, we use the features to reconstruct the code structure of the analyzed firmware. In the final step we calculate a score for the input, which is proportional to the code coverage. We carry out our proof of concept by fuzzing synthetic software and a light-weight AES implementation running on an ARM Cortex-M4 microcontroller. Our results show that the power side-channel carries information relevant for fuzzing.",0.0769230769],["large deviation principle on phase space is proved for a class of Markov processes known as random","Large deviations in a population dynamics with catastrophes","summarize: The large deviation principle on phase space is proved for a class of Markov processes known as random population dynamics with catastrophes. In the paper we study the process which corresponds to the random population dynamics with linear growth and uniform catastrophes, where an eliminating portion of the population is chosen uniformly. The large deviation result provides an optimal trajectory of large fluctuation: it shows how the large fluctuations occur for this class of processes.",0.3928571429],["symbolic model checking for Dynamic Epistemic Logic. the method can be implemented using","Towards Symbolic Factual Change in DEL","summarize: We extend symbolic model checking for Dynamic Epistemic Logic with factual change. Our transformers provide a compact representation of action models with pre- and postconditions, for both S5 and the general case. The method can be implemented using binary decision diagrams and we expect it to improve model checking performance. As an example we give a symbolic representation of the Sally-Anne false belief task.",0.0],["we propose three approximate likelihoods that are computationally tractable. they are based on","Langevin diffusions on the torus: estimation and applications","summarize: We introduce stochastic models for continuous-time evolution of angles and develop their estimation. We focus on studying Langevin diffusions with stationary distributions equal to well-known distributions from directional statistics, since such diffusions can be regarded as toroidal analogues of the Ornstein-Uhlenbeck process. Their likelihood function is a product of transition densities with no analytical expression, but that can be calculated by solving the Fokker-Planck equation numerically through adequate schemes. We propose three approximate likelihoods that are computationally tractable: a likelihood based on the stationary distribution; toroidal adaptations of the Euler and Shoji-Ozaki pseudo-likelihoods; a likelihood based on a specific approximation to the transition density of the wrapped normal process. A simulation study compares, in dimensions one and two, the approximate transition densities to the exact ones, and investigates the empirical performance of the approximate likelihoods. Finally, two diffusions are used to model the evolution of the backbone angles of the protein G during a molecular dynamics simulation. The software package sdetorus implements the estimation methods and applications presented in the paper.",0.125],["reinforcement learning is essential in online systems. there are many practical challenges to implement deep reinforcement learning","MBCAL: Sample Efficient and Variance Reduced Reinforcement Learning for Recommender Systems","summarize: In recommender systems such as news feed stream, it is essential to optimize the long-term utilities in the continuous user-system interaction processes. Previous works have proved the capability of reinforcement learning in this problem. However, there are many practical challenges to implement deep reinforcement learning in online systems, including low sample efficiency, uncontrollable risks, and excessive variances. To address these issues, we propose a novel reinforcement learning method, namely model-based counterfactual advantage learning . The proposed method takes advantage of the characteristics of recommender systems and draws ideas from the model-based reinforcement learning method for higher sample efficiency. It has two components: an environment model that predicts the instant user behavior one-by-one in an auto-regressive form, and a future advantage model that predicts the future utility. To alleviate the impact of excessive variance when learning the future advantage model, we employ counterfactual comparisons derived from the environment model. In consequence, the proposed method possesses high sample efficiency and significantly lower variance; Also, it is able to use existing user logs to avoid the risks of starting from scratch. In contrast to its capability, its implementation cost is relatively low, which fits well with practical systems. Theoretical analysis and elaborate experiments are presented. Results show that the proposed method transcends the other supervised learning and RL-based methods in both sample efficiency and asymptotic performances.",0.0769230769],["metric spaces are mapped using a set of simulation functions. a fixed disc","Fixed-disc results via simulation functions","summarize: In this paper, our aim is to obtain new fixed-disc results on metric spaces. To do this, we present a new approach using the set of simulation functions and some known fixed-point techniques. We do not need to have some strong conditions such as completeness or compactness of the metric space or continuity of the self-mapping in our results. Taking only one geometric condition, we ensure the existence of a fixed disc of a new type contractive mapping.",0.2608695652],["inverse problem is usually solved through a regularized least-squares approach.","Automatic alignment for three-dimensional tomographic reconstruction","summarize: In tomographic reconstruction, the goal is to reconstruct an unknown object from a collection of line integrals. Given a complete sampling of such line integrals for various angles and directions, explicit inverse formulas exist to reconstruct the object. Given noisy and incomplete measurements, the inverse problem is typically solved through a regularized least-squares approach. A challenge for both approaches is that in practice the exact directions and offsets of the x-rays are only known approximately due to, e.g. calibration errors. Such errors lead to artifacts in the reconstructed image. In the case of sufficient sampling and geometrically simple misalignment, the measurements can be corrected by exploiting so-called consistency conditions. In other cases, such conditions may not apply and we have to solve an additional inverse problem to retrieve the angles and shifts. In this paper we propose a general algorithmic framework for retrieving these parameters in conjunction with an algebraic reconstruction technique. The proposed approach is illustrated by numerical examples for both simulated data and an electron tomography dataset.",0.25],["the problem is NP-hard in general but we show that under certain conditions we can recover","Learning discrete Bayesian networks in polynomial time and sample complexity","summarize: In this paper, we study the problem of structure learning for Bayesian networks in which nodes take discrete values. The problem is NP-hard in general but we show that under certain conditions we can recover the true structure of a Bayesian network with sufficient number of samples. We develop a mathematical model which does not assume any specific conditional probability distributions for the nodes. We use a primal-dual witness construction to prove that, under some technical conditions on the interaction between node pairs, we can do exact recovery of the parents and children of a node by performing group l_12-regularized multivariate regression. Thus, we recover the true Bayesian network structure. If degree of a node is bounded then the sample complexity of our proposed approach grows logarithmically with respect to the number of nodes in the Bayesian network. Furthermore, our method runs in polynomial time.",0.1578947368],["this second part of the series treats spin.","Uniform energy bound and Morawetz estimate for extreme components of spin fields in the exterior of a slowly rotating Kerr black hole II: linearized gravity","summarize: This second part of the series treats spin ",0.0447873631],["general model contains many existing plane-based clustering methods. the general model is a","A general model for plane-based clustering with loss function","summarize: In this paper, we propose a general model for plane-based clustering. The general model contains many existing plane-based clustering methods, e.g., k-plane clustering , proximal plane clustering , twin support vector clustering and its extensions. Under this general model, one may obtain an appropriate clustering method for specific purpose. The general model is a procedure corresponding to an optimization problem, where the optimization problem minimizes the total loss of the samples. Thereinto, the loss of a sample derives from both within-cluster and between-cluster. In theory, the termination conditions are discussed, and we prove that the general model terminates in a finite number of steps at a local or weak local optimal point. Furthermore, based on this general model, we propose a plane-based clustering method by introducing a new loss function to capture the data distribution precisely. Experimental results on artificial and public available datasets verify the effectiveness of the proposed method.",0.375],["a local Lipschitz stability estimate for the Schrodinger equation is found","Calder\\'on's Inverse Problem with a Finite Number of Measurements II: Independent Data","summarize: We prove a local Lipschitz stability estimate for Gel'fand-Calder\\'on's inverse problem for the Schr\\odinger equation. The main novelty is that only a finite number of boundary input data is available, and those are independent of the unknown potential, provided it belongs to a known finite-dimensional subspace of ",0.3043669054],["we consider the equation.","Lack of null-controllability for the fractional heat equation and related equations","summarize: We consider the equation ",0.0868869717],["persistent homology is introduced for the first time to quantitatively analyze the intrinsic topological properties of","Persistent homology analysis of ion aggregation and hydrogen-bonding network","summarize: Despite the great advancement of experimental tools and theoretical models, a quantitative characterization of the microscopic structures of ion aggregates and its associated water hydrogen-bonding networks still remains a challenging problem. In this paper, a newly-invented mathematical method called persistent homology is introduced, for the first time, to quantitatively analyze the intrinsic topological properties of ion aggregation systems and hydrogen-bonding networks. Two most distinguishable properties of persistent homology analysis of assembly systems are as follows. First, it does not require a predefined bond length to construct the ion or hydrogen network. Persistent homology results are determined by the morphological structure of the data only. Second, it can directly measure the size of circles or holes in ion aggregates and hydrogen-bonding networks. To validate our model, we consider two well-studied systems, i.e., NaCl and KSCN solutions, generated from molecular dynamics simulations. They are believed to represent two morphological types of aggregation, i.e., local clusters and extended ion network. It has been found that the two aggregation types have distinguishable topological features and can be characterized by our topological model very well. For hydrogen-bonding networks, KSCN systems demonstrate much more dramatic variations in their local circle structures with the concentration increase. A consistent increase of large-sized local circle structures is observed and the sizes of these circles become more and more diverse. In contrast, NaCl systems show no obvious increase of large-sized circles. Instead a consistent decline of the average size of circle structures is observed and the sizes of these circles become more and more uniformed with the concentration increase.",0.2],["quantitative structure-activity relationship has proved invaluable tool in medicinal chemistry. predictive models are","On the Virtues of Automated QSAR The New Kid on the Block","summarize: Quantitative Structure-Activity Relationship has proved an invaluable tool in medicinal chemistry. Data availability at unprecedented levels through various databases have collaborated to a resurgence in the interest for QSAR. In this context, rapid generation of quality predictive models is highly desirable for hit identification and lead optimization. We showcase the application of an automated QSAR approach, which randomly selects multiple training\/test sets and utilizes machine-learning algorithms to generate predictive models. Results demonstrate that AutoQSAR produces models of improved or similar quality to those generated by practitioners in the field but in just a fraction of the time. Despite the potential of the concept to the benefit of the community, the AutoQSAR opportunity has been largely undervalued.",0.0],["model fit with variational Bayes may not equal the underlying data distribution. variation","Decision-Making with Auto-Encoding Variational Bayes","summarize: To make decisions based on a model fit with auto-encoding variational Bayes , practitioners often let the variational distribution serve as a surrogate for the posterior distribution. This approach yields biased estimates of the expected risk, and therefore leads to poor decisions for two reasons. First, the model fit with AEVB may not equal the underlying data distribution. Second, the variational distribution may not equal the posterior distribution under the fitted model. We explore how fitting the variational distribution based on several objective functions other than the ELBO, while continuing to fit the generative model based on the ELBO, affects the quality of downstream decisions. For the probabilistic principal component analysis model, we investigate how importance sampling error, as well as the bias of the model parameter estimates, varies across several approximate posteriors when used as proposal distributions. Our theoretical results suggest that a posterior approximation distinct from the variational distribution should be used for making decisions. Motivated by these theoretical results, we propose learning several approximate proposals for the best model and combining them using multiple importance sampling for decision-making. In addition to toy examples, we present a full-fledged case study of single-cell RNA sequencing. In this challenging instance of multiple hypothesis testing, our proposed approach surpasses the current state of the art.",0.1428571429],["classical sketch has similar effect on optimization properties of MRR. classical sketch does not have this","Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging","summarize: We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee, instead, the approximation error is governed by a subtle interplay between the mass in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR, these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching.",0.0952380952],["ergodic partition of state space in a class of measure-preserving and diss","Invariant Sets in Quasiperiodically Forced Dynamical Systems","summarize: This paper addresses structures of state space in quasiperiodically forced dynamical systems. We develop a theory of ergodic partition of state space in a class of measure-preserving and dissipative flows, which is a natural extension of the existing theory for measure-preserving maps. The ergodic partition result is based on eigenspace at eigenvalue 0 of the associated Koopman operator, which is realized via time-averages of observables, and provides a constructive way to visualize a low-dimensional slice through a high-dimensional invariant set. We apply the result to the systems with a finite number of attractors and show that the time-average of a continuous observable is well-defined and reveals the invariant sets, namely, a finite number of basins of attraction. We provide a characterization of invariant sets in the quasiperiodically forced systems. A theoretical result on uniform boundedness of the invariant sets is presented. The series of theoretical results enables numerical analysis of invariant sets in the quasiperiodically forced systems based on the ergodic partition and time-averages. Using this, we analyze a nonlinear model of complex power grids that represents the short-term swing instability, named the coherent swing instability. We show that our theoretical results can be used to understand stability regions in such complex systems.",0.3333333333],["imageCLEFmed Caption task is to develop a system that labels radiology images with","A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task","summarize: The aim of ImageCLEFmed Caption task is to develop a system that automatically labels radiology images with relevant medical concepts. We describe our Deep Neural Network based approach for tackling this problem. On the challenge test set of 3,534 radiology images, our system achieves an F1 score of 0.375 and ranks high, 12th among all systems that were successfully submitted to the challenge, whereby we only rely on the provided data sources and do not use any external medical knowledge or ontologies, or pretrained models from other medical image repositories or application domains.",0.3],["the results are a consequence of the simultaneaous monomialization presented here","Simultaneous Monomialization","summarize: We give a new proof of the simultaneous embedded local uniformization Theorem in zero characteristic for essentially of finite type rings and for quasi excellent rings. The results are a consequence of the simultaneaous monomialization presented here. The methods develop the key elements theory that is a more subtle notion than the notion of key polynomials.",0.1666666667],["the LTE uses static LTE Key to derive the entire key hierarchy. the encryption","A Console GRID LA Console GRID Leveraged Authentication and Key Agreement Mechanism for LTE\/SAE","summarize: The growing popularity of multimedia applications, pervasive connectivity, higher bandwidth, and euphoric technology penetration among the bulk of the human race that happens to be cellular technology users, has fueled the adaptation to Long Term Evolution \/ System Architecture Evolution . The LTE fulfills the resource demands of the next generation applications for now. We identify security issues in the authentication mechanism used in LTE that without countermeasures might give superuser rights to unauthorized users. The LTE uses static LTE Key to derive the entire key hierarchy such as LTE follows Evolved Packet System-Authentication and Key Agreement based authentication which discloses user identity, location, and other Personally Identifiable Information . To counter this, we propose a public key cryptosystem named International mobile subscriber identity Protected Console Grid-based Authentication and Key Agreement protocol to address the vulnerabilities related to weak key management. From the data obtained from threat modeling and simulation results, we claim that the IPG-AKA scheme not only improves the security of authentication procedures, it also shows improvements in authentication loads and reduction in key generation time. The empirical results and qualitative analysis presented in this paper proves that IPG-AKA improves security in authentication procedure and performance in the LTE.",0.2727272727],["we prove an equivalence of categories, generalizing the equivalence","Group Schemes with ","summarize: Via a construction due to V. Drinfel'd, we prove an equivalence of categories, generalizing the equivalence between commutative flat group schemes in characteristic ",0.0],["sensorimotor controls are mainly based on information gathered from vision, proprio","Towards the Enhancement of Body Standing Balance Recovery by Means of a Wireless Audio-Biofeedback System","summarize: Human maintain their body balance by sensorimotor controls mainly based on information gathered from vision, proprioception and vestibular systems. When there is a lack of information, caused by pathologies, diseases or aging, the subject may fall. In this context, we developed a system to augment information gathering, providing the subject with warning audio-feedback signals related to his\/her equilibrium. The system comprises an inertial measurement unit , a data processing unit, a headphone audio device and a software application. The IMU is a low-weight, small-size wireless instrument that, body-back located between the L2 and L5 lumbar vertebrae, measures the subject's trunk kinematics. The application drives the data processing unit to feeding the headphone with electric signals related to the kinematic measures. Consequently, the user is audio-alerted, via headphone, of his\/her own equilibrium, hearing a pleasant sound when in a stable equilibrium, or an increasing bothering sound when in an increasing unstable condition. Tests were conducted on a group of six older subjects and a group of four young subjects to underline difference in effectiveness of the system, if any, related to the age of the users. For each subject, standing balance tests were performed in normal or altered conditions, such as, open or closed eyes, and on a solid or foam surface The system was evaluated in terms of usability, reliability, and effectiveness in improving the subject's balance in all conditions. As a result, the system successfully helped the subjects in reducing the body swaying within 10.65%-65.90%, differences depending on subjects' age and test conditions.",0.0],["the data was combined with information available in DOAJ, CrossRef, OpenDOAR,","Evidence of Open Access of scientific publications in Google Scholar: a large-scale analysis","summarize: This article uses Google Scholar as a source of data to analyse Open Access levels across all countries and fields of research. All articles and reviews with a DOI and published in 2009 or 2014 and covered by the three main citation indexes in the Web of Science were selected for study. The links to freely available versions of these documents displayed in GS were collected. To differentiate between more reliable forms of access and less reliable ones, the data extracted from GS was combined with information available in DOAJ, CrossRef, OpenDOAR, and ROAR. This allowed us to distinguish the percentage of documents in our sample that are made OA by the publisher from those available as Green OA , and those available from other sources . The data shows an overall free availability of 54.6%, with important differences at the country and subject category levels. The data extracted from GS yielded very similar results to those found by other studies that analysed similar samples of documents, but employed different methods to find evidence of OA, thus suggesting a relative consistency among methods.",0.0641348399],["a vacancy under compression is fissioned into a pair of dislocation","Stress driven fractionalization of vacancies in regular packings of elastic particles","summarize: Elucidating the interplay of defect and stress at the microscopic level is a fundamental physical problem that has strong connection with materials science. Here, based on the two-dimensional crystal model, we show that the instability mode of vacancies with varying size and morphology conforms to a common scenario. A vacancy under compression is fissioned into a pair of dislocations that glide and vanish at the boundary. This neat process is triggered by the local shear stress around the vacancy. The remarkable fractionalization of vacancies creates rich modes of interaction between vacancies and other topological defects, and provides a new dimension for mechanical engineering of defects in extensive crystalline structures.",0.5652173913],["confinement induced ordering is modeled using a film thickness dependent viscosity","Confinement induced control of similarity solutions in premelting dynamics and other thin film problems","summarize: We study the combined effects of nonlocal elasticity and confinement induced ordering on the dynamics of thermomolecular pressure gradient driven premelted films bound by an elastic membrane. The confinement induced ordering is modeled using a film thickness dependent viscosity. When there is no confinement induced ordering, we recover the similarity solution for the evolution of the elastic membrane, which exhibits an infinite sequence of oscillations. However, when the confinement induced viscosity is comparable to the bulk viscosity, the numerical solutions of the full system reveal the conditions under which the oscillations and similarity solutions vanish. Implications of our results for general thermomechanical dynamics, frost heave observations and cryogenic cell preservation are discussed. Finally, through its influence on the viscosity, the confinement effect implicitly introduces a new universal length scale into the volume flux. Thus, there are a host of thin film problems, from droplet breakup to wetting\/dewetting dynamics, whose properties will change under the action of the confinement effect. Therefore, our study suggests revisiting the mathematical structure and experimental implications of a wide range of problems within the framework of the confinement effect.",0.1903250967],["study aims to compare different designs for the world men's handball championships.","A simulation comparison of tournament designs for the World Men's Handball Championships","summarize: The study aims to compare different designs for the World Men's Handball Championships. This event, organised in every two years, has adopted four hybrid formats consisting of knockout and round-robin stages in recent decades, including a change of design between the two recent championships in 2017 and 2019. They are evaluated under two extremal seeding policies with respect to various outcome measures through Monte-Carlo simulations. We find that the ability to give the first four positions to the strongest teams, as well as the expected quality and outcome uncertainty of the final is not necessarily a monotonic function of the number of matches played: the most frugal format is the second best with respect to these outcome measures, making it a good compromise in an unavoidable trade-off. A possible error is identified in a particular design. The relative performance of the formats is independent of the seeding rules and the competitive balance of the teams. The recent reform is demonstrated to have increased the probability of winning for the top teams. Our results have useful implications for the organisers of hybrid tournaments.",0.3333333333],["the government has been one of the most responsive to COVID-19. the government has","The determinants of COVID-19 case fatality rate in the Italian regions and provinces: an analysis of environmental, demographic, and healthcare factors","summarize: The Italian government has been one of the most responsive to COVID-19 emergency, through the adoption of quick and increasingly stringent measures to contain the outbreak. Despite this, Italy has suffered a huge human and social cost, especially in Lombardy. The aim of this paper is dual: i) first, to investigate the reasons of the case fatality rate differences across Italian 20 regions and 107 provinces, using a multivariate OLS regression approach; and ii) second, to build a taxonomy of provinces with similar mortality risk of COVID-19, by using the Ward hierarchical agglomerative clustering method. I considered health system metrics, environmental pollution, climatic conditions, demographic variables, and three ad hoc indexes that represent the health system saturation. The results showed that overall health care efficiency, physician density, and average temperature helped to reduce the CFR. By the contrary, population aged 70 and above, car and firm density, level of air pollutants , relative average humidity, COVID-19 prevalence, and all three indexes of health system saturation were positively associated with the CFR. Population density, social vertical integration, and altitude were not statistically significant. In particular, the risk of dying increases with age, as 90 years old and above had a three-fold greater risk than the 80 to 89 years old and four-fold greater risk than 70 to 79 years old. Moreover, the cluster analysis showed that the highest mortality risk was concentrated in the north of the country, while the lowest risk was associated with southern provinces. Finally, since prevalence and health system saturation indexes played the most important role in explaining the CFR variability, a significant part of the latter may have been caused by the massive stress of the Italian health system.",0.1516326649],["5G mobile networks provide additional benefits in terms of lower latency, higher data rates, and","Blockchain-enabled Authentication Handover with Efficient Privacy Protection in SDN-based 5G Networks","summarize: 5G mobile networks provide additional benefits in terms of lower latency, higher data rates, and more coverage, in comparison to 4G networks, and they are also coming close to standardization. For example, 5G has a new level of data transfer and processing speed that assures users are not disconnected when they move from one cell to another; thus, supporting faster connection. However, it comes with its own technical challenges relating to resource management, authentication handover and user privacy protection. In 5G, the frequent displacement of the users among the cells as a result of repeated authentication handovers often lead to a delay, contradicting the 5G objectives. In this paper, we propose a new authentication approach that utilizes blockchain and software defined networking techniques to remove the re-authentication in repeated handover among heterogeneous cells. The proposed approach is designed to assure the low delay, appropriate for the 5G network in which users can be replaced with the least delay among heterogeneous cells using their public and private keys provided by the devised blockchain component while protecting their privacy. In our comparison between Proof-of-Work -based and network-based models, the delay of our authentication handover was shown to be less than 1ms. Also, our approach demonstrated less signaling overhead and energy consumption compared to peer models.",0.2],["to every half-translation surface, we associate a saddle connection graph. we prove","Affine equivalence and saddle connection graphs of half-translation surfaces","summarize: To every half-translation surface, we associate a saddle connection graph, which is a subgraph of the arc graph. We prove that every isomorphism between two saddle connection graphs is induced by an affine homeomorphism between the underlying half-translation surfaces. We also investigate the automorphism group of the saddle connection graph, and the corresponding quotient graph.",0.5454545455],["efficient algorithm of solution is proposed. the algorithm can be easily adapted to circular or","An efficient algorithm of solution for the flow of generalized Newtonian fluid in channels of simple geometries","summarize: In this paper a problem of stationary flow of generalized Newtonian fluid in a thin channel is considered. An efficient algorithm of solution is proposed that includes a flexible procedure for a continuous approximation of the apparent viscosity by means of elementary functions combined with analytical integration of the governing equations. The algorithm can be easily adapted to circular or elliptic conduits. The accuracy and efficiency of computations are analyzed using an example of the Carreau fluid. The proposed computational scheme proves to be highly efficient and versatile providing excellent accuracy of solution at a very low computational cost.",0.4145557827],["tolman analyzed relation between temperature gradient and gravitational field. he","Heat flux in the presence of a gravitational field in a simple dilute fluid: an approach based in general relativistic kinetic theory to first order in the gradients","summarize: Richard C. Tolman analyzed the relation between a temperature gradient and a gravitational field in an equilibrium situation. In 2012, Tolman\\textquoteright s law was generalized to a non-equilibrium situation for a simple dilute relativistic fluid. The result in that scenario, obtained by introducing the gravitational force through the molecular acceleration, couples the heat flux with the metric coefficients and the gradients of the state variables. In the present paper it is shown, by \\textquotedblleft suppressing\\textquotedblright the molecular acceleration in Boltzmann\\textquoteright s equation, that a gravitational field drives a heat flux. This procedure corresponds to the description of particle motion through geodesics, in which a Newtonian limit to the Schwarzschild metric is assumed. The effect vanishes in the non-relativistic regime, as evidenced by the direct evaluation of the corresponding limit.",0.0661195553],["the solar corona is dominated by Fe lines of charge states VIII, IX,","The Slowly Varying Corona: I --- Daily Differential Emission Measure Distributions Derived from EVE Spectra","summarize: Daily differential emission measure distributions of the solar corona are derived from spectra obtained by the Extreme-ultraviolet Variability Experiment over a 4-year period starting in 2010 near solar minimum and continuing through the maximum of solar cycle 24. The DEMs are calculated using six strong emission features dominated by Fe lines of charge states VIII, IX, XI, XII, XIV, and XVI that sample the non-flaring coronal temperature range 0.3--5 MK. A proxy for the non-XVIII emission in the wavelength band around the 93.9 \\AA\\ line is demonstrated. There is little variability in the cool component of the corona over the four years, suggesting that the quiet-Sun corona does not respond strongly to the solar cycle, whereas the hotter component varies by more than an order of magnitude. A discontinuity in the behavior of coronal diagnostics in 2011 February--March, around the time of the first X-class flare of cycle 24, suggests fundamentally different behavior in the corona under solar minimum and maximum conditions. This global state transition occurs over a period of several months. The DEMs are used to estimate the thermal energy of the visible solar corona , its radiative energy loss rate , and the corresponding energy turnover timescale . The uncertainties associated with the DEMs and these derived values are mostly due to the coronal Fe abundance and density and the CHIANTI atomic line database.",0.0659541476],["study aims to compare different designs for the world men's handball championships.","A simulation comparison of tournament designs for the World Men's Handball Championships","summarize: The study aims to compare different designs for the World Men's Handball Championships. This event, organised in every two years, has adopted four hybrid formats consisting of knockout and round-robin stages in recent decades, including a change of design between the two recent championships in 2017 and 2019. They are evaluated under two extremal seeding policies with respect to various outcome measures through Monte-Carlo simulations. We find that the ability to give the first four positions to the strongest teams, as well as the expected quality and outcome uncertainty of the final is not necessarily a monotonic function of the number of matches played: the most frugal format is the second best with respect to these outcome measures, making it a good compromise in an unavoidable trade-off. A possible error is identified in a particular design. The relative performance of the formats is independent of the seeding rules and the competitive balance of the teams. The recent reform is demonstrated to have increased the probability of winning for the top teams. Our results have useful implications for the organisers of hybrid tournaments.",0.3333333333],["a new analysis of critical wavefunctions is being used to explain the effects of critical wave","Gaussian free fields at the integer quantum Hall plateau transition","summarize: In this work we put forward an effective Gaussian free field description of critical wavefunctions at the transition between plateaus of the integer quantum Hall effect. To this end, we expound our earlier proposal that powers of critical wave intensities prepared via point contacts behave as pure scaling fields obeying an Abelian operator product expansion. Our arguments employ the framework of conformal field theory and, in particular, lead to a multifractality spectrum which is parabolic. We also derive a number of old and new identities that hold exactly at the lattice level and hinge on the correspondence between the Chalker-Coddington network model and a supersymmetric vertex model.",0.3103448276],["mobile networks have connected tens of billions of devices on the ground in the past decades","Mobile-Network Connected Drones: Field Trials, Simulations, and Design Insights","summarize: Drones are becoming increasingly used in a wide variety of industries and services and are delivering profound socioeconomic benefits. Technology needs to be in place to ensure safe operation and management of the growing fleet of drones. Mobile networks have connected tens of billions of devices on the ground in the past decades and are now ready to connect the drones flying in the sky. In this article, we share some of our findings in cellular connectivity for low altitude drones. We first present and analyze field measurement data collected during drone flights in a commercial Long-Term Evolution network. We then present simulation results to shed light on the performance of a network when it is serving many drones simultaneously over a wide area. The results, analysis, and design insights presented in this article help enhance the understanding of the applicability and performance of providing mobile connectivity to low altitude drones.",0.1363636364],["matrix factorization may not be well suited for the matrix completion problem. we show that","Scalable Probabilistic Matrix Factorization with Graph-Based Priors","summarize: In matrix factorization, available graph side-information may not be well suited for the matrix completion problem, having edges that disagree with the latent-feature relations learnt from the incomplete data matrix. We show that removing these ",0.0],["the BAMSDN framework dynamically allocates resources for a MPLS network using a","A SDN\/OpenFlow Framework for Dynamic Resource Allocation based on Bandwidth Allocation Model","summarize: The communication network context in actual systems like 5G, cloud and IoT , presents an ever-increasing number of users, applications, and services that are highly distributed with distinct and heterogeneous communications requirements. Resource allocation in this context requires dynamic, efficient, and customized solutions and Bandwidth Allocation Models are an alternative to support this new trend. This paper proposes the BAMSDN framework that dynamically allocates resources for a MPLS network using a SDN \/OpenFlow strategy with BAM. The framework adopts an innovative implementation approach for BAM systems by controlling the MPLS network using SDN with OpenFlow. Experimental results suggest that using SDN\/OpenFlow with BAM for bandwidth allocation does have effective advantages for MPLS networks requiring flexible resource sharing among applications and facilitates the migration path to a SDN\/OpenFlow network.",0.5416666667],["we introduce a tree-like forcing notion extending some properties of the random forcing in the","A null ideal for inaccessibles","summarize: In this paper we introduce a tree-like forcing notion extending some properties of the random forcing in the context of the generalised Cantor space and study its associated ideal of null sets and notion of measurability. This issue was also addressed by Shelah and concerns the definition of a forcing which is ",0.125],["hash tables are ubiquitous in computer science for efficient access to large datasets. but there","A Faster Algorithm for Cuckoo Insertion and Bipartite Matching in Large Graphs","summarize: Hash tables are ubiquitous in computer science for efficient access to large datasets. However, there is always a need for approaches that offer compact memory utilisation without substantial degradation of lookup performance. Cuckoo hashing is an efficient technique of creating hash tables with high space utilisation and offer a guaranteed constant access time. We are given ",0.125],["decompositions are used to prove the functional CLT for reversible Markov","Remarks on limit theorems for reversible Markov processes","summarize: We propose some backward-forward martingale decompositions for functions of reversible Markov chains. These decompositions are used to prove the functional CLT for reversible Markov chains with asymptotically linear variance of partial sums. We also provide a proof of the equivalence between asymptotic linearity of the variance and convergence of the integral of ",0.3636363636],["the research aims to examine areas of the Compton cross section of annihilation","Probing entanglement in Compton interactions","summarize: This theoretical research aims to examine areas of the Compton cross section of entangled annihilation photons for the purpose of testing for possible break down of theory, which could have consequences for predicted optimal capabilities of Compton PET systems.We provide maps of the cross section for entangled annihilation photons for experimental verification.We introduce a strategy to derive cross sections in a relatively straight forward manner for the Compton scattering of a hypothetical separable, mixed and entangled states. To understand the effect that entanglement has on the cross section for annihilation photons, we derive the cross section so that it is expressed in terms of the cross section of a hypothetical separable state and of a hypothetical forbidden maximally entangled state.We find lobe-like structures in the cross section which are regions where entanglement has the greatest effect.We also find that mixed states do not reproduce the cross section for annihilation photons, contrary to a recent investigation which reported otherwise.We review the motivation and method of the most precise Compton scattering experiment for annihilation photons, in order to resolve conflicting reports regarding the extent to which the cross section itself has been experimentally verified.",0.1111111111],["dynamic landscape models are proposed for prisoner's Dilemma and Snowdrift games","Dynamic landscape models of coevolutionary games","summarize: Players of coevolutionary games may update not only their strategies but also their networks of interaction. Based on interpreting the payoff of players as fitness, dynamic landscape models are proposed. The modeling procedure is carried out for Prisoner's Dilemma and Snowdrift games that both use either birth--death or death--birth strategy updating. The main focus is on using dynamic fitness landscapes as a mathematical model of coevolutionary game dynamics. Hence, an alternative tool for analyzing coevolutionary games becomes available, and landscape measures such as modality, ruggedness and information content can be computed and analyzed. In addition, fixation properties of the games and quantifiers characterizing the interaction networks are calculated numerically. Relations are established between landscape properties expressed by landscape measures and quantifiers of coevolutionary game dynamics such as fixation probabilities, fixation times and network properties.",0.3333333333],["optical flow is a sub-optimal representation for video processing tasks. we design","Video Enhancement with Task-Oriented Flow","summarize: Many video enhancement algorithms rely on optical flow to register frames in a video sequence. Precise flow estimation is however intractable; and optical flow itself is often a sub-optimal representation for particular video processing tasks. In this paper, we propose task-oriented flow , a motion representation learned in a self-supervised, task-specific manner. We design a neural network with a trainable motion estimation component and a video processing component, and train them jointly to learn the task-oriented flow. For evaluation, we build Vimeo-90K, a large-scale, high-quality video dataset for low-level video processing. TOFlow outperforms traditional optical flow on standard benchmarks as well as our Vimeo-90K dataset in three video processing tasks: frame interpolation, video denoising\/deblocking, and video super-resolution.",0.125],["we propose a kernel classifier based on the optimal scoring framework. we provide theoretical","Sparse Feature Selection in Kernel Discriminant Analysis via Optimal Scoring","summarize: We consider the two-group classification problem and propose a kernel classifier based on the optimal scoring framework. Unlike previous approaches, we provide theoretical guarantees on the expected risk consistency of the method. We also allow for feature selection by imposing structured sparsity using weighted kernels. We propose fully-automated methods for selection of all tuning parameters, and in particular adapt kernel shrinkage ideas for ridge parameter selection. Numerical studies demonstrate the superior classification performance of the proposed approach compared to existing nonparametric classifiers.",0.3181818182],["this paper discusses differential stability of convex programming problems in Hausdorff local conve","Subdifferential Stability Analysis for Convex Optimization Problems via Multiplier Sets","summarize: This paper discusses differential stability of convex programming problems in Hausdorff locally convex topological vector spaces. Among other things, we obtain formulas for computing or estimating the subdifferential and the singular subdifferential of the optimal value function via suitable multiplier sets.",0.0666666667],["magnetically frustrated heavy fermion metals are magnetically frustrated. we theoretically study","Magnon Bose-Einstein Condensation and Superconductivity in a Frustrated Kondo Lattice","summarize: Motivated by recent experiments on magnetically frustrated heavy fermion metals, we theoretically study the phase diagram of the Kondo lattice model with a nonmagnetic valence bond solid ground state on a ladder. A similar physical setting may be naturally occurring in YbAl",0.0],["driftless swimmers include swimmers in a 3D Stokes flow or swimmers in","Optimal Strokes for Driftless Swimmers: A General Geometric Approach","summarize: Swimming consists by definition in propelling through a fluid by means of bodily movements. Thus, from a mathematical point of view, swimming turns into a control problem for which the controls are the deformations of the swimmer. The aim of this paper is to present a unified geometric approach for the optimization of the body deformations of so-called driftless swimmers. The class of driftless swimmers includes, among other, swimmers in a 3D Stokes flow or swimmers in a 2D or 3D potential flow. A general framework is introduced, allowing the complete analysis of five usual nonlinear optimization problems to be carried out. The results are illustrated with examples coming from the literature and with an in-depth study of a swimmer in a 2D potential flow. Numerical tests are also provided.",0.0909090909],["we generalize a map by S. Mason regarding two combinatorial models for key poly","A major-index preserving map on fillings","summarize: We generalize a map by S. Mason regarding two combinatorial models for key polynomials, in a way that accounts for the major index. We also define similar variants of this map, that regards alternative models for the modified Macdonald polynomials at ",0.15],["the restoration problem is to resupply the maximum energy of loads. it is intended","A Multi-Step Reconfiguration Model for Active Distribution Network Restoration Integrating DG Start-Up Sequences","summarize: The ever-increasing penetration of Distributed Generators in distribution networks suggests to enable their potentials in better fulfilling the restoration objective. The objective of the restoration problem is to resupply the maximum energy of loads considering their priorities using minimum switching operations. Basically, it is desired to provide a unique configuration that is valid regarding the load and generation profiles along the entire restorative period. However, this unique configuration may not satisfy at the same time: I) the DG start-up requirements at the beginning of the restoration plan and II) the topological conditions that would allow the DG to provide later on the most efficient support for the supply of loads. Therefore, it is proposed in this paper to allow a limited number of reconfiguration steps according to the DG start-up requirements. In addition, this paper presents a novel formulation for the reconfiguration problem that accounts for partial restoration scenarios where the whole unsupplied area cannot be restored. The decision variables of the proposed multi-step restoration problem are: I) the line switching actions at each step of the reconfiguration process, II) the load switching actions during the whole restorative period and, III) the active\/reactive power dispatch of DGs during the whole restorative period. A relaxed AC power flow formulation is integrated to the optimization problem in order to ensure the feasibility of the solution concerning the operational safety constraints. The overall model is formulated in terms of a mixed-integer second-order cone programming. Two simulation scenarios are studied in order to illustrate different features of the proposed strategy and to demonstrate its effectiveness particularly in the case of large-scale outages in distribution networks.",0.1578947368],["the new global digital Elevation Model was created through the TanDEM-X mission","Fusion of Urban TanDEM-X raw DEMs using variational models","summarize: Recently, a new global Digital Elevation Model with pixel spacing of 0.4 arcseconds and relative height accuracy finer than 2m for flat areas and better than 4m for rugged terrain was created through the TanDEM-X mission. One important step of the chain of global DEM generation is to mosaic and fuse multiple raw DEM tiles to reach the target height accuracy. Currently, Weighted Averaging is applied as a fast and simple method for TanDEM-X raw DEM fusion in which the weights are computed from height error maps delivered from the Interferometric TanDEM-X Processor . However, evaluations show that WA is not the perfect DEM fusion method for urban areas especially in confrontation with edges such as building outlines. The main focus of this paper is to investigate more advanced variational approaches such as TV-L1 and Huber models. Furthermore, we also assess the performance of variational models for fusing raw DEMs produced from data takes with different baseline configurations and height of ambiguities. The results illustrate the high efficiency of variational models for TanDEM-X raw DEM fusion in comparison to WA. Using variational models could improve the DEM quality by up to 2m particularly in inner-city subsets.",0.0714285714],["we propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving","Softwarization of Internet of Things Infrastructure for Secure and Smart Healthcare","summarize: We propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving deployment of Internet of Things for smart healthcare applications and services. It integrates state-of-the-art networking and virtualization techniques across IoT, fog and cloud domains, employing Blockchain, Tor and message brokers to provide security and privacy for patients and healthcare providers. We propose a novel platform using Machine-to-Machine messaging and rule-based beacons for seamless data management and discuss the role of data and decision fusion in the cloud and the fog, respectively, for smart healthcare applications and services.",0.2],["event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronous","Event-Based Motion Segmentation by Motion Compensation","summarize: In contrast to traditional cameras, whose pixels have a common exposure time, event-based cameras are novel bio-inspired sensors whose pixels work independently and asynchronously output intensity changes , with microsecond resolution. Since events are caused by the apparent motion of objects, event-based cameras sample visual information based on the scene dynamics and are, therefore, a more natural fit than traditional cameras to acquire motion, especially at high speeds, where traditional cameras suffer from motion blur. However, distinguishing between events caused by different moving objects and by the camera's ego-motion is a challenging task. We present the first per-event segmentation method for splitting a scene into independently moving objects. Our method jointly estimates the event-object associations and the motion parameters of the objects by maximization of an objective function, which builds upon recent results on event-based motion-compensation. We provide a thorough evaluation of our method on a public dataset, outperforming the state-of-the-art by as much as 10%. We also show the first quantitative evaluation of a segmentation algorithm for event cameras, yielding around 90% accuracy at 4 pixels relative displacement.",0.0],["ring solitons model of bubble fluxons in disk-shaped heterogene","Stability of bubble-like fluxons in disk-shaped Josephson junctions in the presence of a coaxial dipole current","summarize: We investigate analytically and numerically the stability of bubble-like fluxons in disk-shaped heterogeneous Josephson junctions. Using ring solitons as a model of bubble fluxons in the two-dimensional sine-Gordon equation, we show that the insertion of coaxial dipole currents prevents their collapse. We characterize the onset of instability by introducing a single parameter that couples the radius of the bubble fluxon with the properties of the injected current. For different combination of parameters, we report the formation of stable oscillating bubbles, the emergence of internal modes, and bubble breakup due to internal mode instability. We show that the critical germ depends on the ratio between its radius and the steepness of the wall separating the different phases in the system. If the steepness of the wall is increased , the critical radius decreases . Our theoretical findings are in good agreement with numerical simulations. We discuss applications in quantum information technologies.",0.2756554944],["this paper generalizes the definition of a multilinear map to arbitrary groups","Multilinear Cryptography using Nilpotent Groups","summarize: In this paper we generalize the definition of a multilinear map to arbitrary groups and develop a novel idea of multilinear cryptosystem using nilpotent group identities.",0.1666666667],["Bayeslands used the Bayesian paradigm to make inference for unknown parameters in the","Multi-core parallel tempering Bayeslands for basin and landscape evolution","summarize: The Bayesian paradigm is becoming an increasingly popular framework for estimation and uncertainty quantification of unknown parameters in geo-physical inversion problems. Badlands is a basin and landscape evolution forward model for simulating topography evolution at a large range of spatial and time scales. Our previous work presented Bayeslands that used the Bayesian paradigm to make inference for unknown parameters in the Badlands model using Markov chain Monte Carlo sampling. Bayeslands faced challenges in convergence due to multi-modal posterior distributions in the selected parameters of Badlands. Parallel tempering is an advanced MCMC method suited for irregular and multi-modal posterior distributions. In this paper, we extend Bayeslands using parallel tempering with high performance computing to address previous limitations in parameter space exploration in the context of the computationally expensive Badlands model. Our results show that PT-Bayeslands not only reduces the computation time, but also provides an improvement of the sampling for multi-modal posterior distributions. This provides an improvement over Bayeslands which used single chain MCMC that face difficulties in convergence and can lead to misleading inference. This motivates its usage in large-scale basin and landscape evolution models.",0.25],["the LTE uses static LTE Key to derive the entire key hierarchy. the encryption","A Console GRID LA Console GRID Leveraged Authentication and Key Agreement Mechanism for LTE\/SAE","summarize: The growing popularity of multimedia applications, pervasive connectivity, higher bandwidth, and euphoric technology penetration among the bulk of the human race that happens to be cellular technology users, has fueled the adaptation to Long Term Evolution \/ System Architecture Evolution . The LTE fulfills the resource demands of the next generation applications for now. We identify security issues in the authentication mechanism used in LTE that without countermeasures might give superuser rights to unauthorized users. The LTE uses static LTE Key to derive the entire key hierarchy such as LTE follows Evolved Packet System-Authentication and Key Agreement based authentication which discloses user identity, location, and other Personally Identifiable Information . To counter this, we propose a public key cryptosystem named International mobile subscriber identity Protected Console Grid-based Authentication and Key Agreement protocol to address the vulnerabilities related to weak key management. From the data obtained from threat modeling and simulation results, we claim that the IPG-AKA scheme not only improves the security of authentication procedures, it also shows improvements in authentication loads and reduction in key generation time. The empirical results and qualitative analysis presented in this paper proves that IPG-AKA improves security in authentication procedure and performance in the LTE.",0.2727272727],["the course will examine the basics of magnetism and quantum mechanics. we will go","Quantum Magnetism, Spin Waves, and Light","summarize: Both magnetic materials and light have always played a predominant role in information technologies, and continue to do so as we move into the realm of quantum technologies. In this course we review the basics of magnetism and quantum mechanics, before going into more advanced subjects. Magnetism is intrinsically quantum mechanical in nature, and magnetic ordering can only be explained by use of quantum theory. We will go over the interactions and the resulting Hamiltonian that governs magnetic phenomena, and discuss its elementary excitations, denominated magnons. After that we will study magneto-optical effects and derive the classical Faraday effect. We will then move on to the quantization of the electric field and the basics of optical cavities. This will allow us to understand a topic of current research denominated Cavity Optomagnonics. These notes were written as the accompanying material to the course I taught in the Summer Semester 2018 at the Friedrich-Alexander University in Erlangen. The course is intended for Master or advanced Bachelor students. Basic knowledge of quantum mechanics, electromagnetism, and solid state at the Bachelor level is assumed. Each section is followed by a couple of simple exercises which should serve as to fill in the blanks of what has been derived, plus specific references to bibliography, and a couple of check-points for the main concepts developed. The figures are pictures of the blackboard taken during the lecture.",0.0555555556],["perovskites have been praised for their exceptional photovoltaic and optoe","Efficient indoor p-i-n hybrid perovskite solar cells using low temperature solution processed NiO as hole extraction layers","summarize: Hybrid perovskites have received tremendous attention due to their exceptional photovoltaic and optoelectronic properties. Among the two widely used perovskite solar cell device architectures of n-ip and p-i-n, the latter is interesting in terms of its simplicity of fabrication and lower energy input. However this structure mostly uses PEDOT:PSS as a hole transporting layer which can accelerate the perovskite solar cell degradation. Hence the development of stable, inorganic hole extraction layers , without compromising the simplicity of device fabrication is crucial in this fast-growing photovoltaic field. Here we demonstrate a low temperature solution - processed and ultrathin NiO nanoparticle thin films as an efficient HEL for CH3NH3PbI3 based perovskite solar cells. We measure a power conversion efficiency of 13.3 % on rigid glass substrates and 8.5 % on flexible substrates. A comparison with PEDOT:PSS based MAPbI3 solar cells shows that NiO based solar cells have higher short circuit current density and improved open circuit voltage . Apart from the photovoltaic performance under 1 Sun, the efficient hole extraction property of NiO is demonstrated for indoor lighting as well with a PCE of 23.0 % for NiO based CH3NH3PbI2.9Cl0.1 p-i-n solar cells under compact fluorescent lighting. Compared to the perovskite solar cells fabricated on PEDOT:PSS HEL, better shelf-life stability is observed for perovskite solar cells fabricated on NiO HEL. Detailed microstructural and photophysical investigations imply uniform morphology, lower recombination losses, and improved charge transfer properties for CH3NH3PbI3 grown on NiO HEL.",0.0],["a series of experiments have been carried out in the past. we compared and","A Comparative Study on Transformer vs RNN in Speech Applications","summarize: Sequence-to-sequence models have been widely used in end-to-end speech processing, for example, automatic speech recognition , speech translation , and text-to-speech . This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance in neural machine translation and other natural language processing applications. We undertook intensive studies in which we experimentally compared and analyzed Transformer and conventional recurrent neural networks in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and significant performance benefits obtained with Transformer for each task including the surprising superiority of Transformer in 13\/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to succeed our exciting outcomes.",0.2631578947],["this article proposes a new approach to modeling high-dimensional time series by treating a","Modeling High-Dimensional Time Series: A Factor Model with Dynamically Dependent Factors and Diverging Eigenvalues","summarize: This article proposes a new approach to modeling high-dimensional time series by treating a ",0.5769230769],["alterations in gel materials are responsible for macroscopically-measured strain values.","Drying-Induced Atomic Structural Rearrangements in Sodium-Based Calcium-Alumino-Silicate-Hydrate Gel and the Mitigating Effects of ZrO","summarize: Conventional drying of colloidal materials and gels can lead to detrimental effects due to the buildup of internal stresses as water evaporates from the nano\/microscopic pores. However, the underlying nanoscopic alterations in these gel materials that are, in part, responsible for macroscopically-measured strain values, especially at low relative humidity, remain a topic of open debate in the literature. In this study, sodium-based calcium-alumino-silicate-hydrate -A-S-H) gel, the major binding phase of silicate-activated blast furnace slag , is investigated from a drying perspective, since it is known to suffer extensively from drying-induced microcracking. By employing in situ synchrotron X-ray total scattering measurements and pair distribution function analysis we show that the significant contributing factor to the strain development in this material at extremely low relative humidity is the local atomic structural rearrangement of the C--A-S-H gel, including collapse of interlayer spacing and slight disintegration of the gel. Moreover, analysis of the medium range ordering in the PDF data reveals that the PDF-derived strain values are in much closer agreement with the macroscopically measured strain data, compared to previous results based on reciprocal space X-ray diffraction data. From a mitigation standpoint, we show that small amounts of ZrO",0.121876372],["this paper develops a more general theory of sequences of dependent categorical random variables","Vertical Dependency in Sequences of Categorical Random Variables","summarize: This paper develops a more general theory of sequences of dependent categorical random variables, extending the works of Korzeniowski and Traylor that studied first-kind dependency in sequences of Bernoulli and categorical random variables, respectively. A more natural form of dependency, sequential dependency, is defined and shown to retain the property of identically distributed but dependent elements in the sequence. The cross-covariance of sequentially dependent categorical random variables is proven to decrease exponentially in the dependency coefficient ",0.347826087],["traffic matrix estimation has always caught attention from researchers. the problem formulation uses a dynamic measurement","A Novel Compressed Sensing Technique for Traffic Matrix Estimation of Software Defined Cloud Networks","summarize: Traffic Matrix estimation has always caught attention from researchers for better network management and future planning. With the advent of high traffic loads due to Cloud Computing platforms and Software Defined Networking based tunable routing and traffic management algorithms on the Internet, it is more necessary as ever to be able to predict current and future traffic volumes on the network. For large networks such origin-destination traffic prediction problem takes the form of a large under-constrained and under-determined system of equations with a dynamic measurement matrix. In this work, we present our Compressed Sensing with Dynamic Model Estimation architecture suitable for modern software defined networks. Our main contributions are: we formulate an approach in which measurement matrix in the compressed sensing scheme can be accurately and dynamically estimated through a reformulation of the problem based on traffic demands. We show that the problem formulation using a dynamic measurement matrix based on instantaneous traffic demands may be used instead of a stationary binary routing matrix which is more suitable to modern Software Defined Networks that are constantly evolving in terms of routing by inspection of its Eigen Spectrum using two real world datasets. We also show that linking this compressed measurement matrix dynamically with the measured parameters can lead to acceptable estimation of Origin Destination Traffic flows with marginally poor results with other state-of-art schemes relying on fixed measurement matrices. Furthermore, using this compressed reformulated problem, a new strategy for selection of vantage points for most efficient traffic matrix estimation is also presented through a secondary compression technique based on subset of link measurements.",0.1428571429],["we investigate the variance of local localities in this article.","A note on quadratic twisting of epsilon factors for modular forms with arbitrary nebentypus","summarize: In this article, we investigate the variance of local ",0.1117200077],["fullerene C is the fullerene C.","Laboratory photo-chemistry of covalently bonded fluorene clusters: observation of an interesting PAH bowl-forming mechanism","summarize: The fullerene C",0.0263597138],["Dedicated short-range communication enables vehicular communication using periodic broadcasts. each vehicle","Inter-Vehicle Range Estimation from Periodic Broadcasts","summarize: Dedicated short-range communication enables vehicular communication using periodic broadcast messages. We propose to use these periodic broadcasts to perform inter-vehicle ranging. Motivated by this scenario, we study the general problem of precise range estimation between pairs of moving vehicles using periodic broadcasts. Each vehicle has its own independent and unsynchronized clock, which can exhibit significant drift between consecutive periodic broadcast transmissions. As a consequence, both the clock offsets and drifts need to be taken into account in addition to the vehicle motion to accurately estimate the vehicle ranges. We develop a range estimation algorithm using local polynomial smoothing of the vehicle motion. The proposed algorithm can be applied to networks with arbitrary number of vehicles and requires no additional message exchanges apart from the periodic broadcasts. We validate our algorithm on experimental data and show that the performance of the proposed approach is close to that obtained using unicast round-trip time ranging. In particular, we are able to achieve sub-meter ranging accuracies in vehicular scenarios. Our scheme requires additional timestamp information to be transmitted as part of the broadcast messages, and we develop a novel timestamp compression algorithm to minimize the resulting overhead.",0.0],["a multidisciplinary approach is presented to analyse precipitation process in a model Al-Cu","A multidisciplinary approach to study precipitation kinetics and hardening in an Al-4Cu alloy","summarize: A multidisciplinary approach is presented to analyse the precipitation process in a model Al-Cu alloy. Although this topic has been extensively studied in the past, most of the investigations are focussed either on transmission electron microscopy or on thermal analysis of the processes. The information obtained from these techniques cannot, however, provide a coherent picture of all the complex transformations that take place during decomposition of supersaturated solid solution. Thermal analysis, high resolution dilatometry, transmission electron microscopy and density functional calculations are combined to study precipitation kinetics, interfacial energies, and the effect of second phase precipitates on the mechanical strength of the alloy. Data on both the coherent and semi-coherent orientations of the \/Al interface are reported for the first time. The combination of the different characterization and modelling techniques provides a detailed picture of the precipitation phenomena that take place during aging and of the different contributions to the strength of the alloy. This strategy can be used to analyse and design more complex alloys.",0.7931034483],["","Inversion symmetry breaking induced triply degenerate points in orderly arranged PtSeTe family materials","summarize: ",0.0000061442],["encoding and maintenance periods of tasks performed by healthy volunteers. encoding and maintenance","Locating Temporal Functional Dynamics of Visual Short-Term Memory Binding using Graph Modular Dirichlet Energy","summarize: Visual short-term memory binding tasks are a promising early marker for Alzheimer's disease . To uncover functional deficits of AD in these tasks it is meaningful to first study unimpaired brain function. Electroencephalogram recordings were obtained from encoding and maintenance periods of tasks performed by healthy young volunteers. We probe the task's transient physiological underpinnings by contrasting shape only and shape-colour binding conditions, displayed in the left and right sides of the screen, separately. Particularly, we introduce and implement a novel technique named Modular Dirichlet Energy which allows robust and flexible analysis of the functional network with unprecedented temporal precision. We find that connectivity in the Bind condition is less integrated with the global network than in the Shape condition in occipital and frontal modules during the encoding period of the right screen condition. Using MDE we are able to discern driving effects in the occipital module between 100-140ms, coinciding with the P100 visually evoked potential, followed by a driving effect in the frontal module between 140-180ms, suggesting that the differences found constitute an information processing difference between these modules. This provides temporally precise information over a heterogeneous population in promising tasks for the detection of AD.",0.0487347936],["the aim this study is discussed on the detection and correction of data containing the additive outlier","Modeling Data Containing Outliers using ARIMA Additive Outlier ","summarize: The aim this study is discussed on the detection and correction of data containing the additive outlier on the model ARIMA . The process of detection and correction of data using an iterative procedure popularized by Box, Jenkins, and Reinsel . By using this method we obtained an ARIMA models were fit to the data containing AO, this model is added to the original model of ARIMA coefficients obtained from the iteration process using regression methods. This shows that there is an improvement of forecasting error rate data.",0.0357142857],["the task of understanding a text and answering questions is not easy. the research is","Enhancing lexical-based approach with external knowledge for Vietnamese multiple-choice machine reading comprehension","summarize: Although Vietnamese is the 17th most popular native-speaker language in the world, there are not many research studies on Vietnamese machine reading comprehension , the task of understanding a text and answering questions about it. One of the reasons is because of the lack of high-quality benchmark datasets for this task. In this work, we construct a dataset which consists of 2,783 pairs of multiple-choice questions and answers based on 417 Vietnamese texts which are commonly used for teaching reading comprehension for elementary school pupils. In addition, we propose a lexical-based MRC method that utilizes semantic similarity measures and external knowledge sources to analyze questions and extract answers from the given text. We compare the performance of the proposed model with several baseline lexical-based and neural network-based models. Our proposed method achieves 61.81% by accuracy, which is 5.51% higher than the best baseline model. We also measure human performance on our dataset and find that there is a big gap between machine-model and human performances. This indicates that significant progress can be made on this task. The dataset is freely available on our website for research purposes.",0.2692307692],["the Hardy--Littlewood inequalities on the inequalities","Optimal exponents for Hardy--Littlewood inequalities for ","summarize: The Hardy--Littlewood inequalities on ",0.33859269],["supervised machine learning classifiers have shown great promise in a multitude of domains","Text Classification of the Precursory Accelerating Seismicity Corpus: Inference on some Theoretical Trends in Earthquake Predictability Research from 1988 to 2018","summarize: Text analytics based on supervised machine learning classifiers has shown great promise in a multitude of domains, but has yet to be applied to Seismology. We test various standard models on a seismological corpus of 100 articles related to the topic of precursory accelerating seismicity, spanning from 1988 to 2010. This corpus was labelled in Mignan with the precursor whether explained by critical processes or by other processes . We investigate rather the classification process can be automatized to help analyze larger corpora in order to better understand trends in earthquake predictability research. We find that the Naive Bayes model performs best, in agreement with the machine learning literature for the case of small datasets, with cross-validation accuracies of 86% for binary classification. For a refined multiclass classification , we obtain up to 78% accuracy. Prediction on a dozen of articles published since 2011 shows however a weak generalization with a F1-score of 60%, only slightly better than a random classifier, which can be explained by a change of authorship and use of different terminologies. Yet, the model shows F1-scores greater than 80% for the two multiclass extremes while it falls to random classifier results for papers labelled 'agnostic' or 'critical process assumed'. Those results are encouraging in view of the small size of the corpus and of the high degree of abstraction of the labelling. Domain knowledge engineering remains essential but can be made transparent by an investigation of Naive Bayes keyword posterior probabilities.",0.245651362],["the gauge field is valued on an octonionic algebra. the gauge","Yang-Mills connections valued on the octonionic algebra","summarize: We consider a formulation of Yang-Mills theory where the gauge field is valued on an octonionic algebra and the gauge transformation is the group of automorphisms of it. We show, under mild assumptions, that the only possible gauge formulations are the usual ",0.4705882353],["online tutoring agents are transforming into customizable, on-demand services driven by the learner","Intelligent Tutoring Systems for Generation Z's Addiction","summarize: As generation Z's big data is flooding the Internet through social nets, neural network based data processing is turning an important cornerstone, showing significant potential for fast extraction of data patterns. Online course delivery and associated tutoring are transforming into customizable, on-demand services driven by the learner. Besides automated grading, strong potential exists for the development and deployment of next generation intelligent tutoring software agents. Self-adaptive, online tutoring agents exhibiting intelligent-like behavior, being capable to learn from the learner, will become the next educational superstars. Over the past decade, computer-based tutoring agents were deployed in a variety of extended reality environments, from patient rehabilitation to psychological trauma healing. Most of these agents are driven by a set of conditional control statements and a large answers\/questions pairs dataset. This article provides a brief introduction on Generation Z's addiction to digital information, highlights important efforts for the development of intelligent dialogue systems, and explains the main components and important design decisions for Intelligent Tutoring System.",0.0],["the team from the new technologies for the Information Society research center of the university of west boh","UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge","summarize: In this paper, we present our system developed by the team from the New Technologies for the Information Society research center of the University of West Bohemia in Pilsen, for the Second DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i\/x-vector extraction, clustering, and resegmentation. The hyperparameters for each of the subsystems were selected according to the domain classifier trained on the development set of DIHARD II. We compared our system with results from the Kaldi diarization and combined these systems. At the time of writing of this abstract, our best submission achieved a DER of 23.47% and a JER of 48.99% on the evaluation set .",0.1515151515],["a multi-task convolutional neural network is used to localize skin lesion","A Multi-task Framework for Skin Lesion Detection and Segmentation","summarize: Early detection and segmentation of skin lesions is crucial for timely diagnosis and treatment, necessary to improve the survival rate of patients. However, manual delineation is time consuming and subject to intra- and inter-observer variations among dermatologists. This underlines the need for an accurate and automatic approach to skin lesion segmentation. To tackle this issue, we propose a multi-task convolutional neural network based, joint detection and segmentation framework, designed to initially localize the lesion and subsequently, segment it. A `Faster region-based convolutional neural network' which comprises a region proposal network , is used to generate bounding boxes\/region proposals, for lesion localization in each image. The proposed regions are subsequently refined using a softmax classifier and a bounding-box regressor. The refined bounding boxes are finally cropped and segmented using `SkinNet', a modified version of U-Net. We trained and evaluated the performance of our network, using the ISBI 2017 challenge and the PH2 datasets, and compared it with the state-of-the-art, using the official test data released as part of the challenge for the former. Our approach outperformed others in terms of Dice coefficients , Jaccard index , accuracy and sensitivity , across five-fold cross validation experiments.",0.2666666667],["traditional detection methods used highly computational machine learning algorithms with intensive hardware set up. a real","Real-Time Apple Detection System Using Embedded Systems With Hardware Accelerators: An Edge AI Application","summarize: Real-time apple detection in orchards is one of the most effective ways of estimating apple yields, which helps in managing apple supplies more effectively. Traditional detection methods used highly computational machine learning algorithms with intensive hardware set up, which are not suitable for infield real-time apple detection due to their weight and power constraints. In this study, a real-time embedded solution inspired from Edge AI is proposed for apple detection with the implementation of YOLOv3-tiny algorithm on various embedded platforms such as Raspberry Pi 3 B+ in combination with Intel Movidius Neural Computing Stick , Nvidia's Jetson Nano and Jetson AGX Xavier. Data set for training were compiled using acquired images during field survey of apple orchard situated in the north region of Italy, and images used for testing were taken from widely used google data set by filtering out the images containing apples in different scenes to ensure the robustness of the algorithm. The proposed study adapts YOLOv3-tiny architecture to detect small objects. It shows the feasibility of deployment of the customized model on cheap and power-efficient embedded hardware without compromising mean average detection accuracy and achieved frame rate up to 30 fps even for the difficult scenarios such as overlapping apples, complex background, less exposure of apple due to leaves and branches. Furthermore, the proposed embedded solution can be deployed on the unmanned ground vehicles to detect, count, and measure the size of the apples in real-time to help the farmers and agronomists in their decision making and management skills.",0.1923076923],["the affected muscles are hyperexcitable and can display involuntary static muscle tone","Isometric force pillow: using air pressure to quantify involuntary finger flexion in the presence of hypertonia","summarize: Survivors of central nervous system injury commonly present with spastic hypertonia. The affected muscles are hyperexcitable and can display involuntary static muscle tone and an exaggerated stretch reflex. These symptoms affect posture and disrupt activities of daily living. Symptoms are typically measured using subjective manual tests such as the Modified Ashworth Scale; however, more quantitative measures are necessary to evaluate potential treatments. The hands are one of the most common targets for intervention, but few investigators attempt to quantify symptoms of spastic hypertonia affecting the fingers. We present the isometric force pillow to quantify involuntary grip force. This lightweight, computerized tool provides a holistic measure of finger flexion force and can be used in various orientations for clinical testing and to measure the impact of assistive devices.",0.1102355862],["a new study is underway to weave ethics into advancing ML research. a","Theories of Parenting and their Application to Artificial Intelligence","summarize: As machine learning systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.",0.3461538462],["the explicit geometric representation provides specific controls on certain design regions while the implicit density representation provides the ultimate","Mixed projection- and density-based topology optimization with applications to structural assemblies","summarize: In this paper we present a mixed projection- and density-based topology optimization approach. The aim is to combine the benefits of both parametrizations: the explicit geometric representation provides specific controls on certain design regions while the implicit density representation provides the ultimate design freedom elsewhere. This approach is particularly suited for structural assemblies, where the optimization of the structural topology is coupled with the optimization of the shape of the interface between the sub-components in a unified formulation. The interface between the assemblies is defined by a segmented profile made of linear geometric entities. The geometric coordinates of the nodes connecting the profile segments are used as shape variables in the problem, together with density variables as in conventional topology optimization. The variable profile is used to locally impose specific geometric constraints or to project particular material properties. Examples of the properties considered herein are a local volume constraint, a local maximum length scale control, a variable Young's modulus for the distributed solid material, and spatially variable minimum and maximum length scale. The resulting optimization approach is general and various geometric entities can be used. The potential for complex design manipulations is demonstrated through several numerical examples.",0.1212121212],["derived structure rationalizes lattice parameters and the general equivalent atomic positions that","Derived Crystal Structure of Martensitic Materials by Solid-Solid Phase Transformation","summarize: We propose a mathematical description of crystal structure: underlying translational periodicity together with the distinct atomic positions up to the symmetry operations in the unit cell. It is consistent with the international table of crystallography. By the Cauchy-Born hypothesis, such a description can be integrated with the theory of continuum mechanics to calculate a derived crystal structure produced by solid-solid phase transformation. In addition, we generalize the expressions for orientation relationship between the parent lattice and the derived lattice. The derived structure rationalizes the lattice parameters and the general equivalent atomic positions that assist the indexing process of X-ray diffraction analysis for low symmetry martensitic materials undergoing phase transformation. The analysis is demonstrated in a CuAlMn shape memory alloy. From its austenite phase , we identify that the derived martensitic structure has the orthorhombic symmetry Pmmm with derived lattice parameters a_dv = 4.36491 \\AA, b_dv = 5.40865 \\AA and c_dv = 4.2402 \\AA, by which the complicated X-ray Laue diffraction pattern can be well indexed, and the orientation relationship can be verified.",0.0],["methylation data may help biomarker discovery in cancer. methylation data is","Classification of large DNA methylation datasets for identifying cancer drivers","summarize: DNA methylation is a well-studied genetic modification crucial to regulate the functioning of the genome. Its alterations play an important role in tumorigenesis and tumor-suppression. Thus, studying DNA methylation data may help biomarker discovery in cancer. Since public data on DNA methylation become abundant, and considering the high number of methylated sites present in the genome, it is important to have a method for efficiently processing such large datasets. Relying on big data technologies, we propose BIGBIOCL an algorithm that can apply supervised classification methods to datasets with hundreds of thousands of features. It is designed for the extraction of alternative and equivalent classification models through iterative deletion of selected features. We run experiments on DNA methylation datasets extracted from The Cancer Genome Atlas, focusing on three tumor types: breast, kidney, and thyroid carcinomas. We perform classifications extracting several methylated sites and their associated genes with accurate performance. Results suggest that BIGBIOCL can perform hundreds of classification iterations on hundreds of thousands of features in few hours. Moreover, we compare the performance of our method with other state-of-the-art classifiers and with a wide-spread DNA methylation analysis method based on network analysis. Finally, we are able to efficiently compute multiple alternative classification models and extract, from DNA-methylation large datasets, a set of candidate genes to be further investigated to determine their active role in cancer. BIGBIOCL, results of experiments, and a guide to carry on new experiments are freely available on GitHub.",0.375],["the agent is trained to reformulate an initial question. the system is trained end-to","Ask the Right Questions: Active Question Reformulation with Reinforcement Learning","summarize: We frame Question Answering as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting and stemming.",0.0952380952],["ICE'19, the 12th Interaction and Concurrency Experience, was held","Proceedings 12th Interaction and Concurrency Experience","summarize: This volume contains the proceedings of ICE'19, the 12th Interaction and Concurrency Experience, which was held in Copenhagen, Denmark on the 20th and 21st of June 2019, as a satellite event of DisCoTec'19. The ICE workshop series features a distinguishing review and selection procedure, allowing PC members to interact anonymously with authors. As in the past 11 editions, this interaction considerably improved the accuracy of the feedback from the reviewers and the quality of accepted papers, and offered the basis for lively discussion during the workshop. The 2019 edition of ICE included double blind reviewing of original research papers, in order to increase fairness and avoid bias in reviewing. Each paper was reviewed by three PC members, and altogether 9 papers were accepted for publication - plus 2 oral presentations which are not part of this volume. We were proud to host 4 invited talks, by Dilian Gurov, Fritz Henglein, Sophia Knight, and Hern\\'an Melgratti. The abstracts of these talks are included in this volume together with the regular papers. Final versions of the contributions, taking into account the discussion at the workshop, are included.",0.4444444444],["research proposes implications of application functions by using traceability data. data is classified by the","Improving risk management by using smart containers for real-time traceability","summarize: This research proposes implications of application functions by using the chain traceability data acquired from the Smart Object attached with Extended Real-time Data to improve risk management at the level of the logistics chain. Recent applications using traceability data and major issues in traceability systems have been explored by an academic literature. Information is classified by the usage of current traceability data for supporting risk detection and decisions in operational, tactical, and strategical levels. It is found that real-time data has been a significant impact on the usage for the transportation activity in all decision levels such the function of food quality control and collaborative planning among partners. However, there are some uncertainties in the aggregation of event-based traceability data captured by various partners which are preventing the adoption of data usage for the chain. Under the environment of Industry 4.0 and the Internet of Things , the SO-ERD enables independent data tracing through the chain in real-time. Its data has potential to overcome current issues and improve the supply chain risk management. Therefore, Implications of risk management are proposed with the usage of SO-ERD data based on the literature review which reveals current concerns of decision functions in the supply chain. The implications can be an impact to the domain needs.",0.2631578947],["conventional ARC strategies require structural knowledge of the system. a new adaptive-rob","Theory and Application on Adaptive-Robust Control of Euler-Lagrange Systems with Linearly Parametrizable Uncertainty Bound","summarize: This work proposes a new adaptive-robust control architecture for a class of uncertain Euler-Lagrange systems where the upper bound of the uncertainty satisfies linear in parameters structure. Conventional ARC strategies either require structural knowledge of the system or presume that the overall uncertainties or its time derivative are norm bounded by a constant. Due to unmodelled dynamics and modelling imperfection, true structural knowledge of the system is not always available. Further, for the class of systems under consideration, prior assumption regarding the uncertainties being upper bounded by a constant, puts a restriction on states beforehand. Conventional ARC laws invite overestimation-underestimation problem of switching gain. Towards this front, Adaptive Switching-gain based Robust Control is proposed which alleviates the overestimation-underestimation problem of switching gain. Moreover, ASRC avoids any presumption of constant upper bound on the overall uncertainties and can negotiate uncertainties regardless of being linear or nonlinear in parameters. Experimental results of ASRC using a wheeled mobile robot notes improved control performance in comparison to adaptive sliding mode control.",0.3485512985],["the goal of this paper is to measure the non-convexity of real algebraic","Measuring the local non-convexity of real algebraic curves","summarize: The goal of this paper is to measure the non-convexity of compact and smooth connected components of real algebraic plane curves. We study these curves first in a general setting and then in an asymptotic one. In particular, we consider sufficiently small levels of a real bivariate polynomial in a small enough neighbourhood of a strict local minimum at the origin of the real affine plane. We introduce and describe a new combinatorial object, called the Poincare-Reeb graph, whose role is to encode the shape of such curves and to allow us to quantify their non-convexity. Moreover, we prove that in this setting the Poincare-Reeb graph is a plane tree and can be used as a tool to study the asymptotic behaviour of level curves near a strict local minimum. Finally, using the real polar curve, we show that locally the shape of the levels stabilises and that no spiralling phenomena occur near the origin.",0.3888888889],["positive topological entropy does not imply a strong version of chaos called DC","Distributional chaos in multifractal analysis, recurrence and transitivity","summarize: There are lots of results to study dynamical complexity on irregular sets and level sets of ergodic average from the perspective of density in base space, Hausdorff dimension, Lebesgue positive measure, positive or full topological entropy etc.. However, it is unknown from the viewpoint of chaos. There are lots of results on the relationship of positive topological entropy and various chaos but it is known that positive topological entropy does not imply a strong version of chaos called DC1 so that it is non-trivial to study DC1 on irregular sets and level sets. In this paper we will show that for dynamical system with specification property, there exist uncountable DC1-scrambled subsets in irregular sets and level sets. On the other hand, we also prove that several recurrent levels of points with different recurrent frequency all have uncountable DC1-scrambled subsets. The main technique established to prove above results is that there exists uncountable DC1-scrambled subset in saturated sets.",0.3125],["we show that for all time-scales the mean squared displacement of Brownian micro","Viscous-Viscoelastic Correspondence Principle for Brownian Motion","summarize: Motivated from the classical expressions of the mean squared displacement and the velocity autocorrelation function of Brownian particles suspended either in a Newtonian viscous fluid or trapped in a harmonic potential, we show that for all time-scales the mean squared displacement of Brownian microspheres with mass ",0.1538461538],["graphene is an ideal solid-state material platform to realize an electronic device based on","Graphene Transistor Based on Tunable Dirac-Fermion-Optics","summarize: The linear energy-momentum dispersion, coupled with pseudo-spinors, makes graphene an ideal solid-state material platform to realize an electronic device based on Dirac-Fermionic relativistic quantum mechanics. Employing local gate control, several examples of electronic devices based on Dirac fermion dynamics have been demonstrated, including Klein tunneling, negative refraction and specular Andreev reflection. In this work, we present a quantum switch based on analogous Dirac-fermion-optics , in which the angle dependence of Klein tunneling is explicitly utilized to build tunable collimators and reflectors for the quantum wave function of Dirac fermions. We employ a novel dual-source design with a single flat reflector, which minimizes diffusive edge scattering and suppresses the background incoherent transmission. Our gate-tunable collimator-reflector device design enables measurement of the net DFO contribution in the switching device operation. We measure a full set of transmission coefficients of DFO wavefunction between multiple leads of the device, separating the classical contribution from that of any disorder in the channel. Since the DFO quantum switch demonstrated in this work requires no explicit energy gap, the switching operation is expected to be robust against thermal fluctuations and inhomogeneity length scales comparable to the Fermi wavelength. We find our quantum switch works at an elevated temperature up to 230 K and large bias current density up to 102 A\/m, over a wide range of carrier densities. The tunable collimator-reflector coupled with the conjugated source electrodes developed in this work provides an additional component to build more efficient DFO electronic devices.",0.3529411765],["distributed ledgers are playing a major role in building security and trust in Internet of Things","Witness-based Approach for Scaling Distributed Ledgers to Massive IoT Scenarios","summarize: Distributed Ledger Technologies are playing a major role in building security and trust in Internet of Things systems. However, IoT deployments with a large number of devices, such as in environment monitoring applications, generate and send massive amounts of data. This would generate vast number of transactions that must be processed within the distributed ledger. In this work, we first demonstrate that the Proof of Work blockchain fails to scale in a sizable IoT connectivity infrastructure. To solve this problem, we present a lightweight distributed ledger scheme to integrate PoW blockchain into IoT. In our scheme, we classify transactions into two types: 1) global transactions, which must be processed by global blockchain nodes and 2) local transactions, which can be processed locally by entities called witnesses. Performance evaluation demonstrates that our proposed scheme improves the scalability of integrated blockchain and IoT monitoring systems by processing a fraction of the transactions, inversely proportional to the number of witnesses, locally. Hence, reducing the number of global transactions.",0.25],["in this paper, we present three studies that explore temporal stages of document authoring.","Characterizing Stage-Aware Writing Assistance in Collaborative Document Authoring","summarize: Writing is a complex non-linear process that begins with a mental model of intent, and progresses through an outline of ideas, to words on paper . Despite past research in understanding writing, Web-scale consumer and enterprise collaborative digital writing environments are yet to greatly benefit from intelligent systems that understand the stages of document evolution, providing opportune assistance based on authors' situated actions and context. In this paper, we present three studies that explore temporal stages of document authoring. We first survey information workers at a large technology company about their writing habits and preferences, concluding that writers do in fact conceptually progress through several distinct phases while authoring documents. We also explore, qualitatively, how writing stages are linked to document lifespan. We supplement these qualitative findings with an analysis of the longitudinal user interaction logs of a popular digital writing platform over several million documents. Finally, as a first step towards facilitating an intelligent digital writing assistant, we conduct a preliminary investigation into the utility of user interaction log data for predicting the temporal stage of a document. Our results support the benefit of tools tailored to writing stages, identify primary tasks associated with these stages, and show that it is possible to predict stages from anonymous interaction logs. Together, these results argue for the benefit and feasibility of more tailored digital writing assistance.",0.1333333333],["dc-precise magnetometers are calibrated in low field range","Magnetic Calibration System With Interference Compensation","summarize: This paper describes a novel method for calibrating dc-precise magnetometers in the low field range , which gives acceptable results even in laboratory conditions with significant magnetic interference. By introducing a closely mounted reference magnetometer and a specific calibration procedure, it is possible to compensate for the external magnetic field disturbances caused, e.g., by the local transportation operated with dc power supplies. The field compensation occurs only shortly after the calibrating coils are energized. In this case, the leakage of the coil's magnetic flux to the reference sensor due to the cancellation of the time-varying compensating field was negligible. When using 60 cm coils and reference sensor in 2.5 m distance, we were able to calculate magnetometer gains with a standard deviation of 91 ppm. We show that an overall uncertainty of 0.1% can be achieved.",0.0],["dynamical features of Bianchi type. dynamical features of bianchi type","Dynamical features of an anisotropic cosmological model","summarize: The dynamical features of Bianchi type ",0.2352941176],["quantum theory is typically modeled as a dynamical process involving decoherence","Measurement and Quantum Dynamics in the Minimal Modal Interpretation of Quantum Theory","summarize: Any realist interpretation of quantum theory must grapple with the measurement problem and the status of state-vector collapse. In a no-collapse approach, measurement is typically modeled as a dynamical process involving decoherence. We describe how the minimal modal interpretation closes a gap in this dynamical description, leading to a complete and consistent resolution to the measurement problem and an effective form of state collapse. Our interpretation also provides insight into the indivisible nature of measurement--the fact that you can't stop a measurement part-way through and uncover the underlying `ontic' dynamics of the system in question. Having discussed the hidden dynamics of a system's ontic state during measurement, we turn to more general forms of open-system dynamics and explore the extent to which the details of the underlying ontic behavior of a system can be described. We construct a space of ontic trajectories and describe obstructions to defining a probability measure on this space.",0.3994815634],["finding spanning trees under various constraints reduces to solving the discrete optimization problems. new","Globally optimal dense and sparse spanning trees, and their applications","summarize: Finding spanning trees under various constraints is a classic problem with applications in many fields. Recently, a novel notion of dense tree, and in particular spanning tree , is introduced as the structure that have a large number of subtrees, or small sum of distances between vertices. We show that finding DST and SST reduces to solving the discrete optimization problems. New and efficient approaches to find such spanning trees is achieved by imposing certain conditions on the vertex degrees which are then used to define an objective function that is minimized over all spanning trees of the graph under consideration. Solving this minimization problem exactly may be prohibitively time consuming for large graphs. Hence, we propose to use genetic algorithm which is one of well known metaheuristics methods to solve DST and SST approximately. As far as we are aware this is the first time GA has been used in this context. We also demonstrate on a number of applications that GA approach is well suited for these types of problems both in computational efficiency and accuracy of the approximate solution. Furthermore, we improve the efficiency of the proposed method by using Kruskal's algorithm in combination with GA. The application of our methods to several practical large graphs and networks is presented. Computational results show that they perform faster than previously proposed heuristic methods and produce more accurate solutions. Furthermore, the new feature of the proposed approach is that it can be applied recursively to sub-trees or spanning trees with additional constraints in order to further investigate the graphical properties of the graph and\/or network. The application of this methodology on the gene network of a cancer cell led to isolating key genes in a network that were not obvious from previous studies.",0.2142857143],["exposition focuses on the analysis of three real-world use cases. FIW","A Standard-based Open Source IoT Platform: FIWARE","summarize: The ever-increasing acceleration of technology evolution in all fields is rapidly changing the architectures of data-driven systems towards the Internet-of-Things concept. Many general and specific-purpose IoT platforms are already available. This article introduces the capabilities of the FIWARE framework that is transitioning from a research to a commercial level. We base our exposition on the analysis of three real-world use cases and their requirements that are addressed with the usage of FIWARE. We highlight the lessons learnt during the design, implementation and deployment phases for each of the use cases and their critical issues. Finally we give two examples showing that FIWARE still maintains openness to innovation: semantics and privacy.",0.0769230769],["a computationally efficient harmonic block uses Discrete Cosine Transform filters in CNN","Harmonic Networks with Limited Training Samples","summarize: Convolutional neural networks are very popular nowadays for image processing. CNNs allow one to learn optimal filters in a supervised machine learning context. However this typically requires abundant labelled training data to estimate the filter parameters. Alternative strategies have been deployed for reducing the number of parameters and \/ or filters to be learned and thus decrease overfitting. In the context of reverting to preset filters, we propose here a computationally efficient harmonic block that uses Discrete Cosine Transform filters in CNNs. In this work we examine the performance of harmonic networks in limited training data scenario. We validate experimentally that its performance compares well against scattering networks that use wavelets as preset filters.",0.2941176471],["Graph Optimal Transport aims to create a graph matching problem. the learned","Graph Optimal Transport for Cross-Domain Alignment","summarize: Cross-domain alignment between two sets of entities is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, with no training signals to explicitly encourage alignment. The learned attention matrices are also dense and lacks interpretability. We propose Graph Optimal Transport , a principled framework that germinates from recent advances in Optimal Transport . In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities into a dynamically-constructed graph. Two types of OT distances are considered: Wasserstein distance for node matching; and Gromov-Wasserstein distance for edge matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer. The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization.",0.35],["article presents method of organizing men-in-the-middle attack and penetration test on","Men-in-the-Middle Attack Simulation on Low Energy Wireless Devices using Software Define Radio","summarize: The article presents a method of organizing men-in-the-middle attack and penetration test on Bluetooth Low Energy devices and ZigBee packets using software define radio with sniffing and spoofing packets, capture and analysis techniques on wireless waves with the focus on Bluetooth. The paper contains the analysis of the latest scientific work in this area, provides a comparative analysis of SDRs and the rationale for the choice of hardware, gives the sequence of actions for collecting wireless data packets and data collection from ZigBee and BLE devices, and analyzes ways to improve captured wireless packet analysis techniques. For the study collected experimental setup, the results of which are analyzed in real time. The collected wireless data packets are compared with those sent. The result of the experiment shows the weaknesses of local wireless networks.",0.2282751791],["elliptic genus of abelian 2d gauge theories corresponding to bra","Elliptic Genera of 2d Gauge Theories from Brane Brick Models","summarize: We compute the elliptic genus of abelian 2d gauge theories corresponding to brane brick models. These theories are worldvolume theories on a single D1-brane probing a toric Calabi-Yau 4-fold singularity. We identify a match with the elliptic genus of the non-linear sigma model on the same Calabi-Yau background, which is computed using a new localization formula. The matching implies that the quantum effects do not drastically alter the correspondence between the geometry and the 2d gauge theory. In theories whose matter sector suffers from abelian gauge anomaly, we propose an ansatz for an anomaly cancelling term in the integral formula for the elliptic genus. We provide an example in which two brane brick models related to each other by Gadde-Gukov-Putrov triality give the same elliptic genus.",0.2],["a corpus based on twitter has been used to analyse language variation. we use","Dialectometric analysis of language variation in Twitter","summarize: In the last few years, microblogging platforms such as Twitter have given rise to a deluge of textual data that can be used for the analysis of informal communication between millions of individuals. In this work, we propose an information-theoretic approach to geographic language variation using a corpus based on Twitter. We test our models with tens of concepts and their associated keywords detected in Spanish tweets geolocated in Spain. We employ dialectometric measures to quantify the linguistic distance on the lexical level between cells created in a uniform grid over the map. This can be done for a single concept or in the general case taking into account an average of the considered variants. The latter permits an analysis of the dialects that naturally emerge from the data. Interestingly, our results reveal the existence of two dialect macrovarieties. The first group includes a region-specific speech spoken in small towns and rural areas whereas the second cluster encompasses cities that tend to use a more uniform variety. Since the results obtained with the two different metrics qualitatively agree, our work suggests that social media corpora can be efficiently used for dialectometric analyses.",0.4583333333],["a total of 150 students were asked to provide effort estimates for different amounts of requirements.","Is it Possible to Disregard Obsolete Requirements? - An Initial Experiment on a Potentially New Bias in Software Effort Estimation","summarize: Effort estimation is a complex area in decision-making, and is influenced by a diversity of factors that could increase the estimation error. The effects on effort estimation accuracy of having obsolete requirements in specifications have not yet been studied. This study aims at filling that gap. A total of 150 students were asked to provide effort estimates for different amounts of requirements, and one group was explicitly told to disregard some of the given requirements. The results show that even the extra text instructing participants to exclude requirements in the estimation task, had the subjects give higher estimates. The effect of having obsolete requirements in requirements specifications and backlogs in software effort estimation is not taken into account enough today, and this study provides empirical evidence that it possibly should. We also suggest different psychological explanations to the found effect.",0.2920502937],["a computationally efficient harmonic block uses Discrete Cosine Transform filters in CNN","Harmonic Networks with Limited Training Samples","summarize: Convolutional neural networks are very popular nowadays for image processing. CNNs allow one to learn optimal filters in a supervised machine learning context. However this typically requires abundant labelled training data to estimate the filter parameters. Alternative strategies have been deployed for reducing the number of parameters and \/ or filters to be learned and thus decrease overfitting. In the context of reverting to preset filters, we propose here a computationally efficient harmonic block that uses Discrete Cosine Transform filters in CNNs. In this work we examine the performance of harmonic networks in limited training data scenario. We validate experimentally that its performance compares well against scattering networks that use wavelets as preset filters.",0.2941176471],["orientable matroids are characterized by regular matroids.","On the regularity of orientable matroids","summarize: We present two characterizations of regular matroids among orientable matroids and use them to give a measure of how far an orientable matroid is from being regular.",0.3333333333],["carsharing is a model of renting vehicles for short periods of time. payment is made","Proposal of a Carsharing System to Improve Urban Mobility","summarize: Carsharing is a model of renting vehicles for short periods of time, where the payment is made according to the time and distance effectively traveled. Carsharing offers a simple, economical and smart alternative to urban mobility, that is already being adopted in the major cities in the world. The proposed methodology consisted in the development of a decision support system that simplifies the process of choosing carsharing services. Adopting the AHP method, the user can indicate their preferences in the choice of vehicles, and the system returns an ordered list of the most suitable available vehicles based on their geographic location. The findings of the project indicate that the use of this system encourage and simplify the use of carsharing services, which will allow to enhance the financial, mobility and environment advantages inherent to their use.",0.3043478261],["hypothesis testing problem where a part of data cannot be observed. our helper can send","New Upper Bounds in the Hypothesis Testing Problem with Information Constraints","summarize: We consider a hypothesis testing problem where a part of data cannot be observed. Our helper observes the missed data and can send us a limited amount of information about them. What kind of this limited information will allow us to make the best statistical inference? In particular, what is the minimum information sufficient to obtain the same results as if we directly observed all the data? We derive estimates for this minimum information and some other similar results.",0.0952380952],["quantitative structure-activity relationship has proved invaluable tool in medicinal chemistry. predictive models are","On the Virtues of Automated QSAR The New Kid on the Block","summarize: Quantitative Structure-Activity Relationship has proved an invaluable tool in medicinal chemistry. Data availability at unprecedented levels through various databases have collaborated to a resurgence in the interest for QSAR. In this context, rapid generation of quality predictive models is highly desirable for hit identification and lead optimization. We showcase the application of an automated QSAR approach, which randomly selects multiple training\/test sets and utilizes machine-learning algorithms to generate predictive models. Results demonstrate that AutoQSAR produces models of improved or similar quality to those generated by practitioners in the field but in just a fraction of the time. Despite the potential of the concept to the benefit of the community, the AutoQSAR opportunity has been largely undervalued.",0.0],["lines restricted to invariant submanifolds generally gives rise to nonlinear dynamics","Nonlinear Dynamics from Linear Quantum Evolutions","summarize: Linear dynamics restricted to invariant submanifolds generally gives rise to nonlinear dynamics. Submanifolds in the quantum framework may emerge for several reasons: one could be interested in specific properties possessed by a given family of states, either as a consequence of experimental constraints or inside an approximation scheme. In this work we investigate such issues in connection with a one parameter group ",0.0],["we consider an extension of the standard model based on the group.","A ","summarize: We consider an extension of the standard model based on the group ",0.0],["we propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving","Softwarization of Internet of Things Infrastructure for Secure and Smart Healthcare","summarize: We propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving deployment of Internet of Things for smart healthcare applications and services. It integrates state-of-the-art networking and virtualization techniques across IoT, fog and cloud domains, employing Blockchain, Tor and message brokers to provide security and privacy for patients and healthcare providers. We propose a novel platform using Machine-to-Machine messaging and rule-based beacons for seamless data management and discuss the role of data and decision fusion in the cloud and the fog, respectively, for smart healthcare applications and services.",0.2],["universal relations for spin-polarized Fermi gases.","Universal relations for a spin-polarized Fermi gas in two dimensions","summarize: We derive the full set of universal relations for spin-polarized Fermi gases with ",0.3422780794],["motifs are a powerful tool for analyzing physiological waveform data. standard motif methods","Contextual Motifs: Increasing the Utility of Motifs using Contextual Data","summarize: Motifs are a powerful tool for analyzing physiological waveform data. Standard motif methods, however, ignore important contextual information . We hypothesize that these additional contextual data could increase the utility of motifs. Thus, we propose an extension to motifs, contextual motifs, that incorporates context. Recognizing that, oftentimes, context may be unobserved or unavailable, we focus on methods to jointly infer motifs and context. Applied to both simulated and real physiological data, our proposed approach improves upon existing motif methods in terms of the discriminative utility of the discovered motifs. In particular, we discovered contextual motifs in continuous glucose monitor data collected from patients with type 1 diabetes. Compared to their contextless counterparts, these contextual motifs led to better predictions of hypo- and hyperglycemic events. Our results suggest that even when inferred, context is useful in both a long- and short-term prediction horizon when processing and interpreting physiological waveform data.",0.2083333333],["electric taxis face several obstacles, including constrained driving range, long recharging duration","Improving Viability of Electric Taxis by Taxi Service Strategy Optimization: A Big Data Study of New York City","summarize: Electrification of transportation is critical for a low-carbon society. In particular, public vehicles provide a crucial opportunity for electrification. Despite the benefits of eco-friendliness and energy efficiency, adoption of electric taxis faces several obstacles, including constrained driving range, long recharging duration, limited charging stations and low gas price, all of which impede taxi drivers' decisions to switch to electric taxis. On the other hand, the popularity of ride-hailing mobile apps facilitates the computerization and optimization of taxi service strategies, which can provide computer-assisted decisions of navigation and roaming for taxi drivers to locate potential customers. This paper examines the viability of electric taxis with the assistance of taxi service strategy optimization, in comparison with conventional taxis with internal combustion engines. A big data study is provided using a large dataset of real-world taxi trips in New York City. Our methodology is to first model the computerized taxi service strategy by Markov Decision Process , and then obtain the optimized taxi service strategy based on NYC taxi trip dataset. The profitability of electric taxi drivers is studied empirically under various battery capacity and charging conditions. Consequently, we shed light on the solutions that can improve viability of electric taxis.",0.0],["we study the dynamics of the fluctuations of the variance.","Dynamics of fluctuations in the Gaussian model with dissipative Langevin Dynamics","summarize: We study the dynamics of the fluctuations of the variance ",0.3016124727],["informal and code-switched content are under-resourced in terms of labeled","Adapting Deep Learning for Sentiment Classification of Code-Switched Informal Short Text","summarize: Nowadays, an abundance of short text is being generated that uses nonstandard writing styles influenced by regional languages. Such informal and code-switched content are under-resourced in terms of labeled datasets and language models even for popular tasks like sentiment classification. In this work, we present a labeled dataset called MultiSenti for sentiment classification of code-switched informal short text, explore the feasibility of adapting resources from a resource-rich language for an informal one, and propose a deep learning-based model for sentiment classification of code-switched informal short text. We aim to achieve this without any lexical normalization, language translation, or code-switching indication. The performance of the proposed models is compared with three existing multilingual sentiment classification models. The results show that the proposed model performs better in general and adapting character-based embeddings yield equivalent performance while being computationally more efficient than training word-based domain-specific embeddings.",0.2467738413],["Graph neural networks are emerging line of deep learning models. it is becoming more popular due","Architectural Implications of Graph Neural Networks","summarize: Graph neural networks represent an emerging line of deep learning models that operate on graph structures. It is becoming more and more popular due to its high accuracy achieved in many graph-related tasks. However, GNN is not as well understood in the system and architecture community as its counterparts such as multi-layer perceptrons and convolutional neural networks. This work tries to introduce the GNN to our community. In contrast to prior work that only presents characterizations of GCNs, our work covers a large portion of the varieties for GNN workloads based on a general GNN description framework. By constructing the models on top of two widely-used libraries, we characterize the GNN computation at inference stage concerning general-purpose and application-specific architectures and hope our work can foster more system and architecture research for GNNs.",0.1875],["the results play an important role in the theory of sheaves with transfers on proper modulus pairs","Topologies on schemes and modulus pairs","summarize: We study relationships between the Nisnevich topology on smooth schemes and certain Grothendieck topologies on proper and not necessarily proper modulus pairs which were introduced respectively in and . Our results play an important role in the theory of sheaves with transfers on proper modulus pairs. This is a revised version of arXiv:1809.05851 .",0.1739130435],["area under the Receiver operating characteristic curve is used for model comparison. two under","Predicting class-imbalanced business risk using resampling, regularization, and model ensembling algorithms","summarize: We aim at developing and improving the imbalanced business risk modeling via jointly using proper evaluation criteria, resampling, cross-validation, classifier regularization, and ensembling techniques. Area Under the Receiver Operating Characteristic Curve is used for model comparison based on 10-fold cross validation. Two undersampling strategies including random undersampling and cluster centroid undersampling , as well as two oversampling methods including random oversampling and Synthetic Minority Oversampling Technique , are applied. Three highly interpretable classifiers, including logistic regression without regularization , L1-regularized LR , and decision tree are implemented. Two ensembling techniques, including Bagging and Boosting, are applied on the DT classifier for further model improvement. The results show that, Boosting on DT by using the oversampled data containing 50% positives via SMOTE is the optimal model and it can achieve AUC, recall, and F1 score valued 0.8633, 0.9260, and 0.8907, respectively.",0.1111111111],["extrasolar giants are expected to possess dipole surface magnetic fields. the magnetic fields","Size and strength of self-excited dynamos in Jupiter-like extrasolar planets","summarize: The magnetization of solar and extrasolar gas giants is critically dependent on electronic and mass transport coefficients of their convective fluid interiors. We analyze recent laboratory experimental results on metallic hydrogen to derive a new conductivity profile for the Jovian-like planets. We combine this revised conductivity with a polytropic-based thermodynamic equation of state to study the dynamo action in 100 extrasolar giant planets varying from synchronous hot jupiters to fast rotators, with masses ranging from 0.3MJ to 15MJ. We find conducting cores larger than previous estimates, but consistent with the results from Juno, suggesting that the dynamos in the more massive planets might be shallow-seated. Our results reveal that most extrasolar giants are expected to possess dipole surface magnetic fields in the range of 0.1-10 Gauss. Assuming radio emission processes similar to our solar giants, most characterized planets should emit radiation with a maximum cyclotron frequency between few and 30 MHz, lower than previous estimates. Our work places new bounds on the observational detectability of extrasolar magnetic fields.",0.0625],["knowledge leakage poses a critical risk to the competitive advantage of knowledge-intensive organisations.","Exploring Knowledge Leakage Risk in Knowledge-Intensive Organisations: Behavioural aspects and Key controls","summarize: Knowledge leakage poses a critical risk to the competitive advantage of knowledge-intensive organisations. Although knowledge leakage is a human-centric security issue, little is known about leakage resulting from individual behaviour and the protective strategies and controls that could be effective in mitigating leakage risk. Therefore, this research explores the perspectives of security practitioners on the key factors that influence knowledge leakage risk in the context of knowledge-intensive organisations. We conduct two focus groups to explore these perspectives. The research highlights three types of behavioural controls that mitigate the risk of knowledge leakage: human resource management practices, knowledge security training and awareness practices, and compartmentalisation practices.",0.3636363636],["the approach relies on Monte Carlo Simulations of the fine-grained CDF equation","An Efficient Solver for Cumulative Density Function-based Solutions of Uncertain Kinematic Wave Models","summarize: We develop a numerical framework to implement the cumulative density function method for obtaining the probability distribution of the system state described by a kinematic wave model. The approach relies on Monte Carlo Simulations of the fine-grained CDF equation of system state, as derived by the CDF method. This fine-grained CDF equation is solved via the method of characteristics. Each method of characteristics solution is far more computationally efficient than the direct solution of the kinematic wave model, and the MCS estimator of the CDF converges relatively quickly. We verify the accuracy and robustness of our procedure via comparison with direct MCS of a particular kinematic wave system, the Saint-Venant equation.",0.1623607791],["composites are assumed to possess periodic microstructure and be subjected to a remote field","Micromechanical analysis of hyperelastic composites with localized damage using a new low-memory Broyden-step-based algorithm","summarize: A multiscale analysis is proposed for the prediction of the finite strain behavior of composites with hyperelastic constituents and embedded localized damage. The composites are assumed to possess periodic microstructure and be subjected to a remote field. At the microscale, finite-strain micromechanical analysis based on the homogenization technique for the composite is employed for the prediction of the effective deformation. At the macroscale, a procedure, based on the representative cell method and the associated higher-order theory, is developed for the determination of the elastic field in the damaged composite. The periodic composite is discretized into identical cells and then reduced to the problem of a single cell by application of the discrete Fourier transform. The resulting governing equations, interfacial and boundary conditions in the Fourier transform domain, are solved by employing the higher-order theory in conjunction with an iterative procedure to treat the effects of damage and material nonlinearity. The initial conditions for the iterative solution are obtained using the weakly-nonlinear material limit and a natural fixed-point iteration. A locally-convergent low-memory Quasi-Newton solver is then employed. A new algorithm for the implementation of the solver is proposed, which allows storing in the memory directly the vector-function history-sequence, which may be advantageous for convergence-control based on specific components of the objective vector-function. The strong-form Fourier transform-based approach employed here, in conjunction with the new solver, enables to extend the application of the method to nonlinear materials and may have computational efficiency comparable or possibly advantageous to that of standard approaches.",0.2631578947],["electrical conductivity is one of the most commonly used geophysical method for reservoir and environmental studies","A physically based model for the electrical conductivity of water-saturated porous media","summarize: Electrical conductivity is one of the most commonly used geophysical method for reservoir and environmental studies. Its main interest lies in its sensitivity to key properties of storage and transport in porous media. Its quantitative use therefore depends on the efficiency of the petrophysical relationship to link them. In this work, we develop a new physically based model for estimating electrical conductivity of saturated porous media. The model is derived assuming that the porous media is represented by a bundle of tortuous capillary tubes with a fractal pore-size distribution. The model is expressed in terms of the porosity, electrical conductivity of the pore liquid and the microstructural parameters of porous media. It takes into account the interface properties between minerals and pore water by introducing a surface conductivity. Expressions for the formation factor and hydraulic tortuosity are also obtained from the model derivation. The model is then successfully compared with published data and performs better than previous models. The proposed approach also permits to relate the electrical conductivity to other transport properties such as the hydraulic conductivity.",0.3125],["we introduce one-way coupling strategies from input properties to our simulation. we efficiently compute","Efficient 2D Simulation on Moving 3D Surfaces","summarize: We present a method to simulate fluid flow on evolving surfaces, e.g., an oil film on a water surface. Given an animated surface in three-dimensional space, we add a second simulation on this base animation. In general, we solve a partial differential equation on a level set surface obtained from the animated input surface. The properties of the input surface are transferred to a sparse volume data structure that is then used for the simulation. We introduce one-way coupling strategies from input properties to our simulation and we add conservation of mass and momentum to existing methods that solve a PDE in a narrow-band using the Closest Point Method. In this way, we efficiently compute high-resolution 2D simulations on coarse input surfaces. Our approach helps visual effects creators easily integrate a workflow to simulate material flow on evolving surfaces into their existing production pipeline.",0.0],["adaptive discontinuous Galerkin methods are used to solve fractional diffusion equations. the","Discontinuous Galerkin methods and their adaptivity for the tempered fractional diffusion equations","summarize: This paper focuses on the adaptive discontinuous Galerkin methods for the tempered fractional diffusion equations. The DG schemes with interior penalty for the diffusion term and numerical flux for the convection term are used to solve the equations, and the detailed stability and convergence analyses are provided. Based on the derived posteriori error estimates, the local error indicator is designed. The theoretical results and the effectiveness of the adaptive DG methods are respectively verified and displayed by the extensive numerical experiments. The strategy of designing adaptive schemes presented in this paper works for the general PDEs with fractional operators.",0.4166666667],["the homogenization of periodic elastic composites is addressed through the reformulation of the local equation","Geometric variational principles for computational homogenization","summarize: The homogenization of periodic elastic composites is addressed through the reformulation of the local equations of the mechanical problem in a geometric functional setting. This relies on the definition of Hilbert spaces of kinematically and statically admissible tensor fields, whose orthogonality and duality properties are recalled. These are endowed with specific energetic scalar products that make use of a reference and uniform elasticity tensor. The corresponding strain and stress Green's operators are introduced and interpreted as orthogonal projection operators in the admissibility spaces. In this context and as an alternative to classical minimum energy principles, two geometric variational principles are investigated with the introduction of functionals that aim at measuring the discrepancy of arbitrary test fields to the kinematic, static or material admissibility conditions of the problem. By relaxing the corresponding local equations, this study aims in particular at laying the groundwork for the homogenization of composites whose constitutive properties are only partially known or uncertain. The local fields in the composite and their macroscopic responses are computed through the minimization of the proposed geometric functionals. To do so, their gradients are computed using the Green's operators and gradient-based optimization schemes are discussed. A FFT-based implementation of these schemes is proposed and they are assessed numerically on a canonical example for which analytical solutions are available.",0.0434782609],["we define an infinite graded graph of ordered pairs and acanonical action of","Universal adic approximation, invariant measures and scaled entropy","summarize: We define an infinite graded graph of ordered pairs and a~canonical action of the group ",0.1764705882],["spectral CT is a modality which incorporates multiple photon energy spectral","Physical Modeling and Performance of Spatial-Spectral Filters for CT Material Decomposition","summarize: Material decomposition for imaging multiple contrast agents in a single acquisition has been made possible by spectral CT: a modality which incorporates multiple photon energy spectral sensitivities into a single data collection. This work presents an investigation of a new approach to spectral CT which does not rely on energy-discriminating detectors or multiple x-ray sources. Instead, a tiled pattern of K-edge filters are placed in front of the x-ray to create spatially encoded spectra data. For improved sampling, the spatial-spectral filter is moved continuously with respect to the source. A model-based material decomposition algorithm is adopted to directly reconstruct multiple material densities from projection data that is sparse in each spectral channel. Physical effects associated with the x-ray focal spot size and motion blur for the moving filter are expected to impact overall performance. In this work, those physical effects are modeled and a performance analysis is conducted. Specifically, experiments are presented with simulated focal spot widths between 0.2 mm and 4.0 mm. Additionally, filter motion blur is simulated for a linear translation speeds between 50 mm\/s and 450 mm\/s. The performance differential between a 0.2 mm and a 1.0 mm focal spot is less than 15% suggesting feasibility of the approach with realistic x-ray tubes. Moreover, for reasonable filter actuation speeds, higher speeds are shown to decrease error despite motion-based spectral blur.",0.3529411765],["the spectrum of massless bosonic and fermionic fluids satisfying the equation of state","Spectrum of ","summarize: The spectrum of massless bosonic and fermionic fluids satisfying the equation of state ",0.1176470588],["this paper considers the","","summarize: This paper considers the ",0.0],["a new hybrid approach for complexity analysis is Badger. it uses fuzz testing to","Badger: Complexity Analysis with Fuzzing and Symbolic Execution","summarize: Hybrid testing approaches that involve fuzz testing and symbolic execution have shown promising results in achieving high code coverage, uncovering subtle errors and vulnerabilities in a variety of software applications. In this paper we describe Badger - a new hybrid approach for complexity analysis, with the goal of discovering vulnerabilities which occur when the worst-case time or space complexity of an application is significantly higher than the average case. Badger uses fuzz testing to generate a diverse set of inputs that aim to increase not only coverage but also a resource-related cost associated with each path. Since fuzzing may fail to execute deep program paths due to its limited knowledge about the conditions that influence these paths, we complement the analysis with a symbolic execution, which is also customized to search for paths that increase the resource-related cost. Symbolic execution is particularly good at generating inputs that satisfy various program conditions but by itself suffers from path explosion. Therefore, Badger uses fuzzing and symbolic execution in tandem, to leverage their benefits and overcome their weaknesses. We implemented our approach for the analysis of Java programs, based on Kelinci and Symbolic PathFinder. We evaluated Badger on Java applications, showing that our approach is significantly faster in generating worst-case executions compared to fuzzing or symbolic execution on their own.",0.3333333333],["a study of the relationship between the radius and the radius is carried out in this paper.","On the radius and the attachment number of tetravalent half-arc-transitive graphs","summarize: In this paper, we study the relationship between the radius ",0.53125],["opportunistic routing and network coding are two recognized innovative ideas to improve the","Joint Inter-flow Network Coding and Opportunistic Routing in Multi-hop Wireless Mesh Networks: A Comprehensive Survey","summarize: Network coding and opportunistic routing are two recognized innovative ideas to improve the performance of wireless networks by utilizing the broadcast nature of the wireless medium. In the last decade, there has been considerable research on how to synergize inter-flow network coding and opportunistic routing in a single joint protocol outperforming each in any scenario. This paper explains the motivation behind the integration of these two techniques, and highlights certain scenarios in which the joint approach may even degrade the performance, emphasizing the fact that their synergistic effect cannot be accomplished with a naive and perfunctory combination. This survey paper also provides a comprehensive taxonomy of the joint protocols in terms of their fundamental components and associated challenges, and compares existing joint protocols. We also present concluding remarks along with an outline of future research directions.",0.1837294112],["multi-level summarizer supervised method to construct abstractive summaries.","Interpretable Multi-Headed Attention for Abstractive Summarization at Controllable Lengths","summarize: Abstractive summarization at controllable lengths is a challenging task in natural language processing. It is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand. At the same time, when it comes to trusting machine-generated summaries, explaining how a summary was constructed in human-understandable terms may be critical. We propose Multi-level Summarizer , a supervised method to construct abstractive summaries of a text document at controllable lengths. The key enabler of our method is an interpretable multi-headed attention mechanism that computes attention distribution over an input document using an array of timestep independent semantic kernels. Each kernel optimizes a human-interpretable syntactic or semantic property. Exhaustive experiments on two low-resource datasets in the English language show that MLS outperforms strong baselines by up to 14.70% in the METEOR score. Human evaluation of the summaries also suggests that they capture the key concepts of the document at various length-budgets.",0.0],["3D shape and motion priors can be used to regularize the estimation of the trajectory and","SAMP: Shape and Motion Priors for 4D Vehicle Reconstruction","summarize: Inferring the pose and shape of vehicles in 3D from a movable platform still remains a challenging task due to the projective sensing principle of cameras, difficult surface properties e.g. reflections or transparency, and illumination changes between images. In this paper, we propose to use 3D shape and motion priors to regularize the estimation of the trajectory and the shape of vehicles in sequences of stereo images. We represent shapes by 3D signed distance functions and embed them in a low-dimensional manifold. Our optimization method allows for imposing a common shape across all image observations along an object track. We employ a motion model to regularize the trajectory to plausible object motions. We evaluate our method on the KITTI dataset and show state-of-the-art results in terms of shape reconstruction and pose estimation accuracy.",0.0952380952],["the results were calculated for coherent neutrinos. this was done with two different models for","Meaningful Details: The value of adding baseline dependence to the Neutrino-Dark Matter Effect","summarize: The possible effect on the flavour spectra of astronomical neutrinos from a neutrino-dark matter interaction has been investigated for decoherent neutrinos. In this work, we report results calculated for coherent neutrinos. This was done with two different models for the neutrino dark-matter interactions: a flavour state interaction,as for the weak interaction in the Standard Model, and a mass state interaction, which is predicted by certain non-Standard Models . It was found that using a coherent analysis dramatically increased the explorable parameter space for the neutrino-dark matter interaction. However, the detection of coherent astronomical neutrinos presents a significant challenge to experimentalists, because such a detection would require an improvement in energy resolution by at least six orders of magnitude, with similar improvements in astronomical distance determinations.",0.0588235294],["the pseudo-metric defined on the metric graph is a random continuous function that generali","The random pseudo-metric on a graph defined via the zero-set of the Gaussian free field on its metric graph","summarize: We further investigate properties of the Gaussian free field on the metric graph associated to a discrete weighted graph that has been introduced by the first author. On such a metric graph, the GFF is a random continuous function that generalises one-dimensional Brownian bridges so that one-dimensional techniques can be used. In the present paper, we define and study the pseudo-metric defined on the metric graph , where the length of a path on the metric graph is defined to be the local time at level zero accumulated by the Gaussian free field along this path. We first derive a pathwise transformation that relates the GFF on the metric graph with the reflected GFF on the metric graph via the pseudo-distance defined by the latter. This is a generalisation of Paul L\\'evy's result relating the local time at zero of Brownian motion to the supremum of another Brownian motion. We also compute explicitly the distribution of certain functionals of this pseudo-metric and of the GFF. In particular, we point out that when the boundary consists of just two points, the law of the pseudo-distance between them depends solely on the resistance of the network between them. We then discuss questions related to the scaling limit of this pseudo-metric in the two-dimensional case, which should be the conformally invariant way to measure distances between CLE loops introduced and studied by the second author with Wu, and by Sheffield, Watson and Wu. Our explicit laws on metric graphs also lead to new conjectures for related functionals of the continuum GFF on fairly general Riemann surfaces.",0.5171492668],["the atomic dynamic dipole polarizability of dysprosium depend on the","Optical trapping of ultracold dysprosium atoms: transition probabilities, dynamic dipole polarizabilities and van der Waals ","summarize: The efficiency of optical trapping of ultracold atoms depend on the atomic dynamic dipole polarizability governing the atom-field interaction. In this article, we have calculated the real and imaginary parts of the dynamic dipole polarizability of dysprosium in the ground and first excited level. Due to the high electronic angular momentum of those two states, the polarizabilities possess scalar, vector and tensor contributions that we have computed, on a wide range of trapping wavelengths, using the sum-over-state formula. Using the same formalism, we have also calculated the ",0.228671515],["tight laminations are representatives of minimal complexity. the geometric complexity of a braid is","The Relaxation Normal Form of Braids is Regular","summarize: Braids can be represented geometrically as laminations of punctured disks. The geometric complexity of a braid is the minimal complexity of a lamination that represents it, and tight laminations are representatives of minimal complexity. These laminations give rise to a normal form of braids, via a relaxation algorithm. We study here this relaxation algorithm and the associated normal form. We prove that this normal form is regular and prefix-closed. We provide an effective construction of a deterministic automaton that recognizes this normal form.",0.3333333333],["composition dependence of gravitational constant is dependent on the composition of the constant.","Short-range test of the universality of gravitational constant ","summarize: The composition dependence of gravitational constant ",0.4230769231],["Miles et al. argue that the lockdown costed more than the benefit","Counting the costs of COVID-19: why future treatment option values matter","summarize: I critique a recent analysis of COVID-19 lockdown costs and benefits, focussing on the United Kingdom . Miles et al. argue that the March-June UK lockdown was more costly than the benefit of lives saved, evaluated using the NICE threshold of 30000 for a quality-adjusted life year and that the costs of a lockdown for 13 weeks from mid-June would be vastly greater than any plausible estimate of the benefits, even if easing produced a second infection wave causing over 7000 deaths weekly by mid-September. I note here two key problems that significantly affect their estimates and cast doubt on their conclusions. Firstly, their calculations arbitrarily cut off after 13 weeks, without costing the epidemic end state. That is, they assume indifference between mid-September states of 13 or 7500 weekly deaths and corresponding infection rates. This seems indefensible unless one assumes that there is little chance of any effective vaccine or improved medical or social interventions for the foreseeable future, notwithstanding temporary lockdowns, COVID-19 will very likely propagate until herd immunity. Even under these assumptions it is very questionable. Secondly, they ignore the costs of serious illness, possible long-term lowering of life quality and expectancy for survivors. These are uncertain, but plausibly at least as large as the costs in deaths. In summary, policy on tackling COVID-19 cannot be rationally made without estimating probabilities of future medical interventions and long-term illness costs. More work on modelling these uncertainties is urgently needed.",0.2142857143],["a partial differential equations model for ELISPOT and Fluorospot immunoa","Cell detection on image-based immunoassays","summarize: Cell detection and counting in the image-based ELISPOT and Fluorospot immunoassays is considered a bottleneck. The task has remained hard to automatize, and biomedical researchers often have to rely on results that are not accurate. Previously proposed solutions are heuristic, and data-based solutions are subject to a lack of objective ground truth data. In this paper, we analyze a partial differential equations model for ELISPOT, Fluorospot, and assays of similar design. This leads us to a mathematical observation model for the images generated by these assays. We use this model to motivate a methodology for cell detection. Finally, we provide a real-data example that suggests that this cell detection methodology and a human expert perform comparably.",0.3125],["a paper by Grochenig retraces the steps of a","Almost Diagonalization of Pseudodifferential Operators","summarize: In this review we focus on the almost diagonalization of pseudodifferential operators and highlight the advantages that time-frequency techniques provide here. In particular, we retrace the steps of an insightful paper by Gr\\ochenig, who succeeded in characterizing a class of symbols previously investigated by Se\\ostrand by noticing that Gabor frames almost diagonalize the corresponding Weyl operators. This approach also allows to give new and more natural proofs of related results such as boundedness of operators or algebra and Wiener properties of the symbol class. Then, we discuss some recent developments on the theme, namely an extension of these results to a more general family of pseudodifferential operators and similar outcomes for a symbol class closely related to Sj\\ostrand's one.",0.6],["the overdamped equation of motion is a particle subject to spatially varying Lorent","Anomalous fluxes in overdamped Brownian dynamics with Lorentz force","summarize: We study the stochastic motion of a particle subject to spatially varying Lorentz force in the small-mass limit. The limiting procedure yields an additional drift term in the overdamped equation that cannot be obtained by simply setting mass to zero in the velocity Langevin equation. We show that whereas the overdamped equation of motion accurately captures the position statistics of the particle, it leads to unphysical fluxes in the system that persist in the long time limit; an anomalous result inconsistent with thermal equilibrium. These fluxes are calculated analytically from the overdamped equation of motion and found to be in quantitative agreement with Brownian dynamics simulations. Our study suggests that the overdamped approximation, though perfectly suited for position statistics, can yield unphysical values for velocity-dependent variables such as flux and entropy production.",0.3157894737],["autophagosome formation involves dynamic morphological changes. disk-shaped membrane c","Modeling membrane morphological change during autophagosome formation","summarize: Autophagy is an intracellular degradation process that is mediated by de novo formation of autophagosomes. Autophagosome formation involves dynamic morphological changes; a disk-shaped membrane cisterna grows, bends to become a cup-shaped structure, and finally develops into a spherical autophagosome. We have constructed a theoretical model that integrates the membrane morphological change and entropic partitioning of putative curvature generators, which we have used to investigate the autophagosome formation process quantitatively. We show that the membrane curvature and the distribution of the curvature generators stabilize disk- and cup-shaped intermediate structures during autophagosome formation, which is quantitatively consistent with in vivo observations. These results suggest that various autophagy proteins with membrane curvature-sensing properties control morphological change by stabilizing these intermediate structures. Our model provides a framework for understanding autophagosome formation.",0.5],["we propose a natural greedy algorithm for response-dependent costs. we bound the app","Submodular Learning and Covering with Response-Dependent Costs","summarize: We consider interactive learning and covering problems, in a setting where actions may incur different costs, depending on the response to the action. We propose a natural greedy algorithm for response-dependent costs. We bound the approximation factor of this greedy algorithm in active learning settings as well as in the general setting. We show that a different property of the cost function controls the approximation factor in each of these scenarios. We further show that in both settings, the approximation factor of this greedy algorithm is near-optimal among all greedy algorithms. Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting.",0.1578947368],["experience replay is one of the most commonly used approaches to improve reinforcement learning algorithms. we propose","Experience Replay Using Transition Sequences","summarize: Experience replay is one of the most commonly used approaches to improve the sample efficiency of reinforcement learning algorithms. In this work, we propose an approach to select and replay sequences of transitions in order to accelerate the learning of a reinforcement learning agent in an off-policy setting. In addition to selecting appropriate sequences, we also artificially construct transition sequences using information gathered from previous agent-environment interactions. These sequences, when replayed, allow value function information to trickle down to larger sections of the state\/state-action space, thereby making the most of the agent's experience. We demonstrate our approach on modified versions of standard reinforcement learning tasks such as the mountain car and puddle world problems and empirically show that it enables better learning of value functions as compared to other forms of experience replay. Further, we briefly discuss some of the possible extensions to this work, as well as applications and situations where this approach could be particularly useful.",0.0],["we derive Born's rule and the density-operator formalism for quantum systems","A Gleason-type theorem for qubits based on mixtures of projective measurements","summarize: We derive Born's rule and the density-operator formalism for quantum systems with Hilbert spaces of dimension two or larger. Our extension of Gleason's theorem only relies upon the consistent assignment of probabilities to the outcomes of projective measurements and their classical mixtures. This assumption is significantly weaker than those required for existing Gleason-type theorems valid in dimension two.",0.1666666667],["the 2798AA Mg II broad emission line responds to the continuum. the","Behaviour of the MgII 2798AA Line Over the Full Range of AGN Variability","summarize: We investigate the responsiveness of the 2798AA Mg II broad emission line in AGN on timescales of several years. Our study is based on a sample of extremely variable AGN as well as a broad population sample. The observed response of the line in previous studies has been mixed. By focussing on extreme variability we find that Mg II clearly does respond to the continuum. However, the degree of responsiveness varies strikingly from one object to another; we see cases of Mg II changing by as much as the continuum, more than the continuum, or very little at all. In 74% of the highly variable sample the behaviour of Mg II corresponds with that of H",0.4600222073],["time-fractional phase field models admit energy dissipation law of integral type","On energy dissipation theory and numerical stability for time-fractional phase field equations","summarize: For the time-fractional phase field models, the corresponding energy dissipation law has not been settled on both the continuous level and the discrete level. In this work, we shall address this open issue. More precisely, we prove for the first time that the time-fractional phase field models indeed admit an energy dissipation law of an integral type. In the discrete level, we propose a class of finite difference schemes that can inherit the theoretical energy stability. Our discussion covers the time-fractional gradient systems, including the time-fractional Allen-Cahn equation, the time-fractional Cahn-Hilliard equation, and the time-fractional molecular beam epitaxy models. Numerical examples are presented to confirm the theoretical results. Moreover, a numerical study of the coarsening rate of random initial states depending on the fractional parameter ",0.4150457801],["side-effect modulation is a variation in the intensity of the light emitted by","Mitigation of Side-Effect Modulation in Optical OFDM VLC Systems","summarize: Side-effect modulation has the potential to be a significant source of interference in future visible light communication systems. SEM is a variation in the intensity of the light emitted by a luminaire and is usually a side-effect caused by the power supply used to drive the luminaires. For LED luminaires powered by a switched mode power supply, the SEM can be at much higher frequencies than that emitted by conventional incandescent or fluorescent lighting. It has been shown that the SEM caused by commercially available LED luminaires is often periodic and of low power. In this paper, we investigate the impact of typical forms of SEM on the performance of optical OFDM VLC systems; both ACO-OFDM and DCO-OFDM are considered. Our results show that even low levels of SEM power can significantly degrade the bit-error-rate performance. To solve this problem, an SEM mitigation scheme is described. The mitigation scheme is decision-directed and is based on estimating and subtracting the fundamental component of the SEM from the received signal. We describe two forms of the algorithm; one uses blind estimation while the other uses pilot-assisted estimation based on a training sequence. Decision errors, resulting in decision noise, limit the performance of the blind estimator even when estimation is based on very long signals. However, the pilot system can achieve more accurate estimations, thus better performance. Results are first presented for typical SEM waveforms for the case where the fundamental frequency of the SEM is known. The algorithms are then extended to include a frequency estimation step and the mitigation algorithm is shown also to be effective in this case.",0.2631578947],["we investigate the variance of local localities in this article.","A note on quadratic twisting of epsilon factors for modular forms with arbitrary nebentypus","summarize: In this article, we investigate the variance of local ",0.1117200077],["we introduce a further new family of discrete objects, called coalescent-walk processes,","Scaling and local limits of Baxter permutations through coalescent-walk processes","summarize: Baxter permutations, plane bipolar orientations, and a specific family of walks in the non-negative quadrant are well-known to be related to each other through several bijections. We introduce a further new family of discrete objects, called coalescent-walk processes, that are fundamental for our results. We relate these new objects with the other previously mentioned families introducing some new bijections. We prove joint Benjamini--Schramm convergence for uniform objects in the four families. Furthermore, we explicitly construct a new fractal random measure of the unit square, called the coalescent Baxter permuton and we show that it is the scaling limit of uniform Baxter permutations. To prove the latter result, we study the scaling limit of the associated random coalescent-walk processes. We show that they converge in law to a continuous random coalescent-walk process encoded by a perturbed version of the Tanaka stochastic differential equation. This result has connections with the results of Gwynne, Holden, Sun on scaling limits of plane bipolar triangulations. We further prove some results that relate the limiting objects of the four families to each other, both in the local and scaling limit case.",0.4375],["wholeness is defined as a mathematical structure of physical space in our surroundings. but there","A Recursive Definition of Goodness of Space for Bridging the Concepts of Space and Place for Sustainability","summarize: Conceived and developed by Christopher Alexander through his life's work: The Nature of Order, wholeness is defined as a mathematical structure of physical space in our surroundings. Yet, there was no mathematics, as Alexander admitted then, that was powerful enough to capture his notion of wholeness. Recently, a mathematical model of wholeness, together with its topological representation, has been developed that is capable of addressing not only why a space is good, but also how much goodness the space has. This paper develops a structural perspective on goodness of space - both large- and small-scale - in order to bridge two basic concepts of space and place through the very concept of wholeness. The wholeness provides a de facto recursive definition of goodness of space from a holistic and organic point of view. A space is good, genuinely and objectively, if its adjacent spaces are good, the larger space to which it belongs is good, and what is contained in the space is also good. Eventually, goodness of space - sustainability of space - is considered a matter of fact rather than of opinion under the new view of space: space is neither lifeless nor neutral, but a living structure capable of being more living or less living, or more sustainable or less sustainable. Under the new view of space, geography or architecture will become part of complexity science, not only for understanding complexity, but also for making and remaking complex or living structures. Keywords: Scaling law, head\/tail breaks, living structure, beauty, streets, cities",0.3805101387],["we consider a class of numerical approximations to the Caputo fractional derivative","A discrete Gr\\nwall inequality with application to numerical schemes for subdiffusion problems","summarize: We consider a class of numerical approximations to the Caputo fractional derivative. Our assumptions permit the use of nonuniform time steps, such as is appropriate for accurately resolving the behavior of a solution whose derivatives are singular at~",0.3333333333],["this paper introduces an expressive class of quotient-inductive types. the","Constructing Infinitary Quotient-Inductive Types","summarize: This paper introduces an expressive class of quotient-inductive types, called QW-types. We show that in dependent type theory with uniqueness of identity proofs, even the infinitary case of QW-types can be encoded using the combination of inductive-inductive definitions involving strictly positive occurrences of Hofmann-style quotient types, and Abel's size types. The latter, which provide a convenient constructive abstraction of what classically would be accomplished with transfinite ordinals, are used to prove termination of the recursive definitions of the elimination and computation properties of our encoding of QW-types. The development is formalized using the Agda theorem prover.",0.0],["a quantum stochastic walk model is a generalization of coherent, i.","Properties of quantum stochastic walks from the asymptotic scaling exponent","summarize: This work focuses on the study of quantum stochastic walks, which are a generalization of coherent, i. e. unitary quantum walks. Our main goal is to present a measure of a coherence of the walk. To this end, we utilize the asymptotic scaling exponent of the second moment of the walk i. e. of the mean squared distance covered by a walk. As the quantum stochastic walk model encompasses both classical random walks and quantum walks, we are interested how the continuous change from one regime to the other influences the asymptotic scaling exponent. Moreover this model allows for behavior which is not found in any of the previously mentioned model -- the model with global dissipation. We derive the probability distribution for the walker, and determine the asymptotic scaling exponent analytically, showing that ballistic regime of the walk is maintained even at large dissipation strength.",0.6666666667],["algorithms for strain tomography from energy-resolved neutron transmission measurements have been","Neutron Transmission Strain Tomography for Non-Constant Stress-Free Lattice Spacing","summarize: Recently, several algorithms for strain tomography from energy-resolved neutron transmission measurements have been proposed. These methods assume that the stress-free lattice spacing ",0.0909090909],["quantizers are constrained to use power-of-2 scale-factors and per-","Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks","summarize: We propose a method of training quantization thresholds for uniform symmetric quantizers using standard backpropagation and gradient descent. Contrary to prior work, we show that a careful analysis of the straight-through estimator for threshold gradients allows for a natural range-precision trade-off leading to better optima. Our quantizers are constrained to use power-of-2 scale-factors and per-tensor scaling of weights and activations to make it amenable for hardware implementations. We present analytical support for the general robustness of our methods and empirically validate them on various CNNs for ImageNet classification. We are able to achieve near-floating-point accuracy on traditionally difficult networks such as MobileNets with less than 5 epochs of quantized retraining. Finally, we present Graffitist, a framework that enables automatic quantization of TensorFlow graphs for TQT .",0.0641180388],["we aim at avoiding single points of failure, particularly in situations in which the failure of","Stop, Think, and Roll: Online Gain Optimization for Resilient Multi-robot Topologies","summarize: Efficient networking of many-robot systems is considered one of the grand challenges of robotics. In this article, we address the problem of achieving resilient, dynamic interconnection topologies in multi-robot systems. In scenarios in which the overall network topology is constantly changing, we aim at avoiding the onset of single points of failure, particularly situations in which the failure of a single robot causes the loss of connectivity for the overall network. We propose a method based on the combination of multiple control objectives and we introduce an online distributed optimization strategy that computes the optimal choice of control parameters for each robot. This ensures that the connectivity of the multi-robot system is not only preserved but also made more resilient to failures, as the network topology evolves. We provide simulation results, as well as experiments with real robots to validate theoretical findings and demonstrate the portability to robotic hardware.",0.25],["we present new connections between quantum information and classical cryptography. examples consist of a quantum","Using Simon's Algorithm to Attack Symmetric-Key Cryptographic Primitives","summarize: We present new connections between quantum information and the field of classical cryptography. In particular, we provide examples where Simon's algorithm can be used to show insecurity of commonly used cryptographic symmetric-key primitives. Specifically, these examples consist of a quantum distinguisher for the 3-round Feistel network and a forgery attack on CBC-MAC which forges a tag for a chosen-prefix message querying only other messages . We assume that an adversary has quantum-oracle access to the respective classical primitives. Similar results have been achieved recently in independent work by Kaplan et al. Our findings shed new light on the post-quantum security of cryptographic schemes and underline that classical security proofs of cryptographic constructions need to be revisited in light of quantum attackers.",0.0769230769],["a quasi-continuous composite perfect electric conductor metasurface and a systematic metasurface","Quasi-Continuous Metasurfaces for Orbital Angular Momentum Generation","summarize: A quasi-continuous composite perfect electric conductor-perfect magnetic conductor metasurface and a systematic metasurface design process are proposed for the orbital angular momentum generation. The metasurfaces reflect the incident left circularly polarized \/right circularly polarized plane wave to RCP\/LCP vortex beams carrying OAM at normal or oblique direction. Unlike conventional metasurfaces that are composed of discrete scatterers, the scatterers on the proposed metasurface form a quasi-continuous pattern. The patterning of the metasurface is calculated through grating vectors, and no optimization of single scatterer is required. Furthermore, the distortions from local-response discontinuity of discrete scatterers are avoided. This letter provides great convenience to high-quality OAM generation.",0.4137931034],["a new tool is developed by a hardware agnostic profiling tool","Platform Independent Software Analysis for Near Memory Computing","summarize: Near-memory Computing promises improved performance for the applications that can exploit the features of emerging memory technologies such as 3D-stacked memory. However, it is not trivial to find such applications and specialized tools are needed to identify them. In this paper, we present PISA-NMC, which extends a state-of-the-art hardware agnostic profiling tool with metrics concerning memory and parallelism, which are relevant for NMC. The metrics include memory entropy, spatial locality, data-level, and basic-block-level parallelism. By profiling a set of representative applications and correlating the metrics with the application's performance on a simulated NMC system, we verify the importance of those metrics. Finally, we demonstrate which metrics are useful in identifying applications suitable for NMC architectures.",0.4285714286],["noncrossing arc diagrams are combinatorial models for the equivalence","Hopf algebras on decorated noncrossing arc diagrams","summarize: Noncrossing arc diagrams are combinatorial models for the equivalence classes of the lattice congruences of the weak order on permutations. In this paper, we provide a general method to endow these objects with Hopf algebra structures. Specific instances of this method produce relevant Hopf algebras that appeared earlier in the literature.",0.3333333333],["covariates are included in the estimation. we recommend a covariate-a","Regression Discontinuity Designs Using Covariates","summarize: We study regression discontinuity designs when covariates are included in the estimation. We examine local polynomial estimators that include discrete or continuous covariates in an additive separable way, but without imposing any parametric restrictions on the underlying population regression functions. We recommend a covariate-adjustment approach that retains consistency under intuitive conditions, and characterize the potential for estimation and inference improvements. We also present new covariate-adjusted mean squared error expansions and robust bias-corrected inference procedures, with heteroskedasticity-consistent and cluster-robust standard errors. An empirical illustration and an extensive simulation study is presented. All methods are implemented in \\texttt and \\texttt software packages.",0.2222222222],["this study uses deep learning based multiple regression algorithm. we predict the TCWV with","Deep Learning based Multiple Regression to Predict Total Column Water Vapor from Physical Parameters in West Africa by using Keras Library","summarize: Total column water vapor is an important factor for the weather and climate. This study apply deep learning based multiple regression to map the TCWV with elements that can improve spatiotemporal prediction. In this study, we predict the TCWV with the use of ERA5 that is the fifth generation ECMWF atmospheric reanalysis of the global climate. We use an appropriate deep learning based multiple regression algorithm using Keras library to improve nonlinear prediction between Total Column water vapor and predictors as Mean sea level pressure, Surface pressure, Sea surface temperature, 100 metre U wind component, 100 metre V wind component, 10 metre U wind component, 10 metre V wind component, 2 metre dew point temperature, 2 metre temperature. The results obtained permit to build a predictor which modelling TCWV with a mean abs error equal to 3.60 kg\/m2 and a coefficient of determination R2 equal to 0.90.",0.0433236186],["Monte Carlo methods are essential tools for Bayesian inference. the algorithm is widely","The Recycling Gibbs Sampler for Efficient Learning","summarize: Monte Carlo methods are essential tools for Bayesian inference. Gibbs sampling is a well-known Markov chain Monte Carlo algorithm, extensively used in signal processing, machine learning, and statistics, employed to draw samples from complicated high-dimensional posterior distributions. The key point for the successful application of the Gibbs sampler is the ability to draw efficiently samples from the full-conditional probability density functions. Since in the general case this is not possible, in order to speed up the convergence of the chain, it is required to generate auxiliary samples whose information is eventually disregarded. In this work, we show that these auxiliary samples can be recycled within the Gibbs estimators, improving their efficiency with no extra cost. This novel scheme arises naturally after pointing out the relationship between the standard Gibbs sampler and the chain rule used for sampling purposes. Numerical simulations involving simple and real inference problems confirm the excellent performance of the proposed scheme in terms of accuracy and computational efficiency. In particular we give empirical evidence of performance in a toy example, inference of Gaussian processes hyperparameters, and learning dependence graphs through regression.",0.0769230769],["inductive invariants belong to an abstract domain. we consider inductive invariants","Decidability and Synthesis of Abstract Inductive Invariants","summarize: Decidability and synthesis of inductive invariants ranging in a given domain play an important role in many software and hardware verification systems. We consider here inductive invariants belonging to an abstract domain ",0.1176470588],["cryo-electron microscopy is a powerful technique for determining the structure","Reconstructing continuous distributions of 3D protein structure from cryo-EM images","summarize: Cryo-electron microscopy is a powerful technique for determining the structure of proteins and other macromolecular complexes at near-atomic resolution. In single particle cryo-EM, the central problem is to reconstruct the three-dimensional structure of a macromolecule from ",0.3],["the informal learning in the workplace is realized during daily collaborators' activities. the informal learning","Adaptation des rseaux sociaux d'entreprise pour favoriser l'apprentissage informel sur le lieu de travail","summarize: The informal learning in the workplace is realized during daily collaborators' activities and represent more than 75 % of the learning occurring in a company. Enterprise social networks are currently massively used to promote this type of learning. From a pilot study, we show that they are actually adapted concerning the social aspects, but that the design must be rethought to consider user contextual needs linked to the content and access to informational corpus, the information quality indicators and the forms of moderation and control.",0.0666666667],["the article describes the process of development and implementation of a full-scale model of noise-","Research of Stability in Ad Hoc Self-Organizated Wireless Networks","summarize: To date, there is a need for the development of efficient data and device exchange protocols that this exchange will provide, since standard protocols used in traditional networks can not fully meet the needs of a new type of network. The article describes the process of development and implementation of a full-scale model of noise-resistant and sensor network breaks. The stability of this network is achieved by building a distributed network, in which all nodes send messages to all available nodes. Wireless mobile peer-to-peer network can be configured automatically, so the nodes in it can move freely. Wireless networks do not have the complexity of infrastructure and management, which allows devices to create and join on-the-go networks - anywhere, anytime. In this paper, the theoretical part of the functioning of such networks and the field of their use is considered. After that, an initial analysis of the available equipment used for constructing such hardware solutions was conducted. The software for developing such solutions is considered in detail, as well as examples of finished models that implement the investigated functional. After that, several variants of the model of network nodes, as well as the test device for creating a payload on the network, are collected. For this purpose, third-party open solutions were used in conjunction with their own developments. The system received a series of tests that made it possible to understand the weak and strong points of such a network and draw conclusions for the further development of the project and the creation of an improved working prototype. The article presents the basic electrical circuits of devices, the list of used equipment and software used and the photographic material of prototypes of the created system.",0.2592592593],["early detection of Parkinson's disease is important. but methods for early detection still remain an","Early Detection of Parkinson's Disease through Patient Questionnaire and Predictive Modelling","summarize: Early detection of Parkinson's disease is important which can enable early initiation of therapeutic interventions and management strategies. However, methods for early detection still remain an unmet clinical need in PD. In this study, we use the Patient Questionnaire portion from the widely used Movement Disorder Society-Unified Parkinson's Disease Rating Scale to develop prediction models that can classify early PD from healthy normal using machine learning techniques that are becoming popular in biomedicine: logistic regression, random forests, boosted trees and support vector machine . We carried out both subject-wise and record-wise validation for evaluating the machine learning techniques. We observe that these techniques perform with high accuracy and high area under the ROC curve in classifying early PD and healthy normal. The logistic model demonstrated statistically significant fit to the data indicating its usefulness as a predictive model. It is inferred that these prediction models have the potential to aid clinicians in the diagnostic process by joining the items of a questionnaire through machine learning.",0.1904761905],["the paper adapts the large deformation diffeomorphic metric mapping framework for image","Indirect Image Registration with Large Diffeomorphic Deformations","summarize: The paper adapts the large deformation diffeomorphic metric mapping framework for image registration to the indirect setting where a template is registered against a target that is given through indirect noisy observations. The registration uses diffeomorphisms that transform the template through a action. These diffeomorphisms are generated by solving a flow equation that is defined by a velocity field with certain regularity. The theoretical analysis includes a proof that indirect image registration has solutions that are stable and that converge as the data error tends so zero, so it becomes a well-defined regularization method. The paper concludes with examples of indirect image registration in 2D tomography with very sparse and\/or highly noisy data.",0.0666666667],["reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals. it","Brain-inspired photonic signal processor for periodic pattern generation and chaotic system emulation","summarize: Reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals. Its hardware implementations have received much attention because of their simplicity and remarkable performance on a series of benchmark tasks. In previous experiments the output was uncoupled from the system and in most cases simply computed offline on a post-processing computer. However, numerical investigations have shown that feeding the output back into the reservoir would open the possibility of long-horizon time series forecasting. Here we present a photonic reservoir computer with output feedback, and demonstrate its capacity to generate periodic time series and to emulate chaotic systems. We study in detail the effect of experimental noise on system performance. In the case of chaotic systems, this leads us to introduce several metrics, based on standard signal processing techniques, to evaluate the quality of the emulation. Our work significantly enlarges the range of tasks that can be solved by hardware reservoir computers, and therefore the range of applications they could potentially tackle. It also raises novel questions in nonlinear dynamics and chaos theory.",0.2941176471],["a wireless broadcast relative localization and clock synchronization system is a difficult problem","BLAS: Broadcast Relative Localization and Clock Synchronization for Dynamic Dense Multi-Agent Systems","summarize: The spatiotemporal information plays crucial roles in a multi-agent system . However, for a highly dynamic and dense MAS in unknown environments, estimating its spatiotemporal states is a difficult problem. In this paper, we present BLAS: a wireless broadcast relative localization and clock synchronization system to address these challenges. Our BLAS system exploits a broadcast architecture, under which a MAS is categorized into parent agents that broadcast wireless packets and child agents that are passive receivers, to reduce the number of required packets among agents for relative localization and clock synchronization. We first propose an asynchronous broadcasting and passively receiving protocol. The protocol schedules the broadcast of parent agents using a distributed time division multiple access scheme and delivers inter-agent information used for joint relative localization and clock synchronization. We then present distributed state estimation approaches in parent and child agents that utilize the broadcast inter-agent information for joint estimation of spatiotemporal states. The simulations and real-world experiments based on ultra-wideband illustrate that our proposed BLAS cannot only enable accurate, high-frequency and real-time estimation of relative position and clock parameters but also support theoretically an unlimited number of agents.",0.5862068966],["experimental results are in good agreement with the analytical expressions that explicitly take into account the fraction of","Missing level statistics and the power spectrum of level fluctuations analysis of three-dimensional chaotic microwave cavities","summarize: We present an experimental study of missing level statistics of three-dimensional chaotic microwave cavities. The investigation is reinforced by the power spectrum of level fluctuations analysis which also takes into account the missing levels. On the basis of our data sets we demonstrate that the power spectrum of level fluctuations in combination with short- and long-range spectral fluctuations provides a powerful tool for the determination of the fraction of randomly missing levels in systems that display wave chaos such as the three-dimensional chaotic microwave cavities. The experimental results are in good agreement with the analytical expressions that explicitly take into account the fraction of observed levels. We also show that in the case of incomplete spectra with many unresolved states the above procedures may fail. In such a case the random matrix theory calculations can be useful for the determination of missing levels.",0.1904761905],["the MNO's optimal multi-cap data plans with time flexibility are studied. the","Multi-Cap Optimization for Wireless Data Plans with Time Flexibility","summarize: An effective way for a Mobile network operator to improve its revenue is price discrimination, i.e., providing different combinations of data caps and subscription fees. Rollover data plan is an innovative data mechanism with time flexibility. In this paper, we study the MNO's optimal multi-cap data plans with time flexibility in a realistic asymmetric information scenario. Specifically, users are associated with multi-dimensional private information, and the MNO designs a contract to induce users to truthfully reveal their private information. This problem is quite challenging due to the multi-dimensional private information. We address the challenge in two aspects. First, we find that a feasible contract should allocate the data caps according to users' willingness-to-pay . Second, for the non-convex data cap allocation problem, we propose a Dynamic Quota Allocation Algorithm, which has a low complexity and guarantees the global optimality. Numerical results show that the time-flexible data mechanisms increase both the MNO's profit and users' payoffs under price discrimination.",0.0714285714],["a group G is called special p-group of rank k if the","Schur multipliers of special p-groups of rank 2","summarize: A group G is called special p-group of rank k if the commutator subgroup and centre Z are equal, which is elementary abelian p-group of rank k and G\/ is also elementary abelian p-group. In this article we determine the Schur multiplier of special p-groups of rank 2 explicitly.",0.4705882353],["bow shocks are created by a magnetized shock model. the molecules formed in the","H","summarize: When a fast moving star or a protostellar jet hits an interstellar cloud, the surrounding gas gets heated and illuminated: a bow shock is born which delineates the wake of the impact. In such a process, the new molecules that are formed and excited in the gas phase become accessible to observations. In this article, we revisit models of H2 emission in these bow shocks. We approximate the bow shock by a statistical distribution of planar shocks computed with a magnetized shock model. We improve on previous works by considering arbitrary bow shapes, a finite irradiation field, and by including the age effect of non-stationary C-type shocks on the excitation diagram and line profiles of H2. We also examine the dependence of the line profiles on the shock velocity and on the viewing angle: we suggest that spectrally resolved observations may greatly help to probe the dynamics inside the bow shock. For reasonable bow shapes, our analysis shows that low velocity shocks largely contribute to H2 excitation diagram. This can result in an observational bias towards low velocities when planar shocks are used to interpret H2 emission from an unresolved bow. We also report a large magnetization bias when the velocity of the planar model is set independently. Our 3D models reproduce excitation diagrams in BHR71 and Orion bow shocks better than previous 1D models. Our 3D model is also able to reproduce the shape and width of the broad H2 1-0S line profile in an Orion bow shock.",0.0],["the goal is to reconstruct the electron density and temperature distributions in the solar corona","Coronal electron density, temperature and solar spectral irradiance during the solar cycle 23 and 24","summarize: The plasma parameters such as the electron density and temperature plays a key role in the dynamics of the solar atmosphere. These characteristics are important in solar physics, because they can help to understand the physics in the solar corona. The goal is to reconstruct the electron density and temperature distributions in the solar corona. The relations between emission and plasma parameters in different time scales are studied. We present a physics-based model to reconstruct the density, temperature and emission in the EUV band. This model called CODET is composed of a flux transport model, an extrapolation model, an emission model and an optimization algorithm. The CODET model parameters were constrained by comparing the model's output to the TIMED\/SEE record instead of direct observations because it covers a longer time interval than the direct solar observations currently available. The most important results of the current work that the recovery of SSI variability in specific wavelengths in the EUV band, also the variations in density and temperature in large time scale through the solar atmosphere, with the CODET model. The evolution of the electron density and temperature profiles through the solar corona in different layers during the solar cycle 23 and 24, will be presented. The emission maps were obtained and they are in accordance to the observations. Also, the density and temperature maps are related to the variations of the magnetic field in different layers through the solar atmosphere.",0.4090909091],["the validity of the measurement has been double-checked in the well-mixed","Moran-evolution of cooperation: From well-mixed to heterogeneous complex networks","summarize: Configurational arrangement of network architecture and interaction character of individuals are two most influential factors on the mechanisms underlying the evolutionary outcome of cooperation, which is explained by the well-established framework of evolutionary game theory. In the current study, not only qualitatively but also quantitatively, we measure Moran-evolution of cooperation to support an analytical agreement based on the consequences of the replicator equation in a finite population. The validity of the measurement has been double-checked in the well-mixed network by the Langevin stochastic differential equation and the Gillespie-algorithmic version of Moran-evolution, while in a structured network, the measurement of accuracy is verified by the standard numerical simulation. Considering the Birth-Death and Death-Birth updating rules through diffusion of individuals, the investigation is carried out in the wide range of game environments those relate to the various social dilemmas where we are able to draw a new rigorous mathematical track to tackle the heterogeneity of complex networks. The set of modified criteria reveals the exact fact about the emergence and maintenance of cooperation in the structured population. We find that in general, nature promotes the environment of coexistent traits.",0.1176470588],["van der Waals tunnel junctions are attractive due to their atomically sharp interface, gate","Astability versus Bistability in van der Waals Tunnel Diode for Voltage Controlled Oscillator and Memory Applications","summarize: Van der Waals tunnel junctions are attractive due to their atomically sharp interface, gate tunablity, and robustness against lattice mismatch between the successive layers. However, the negative differential resistance demonstrated in this class of tunnel diodes often exhibits noisy behaviour with low peak current density, and lacks robustness and repeatability, limiting their practical circuit applications. Here we propose a strategy of using a 1L-WS",0.2311674399],["Existing Recommender Systems mainly focus on exploiting users' feedback.","Enabling the Analysis of Personality Aspects in Recommender Systems","summarize: Existing Recommender Systems mainly focus on exploiting users' feedback, e.g., ratings, and reviews on common items to detect similar users. Thus, they might fail when there are no common items of interest among users. We call this problem the Data Sparsity With no Feedback on Common Items . Personality-based recommender systems have shown a great success to identify similar users based on their personality types. However, there are only a few personality-based recommender systems in the literature which either discover personality explicitly through filling a questionnaire that is a tedious task, or neglect the impact of users' personal interests and level of knowledge, as a key factor to increase recommendations' acceptance. Differently, we identifying users' personality type implicitly with no burden on users and incorporate it along with users' personal interests and their level of knowledge. Experimental results on a real-world dataset demonstrate the effectiveness of our model, especially in DSW-n-FCI situations.",0.3333333333],["eigensolvers dealing with low rank perturbations of unitary and unitary matric","When is a matrix unitary or Hermitian plus low rank?","summarize: Hermitian and unitary matrices are two representatives of the class of normal matrices whose full eigenvalue decomposition can be stably computed in quadratic computing com plexity. Recently, fast and reliable eigensolvers dealing with low rank perturbations of unitary and Hermitian matrices were proposed. These structured eigenvalue problems appear naturally when computing roots, via confederate linearizations, of polynomials expressed in, e.g., the monomial or Chebyshev basis. Often, however, it is not known beforehand whether or not a matrix can be written as the sum of an Hermitian or unitary matrix plus a low rank perturbation. We propose necessary and sufficient conditions characterizing the class of Hermitian or unitary plus low rank matrices. The number of singular values deviating from 1 determines the rank of a perturbation to bring a matrix to unitary form. A similar condition holds for Hermitian matrices; the eigenvalues of the skew-Hermitian part differing from 0 dictate the rank of the perturbation. We prove that these relations are linked via the Cayley transform. Based on these conditions we are able to identify the closest Hermitian and unitary plus low rank matrix in Frobenius and spectral norm and a practical Lanczos iteration to detect the low rank perturbation is presented. Numerical tests prove that this straightforward algorithm is robust with respect to noise.",0.3076923077],["the woods-Saxon potential and the schematic separable-type interaction are employed as","Rotational motion of triaxially deformed nuclei studied by microscopic angular-momentum-projection method II: Chiral doublet band","summarize: In the sequel of the present study, we have investigated the rotational motion of triaxially deformed nucleus by using the microscopic framework of angular-momentum projection. The Woods-Saxon potential and the schematic separable-type interaction are employed as a microscopic Hamiltonian. As the first example nuclear wobbling motion was studied in detail in the part~I of the series. This second part reports on another interesting rotational mode, chiral doublet bands: two prototype examples, ",0.0534726099],["the detailed observation of the distribution of redshifts and chirp masses of binary black hole","Effect of gravitational lensing on the distribution of gravitational waves from distant binary black hole mergers","summarize: The detailed observation of the distribution of redshifts and chirp masses of binary black hole mergers is expected to provide a clue to their origin. In this paper, we develop a hybrid model of the probability distribution function of gravitational lensing magnification taking account of both strong and weak gravitational lensing, and use it to study the effect of gravitational lensing magnification on the distribution of gravitational waves from distant binary black hole mergers detected in ongoing and future gravitational wave observations. We find that the effect of gravitational lensing magnification is significant at high ends of observed chirp mass and redshift distributions. While a high mass tail in the observed chirp mass distribution is produced by highly magnified gravitational lensing events, we find that highly demagnified images of strong lensing events produce a high redshift tail in the observed redshift distribution, which can easily be observed in the third-generation gravitational wave observatories. Such a demagnified, apparently high redshift event is expected to be accompanied by a magnified image that is observed typically ",0.4880906009],["GPLL relaxes supervision assumption from instance-level to group-level.","General Partial Label Learning via Dual Bipartite Graph Autoencoder","summarize: We formulate a practical yet challenging problem: General Partial Label Learning . Compared to the traditional Partial Label Learning problem, GPLL relaxes the supervision assumption from instance-level --- a label set partially labels an instance --- to group-level: 1) a label set partially labels a group of instances, where the within-group instance-label link annotations are missing, and 2) cross-group links are allowed --- instances in a group may be partially linked to the label set from another group. Such ambiguous group-level supervision is more practical in real-world scenarios as additional annotation on the instance-level is no longer required, e.g., face-naming in videos where the group consists of faces in a frame, labeled by a name set in the corresponding caption. In this paper, we propose a novel graph convolutional network called Dual Bipartite Graph Autoencoder to tackle the label ambiguity challenge of GPLL. First, we exploit the cross-group correlations to represent the instance groups as dual bipartite graphs: within-group and cross-group, which reciprocally complements each other to resolve the linking ambiguities. Second, we design a GCN autoencoder to encode and decode them, where the decodings are considered as the refined results. It is worth noting that DB-GAE is self-supervised and transductive, as it only uses the group-level supervision without a separate offline training stage. Extensive experiments on two real-world datasets demonstrate that DB-GAE significantly outperforms the best baseline over absolute 0.159 F1-score and 24.8% accuracy. We further offer analysis on various levels of label ambiguities.",0.1103121128],["the moment generating function of the Kullback-Leibler divergence between the","Finite-Sample Concentration of the Multinomial in Relative Entropy","summarize: We show that the moment generating function of the Kullback-Leibler divergence between the empirical distribution of ",0.25],["urban displacement often results from rising housing costs. new settlements form around a spatial am","The Effects of Inequality, Density, and Heterogeneous Residential Preferences on Urban Displacement and Metropolitan Structure: An Agent-Based Model","summarize: Urban displacement - when a household is forced to relocate due to conditions affecting its home or surroundings - often results from rising housing costs, particularly in wealthy, prosperous cities. However, its dynamics are complex and often difficult to understand. This paper presents an agent-based model of urban settlement, agglomeration, displacement, and sprawl. New settlements form around a spatial amenity that draws initial, poor settlers to subsist on the resource. As the settlement grows, subsequent settlers of varying income, skills, and interests are heterogeneously drawn to either the original amenity or to the emerging human agglomeration. As this agglomeration grows and densifies, land values increase, and the initial poor settlers are displaced from the spatial amenity on which they relied. Through path dependence, high-income residents remain clustered around this original amenity for which they have no direct use or interest. This toy model explores these dynamics, demonstrating a simplified mechanism of how urban displacement and gentrification can be sensitive to income inequality, density, and varied preferences for different types of amenities.",0.272910251],["two ellipses located in the mawrth Vallis region were proposed and evaluated","Mawrth Vallis, Mars: a fascinating place for future in situ exploration","summarize: After the successful landing of the Mars Science Laboratory rover, both NASA and ESA initiated a selection process for potential landing sites for the Mars2020 and ExoMars missions, respectively. Two ellipses located in the Mawrth Vallis region were proposed and evaluated during a series of meetings . We describe here the regional context of the two proposed ellipses as well as the framework of the objectives of these two missions. Key science targets of the ellipses and their astrobiological interests are reported. This work confirms the proposed ellipses contain multiple past Martian wet environments of subaerial, subsurface and\/or subaqueous character, in which to probe the past climate of Mars, build a broad picture of possible past habitable environments, evaluate their exobiological potentials and search for biosignatures in well-preserved rocks. A mission scenario covering several key investigations during the nominal mission of each rover is also presented, as well as descriptions of how the site fulfills the science requirements and expectations of in situ martian exploration. These serve as a basis for potential future exploration of the Mawrth Vallis region with new missions and describe opportunities for human exploration of Mars in terms of resources and science discoveries.",0.1666666667],["system with collection of random walks relays a signal in one dimension. delay is introduced","Delayed Random Relays","summarize: We present here a system with collection of random walks relaying a signal in one dimension in the presence of delays. We are interested in the time for a signal to travel from one end to the other end of the lined group of random walkers. The delay is introduced at the point when the signal is transferred from each walker to the next one. It is found that there is an optimal number of walkers for the signal to travel fastest when delays are present. We discuss implications of this model and associated behaviors to physical and biological systems.",0.1428571429],["we prove uniqueness theorems for inverse Sturm-Liouville","A new kind of uniqueness theorems for inverse Sturm-Liouville problems","summarize: We prove Marchenko-type uniqueness theorems for inverse Sturm-Liouville problems. Moreover, we prove a generalization of Ambarzumyans theorem.",0.4653136125],["china is currently suffering from serious PM2.5 pollution. several models have been developed to obtain spatial","Point-surface fusion of station measurements and satellite observations for mapping PM2.5 distribution in China: methods and assessment","summarize: Fine particulate matter is associated with adverse human health effects, and China is currently suffering from serious PM2.5 pollution. To obtain spatially continuous ground-level PM2.5 concentrations, several models established by point-surface fusion of ground station and satellite observations have been developed. However, how well do these models perform at national scale in China? Is there space to improve the estimation accuracy of PM2.5 concentration? The contribution of this study is threefold. Firstly, taking advantage of the newly established national monitoring network, we develop a national-scale generalized regression neural network model to estimate PM2.5 concentrations. Secondly, different assessment experiments are undertaken in time and space, to comprehensively evaluate and compare the performance of the widely used models. Finally, to map the yearly and seasonal mean distribution of PM2.5 concentrations in China, a pixel-based merging strategy is proposed. The results indicate that the conventional models do not perform well at national scale, with cross-validation R values of 0.488~0.552 and RMSEs of 30.80~31.51 g\/m3, respectively. In contrast, the more advanced models have great advantages in PM2.5 estimation, with R values ranging from 0.610 to 0.816 and RMSEs from 20.93 to 28.68 g\/m3, respectively. In particular, the proposed GRNN model obtains the best performance. Furthermore, the mapped PM2.5 distribution retrieved from 3-km MODIS aerosol optical depth products, agrees quite well with the station measurements. The results also show that our study has the capacity to provide reasonable information for the global monitoring of PM2.5 pollution in China.",0.1174266329],["the polynomial-time 3\/2-approximation algorithm was presented by Christ","A historical note on the 3\/2-approximation algorithm for the metric traveling salesman problem","summarize: One of the most fundamental results in combinatorial optimization is the polynomial-time 3\/2-approximation algorithm for the metric traveling salesman problem. It was presented by Christofides in 1976 and is well known as the Christofides algorithm. Recently, some authors started calling it Christofides-Serdyukov algorithm, pointing out that it was published independently in the USSR in 1978. We provide some historic background on Serdyukov's findings and a translation of his article from Russian into English.",0.2007230357],["optimisation technique employs Monte-Carlo Basin-Hopping. s","A global optimisation study of the low-lying isomers of the alumina octomer ","summarize: We employ the Monte-Carlo Basin-Hopping global optimisation technique with inter- atomic pair potentials to generate low-energy candidates of stoichiometric alumina octomers ",0.1730017911],["the detuning mechanism has been used to experimentally achieve silicon two-qubit quantum","Computational Assessment of Silicon Quantum Gate Based on Detuning Mechanism for Quantum Computing","summarize: Silicon-based quantum computing has the potential advantages of low cost, high integration density, and compatibility with CMOS technologies. The detuning mechanism has been used to experimentally achieve silicon two-qubit quantum gates and programmable quantum processors. In this paper, the scaling behaviors and variability issues are explored by numerical device simulations of a model silicon quantum gate based on the detuning mechanism. The device physics of quantum gates modulation, tradeoff between device speed and quantum fidelity, and impact of variability on the implementation of a quantum algorithm are examined. The results indicate the attractive potential to achieve high speed and fidelity silicon quantum gates with a low operation voltage. To scale up, reducing the device variability and mitigating the variability effect are identified to be indispensable for reliable implementing a quantum computing algorithm with the silicon quantum gates based on the detuning mechanism. A scheme to use the control electronics for mitigating the variability of quantum gates is proposed.",0.0],["the double descent risk curve was proposed to qualitatively describe the out-of-sample","Two models of double descent for weak features","summarize: The double descent risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares\/least norm predictor. Specifically, it is shown that the risk peaks when the number of features ",0.1428571429],["human cooperation does not require sheer computational power, but rather relies on intuition, cultural norm","Cooperating with Machines","summarize: Since Alan Turing envisioned Artificial Intelligence , a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks , or defeating humans in strategic zero-sum encounters . In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition , cultural norms , emotions and signals , and pre-evolved dispositions toward cooperation , common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",0.0],["coaxial nanowires are designed to propagate the surface plasmon polaritons","Coaxial nanowires as plasmon-mediated remote nanosensors","summarize: This study reports on the plasmon-mediated remote Raman sensing promoted by specially designed coaxial nanowires. This unusual geometry for Raman study is based on the separation, by several micrometres, of the excitation laser spot, on one tip of the nanowire, and the Raman detection at the other tip. The very weak efficiency of Raman emission makes it challenging in a remote configuration. For the proof-of-concept, we designed coaxial nanowires consisting in a gold core to propagate the surface plasmon polaritons and a Raman-emitting shell of poly. The success of the fabrication was demonstrated by correlating, for the same single nanowire, a morphological analysis by electron microscopy and Raman spectroscopy analysis. Importantly for probing remote-Raman effect, the original hard template-based process allows to control the location of the polymer shell all along the nanowire, or only close to one or the two nanowire tips. Such all-in-one single nanowires could have applications in the remote detection of photo-degradable substances and for exploring 1D nanosources for integrated photonic and plasmonic systems.",0.1818181818],["emph for a graph for a graph.","Sequences of radius ","summarize: A \\emph for a graph ",0.1176470588],["the PAMELA cosmic ray detector was launched on the Russian satellite in 2006. it","Ten Years of PAMELA in Space","summarize: The PAMELA cosmic ray detector was launched on June 15th 2006 on board the Russian Resurs-DK1 satellite, and during ten years of nearly continuous data-taking it has observed new interesting features in cosmic rays . In a decade of operation it has provided plenty of scientific data, covering different issues related to cosmic ray physics. Its discoveries might change our basic vision of the mechanisms of production, acceleration and propagation of cosmic rays in the Galaxy. The antimatter measurements, focus of the experiment, have set strong constraints to the nature of Dark Matter. Search for signatures of more exotic processes was also pursued. Furthermore, the long-term operation of the instrument had allowed a constant monitoring of the solar activity during its maximum and a detailed and prolonged study of the solar modulation, improving the comprehension of the heliosphere mechanisms. PAMELA had also measured the radiation environment around the Earth, and it detected for the first time the presence of an antiproton radiation belt surrounding our planet. The operation of Resurs-DK1 was terminated in 2016. In this article we will review the main features of the PAMELA instrument and its constructing phases. Main part of the article will be dedicated to the summary of the most relevant PAMELA results over a decade of observation",0.1176470588],["natural language processing is a branch of computer science that combines artificial intelligence with linguistics","Challenges Encountered in Turkish Natural Language Processing Studies","summarize: Natural language processing is a branch of computer science that combines artificial intelligence with linguistics. It aims to analyze a language element such as writing or speaking with software and convert it into information. Considering that each language has its own grammatical rules and vocabulary diversity, the complexity of the studies in this field is somewhat understandable. For instance, Turkish is a very interesting language in many ways. Examples of this are agglutinative word structure, consonant\/vowel harmony, a large number of productive derivational morphemes , derivation and syntactic relations, a complex emphasis on vocabulary and phonological rules. In this study, the interesting features of Turkish in terms of natural language processing are mentioned. In addition, summary info about natural language processing techniques, systems and various sources developed for Turkish are given.",0.25],["krypton produces high-order harmonic generation based on a low-repet","Compact 200 kHz HHG source driven by a few-cycle OPCPA","summarize: We present efficient high-order harmonic generation based on a high-repetition rate, few-cycle, near infrared , carrier-envelope phase stable, optical parametric chirped pulse amplifier , emitting 6fs pulses with 9J pulse energy at 200kHz repetition rate. In krypton, we reach conversion efficiencies from the NIR to the extreme ultraviolet radiation pulse energy on the order of ~10^ with less than 3J driving pulse energy. This is achieved by optimizing the OPCPA for a spatially and temporally clean pulse and by a specially designed high-pressure gas target. In the future, the high efficiency of the HHG source will be beneficial for high-repetition rate two-colour pumpprobe experiments, where the available pulse energy from the laser has to be distributed economically between pump and probe pulses.",0.1193119089],["reID methods focus on learning discriminative features but robust to only a particular factor of","Learning Disentangled Representation for Robust Person Re-identification","summarize: We address the problem of person re-identification , that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations , which requires corresponding supervisory signals . To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person , while identity-unrelated ones hold other factors . To this end, we introduce a new generative adversarial network, dubbed \\emph , that factorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID. Our code and models are available online: https:\/\/cvlab-yonsei.github.io\/projects\/ISGAN\/.",0.2608695652],["multi-patterns are a combinatorial abstraction of polyphonic musical phrases","Generation of musical patterns through operads","summarize: We introduce the notion of multi-pattern, a combinatorial abstraction of polyphonic musical phrases. The interest of this approach lies in the fact that this offers a way to compose two multi-patterns in order to produce a longer one. This dives musical phrases into an algebraic context since the set of multi-patterns has the structure of an operad; operads being structures offering a formalization of the notion of operators and their compositions. Seeing musical phrases as operators allows us to perform computations on phrases and admits applications in generative music: given a set of short patterns, we propose various algorithms to randomly generate a new and longer phrase inspired by the inputted patterns.",0.3529411765],["proposed algorithm is based on the least mean square algorithm. the algorithm is based on","Training Symbol-Based Equalization for Quadrature Duobinary PDM-FTN Systems","summarize: A training symbol-based equalization algorithm is proposed for polarization de-multiplexing in quadrature duobinary modulated polarization division multiplexedfaster-than-Nyquist coherent optical systems. The proposed algorithm is based on the least mean square algorithm, and multiple location candidates of a symbol are considered in order to make use of the training symbols with QDB modulation.Results show that an excellent convergence performance is obtained using the proposed algorithm under different polarization alignment scenarios. The optical signal-to-noise ratio required to attain a bit error rate of 2*10-2 is reduced by 1.7 and 1.8 dB using the proposed algorithm, compared to systems using the constant modulus algorithm with differential coding for 4-ary quadrature amplitude modulation and 16-QAM systems with symbol-by-symbol detection, respectively.Furthermore, comparisons with the Tomlinson-Harashima precoding-based FTN systems illustrate that QDB is preferable when 4-QAM is utilized.",0.0689655172],["coupling is based on a multiplicative decomposition of the surface deformation","The multiplicative deformation split for shells with application to growth, chemical swelling, thermoelasticity, viscoelasticity and elastoplasticity","summarize: This work presents a general unified theory for coupled nonlinear elastic and inelastic deformations of curved thin shells. The coupling is based on a multiplicative decomposition of the surface deformation gradient. The kinematics of this decomposition is examined in detail. In particular, the dependency of various kinematical quantities, such as area change and curvature, on the elastic and inelastic strains is discussed. This is essential for the development of general constitutive models. In order to fully explore the coupling between elastic and different inelastic deformations, the surface balance laws for mass, momentum, energy and entropy are examined in the context of the multiplicative decomposition. Based on the second law of thermodynamics, the general constitutive relations are then derived. Two cases are considered: Independent inelastic strains, and inelastic strains that are functions of temperature and concentration. The constitutive relations are illustrated by several nonlinear examples on growth, chemical swelling, thermoelasticity, viscoelasticity and elastoplasticity of shells. The formulation is fully expressed in curvilinear coordinates leading to compact and elegant expressions for the kinematics, balance laws and constitutive relations.",0.4107118005],["the authors describe a case study of the migration of an interactive diagramming tool written in","Lessons Learned in Migrating from Swing to JavaFX","summarize: The authors describe a case study of the migration of an interactive diagramming tool written in Java from the Swing Graphical User Interface framework to the more recent JavaFX framework. The study distills the authors' experience identifying what information was needed to support the migration effort, and how the information was ultimately discovered. The outcome is presented in a set of five lessons about the discrepancies between expectations and reality in the search for information when migrating software between major frameworks.",0.2413793103],["smart contracts are self-enforcing pieces of software, which reside and run over","Empirical Vulnerability Analysis of Automated Smart Contracts Security Testing on Blockchains","summarize: The emerging blockchain technology supports decentralized computing paradigm shift and is a rapidly approaching phenomenon. While blockchain is thought primarily as the basis of Bitcoin, its application has grown far beyond cryptocurrencies due to the introduction of smart contracts. Smart contracts are self-enforcing pieces of software, which reside and run over a hosting blockchain. Using blockchain-based smart contracts for secure and transparent management to govern interactions in Internet-enabled environments, mostly IoT, is a niche area of research and practice. However, writing trustworthy and safe smart contracts can be tremendously challenging because of the complicated semantics of underlying domain-specific languages and its testability. There have been high-profile incidents that indicate blockchain smart contracts could contain various code-security vulnerabilities, instigating financial harms. When it involves security of smart contracts, developers embracing the ability to write the contracts should be capable of testing their code, for diagnosing security vulnerabilities, before deploying them to the immutable environments on blockchains. However, there are only a handful of security testing tools for smart contracts. This implies that the existing research on automatic smart contracts security testing is not adequate and remains in a very stage of infancy. With a specific goal to more readily realize the application of blockchain smart contracts in security and privacy, we should first understand their vulnerabilities before widespread implementation. Accordingly, the goal of this paper is to carry out a far-reaching experimental assessment of current static smart contracts security testing tools, for the most widely used blockchain, the Ethereum and its domain-specific programming language, Solidity to provide the first...",0.0714285714],["flow-based learning framework provides detailed expression of learning process. a subgraph can be","Interpretable Graph-Based Semi-Supervised Learning via Flows","summarize: In this paper, we consider the interpretability of the foundational Laplacian-based semi-supervised learning approaches on graphs. We introduce a novel flow-based learning framework that subsumes the foundational approaches and additionally provides a detailed, transparent, and easily understood expression of the learning process in terms of graph flows. As a result, one can visualize and interactively explore the precise subgraph along which the information from labeled nodes flows to an unlabeled node of interest. Surprisingly, the proposed framework avoids trading accuracy for interpretability, but in fact leads to improved prediction accuracy, which is supported both by theoretical considerations and empirical results. The flow-based framework guarantees the maximum principle by construction and can handle directed graphs in an out-of-the-box manner.",0.2272727273],["model is constructed to describe the black hole enclosed in dust cosmological background. it is","The model of the black hole enclosed in dust. The flat space case","summarize: In this work the model is constructed to describe the black hole enclosed in the dust cosmological background in case of zero spatial curvature. This model is based on our exact solution of the class of LTB inhomogeneous solutions. We considered the properties of the model and built the R-T-structure of the resulting space-time. It was shown that central region includes the Schwarzchild-like black hole. We derived the equations of motion of the test particle from the point of view of the observer comoving with cosmological expansion. We found analytical expressions for observable orbital and radial velocities of the particle and plotted the surface profile of the total velocity in this case. In comoving coordinate frame it is impossible to study the questions concerning the black hole horizon but one can observe the local motion of the particles influenced by the cosmological expansion.",0.4117647059],["triboelectrification in conducting materials can be explained by electron transfer between different","Atomistic Field Theory for contact electrification of dielectrics","summarize: The triboelectrification of conducting materials can be explained by electron transfer between different Fermi levels. However, triboelectrification in dielectrics is poorly understood. The surface dipole formations are shown to be caused by the contact-induced surface lattice deformations. An Atomistic Field Theory based formulation is utilized to calculate the distribution of the polarization, electric and potential fields. The induced fields are considered as the driving force for charge transfer. The simulation results show that a MgO\/BaTiO3 tribopair can generate up to 104 V\/cm^2, which is comprable with the data in the published literature.",0.0],["matrix metalloproteinase modulating functionality is highly desirable. the introduction of","A hydroxamic acid-methacrylated collagen conjugate for the modulation of inflammation-related MMP upregulation","summarize: Medical devices with matrix metalloproteinase modulating functionality are highly desirable to restore tissue homeostasis in critical inflammation states, such as chronic wounds, rotator cuff tears and cancer. The introduction of MMP-modulating functionality in such devices is typically achieved via loading of either rapidly-diffusing chelating factors, e.g. EDTA, or MMP-cleavable substrates, raising issues in terms of non-controllable pharmacokinetics and enzymatic degradability, respectively. Aiming to accomplish inherent, long-term, device-induced MMP regulation, this study investigated the synthesis of a hydroxamic acid -methacrylated collagen conjugate as the building block of a soluble factor-free MMP-modulating hydrogel network with controlled enzymatic degradability. This was realised via a two-step synthetic route: type I collagen was functionalised with photonetwork-inducing methacrylic anhydride adducts; this methacrylated product was activated with a water-soluble carbodiimide prior to reaction with hydroxylamine, resulting in MMP-chelating HA functions. Nearly-quantitative methacrylation of collagen amines was observed via 2,4,6-trinitrobenzenesulfonic acid assay. The molar content of HA adducts was indirectly quantified via conversion of remaining carboxyl functions into ethylenediamine , so that 12-16 mol.% HA was revealed in the conjugate. Resulting UV cured, HA-bearing collagen hydrogels proved to induce up to ~13 and ~32 RFU% activity reduction of MMP-9 and MMP-3, respectively, following 4-day incubation in vitro. No hydrogel-induced toxic response was observed following 4-day culture of G292 cells. The novel synthetic strategies described in this work provide a new insight into the systematic chemical manipulation of collagen materials aiming at the design of biomimetic, inflammation-responsive medical devices.",0.1637461506],["study of algorithmic complexity of election control is known as election control. it is proposed to","Election Control by Manipulating Issue Significance","summarize: Integrity of elections is vital to democratic systems, but it is frequently threatened by malicious actors. The study of algorithmic complexity of the problem of manipulating election outcomes by changing its structural features is known as election control. One means of election control that has been proposed is to select a subset of issues that determine voter preferences over candidates. We study a variation of this model in which voters have judgments about relative importance of issues, and a malicious actor can manipulate these judgments. We show that computing effective manipulations in this model is NP-hard even with two candidates or binary issues. However, we demonstrate that the problem is tractable with a constant number of voters or issues. Additionally, while it remains intractable when voters can vote stochastically, we exhibit an important special case in which stochastic voting enables tractable manipulation.",0.0],["a new algorithm of label propagation with Augmented Anchors could improve LP.","Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation","summarize: Motivated by the problem relatedness between unsupervised domain adaptation and semi-supervised learning , many state-of-the-art UDA methods adopt SSL principles as their learning ingredients. However, they tend to overlook the very domain-shift nature of UDA. In this work, we take a step further to study the proper extensions of SSL techniques for UDA. Taking the algorithm of label propagation as an example, we analyze the challenges of adopting LP to UDA and theoretically analyze the conditions of affinity graph\/matrix construction in order to achieve better propagation of true labels to unlabeled instances. Our analysis suggests a new algorithm of Label Propagation with Augmented Anchors , which could potentially improve LP via generation of unlabeled virtual instances with high-confidence label predictions. To make the proposed A",0.4232408624],["the aleksandrov-Bakelman-Pucci estimate is applied to the","The Aleksandrov-Bakelman-Pucci estimate and the Calabi-Yau equation","summarize: We give two applications of the Aleksandrov-Bakelman-Pucci estimate to the Calabi-Yau equation on symplectic four-manifolds. The first is solvability of the equation on the Kodaira-Thurston manifold for certain almost-Kahler structures assuming ",0.3333333333],["photonic graphene has a honeycomb structure. the elliptic operator","Wave packet dynamics in slowly modulated photonic graphene","summarize: Mathematical analysis on electromagnetic waves in photonic graphene, a photonic topological material which has a honeycomb structure, is one of the most important current research topics. By modulating the honeycomb structure, numerous topological phenomena have been observed recently. The electromagnetic waves in such a media are generally described by the 2-dimensional wave equation. It has been shown that the corresponding elliptic operator with a honeycomb material weight has Dirac points in its dispersion surfaces. In this paper, we study the time evolution of the wave packets spectrally concentrated at such Dirac points in a modulated honeycomb material weight. We prove that such wave packet dynamics is governed by the Dirac equation with a varying mass in a large but finite time. Our analysis provides mathematical insights to those topological phenomena in photonic graphene.",0.5],["the thermal effects give rise to new waves and generate couplings with longitudinal waves which are not","On the dispersion of waves for the linear thermoelastic relaxed micromorphic model","summarize: We present the complete set of constitutive relations and field equations for the linear thermoelastic relaxed micromorphic continuum and investigate its variants for wave propagation. It is found that the additional thermal effects give rise to new waves and generate couplings with longitudinal waves which are not existing in the relaxed micromorphic continuum without thermal effects. However, transverse waves go un-affected by the thermal properties. The dispersion curves have been computed numerically for a particular model and comparison is made with the result obtained previously.",0.2],["general model contains many existing plane-based clustering methods. the general model is a","A general model for plane-based clustering with loss function","summarize: In this paper, we propose a general model for plane-based clustering. The general model contains many existing plane-based clustering methods, e.g., k-plane clustering , proximal plane clustering , twin support vector clustering and its extensions. Under this general model, one may obtain an appropriate clustering method for specific purpose. The general model is a procedure corresponding to an optimization problem, where the optimization problem minimizes the total loss of the samples. Thereinto, the loss of a sample derives from both within-cluster and between-cluster. In theory, the termination conditions are discussed, and we prove that the general model terminates in a finite number of steps at a local or weak local optimal point. Furthermore, based on this general model, we propose a plane-based clustering method by introducing a new loss function to capture the data distribution precisely. Experimental results on artificial and public available datasets verify the effectiveness of the proposed method.",0.375],["the optimal overlap, circulant power optimizer approach was introduced to construct high performance SC codes","A Channel-Aware Combinatorial Approach to Design High Performance Spatially-Coupled Codes for Magnetic Recording Systems","summarize: Because of their capacity-approaching performance and their complexity\/latency advantages, spatially-coupled codes are among the most attractive error-correcting codes for use in modern dense data storage systems. SC codes are constructed by partitioning an underlying block code and coupling the partitioned components. Here, we focus on circulant-based SC codes. Recently, the optimal overlap , circulant power optimizer approach was introduced to construct high performance SC codes for additive white Gaussian noise and Flash channels. The OO stage operates on the protograph of the SC code to derive the optimal partitioning that minimizes the number of graphical objects that undermine the performance of SC codes under iterative decoding. Then, the CPO optimizes the circulant powers to further reduce this number. Since the nature of detrimental objects in the graph of a code critically depends on the characteristics of the channel of interest, extending the OO-CPO approach to construct SC codes for channels with intrinsic memory is not a straightforward task. In this paper, we tackle one relevant extension; we construct high performance SC codes for practical 1-D magnetic recording channels, i.e., partial-response channels. Via combinatorial techniques, we carefully build and solve the optimization problem of the OO partitioning, focusing on the objects of interest in the case of PR channels. Then, we customize the CPO to further reduce the number of these objects in the graph of the code. SC codes designed using the proposed OO-CPO approach for PR channels outperform prior state-of-the-art SC codes by up to around 3 orders of magnitude in frame error rate and 1.1 dB in signal-to-noise ratio . More intriguingly, our SC codes outperform structured block codes of the same length and rate by up to around 1.8 orders of magnitude in FER and 0.4 dB in SNR.",0.0666666667],["characterization of the RF effect of memory switching on Nb-Al\/Al","Properties of ferromagnetic Josephson junctions for memory applications","summarize: In this work we give a characterization of the RF effect of memory switching on Nb-Al\/AlOx--Pd",0.3846153846],["sums involving a large number of free space Green's functions appear in boundary integral","Fast Ewald summation for free-space Stokes potentials","summarize: We present a spectrally accurate method for the rapid evaluation of free-space Stokes potentials, i.e. sums involving a large number of free space Green's functions. We consider sums involving stokeslets, stresslets and rotlets that appear in boundary integral methods and potential methods for solving Stokes equations. The method combines the framework of the Spectral Ewald method for periodic problems, with a very recent approach to solving the free-space harmonic and biharmonic equations using fast Fourier transforms on a uniform grid. Convolution with a truncated Gaussian function is used to place point sources on a grid. With precomputation of a scalar grid quantity that does not depend on these sources, the amount of oversampling of the grids with Gaussians can be kept at a factor of two, the minimum for aperiodic convolutions by FFTs. The resulting algorithm has a computational complexity of O for problems with N sources and targets. Comparison is made with a fast multipole method to show that the performance of the new method is competitive.",0.3043478261],["moTe2 involves attractive polymorphic TMD crystals. they can exist in","Mechanical responses of two-dimensional MoTe2; pristine 2H, 1T and 1T' and 1T'\/2H heterostructure","summarize: Transition metal dichalcogenides are currently among the most interesting two-dimensional materials due to their outstanding properties. MoTe2 involves attractive polymorphic TMD crystals which can exist in three different 2D atomic lattices of 2H, 1T and 1T', with diverse properties, like semiconducting and metallic electronic characters. Using the polymorphic heteroepitaxy, most recently coplanar semiconductor\/metal few-layer MoTe2 heterostructures were experimentally synthesized, highly promising to build circuit components for next generation nanoelectronics. Motivated by the recent experimental advances, we conducted first-principles calculations to explore the mechanical properties of single-layer MoTe2 structures. We first studied the mechanical responses of pristine and single-layer 2H-, 1T- and 1T'-MoTe2. In these cases we particularly analyzed the possibility of engineering of the electronic properties of these attractive 2D structures using the biaxial or uniaxial tensile loadings. Finally, the mechanical-failure responses of 1T'\/2H-MoTe2 heterostructure were explored, which confirms the remarkable strength of this novel 2D system.",0.067347111],["the determination of the gravitofluid-static field required as initial field in forthcoming fluid-","Fluid statics of a self-gravitating perfect-gas isothermal sphere","summarize: We open the paper with introductory considerations describing the motivations of our long-term research plan targeting gravitomagnetism, illustrating the fluid-dynamics numerical test case selected for that purpose, that is, a perfect-gas sphere contained in a solid shell located in empty space sufficiently away from other masses, and defining the main objective of this study: the determination of the gravitofluid-static field required as initial field in forthcoming fluid-dynamics calculations. The determination of the gravitofluid-static field requires the solution of the isothermal-sphere Lane-Emden equation. We do not follow the habitual approach of the literature based on the prescription of the central density as boundary condition; we impose the gravitational field at the solid-shell internal wall. As the discourse develops, we point out differences and similarities between the literature's and our approach. We show that the nondimensional formulation of the problem hinges on a unique physical characteristic number that we call gravitational number because it gauges the self-gravity effects on the gas' fluid statics. We illustrate and discuss numerical results; some peculiarities, such as gravitational-number upper bound and multiple solutions, lead us to investigate the thermodynamics of the physical system, particularly entropy and energy, and preliminarily explore whether or not thermodynamic-stability reasons could provide justification for either selection or exclusion of multiple solutions. We close the paper with a summary of the present study in which we draw conclusions and describe future work.",0.2272727273],["cascade approach to generate talking face video is robust to different face shapes, view angles, facial","Hierarchical Cross-Modal Talking Face Generationwith Dynamic Pixel-Wise Loss","summarize: We devise a cascade GAN approach to generate talking face video, which is robust to different face shapes, view angles, facial characteristics, and noisy audio conditions. Instead of learning a direct mapping from audio to video frames, we propose first to transfer audio to high-level structure, i.e., the facial landmarks, and then to generate video frames conditioned on the landmarks. Compared to a direct audio-to-image approach, our cascade approach avoids fitting spurious correlations between audiovisual signals that are irrelevant to the speech content. We, humans, are sensitive to temporal discontinuities and subtle artifacts in video. To avoid those pixel jittering problems and to enforce the network to focus on audiovisual-correlated regions, we propose a novel dynamically adjustable pixel-wise loss with an attention mechanism. Furthermore, to generate a sharper image with well-synchronized facial movements, we propose a novel regression-based discriminator structure, which considers sequence-level information along with frame-level information. Thoughtful experiments on several datasets and real-world samples demonstrate significantly better results obtained by our method than the state-of-the-art methods in both quantitative and qualitative comparisons.",0.05],["we analyse a group of participants with more than 10,000 participants. we find that engagement pe","Is this pofma? Analysing public opinion and misinformation in a COVID-19 Telegram group chat","summarize: We analyse a Singapore-based COVID-19 Telegram group with more than 10,000 participants. First, we study the group's opinion over time, focusing on four dimensions: participation, sentiment, topics, and psychological features. We find that engagement peaked when the Ministry of Health raised the disease alert level, but this engagement was not sustained. Second, we search for government-identified misinformation in the group. We find that government-identified misinformation is rare, and that messages discussing these pieces of misinformation express skepticism.",0.3103448276],["meshfree solutions for incompressible Navier--Stokes equations are usually","On Meshfree GFDM Solvers for the Incompressible Navier-Stokes Equations","summarize: Meshfree solution schemes for the incompressible Navier--Stokes equations are usually based on algorithms commonly used in finite volume methods, such as projection methods, SIMPLE and PISO algorithms. However, drawbacks of these algorithms that are specific to meshfree methods have often been overlooked. In this paper, we study the drawbacks of conventionally used meshfree Generalized Finite Difference Method~ schemes for Lagrangian incompressible Navier-Stokes equations, both operator splitting schemes and monolithic schemes. The major drawback of most of these schemes is inaccurate local approximations to the mass conservation condition. Further, we propose a new modification of a commonly used monolithic scheme that overcomes these problems and shows a better approximation for the velocity divergence condition. We then perform a numerical comparison which shows the new monolithic scheme to be more accurate than existing schemes.",0.1103121128],["holograms are recorded by directive antennas aligned along each other's bore","Early Detection of Cancerous Tissues in Human Breast utilizing Near field Microwave Holography","summarize: This work demonstrates an application of near field indirect microwave holography for the detection of malignant tissues in the human breast in an effective way. The holograms are recorded by two directive antennas aligned along each other's boresight while performing a raster scan over a 2D plane utilizing XY-linear motorized translation stage and a uniform reference wave. The whole information i.e. amplitude and phase of an object has been provided by indirect holography at microwave frequencies. The extracted phase values are used to determine the dielectric permittivity values which are further utilized for the identification and validating the positions of malignant tissues in the breast phantom. The experimental evaluations performed on the in-house designed and developed tissue mimicking 3D printed breast phantoms. The experimental results demonstrate the ability of microwave holography using directive antennas in locating and identifying the tumors up to the minimum size of 4mm and a maximum depth of 25mm in fabricated phantom. The preliminary results present the potential of the Near Field Indirect Holographic Imaging in order to develop an efficient and economical tool for breast cancer detection.",0.0],["the impact of imperfect channel state information on a downlink coordinated multipoint transmission system is investigated","Exploiting non-orthogonal multiple access in downlink coordinated multipoint transmission with the presence of imperfect channel state information","summarize: In this paper, the impact of imperfect channel state information on a downlink coordinated multipoint transmission system with non-orthogonal multiple access is investigated since perfect knowledge of a channel can not be guaranteed in practice. Furthermore, the channel estimation error is applied to estimate the channel information wherein its priori of variance is assumed to be known. The impact of the number of coordinated base stations on downlink CoMP NOMA is investigated. Users are classified into one of two groups according to their position within the cell, namely cell-center user and cell-edge user . In this paper, ergodic capacity and sum capacity for both CCU and CEU are derived as closed form. In addition, various experiments are conducted with different parameters such as SNR, error variance, and power allocation to show their impact on the CoMP method. The results show that CoMP NOMA outperforms the CoMP orthogonal multiple access wherein the condition of the channel impacts the performance of CoMP NOMA less. It is worth noting that a higher number of coordinated BSs enhances the total capacity of CoMP NOMA. Finally, the performance analysis is validated due to the close accordance between the analytical and simulation results.",0.7587567046],["the Feynman propagator of a causal set contains the complete information about the causal","Towards Spectral Geometry for Causal Sets","summarize: We show that the Feynman propagator of a causal set contains the complete information about the causal set. Intuitively, this is because the Feynman propagator, being a correlator that decays with distance, provides a measure for the invariant distance between pairs of events. Further, we show that even the spectra alone of the propagator and d'Alembertian already carry large amounts of geometric information about their causal set. This geometric information is basis independent and also gauge invariant in the sense that it is relabeling invariant . We provide numerical evidence that the associated spectral distance between causal sets can serve as a measure for the geometric similarity between causal sets.",0.125],["Given a fusion system, the system is a fusion system.","Cohomology of infinite groups realizing fusion systems","summarize: Given a fusion system ",0.3333333333],["we develop a non-cooperative game-theoretic model for graph minor-e","Compiling Adiabatic Quantum Programs","summarize: We develop a non-cooperative game-theoretic model for the problem of graph minor-embedding to show that optimal compiling of adiabatic quantum programs in the sense of Nash equilibrium is possible.",0.3333333333],["a widely studied game that plays an important role in Game Theory. a numerical computation","Numerical Asymptotic Results in Game Theory Using Sergeyev's Infinity Computing","summarize: Prisoner's Dilemma is a widely studied game that plays an important role in Game Theory. This paper aims at extending PD Tournaments to the case of infinite, finite or infinitesimal payoffs using Sergeyev's Infinity Computing . By exploiting IC, we are able to show the limits of the classical approach to PD Tournaments analysis of the classical theory, extending both the sets of the feasible and numerically computable tournaments. In particular we provide a numerical computation of the exact outcome of a simple PD Tournament where one player meets every other an infinite number of times, for both its deterministic and stochastic formulations.",0.2058823529],["hydraulic conductivity is a nonlinear function of pressure head. a vadose","A numerical method for efficient 3D inversions using Richards equation","summarize: Fluid flow in the vadose zone is governed by Richards equation; it is parameterized by hydraulic conductivity, which is a nonlinear function of pressure head. Investigations in the vadose zone typically require characterizing distributed hydraulic properties. Saturation or pressure head data may include direct measurements made from boreholes. Increasingly, proxy measurements from hydrogeophysics are being used to supply more spatially and temporally dense data sets. Inferring hydraulic parameters from such datasets requires the ability to efficiently solve and deterministically optimize the nonlinear time domain Richards equation. This is particularly important as the number of parameters to be estimated in a vadose zone inversion continues to grow. In this paper, we describe an efficient technique to invert for distributed hydraulic properties in 1D, 2D, and 3D. Our algorithm does not store the Jacobian, but rather computes the product with a vector, which allows the size of the inversion problem to become much larger than methods such as finite difference or automatic differentiation; which are constrained by computation and memory, respectively. We show our algorithm in practice for a 3D inversion of saturated hydraulic conductivity using saturation data through time. The code to run our examples is open source and the algorithm presented allows this inversion process to run on modest computational resources.",0.2857142857],["a portfolio that includes an illiquid asset is an important problem of modern mathematical finance","Optimization problem for a portfolio with an illiquid asset: Lie group analysis","summarize: Management of a portfolio that includes an illiquid asset is an important problem of modern mathematical finance. One of the ways to model illiquidity among others is to build an optimization problem and assume that one of the assets in a portfolio can not be sold until a certain finite, infinite or random moment of time. This approach arises a certain amount of models that are actively studied at the moment. Working in the Merton's optimal consumption framework with continuous time we consider an optimization problem for a portfolio with an illiquid, a risky and a risk-free asset. Our goal in this paper is to carry out a complete Lie group analysis of PDEs describing value function and investment and consumption strategies for a portfolio with an illiquid asset that is sold in an exogenous random moment of time with a prescribed liquidation time distribution. The problem of such type leads to three dimensional nonlinear Hamilton-Jacobi-Bellman equations. Such equations are not only tedious for analytical methods but are also quite challenging form a numeric point of view. To reduce the three-dimensional problem to a two-dimensional one or even to an ODE one usually uses some substitutions, yet the methods used to find such substitutions are rarely discussed by the authors. We find the admitted Lie algebra for a broad class of liquidation time distributions in cases of HARA and log utility functions and formulate corresponding theorems for all these cases. We use found Lie algebras to obtain reductions of the studied equations. Several of similar substitutions were used in other papers before whereas others are new to our knowledge. This method gives us the possibility to provide a complete set of non-equivalent substitutions and reduced equations.",0.5],["graph theory and markov chains are explored. we analyze the topological dynamics for semiflow","Topological dynamics on finite directed graphs","summarize: In this work we establish that finite directed graphs give rise to semiflows on the power set of their nodes. We analyze the topological dynamics for semiflows on finite directed graphs by characterizing Morse decompositions, recurrence behavior and attractor-repeller pairs under weaker assumptions. As is expected, the discrete metric plays an important role in our constructions and their consequences. The connections between the semiflow, graph theory and Markov chains are here explored. We lay the foundation for a dynamical systems approach to hybrid systems with Markov chain type perturbations.",0.1333333333],["collision energy is related to the chemical potential used in the thermal - statistical models that assume approximate","Flow and vorticity with varying chemical potential in relativistic heavy ion collisions","summarize: We study the vorticity patterns in relativistic heavy ion collisions with respect to the collision energy. The collision energy is related to the chemical potential used in the thermal - statistical models that assume approximate chemical equilibrium after the relativistic collision. We use the multiphase transport model to study the vorticity in the initial parton phase as well as the final hadronic phase of the relativistic heavy ion collision. We find that as the chemical potential increases,the vortices are larger in size. Using different definitions of vorticity, we find that vorticity plays a greater role at lower collision energies than at higher collision energies. We also look at other effects of the flow patterns related to the bulk viscosity and the shear viscosity at different collision energies. We find that the shear viscosity obtained is almost a constant with a small decrease at higher collision energies. We also look at the elliptic flow as it is related to viscous effects in the final stages after the collision. Our results indicate that viscosity plays a greater role at higher chemical potential and lower collision energies.",0.25],["in-vehicle human object identification plays an important role in vision-based automated vehicle","In-Vehicle Object Detection in the Wild for Driverless Vehicles","summarize: In-vehicle human object identification plays an important role in vision-based automated vehicle driving systems while objects such as pedestrians and vehicles on roads or streets are the primary targets to protect from driverless vehicles. A challenge is the difficulty to detect objects in moving under the wild conditions, while illumination and image quality could drastically vary. In this work, to address this challenge, we exploit Deep Convolutional Generative Adversarial Networks with Single Shot Detector to handle with the wild conditions. In our work, a GAN was trained with low-quality images to handle with the challenges arising from the wild conditions in smart cities, while a cascaded SSD is employed as the object detector to perform with the GAN. We used tested our approach under wild conditions using taxi driver videos on London street in both daylight and night times, and the tests from in-vehicle videos demonstrate that this strategy can drastically achieve a better detection rate under the wild conditions.",0.0625],["the Wasserstein metric or earth mover's distance is a useful tool in statistics","DOTmark - A Benchmark for Discrete Optimal Transport","summarize: The Wasserstein metric or earth mover's distance is a useful tool in statistics, machine learning and computer science with many applications to biological or medical imaging, among others. Especially in the light of increasingly complex data, the computation of these distances via optimal transport is often the limiting factor. Inspired by this challenge, a variety of new approaches to optimal transport has been proposed in recent years and along with these new methods comes the need for a meaningful comparison. In this paper, we introduce a benchmark for discrete optimal transport, called DOTmark, which is designed to serve as a neutral collection of problems, where discrete optimal transport methods can be tested, compared to one another, and brought to their limits on large-scale instances. It consists of a variety of grayscale images, in various resolutions and classes, such as several types of randomly generated images, classical test images and real data from microscopy. Along with the DOTmark we present a survey and a performance test for a cross section of established methods ranging from more traditional algorithms, such as the transportation simplex, to recently developed approaches, such as the shielding neighborhood method, and including also a comparison with commercial solvers.",0.3],["graphene is an ideal solid-state material platform to realize an electronic device based on","Graphene Transistor Based on Tunable Dirac-Fermion-Optics","summarize: The linear energy-momentum dispersion, coupled with pseudo-spinors, makes graphene an ideal solid-state material platform to realize an electronic device based on Dirac-Fermionic relativistic quantum mechanics. Employing local gate control, several examples of electronic devices based on Dirac fermion dynamics have been demonstrated, including Klein tunneling, negative refraction and specular Andreev reflection. In this work, we present a quantum switch based on analogous Dirac-fermion-optics , in which the angle dependence of Klein tunneling is explicitly utilized to build tunable collimators and reflectors for the quantum wave function of Dirac fermions. We employ a novel dual-source design with a single flat reflector, which minimizes diffusive edge scattering and suppresses the background incoherent transmission. Our gate-tunable collimator-reflector device design enables measurement of the net DFO contribution in the switching device operation. We measure a full set of transmission coefficients of DFO wavefunction between multiple leads of the device, separating the classical contribution from that of any disorder in the channel. Since the DFO quantum switch demonstrated in this work requires no explicit energy gap, the switching operation is expected to be robust against thermal fluctuations and inhomogeneity length scales comparable to the Fermi wavelength. We find our quantum switch works at an elevated temperature up to 230 K and large bias current density up to 102 A\/m, over a wide range of carrier densities. The tunable collimator-reflector coupled with the conjugated source electrodes developed in this work provides an additional component to build more efficient DFO electronic devices.",0.3529411765],["geospatial datasets are split into chunks that are processed individually. geoRock","GeoRocket: A scalable and cloud-based data store for big geospatial files","summarize: We present GeoRocket, a software for the management of very large geospatial datasets in the cloud. GeoRocket employs a novel way to handle arbitrarily large datasets by splitting them into chunks that are processed individually. The software has a modern reactive architecture and makes use of existing services including Elasticsearch and storage back ends such as MongoDB or Amazon S3. GeoRocket is schema-agnostic and supports a wide range of heterogeneous geospatial file formats. It is also format-preserving and does not alter imported data in any way. The main benefits of GeoRocket are its performance, scalability, and usability, which make it suitable for a number of scientific and commercial use cases dealing with very high data volumes, complex datasets, and high velocity . GeoRocket also provides many opportunities for further research in the area of geospatial data management.",0.0769230769],["voltage phase angle measurements are used in distribution-level microgrid interconnections. these","Distributed Mixed Voltage Angle and Frequency Droop Control of Microgrid Interconnections with Loss of Distribution-PMU Measurements","summarize: Recent advances in distribution-level phasor measurement unit technology have enabled the use of voltage phase angle measurements for direct load sharing control in distribution-level microgrid interconnections with high penetration of renewable distributed energy resources . In particular, D-PMU enabled voltage angle droop control has the potential to enhance stability and transient performance in such microgrid interconnections. However, these angle droop control designs are vulnerable to D-PMU angle measurement losses that frequently occur due to the unavailability of a GPS signal for synchronization. In the event of such measurement losses, angle droop controlled microgrid interconnections may suffer from poor performance and potentially lose stability. In this paper, we propose a novel distributed mixed voltage angle and frequency droop control framework to improve the reliability of angle droop controlled microgrid interconnections. In this framework, when the D-PMU phase angle measurement is lost at a microgrid, conventional frequency droop control is temporarily used for primary control in place of angle droop control to guarantee stability. We model the microgrid interconnection with this primary control architecture as a nonlinear switched system and design distributed secondary controllers to guarantee transient stability of the network. Further, we incorporate performance specifications such as robustness to generation-load mismatch and network topology changes in the distributed control design. We demonstrate the performance of this control framework by simulation on a test 123-feeder distribution network.",0.0],["logical formalisms are a complex formalism, a model for problem solving","On the Existence of Characterization Logics and Fundamental Properties of Argumentation Semantics","summarize: Given the large variety of existing logical formalisms it is of utmost importance to select the most adequate one for a specific purpose, e.g. for representing the knowledge relevant for a particular application or for using the formalism as a modeling tool for problem solving. Awareness of the nature of a logical formalism, in other words, of its fundamental intrinsic properties, is indispensable and provides the basis of an informed choice. In this treatise we consider the existence characterization logics as well as properties like existence and uniqueness, expressibility, replaceability and verifiability in the realm of abstract argumentation",0.4764003737],["maximum coordinates are formulated in maximum coordinates. the algorithm does not suffer from constrain","Linear-Time Variational Integrators in Maximal Coordinates","summarize: Most dynamic simulation tools parameterize the configuration of multi-body robotic systems using minimal coordinates, also called generalized or joint coordinates. However, maximal-coordinate approaches have several advantages over minimal-coordinate parameterizations, including native handling of closed kinematic loops and nonholonomic constraints. This paper describes a linear-time variational integrator that is formulated in maximal coordinates. Due to its variational formulation, the algorithm does not suffer from constraint drift and has favorable energy and momentum conservation properties. A sparse matrix factorization technique allows the dynamics of a loop-free articulated mechanism with ",0.1428571429],["the ground-state of two-dimensional systems of classical particles interacting pairwisely is studied","Two-dimensional Wigner crystals of classical Lennard-Jones particles","summarize: The ground-state of two-dimensional systems of classical particles interacting pairwisely by the generalized Lennard-Jones potential is studied. Taking the surface area per particle ",0.2666666667],["argon isotopes in comet 67P\/Churyu","Krypton isotopes and noble gas abundances in the coma of comet 67P\/Churyumov-Gerasimenko","summarize: The ROSINA mass spectrometer DFMS on board ESA's Rosetta spacecraft detected the major isotopes of the noble gases argon, krypton, and xenon in the coma of comet 67P\/Churyumov-Gerasimenko. Earlier, it has been shown that xenon exhibits an isotopic composition distinct from anywhere else in the solar system. However, argon isotopes, within error, were shown to be consistent with solar isotope abundances. This discrepancy suggested an additional exotic component of xenon in comet 67P\/Churyumov-Gerasimenko. Here we show that also krypton exhibits an isotopic composition close to solar. Furthermore, we found a depletion compared to solar of argon with respect to krypton and of krypton with respect to xenon, which is a necessity to postulate an addition of exotic xenon in the comet.",0.1972775712],["decompositions are used to prove the functional CLT for reversible Markov","Remarks on limit theorems for reversible Markov processes","summarize: We propose some backward-forward martingale decompositions for functions of reversible Markov chains. These decompositions are used to prove the functional CLT for reversible Markov chains with asymptotically linear variance of partial sums. We also provide a proof of the equivalence between asymptotic linearity of the variance and convergence of the integral of ",0.3636363636],["the strength of the as-deposited multilayers was independent of the layer thickness. the","Effects of layer thickness on the mechanical behavior of oxidation-strengthened Zr\/Nb nanoscale multilayers","summarize: The mechanical behaviour and deformation mechanisms of magnetron-sputtered Zr\/Nb nanoscale multilayers were analysed as a function of the periodicity and annealing time at 350C . The strength of the as-deposited multilayers was independent of the layer thickness and it was controlled by the co-deformation of the Zr and Nb layers. Annealing led to transformation of the Zr layers into ZrO2 after a few hours, while the Nb layers oxidised at a much slower rate. The volumetric expansion associated with the oxidation led to the formation of cracks at the interfaces and within the ZrO2 layers for the multilayers with L = 75 nm but not in the case of L = 10 nm. The nanoindentation hardness increased significantly after annealing due to the contribution of the residual stresses associated with the volume increase due to oxidation and to the higher strength of the oxides. The strength increase after annealing, as measured by micropillar compression tests, was smaller than that measured by nanoindentation, as it did not include the contribution of the residual stresses, which were relieved during micropillar fabrication. The nanolaminate with L = 10 nm presented the highest strength and toughness as damage during oxidation was suppressed, and a deformation mechanism controlled by the formation of shear bands.",0.6071428571],["ciphers generate a primitive group, according to hypotheses. cip","On the primitivity of PRESENT and other lightweight ciphers","summarize: We provide two sufficient conditions to guarantee that the round functions of a translation based cipher generate a primitive group. Furthermore, under the same hypotheses, and assuming that a round of the cipher is strongly proper and consists of m-bit S-Boxes, with m = 3; 4 or 5, we prove that such a group is the alternating group. As an immediate consequence, we deduce that the round functions of some lightweight translation based ciphers, such as the PRESENT cipher, generate the alternating group.",0.25],["neural responses from mouse visual cortex encode features of natural and artificial objects in a distinct manner","Decoding Neural Responses in Mouse Visual Cortex through a Deep Neural Network","summarize: Finding a code to unravel the population of neural responses that leads to a distinct animal behavior has been a long-standing question in the field of neuroscience. With the recent advances in machine learning, it is shown that the hierarchically Deep Neural Networks perform optimally in decoding unique features out of complex datasets. In this study, we utilize the power of a DNN to explore the computational principles in the mammalian brain by exploiting the Neuropixel data from Allen Brain Institute. We decode the neural responses from mouse visual cortex to predict the presented stimuli to the animal for natural and artificial classes. Our results indicate that neurons in mouse visual cortex encode the features of natural and artificial objects in a distinct manner, and such neural code is consistent across animals. We investigate this by applying transfer learning to train a DNN on the neural responses of a single animal and test its generalized performance across multiple animals. Within a single animal, DNN is able to decode the neural responses with as much as 100% classification accuracy. Across animals, this accuracy is reduced to 91%. This study demonstrates the potential of utilizing the DNN models as a computational framework to understand the neural coding principles in the mammalian brain.",0.2222222222],["imageCLEFmed Caption task is to develop a system that labels radiology images with","A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task","summarize: The aim of ImageCLEFmed Caption task is to develop a system that automatically labels radiology images with relevant medical concepts. We describe our Deep Neural Network based approach for tackling this problem. On the challenge test set of 3,534 radiology images, our system achieves an F1 score of 0.375 and ranks high, 12th among all systems that were successfully submitted to the challenge, whereby we only rely on the provided data sources and do not use any external medical knowledge or ontologies, or pretrained models from other medical image repositories or application domains.",0.3],["human cooperation does not require sheer computational power, but rather relies on intuition, cultural norm","Cooperating with Machines","summarize: Since Alan Turing envisioned Artificial Intelligence , a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks , or defeating humans in strategic zero-sum encounters . In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition , cultural norms , emotions and signals , and pre-evolved dispositions toward cooperation , common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",0.0],["recent work on fairness in machine learning has primarily emphasized how to define, quantify,","On Consequentialism and Fairness","summarize: Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage fair outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions , it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.",0.0555555556],["a generic decomposition of the option pricing formula is introduced for a popular class of","Decomposition formula for jump diffusion models","summarize: In this paper we derive a generic decomposition of the option pricing formula for models with finite activity jumps in the underlying asset price process . This is an extension of the well-known result by Alos for Heston SV model. Moreover, explicit approximation formulas for option prices are introduced for a popular class of SVJ models - models utilizing a variance process postulated by Heston . In particular, we inspect in detail the approximation formula for the Bates model with log-normal jump sizes and we provide a numerical comparison with the industry standard - Fourier transform pricing methodology. For this model, we also reformulate the approximation formula in terms of implied volatilities. The main advantages of the introduced pricing approximations are twofold. Firstly, we are able to significantly improve computation efficiency and secondly, the formula can provide an intuition on the volatility smile behaviour under a specific SVJ model.",0.1923076923],["the procedure relies on the formalism and mimetic properties of diagonal-norm","Conservative and entropy stable solid wall boundary conditions for the compressible Navier-Stokes equations: Adiabatic wall and heat entropy transfer","summarize: We present a novel technique for the imposition of non-linear entropy conservative and entropy stable solid wall boundary conditions for the compressible Navier-Stokes equations in the presence of an adiabatic wall, or a wall with a prescribed heat entropy flow. The procedure relies on the formalism and mimetic properties of diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators, and is a generalization of previous works on discontinuous interface coupling and solid wall boundary conditions . Using the method of lines, a semi-discrete entropy estimate for the entire domain is obtained when the proposed numerical imposition of boundary conditions are coupled with an entropy-conservative or entropy-stable discrete interior operator. The resulting estimate mimics the global entropy estimate obtained at the continuous level. The boundary data at the wall are weakly imposed using a penalty flux approach and a simultaneous-approximation-term technique for both the conservative variables and the gradient of the entropy variables. Discontinuous spectral collocation operators , on high-order unstructured grids, are used for the purpose of demonstrating the robustness and efficacy of the new procedure for weakly enforcing boundary conditions. Numerical simulations confirm the non-linear stability of the proposed technique, with applications to three-dimensional subsonic and supersonic flows. The procedure described is compatible with any diagonal-norm summation-by-parts spatial operator, including finite element, finite difference, finite volume, discontinuous Galerkin, and flux reconstruction schemes.",0.1725803861],["the model calculates inter-aural time and level difference cues. it compare","Localization Uncertainty in Time-Amplitude Stereophonic Reproduction","summarize: This article studies the effects of inter-channel time and level differences in stereophonic reproduction on perceived localization uncertainty, which is defined as how difficult it is for a listener to tell where a sound source is located. Towards this end, a computational model of localization uncertainty is proposed first. The model calculates inter-aural time and level difference cues, and compares them to those associated to free-field point-like sources. The comparison is carried out using a particular distance functional that replicates the increased uncertainty observed experimentally with inconsistent inter-aural time and level difference cues. The model is validated by formal listening tests, achieving a Pearson correlation of 0.99. The model is then used to predict localization uncertainty for stereophonic setups and a listener in central and off-central positions. Results show that amplitude methods achieve a slightly lower localization uncertainty for a listener positioned exactly in the center of the sweet spot. As soon as the listener moves away from that position, the situation reverses, with time-amplitude methods achieving a lower localization uncertainty.",0.0909090909],["source: the source: the source is Gauss-Markov.","The Dispersion of the Gauss-Markov Source","summarize: The Gauss-Markov source produces ",0.3076923077],["fgivenx is a Python package for functional posterior plotting. it will","fgivenx: A Python package for functional posterior plotting","summarize: fgivenx is a Python package for functional posterior plotting, currently used in astronomy, but will be of use to scientists performing any Bayesian analysis which has predictive posteriors that are functions. The source code for fgivenx is available on GitHub at https:\/\/github.com\/williamjameshandley\/fgivenx",0.6428571429],["previous work either uses fixed-latency policies, or trains a complicated two-staged","Simultaneous Translation with Flexible Policy via Restricted Imitation Learning","summarize: Simultaneous translation is widely useful but remains one of the most difficult tasks in NLP. Previous work either uses fixed-latency policies, or train a complicated two-staged model using reinforcement learning. We propose a much simpler single model that adds a `delay' token to the target vocabulary, and design a restricted dynamic oracle to greatly simplify training. Experiments on ChineseEnglish simultaneous translation show that our work leads to flexible policies that achieve better BLEU scores and lower latencies compared to both fixed and RL-learned policies.",0.3125],["in the presence of additive Gaussian noise, the statistics of the nonlinear Fourier","Statistics of the Nonlinear Discrete Spectrum of a Noisy Pulse","summarize: In the presence of additive Gaussian noise, the statistics of the nonlinear Fourier transform of a pulse are not yet completely known in closed form. In this paper, we propose a novel approach to study this problem. Our contributions are twofold: first, we extend the existing Fourier Collocation method to compute the whole discrete spectrum . We show numerically that the accuracy of FC is comparable to the state-of-the-art NFT algorithms. Second, we apply perturbation theory of linear operators to derive analytic expressions for the joint statistics of the eigenvalues and the spectral amplitudes when a pulse is contaminated by additive Gaussian noise. Our analytic expressions closely match the empirical statistics obtained through simulations.",0.3636363636],["a new algorithm is proposed to reduce the performance of random fields. the results of the","Bayesian Spatial Field Reconstruction with Unknown Distortions in Sensor Networks","summarize: Spatial regression of random fields based on potentially biased sensing information is proposed in this paper. One major concern in such applications is that since it is not known a-priori what the accuracy of the collected data from each sensor is, the performance can be negatively affected if the collected information is not fused appropriately. For example, the data collector may measure the phenomenon inappropriately, or alternatively, the sensors could be out of calibration, thus introducing random gain and bias to the measurement process. Such readings would be systematically distorted, leading to incorrect estimation of the spatial field. To combat this detrimental effect, we develop a robust version of the spatial field model based on a mixture of Gaussian process experts. We then develop two different approaches for Bayesian spatial field reconstruction: the first algorithm is the Spatial Best Linear Unbiased Estimator , in which one considers the quadratic loss function and restricts the estimator to the linear family of transformations; the second algorithm is based on empirical Bayes, which utilises a two-stage estimation procedure to produce accurate predictive inference in the presence of misbehaving sensors. In addition, we develop the distributed version of these two approaches to drastically improve the computational efficiency in large-scale settings. We present extensive simulation results using both synthetic datasets and semi-synthetic datasets with real temperature measurements and simulated distortions to draw useful conclusions regarding the performance of each of the algorithms.",0.2222222222],["frequency dependent scalar, vector, and tensor dynamical polariza","Observation of vector and tensor light shifts in 87Rb using near-resonant, stimulated Raman spectroscopy","summarize: We present the derivation of the frequency dependent scalar, vector, and tensor dynamical polarizabilities for the two hyperfine levels of the 87Rb atom 5s ground state. Based on the characterization of the dynamical polarizabilities, we analyze and measure the differential vector and tensor light shift between the 5s ground state sub-levels with near-resonant, stimulated Raman transitions. These results clarify that the tensor polarizabilities for the ground states of alkali atoms are absent when the light field is far-detuned from the atomic resonance and the total electronic angular momentum J is a good quantum number. In the near resonant case, the light shifts are non-trivial and the determination of the frequency dependent vector and tensor dynamic polarizabilities will help to achieve higher fidelities for applications of neutral atoms in quantum information and precision measurements.",0.1180916382],["the steps in such a process have been carried out using extract-transform-Lo","Data Context Informed Data Wrangling","summarize: The process of preparing potentially large and complex data sets for further analysis or manual examination is often called data wrangling. In classical warehousing environments, the steps in such a process have been carried out using Extract-Transform-Load platforms, with significant manual involvement in specifying, configuring or tuning many of them. Cost-effective data wrangling processes need to ensure that data wrangling steps benefit from automation wherever possible. In this paper, we define a methodology to fully automate an end-to-end data wrangling process incorporating data context, which associates portions of a target schema with potentially spurious extensional data of types that are commonly available. Instance-based evidence together with data profiling paves the way to inform automation in several steps within the wrangling process, specifically, matching, mapping validation, value format transformation, and data repair. The approach is evaluated with real estate data showing substantial improvements in the results of automated wrangling.",0.3529411765],["we quantify the redistribution of energy in the drop and the surrounding fluid during the impact","Local velocity variations for a drop moving through an orifice: effects of edge geometry and surface wettability","summarize: We investigate velocity variations inside of and surrounding a gravity driven drop impacting on and moving through a confining orifice, wherein the effects of edge geometry and surface wettability of the orifice are considered. Using refractive index matching and time-resolved PIV, we quantify the redistribution of energy in the drop and the surrounding fluid during the drop's impact and motion through a round-edged orifice. The measurements show the importance of a) drop kinetic energy transferred to and dissipated within the surrounding liquid, and b) the drop kinetic energy due to internal deformation and rotation during impact and passage through the orifice. While a rounded orifice edge prevents contact between the drop and orifice surface, a sharp edge promotes contact immediately upon impact, changing the near surface flow field as well as the drop passage dynamics. For a sharp-edged hydrophobic orifice, the contact lines remain localized near the orifice edge, but slipping and pinning strongly affect the drop propagation and outcome. For a sharp-edged hydrophilic orifice, on the other hand, the contact lines propagate away from the orifice edge, and their motion is coupled with the global velocity fields in the drop and the surrounding fluid. By examining the contact line propagation over a hydrophilic orifice surface with minimal drop penetration, we characterize two stages of drop spreading that exhibit power-law dependence with variable exponent. In the first stage, the contact line propagates under the influence of impact inertia and gravity. In the second stage, inertial influence subsides, and the contact line propagates mainly due to wettability.",0.1565688438],["global spherical and planar mapping techniques simplify complex geometry. but these techniques","LMap: Shape-Preserving Local Mappings for Biomedical Visualization","summarize: Visualization of medical organs and biological structures is a challenging task because of their complex geometry and the resultant occlusions. Global spherical and planar mapping techniques simplify the complex geometry and resolve the occlusions to aid in visualization. However, while resolving the occlusions these techniques do not preserve the geometric context, making them less suitable for mission-critical biomedical visualization tasks. In this paper, we present a shape-preserving local mapping technique for resolving occlusions locally while preserving the overall geometric context. More specifically, we present a novel visualization algorithm, LMap, for conformally parameterizing and deforming a selected local region-of-interest on an arbitrary surface. The resultant shape-preserving local mappings help to visualize complex surfaces while preserving the overall geometric context. The algorithm is based on the robust and efficient extrinsic Ricci flow technique, and uses the dynamic Ricci flow algorithm to guarantee the existence of a local map for a selected ROI on an arbitrary surface. We show the effectiveness and efficacy of our method in three challenging use cases: multimodal brain visualization, optimal coverage of virtual colonoscopy centerline flythrough, and molecular surface visualization.",0.0],["arterial pulse wave propagates along the artery. the displacement at multiple parts of the human","Signal Separation Using a Mathematical Model of Physiological Signals for the Measurement of Heart Pulse Wave Propagation With Array Radar","summarize: The arterial pulse wave, which propagates along the artery, is an important indicator of various cardiovascular diseases. By measuring the displacement at multiple parts of the human body, pulse wave velocity can be estimated from the pulse transit time. This paper proposes a technique for signal separation using an antenna array, so that pulse wave propagation can be measured in a non-contact manner. The body displacements due to the pulse wave at different body parts are highly correlated, and cannot be accurately separated using techniques that assume independent or uncorrelated signals. The proposed method formulates the signal separation as an optimization problem, based on a mathematical model of the arterial pulse wave. The objective function in the optimization comprises four terms that are derived based on a small-displacement approximation, unimodal impulse response approximation, and a causality condition. The optimization process was implemented using a genetic algorithm. The effectiveness of the proposed method is demonstrated through numerical simulations and experiments.",0.2931264452],["polarization theory is to determine the binary operations that always lead to polarization when they","Ergodic Theory Meets Polarization. II: A Foundation of Polarization Theory","summarize: An open problem in polarization theory is to determine the binary operations that always lead to polarization when they are used in Arkan style constructions. This paper, which is presented in two parts, solves this problem by providing a necessary and sufficient condition for a binary operation to be polarizing. This part provides a foundation of polarization theory based on the ergodic theory of binary operations which we developed in the first part. We show that a binary operation is polarizing if and only if it is uniformity preserving and its right-inverse is strongly ergodic. The rate of polarization of single user channels is studied. It is shown that the exponent of any polarizing operation cannot exceed ",0.0],["the KdV equation can be derived in the shallow water limit of the Euler equation","Superposition solutions to the extended KdV equation for water surface waves","summarize: The KdV equation can be derived in the shallow water limit of the Euler equations. Over the last few decades, this equation has been extended to include higher order effects. Although this equation has only one conservation law, exact periodic and solitonic solutions exist. Khare and Saxena \\cite demonstrated the possibility of generating new exact solutions by combining known ones for several fundamental equations . Here we find that this construction can be repeated for higher order, non-integrable extensions of these equations. Contrary to many statements in the literature, there seems to be no correlation between integrability and the number of nonlinear one variable wave solutions.",0.3043478261],["a new control method is proprioceptive sonomyographic control. it","Proprioceptive Sonomyographic Control: A novel method of intuitive proportional control of multiple degrees of freedom for upper-extremity amputees","summarize: Technological advances in multi-articulated prosthetic hands have outpaced the methods available to amputees to intuitively control these devices. Amputees often cite difficulty of use as a key contributing factor for abandoning their prosthesis, creating a pressing need for improved control technology. A major challenge of traditional myoelectric control strategies using surface electromyography electrodes has been the difficulty in achieving intuitive and robust proportional control of multiple degrees of freedom. In this paper, we describe a new control method, proprioceptive sonomyographic control that overcomes several limitations of myoelectric control. In sonomyography, muscle mechanical deformation is sensed using ultrasound, as compared to electrical activation, and therefore the resulting control signals can directly control the position of the end effector. Compared to myoelectric control which controls the velocity of the end-effector device, sonomyographic control is more congruent with residual proprioception in the residual limb. We tested our approach with 5 upper-extremity amputees and able-bodied subjects using a virtual target achievement and holding task. Amputees and able-bodied participants demonstrated the ability to achieve positional control for 5 degrees of freedom with an hour of training. Our results demonstrate the potential of proprioceptive sonomyographic control for intuitive dexterous control of multiarticulated prostheses.",0.1839397206],["proposed surfel mapping system can fuse intensity images and depth images into a globally consistent model","Real-time Scalable Dense Surfel Mapping","summarize: In this paper, we propose a novel dense surfel mapping system that scales well in different environments with only CPU computation. Using a sparse SLAM system to estimate camera poses, the proposed mapping system can fuse intensity images and depth images into a globally consistent model. The system is carefully designed so that it can build from room-scale environments to urban-scale environments using depth images from RGB-D cameras, stereo cameras or even a monocular camera. First, superpixels extracted from both intensity and depth images are used to model surfels in the system. superpixel-based surfels make our method both run-time efficient and memory efficient. Second, surfels are further organized according to the pose graph of the SLAM system to achieve ",0.1666666667],["the determination of the gravitofluid-static field required as initial field in forthcoming fluid-","Fluid statics of a self-gravitating perfect-gas isothermal sphere","summarize: We open the paper with introductory considerations describing the motivations of our long-term research plan targeting gravitomagnetism, illustrating the fluid-dynamics numerical test case selected for that purpose, that is, a perfect-gas sphere contained in a solid shell located in empty space sufficiently away from other masses, and defining the main objective of this study: the determination of the gravitofluid-static field required as initial field in forthcoming fluid-dynamics calculations. The determination of the gravitofluid-static field requires the solution of the isothermal-sphere Lane-Emden equation. We do not follow the habitual approach of the literature based on the prescription of the central density as boundary condition; we impose the gravitational field at the solid-shell internal wall. As the discourse develops, we point out differences and similarities between the literature's and our approach. We show that the nondimensional formulation of the problem hinges on a unique physical characteristic number that we call gravitational number because it gauges the self-gravity effects on the gas' fluid statics. We illustrate and discuss numerical results; some peculiarities, such as gravitational-number upper bound and multiple solutions, lead us to investigate the thermodynamics of the physical system, particularly entropy and energy, and preliminarily explore whether or not thermodynamic-stability reasons could provide justification for either selection or exclusion of multiple solutions. We close the paper with a summary of the present study in which we draw conclusions and describe future work.",0.2272727273],["quantum computers promise significant speedups in solving problems intractable for conventional computers. but","Just Like the Real Thing: Fast Weak Simulation of Quantum Computation","summarize: Quantum computers promise significant speedups in solving problems intractable for conventional computers but, despite recent progress, remain limited in scaling and availability. Therefore, quantum software and hardware development heavily rely on simulation that runs on conventional computers. Most such approaches perform strong simulation in that they explicitly compute amplitudes of quantum states. However, such information is not directly observable from a physical quantum computer because quantum measurements produce random samples from probability distributions defined by those amplitudes. In this work, we focus on weak simulation that aims to produce outputs which are statistically indistinguishable from those of error-free quantum computers. We develop algorithms for weak simulation based on quantum state representation in terms of decision diagrams. We compare them to using state-vector arrays and binary search on prefix sums to perform sampling. Empirical validation shows, for the first time, that this enables mimicking of physical quantum computers of significant scale.",0.0588235294],["magnetic clouds are observed at 1 AU and predict the helical handedness of these clouds","Solar Sources of Interplanetary Magnetic Clouds Leading to Helicity Prediction","summarize: This study identifies the solar origins of magnetic clouds that are observed at 1 AU and predicts the helical handedness of these clouds from the solar surface magnetic fields. We started with the magnetic clouds listed by the Magnetic Field Investigation team supporting NASA's WIND spacecraft in what is known as the MFI table and worked backwards in time to identify solar events that produced these clouds. Our methods utilize magnetograms from the Helioseismic and Magnetic Imager instrument on the Solar Dynamics Observatory spacecraft so that we could only analyze MFI entries after the beginning of 2011. This start date and the end date of the MFI table gave us 37 cases to study. Of these we were able to associate only eight surface events with clouds detected by WIND at 1 AU. We developed a simple algorithm for predicting the cloud helicity which gave the correct handedness in all eight cases. The algorithm is based on the conceptual model that an ejected flux tube has two magnetic origination points at the positions of the strongest radial magnetic field regions of opposite polarity near the places where the ejected arches end at the solar surface. We were unable to find events for the remaining 29 cases: lack of a halo or partial halo CME in an appropriate time window, lack of magnetic and\/or filament activity in the proper part of the solar disk, or the event was too far from disk center. The occurrence of a flare was not a requirement for making the identification but in fact flares, often weak, did occur for seven of the eight cases.",0.0526315789],["a 'Immunity Passport' has been mooted as a","COVID-19 Antibody Test \/ Vaccination Certification: There's an app for that","summarize: Goal: As the Coronavirus Pandemic of 2019\/2020 unfolds, a COVID-19 'Immunity Passport' has been mooted as a way to enable individuals to return back to work. While the quality of antibody testing, the availability of vaccines, and the likelihood of even attaining COVID-19 immunity continue to be researched, we address the issues involved in providing tamper-proof and privacy-preserving certification for test results and vaccinations. Methods: We developed a prototype mobile phone app and requisite decentralized server architecture that facilitates instant verification of tamper-proof test results. Personally identifiable information is only stored at the user's discretion, and the app allows the end-user selectively to present only the specific test result with no other personal information revealed. The architecture, designed for scalability, relies upon the 2019 World Wide Web Consortium standard called 'Verifiable Credentials', Tim Berners-Lee's decentralized personal data platform 'Solid', and a Consortium Ethereum-based blockchain. Results: Our mobile phone app and decentralized server architecture enable the mixture of verifiability and privacy in a manner derived from public\/private key pairs and digital signatures, generalized to avoid restrictive ownership of sensitive digital keys and\/or data. Benchmark performance tests show it to scale linearly in the worst case, as significant processing is done locally on each app. For the test certificate Holder, Issuer and Verifier , it is 'just another app' which takes only minutes to use. Conclusions: The app and decentralized server architecture offer a prototype proof of concept that is readily scalable, applicable generically, and in effect 'waiting in the wings' for the biological issues, plus key ethical issues raised in the discussion section, to be resolved.",0.3818273771],["magnetic field source can switch from a high field to a low field configuration by rotation by","A topology optimized switchable permanent magnet system","summarize: The design of a magnetic field source that can switch from a high field to a low field configuration by rotation by ",0.2285714286],["kernel regression is a function of the number of training samples. we derive analytical expression","Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks","summarize: We derive analytical expressions for the generalization performance of kernel regression as a function of the number of training samples using theoretical methods from Gaussian processes and statistical physics. Our expressions apply to wide neural networks due to an equivalence between training them and kernel regression with the Neural Tangent Kernel . By computing the decomposition of the total generalization error due to different spectral components of the kernel, we identify a new spectral principle: as the size of the training set grows, kernel machines and neural networks fit successively higher spectral modes of the target function. When data are sampled from a uniform distribution on a high-dimensional hypersphere, dot product kernels, including NTK, exhibit learning stages where different frequency modes of the target function are learned. We verify our theory with simulations on synthetic data and MNIST dataset.",0.1363636364],["multiBUGS is a new version of the general-purpose bayesian modelling software","MultiBUGS: A parallel implementation of the BUGS modelling framework for faster Bayesian inference","summarize: MultiBUGS is a new version of the general-purpose Bayesian modelling software BUGS that implements a generic algorithm for parallelising Markov chain Monte Carlo algorithms to speed up posterior inference of Bayesian models. The algorithm parallelises evaluation of the product-form likelihoods formed when a parameter has many children in the directed acyclic graph representation; and parallelises sampling of conditionally-independent sets of parameters. A heuristic algorithm is used to decide which approach to use for each parameter and to apportion computation across computational cores. This enables MultiBUGS to automatically parallelise the broad range of statistical models that can be fitted using BUGS-language software, making the dramatic speed-ups of modern multi-core computing accessible to applied statisticians, without requiring any experience of parallel programming. We demonstrate the use of MultiBUGS on simulated data designed to mimic a hierarchical e-health linked-data study of methadone prescriptions including 425,112 observations and 20,426 random effects. Posterior inference for the e-health model takes several hours in existing software, but MultiBUGS can perform inference in only 28 minutes using 48 computational cores.",0.416876459],["reversible computing models settings in which all processes can be reversed. it is","Reversible effects as inverse arrows","summarize: Reversible computing models settings in which all processes can be reversed. Applications include low-power computing, quantum computing, and robotics. It is unclear how to represent side-effects in this setting, because conventional methods need not respect reversibility. We model reversible effects by adapting Hughes' arrows to dagger arrows and inverse arrows. This captures several fundamental reversible effects, including serialization and mutable store computations. Whereas arrows are monoids in the category of profunctors, dagger arrows are involutive monoids in the category of profunctors, and inverse arrows satisfy certain additional properties. These semantics inform the design of functional reversible programs supporting side-effects.",0.0666666667],["union of the u.s.","The Number of Holes in the Union of Translates of a Convex Set in Three Dimensions","summarize: We show that the union of ",0.0248935342],["a graph with edge weights and a subset is a graph with edge weight","On approximate data reduction for the Rural Postman Problem: Theory and experiments","summarize: Given an undirected graph with edge weights and a subset ",0.5277777778],["crystallographic T-duality is a crystallographic concept inspired by the appearance of","Crystallographic T-duality","summarize: We introduce the notion of crystallographic T-duality, inspired by the appearance of ",0.1904761905],["the results of the Big-Bang nucleosynthesis are reviewed. the standard and","Big-Bang nucleosynthyesis: constraints on nuclear reaction rates, neutrino degeneracy, inhomogeneous and Brans-Dicke models","summarize: We review the recent progress in the Big-Bang nucleosynthesis which includes the standard and non-standard theory of cosmology, effects of neutrino degeneracy, and inhomogeneous nucleosynthesis within the framework of a Friedmann model. As for a non-standard theory of gravitation, we adopt a Brans-Dicke theory which incorporate a cosmological constant. We constrain various parameters associated with each subject.",0.0794050398],["we consider dispatching a fleet of distributed energy reserve devices to meet a sequence of power","Robustly Maximal Utilisation of Energy-Constrained Distributed Resources","summarize: We consider the problem of dispatching a fleet of distributed energy reserve devices to collectively meet a sequence of power requests over time. Under the restriction that reserves cannot be replenished, we aim to maximise the survival time of an energy-constrained islanded electrical system; and we discuss realistic scenarios in which this might be the ultimate goal of the grid operator. We present a policy that achieves this optimality, and generalise this into a set-theoretic result that implies there is no better policy available, regardless of the realised energy requirement scenario.",0.347826087],["proposed scheme combines a typical gradient clustering protocol with evolutionary optimization method. it","Energy Balanced Two-level Clustering for Large-Scale Wireless Sensor Networks based on the Gravitational Search Algorithm","summarize: Organizing sensor nodes in clusters is an effective method for energy preservation in a Wireless Sensor Network . Throughout this research work we present a novel hybrid clustering scheme, that combines a typical gradient clustering protocol with an evolutionary optimization method that is mainly based on the Gravitational Search Algorithm . The proposed scheme aims at improved performance over large in size networks, where classical schemes in most cases lead to non-efficient solutions. It first creates suitably balanced multihop clusters, in which the sensors energy gets larger as coming closer to the cluster head . In the next phase of the proposed scheme a suitable protocol based on the GSA runs to associate sets of cluster heads to specific gateway nodes for the eventual relaying of data to the base station . The fitness function was appropriately chosen considering both the distance from the cluster heads to the gateway nodes and the remaining energy of the gateway nodes, and it was further optimized in order to gain more accurate results for large instances. Extended experimental measurements demonstrate the efficiency and scalability of the presented approach over very large WSNs, as well as its superiority over other known clustering approaches presented in the literature.",0.3334348575],["a new framework for reasoning about choice of effect measure is proposed. effects are defined in","The choice of effect measure for binary outcomes: Introducing counterfactual outcome state transition parameters","summarize: Standard measures of effect, including the risk ratio, the odds ratio, and the risk difference, are associated with a number of well-described shortcomings, and no consensus exists about the conditions under which investigators should choose one effect measure over another. In this paper, we introduce a new framework for reasoning about choice of effect measure by linking two separate versions of the risk ratio to a counterfactual causal model. In our approach, effects are defined in terms of counterfactual outcome state transition parameters, that is, the proportion of those individuals who would not have been a case by the end of follow-up if untreated, who would have responded to treatment by becoming a case; and the proportion of those individuals who would have become a case by the end of follow-up if untreated who would have responded to treatment by not becoming a case. Although counterfactual outcome state transition parameters are generally not identified from the data without strong monotonicity assumptions, we show that when they stay constant between populations, there are important implications for model specification, meta-analysis, and research generalization.",0.5416666667],["the original minimal path model computes the globally minimal geodesic by solving an Eikonal","Global Minimum for a Finsler Elastica Minimal Path Approach","summarize: In this paper, we propose a novel curvature-penalized minimal path model via an orientation-lifted Finsler metric and the Euler elastica curve. The original minimal path model computes the globally minimal geodesic by solving an Eikonal partial differential equation . Essentially, this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models. To solve this problem, we present an Eikonal PDE-based Finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem. We were successful at adding the curvature penalization to the classical geodesic energy. The basic idea of this work is to interpret the Euler elastica bending energy via a novel Finsler elastica metric that embeds a curvature penalty. This metric is non-Riemannian, anisotropic and asymmetric, and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension. Based on this orientation lifting, the proposed minimal path model can benefit from both the curvature and orientation of the paths. Thanks to the fast marching method, the global minimum of the curvature-penalized geodesic energy can be computed efficiently. We introduce two anisotropic image data-driven speed functions that are computed by steerable filters. Based on these orientation-dependent speed functions, we can apply the proposed Finsler elastica minimal path model to the applications of closed contour detection, perceptual grouping and tubular structure extraction. Numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results.",0.0],["ad exchanges are widely used in platforms for online display advertising. agents must learn","Learning Best Response Strategies for Agents in Ad Exchanges","summarize: Ad exchanges are widely used in platforms for online display advertising. Autonomous agents operating in these exchanges must learn policies for interacting profitably with a diverse, continually changing, but unknown market. We consider this problem from the perspective of a publisher, strategically interacting with an advertiser through a posted price mechanism. The learning problem for this agent is made difficult by the fact that information is censored, i.e., the publisher knows if an impression is sold but no other quantitative information. We address this problem using the Harsanyi-Bellman Ad Hoc Coordination algorithm, which conceptualises this interaction in terms of a Stochastic Bayesian Game and arrives at optimal actions by best responding with respect to probabilistic beliefs maintained over a candidate set of opponent behaviour profiles. We adapt and apply HBA to the censored information setting of ad exchanges. Also, addressing the case of stochastic opponents, we devise a strategy based on a Kaplan-Meier estimator for opponent modelling. We evaluate the proposed method using simulations wherein we show that HBA-KM achieves substantially better competitive ratio and lower variance of return than baselines, including a Q-learning agent and a UCB-based online learning agent, and comparable to the offline optimal algorithm.",0.1666666667],["non-free data types are data types whose data have no canonical forms.","Non-linear Pattern Matching with Backtracking for Non-free Data Types","summarize: Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset ",0.0],["mm-wave\/THz-frequency range is based on homogenized, frequency","Limits of effective material properties in the context of an electromagnetic tissue model","summarize: Most calibration schemes for reflection-based tissue spectroscopy in the mm-wave\/THz-frequency range are based on homogenized, frequency-dependent tissue models where macroscopic material parameters have either been determined by measurement or calculated using effective material theory. However, as the resolution of measurement at these frequencies captures the underlying microstructure of the tissue, here we will investigate the validity limits of such effective material models over a wide frequency range . Embedded in a parameterizable virtual workbench, we implemented a numerical homogenization method using a hierarchical multiscale approach to capture both the dispersive and tensorial electromagnetic properties of the tissue, and determined at which frequency this homogenized model deviated from a full-wave electromagnetic reference model within the framework of a Monte Carlo analysis. Simulations were carried out using a generic hypodermal tissue that emulated the morphology of the microstructure. Results showed that the validity limit occurred at surprisingly low frequencies and thus contradicted the traditional usage of homogenized tissue models. The reasons for this are explained in detail and thus it is shown how both the lower allowed and upper forbidden frequency ranges can be used for frequency-selective classification\/identification of specific material and structural properties employing a supervised machine-learning approach. Using the implemented classifier, we developed a method to identify specific frequency bands in the forbidden frequency range to optimize the reliability of material classification.",0.1060932114],["cross-lingual stance detection aims to leverage labeled data in one language to identify","Contrastive Language Adaptation for Cross-Lingual Stance Detection","summarize: We study cross-lingual stance detection, which aims to leverage labeled data in one language to identify the relative perspective of a given document with respect to a claim in a different target language. In particular, we introduce a novel contrastive language adaptation approach applied to memory networks, which ensures accurate alignment of stances in the source and target languages, and can effectively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach.",0.0625],["the organic molecular crystal has the properties of the crystal. the crystal is a","Phonon Lifetimes and Thermal Conductivity of the Molecular Crystal ","summarize: The heat transfer properties of the organic molecular crystal ",0.2368421053],["moTe2 involves attractive polymorphic TMD crystals. they can exist in","Mechanical responses of two-dimensional MoTe2; pristine 2H, 1T and 1T' and 1T'\/2H heterostructure","summarize: Transition metal dichalcogenides are currently among the most interesting two-dimensional materials due to their outstanding properties. MoTe2 involves attractive polymorphic TMD crystals which can exist in three different 2D atomic lattices of 2H, 1T and 1T', with diverse properties, like semiconducting and metallic electronic characters. Using the polymorphic heteroepitaxy, most recently coplanar semiconductor\/metal few-layer MoTe2 heterostructures were experimentally synthesized, highly promising to build circuit components for next generation nanoelectronics. Motivated by the recent experimental advances, we conducted first-principles calculations to explore the mechanical properties of single-layer MoTe2 structures. We first studied the mechanical responses of pristine and single-layer 2H-, 1T- and 1T'-MoTe2. In these cases we particularly analyzed the possibility of engineering of the electronic properties of these attractive 2D structures using the biaxial or uniaxial tensile loadings. Finally, the mechanical-failure responses of 1T'\/2H-MoTe2 heterostructure were explored, which confirms the remarkable strength of this novel 2D system.",0.067347111],["the answer appears to be no, but a number of interesting related results were obtained.","Variations on a Hypergeometric Theme","summarize: The question was asked: Is it possible to express the function \\begin \\tag h\\equiv\\, \\label \\end in closed form? After considerable analysis, the answer appears to be no, but during the attempt to answer this question, a number of interesting related results were obtained, either as specialized transformations, or as closed-form expressions for several related functions. The purpose of this paper is to record and review both the methods attempted and the related identities obtained ",0.1428571429],["proposed technique is used to maximise probability of detection of the RS. null-space","Coexistence of MIMO Radar and FD MIMO Cellular Systems with QoS Considerations","summarize: In this work, the feasibility of spectrum sharing between a multiple-input multiple-output radar system and a MIMO cellular system , comprising of a full duplex base station serving multiple downlink and uplink users at the same time and frequency is investigated. While a joint transceiver design technique at the CS's BS and users is proposed to maximise the probability of detection of the MIMO RS, subject to constraints of quality of service of users and transmit power at the CS, null-space based waveform projection is used to mitigate the interference from RS towards CS. In particular, the proposed technique optimises the performance of PoD of RS by maximising its lower bound, which is obtained by exploiting the monotonically increasing relationship of PoD and its non-centrality parameter. Numerical results show the utility of the proposed spectrum sharing framework, but with certain trade-offs in performance corresponding to RS's transmit power, RS's PoD, CS's residual self interference power at the FD BS and QoS of users.",0.1875],["we show that all classes that are neither semisimple nor unipotent in fi","Finite-dimensional pointed Hopf algebras over finite simple groups of Lie type V. Mixed classes in Chevalley and Steinberg groups","summarize: We show that all classes that are neither semisimple nor unipotent in finite simple Chevalley or Steinberg groups different from ",0.1680835164],["this paper develops a more general theory of sequences of dependent categorical random variables","Vertical Dependency in Sequences of Categorical Random Variables","summarize: This paper develops a more general theory of sequences of dependent categorical random variables, extending the works of Korzeniowski and Traylor that studied first-kind dependency in sequences of Bernoulli and categorical random variables, respectively. A more natural form of dependency, sequential dependency, is defined and shown to retain the property of identically distributed but dependent elements in the sequence. The cross-covariance of sequentially dependent categorical random variables is proven to decrease exponentially in the dependency coefficient ",0.347826087],["the classifier is a significant goal in space situational awareness. the aim is to","Development of a High Fidelity Simulator for Generalised Photometric Based Space Object Classification using Machine Learning","summarize: This paper presents the initial stages in the development of a deep learning classifier for generalised Resident Space Object characterisation that combines high-fidelity simulated light curves with transfer learning to improve the performance of object characterisation models that are trained on real data. The classification and characterisation of RSOs is a significant goal in Space Situational Awareness in order to improve the accuracy of orbital predictions. The specific focus of this paper is the development of a high-fidelity simulation environment for generating realistic light curves. The simulator takes in a textured geometric model of an RSO as well as the objects ephemeris and uses Blender to generate photo-realistic images of the RSO that are then processed to extract the light curve. Simulated light curves have been compared with real light curves extracted from telescope imagery to provide validation for the simulation environment. Future work will involve further validation and the use of the simulator to generate a dataset of realistic light curves for the purpose of training neural networks.",0.4173856554],["a 2 to 3-cm thick layer of silica aerogel will transmit sufficient visible","Enabling martian habitability with silica aerogel via the solid-state greenhouse effect","summarize: The low temperatures and high ultraviolet radiation levels at the surface of Mars today currently preclude the survival of life anywhere except perhaps in limited subsurface niches. Several ideas for making the martian surface more habitable have been put forward previously, but they all involve massive environmental modification that will be well beyond human capability for the foreseeable future. Here we present a new approach to this problem. We show that widespread regions of the surface of Mars could be made habitable to photosynthetic life in the future via a solid-state analogue to Earth's atmospheric greenhouse effect. Specifically, we demonstrate via experiments and modelling that under martian environmental conditions, a 2 to 3-cm thick layer of silica aerogel will simultaneously transmit sufficient visible light for photosynthesis, block hazardous ultraviolet radiation, and raise temperatures underneath permanently to above the melting point of water, without the need for any internal heat source. Placing silica aerogel shields over sufficiently ice-rich regions of the martian surface could therefore allow photosynthetic life to survive there with minimal subsequent intervention. This regional approach to making Mars habitable is much more achievable than global atmospheric modification. In addition, it can be developed systematically starting from minimal resources, and can be further tested in extreme environments on Earth today.",0.4117647059],["a cylinder is a homogeneous cylinder that scatters ob","The inverse electromagnetic scattering problem by a penetrable cylinder at oblique incidence","summarize: In this work we consider the method of non-linear boundary integral equation for solving numerically the inverse scattering problem of obliquely incident electromagnetic waves by a penetrable homogeneous cylinder in three dimensions. We consider the indirect method and simple representations for the electric and the magnetic fields in order to derive a system of five integral equations, four on the boundary of the cylinder and one on the unit circle where we measure the far-field pattern of the scattered wave. We solve the system iteratively by linearizing only the far-field equation. Numerical results illustrate the feasibility of the proposed scheme.",0.4636379068],["the K3 quantity is studied for the neutrino oscillations in matter. it","The Leggett--Garg K3 quantity discriminates Dirac and Majorana neutrinos","summarize: The K3 quantity, introduced in a context of the Leggett--Garg inequality violation, is studied for the neutrino oscillations in matter with phenomenologically modelled dissipative environment. It is shown that the K3 function acquires different values depending on whether neutrino is Dirac or Majorana particle, provided that there is a dissipative interaction between matter and neutrinos. The difference occurs for various matter densities and can serve as a potential quantifier verifying the neutrino nature. Moreover, working within phenomenological model one can suggest the values of the matter density and dissipation for which the difference is the most visible. There exist also special conditions in which the violation of the Leggett--Garg inequality, to a different extent for both kinds of neutrino, is observed.",0.4375],["the first projection method is a generalization of the classical primal-dual method","Projection based model order reduction methods for the estimation of vector-valued variables of interest","summarize: We propose and compare goal-oriented projection based model order reduction methods for the estimation of vector-valued functionals of the solution of parameter-dependent equations. The first projection method is a generalization of the classical primal-dual method to the case of vector-valued variables of interest. We highlight the role played by three reduced spaces: the approximation space and the test space associated to the primal variable, and the approximation space associated to the dual variable. Then we propose a Petrov-Galerkin projection method based on a saddle point problem involving an approximation space for the primal variable and an approximation space for an auxiliary variable. A goal-oriented choice of the latter space, defined as the sum of two spaces, allows us to improve the approximation of the variable of interest compared to a primal-dual method using the same reduced spaces. Then, for both approaches, we derive computable error estimates for the approximations of the variable of interest and we propose greedy algorithms for the goal-oriented construction of reduced spaces. The performance of the algorithms are illustrated on numerical examples and compared to standard algorithms.",0.3847644204],["we present a structural study of 182 obscured Active Galactic Nuclei at","Obscured Active Galactic Nuclei triggered in compact star-forming galaxies","summarize: We present a structural study of 182 obscured Active Galactic Nuclei at z<=1.5, selected in the COSMOS field from their extreme infrared to X-ray luminosity ratio and their negligible emission at optical wavelengths. We fit optical to far-infrared spectral energy distributions and analyze deep HST imaging to derive the physical and morphological properties of their host galaxies. We find that such galaxies are more compact than normal star-forming sources at similar redshift and stellar mass, and we show that it is not an observational bias related to the emission of the AGN. Based on the distribution of their UVJ colors, we also argue that this increased compactness is not due to the additional contribution of a passive bulge. We thus postulate that a vast majority of obscured AGNs reside in galaxies undergoing dynamical compaction, similar to processes recently invoked to explain the formation of compact star-forming sources at high redshift.",0.5],["the huge volume of data that is customary generated by hospitals and pharmaceutical companies across the world could","IASIS and BigMedilytics: Towards personalized medicine in Europe","summarize: One field of application of Big Data and Artificial Intelligence that is receiving increasing attention is the biomedical domain. The huge volume of data that is customary generated by hospitals and pharmaceutical companies all over the world could potentially enable a plethora of new applications. Yet, due to the complexity of such data, this comes at a high cost. We here review the activities of the research group composed by people of the Universidad Polit\\'ecnica de Madrid and the Hospital Universitario Puerta de Hierro de Majadahonda, Spain; discuss their activities within two European projects, IASIS and BigMedilytics; and present some initial results.",0.05],["CRAIG is a method to select a weighted subset of training data","Coresets for Data-efficient Training of Machine Learning Models","summarize: Incremental gradient methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.",0.55],["the vulnerability of global positioning system receivers to jammers is a major concern. the","Single-Antenna-Based GPS Antijamming Method Exploiting Polarization Diversity","summarize: The vulnerability of Global Positioning System receivers to jammers is a major concern owing to the extremely weak received signal power of GPS. Researches have been conducted on a variety of antenna array techniques to be used as countermeasures to GPS jammers, and their antijamming performance is known to be greater than that of single antenna methods. However, the application of antenna arrays remains limited because of their size, cost, and computational complexity. This study proposes and experimentally validates a novel space-time-polarization domain adaptive processing for a single-element dual-polarized antenna by focusing on the polarization diversity of a dual-polarized antenna. The mathematical models of arbitrarily polarized signals received by dual-polarized antenna are derived, and an appropriate constraint matrix for dual-polarized-antenna-based GPS antijam is suggested. To reduce the computational complexity of the constraint matrix approach, the eigenvector constraint design scheme is adopted. The performance of STPAPS is quantitively and qualitatively evaluated through experiments as follows. 1) The carrier-to-noise-density ratio of STPAPS under synthetic jamming is demonstrated to be higher than that of the previous minimum mean squared error or minimum variance distortionless response based dual-polarized antenna methods. 2) The strengths and weaknesses of STPAPS are qualitatively compared with those of the previous single-element dual-polarized antenna methods that are not based on the MMSE or MVDR algorithms. 3) The characteristics of STPAPS are compared with those of the conventional two-element single-polarized antenna array method, which has the same degree of freedom as that of STPAPS.",0.25],["Let us know what you think about it!","Existence and Nonexistence results for anisotropic p-Laplace equation with singular nonlinearities","summarize: Let ",0.0],["neural network approaches to create visually grounded embeddings for spoken utterances.","Language learning using Speech to Image retrieval","summarize: Humans learn language by interaction with their environment and listening to other humans. It should also be possible for computational models to learn language directly from speech but so far most approaches require text. We improve on existing neural network approaches to create visually grounded embeddings for spoken utterances. Using a combination of a multi-layer GRU, importance sampling, cyclic learning rates, ensembling and vectorial self-attention our results show a remarkable increase in image-caption retrieval performance over previous work. Furthermore, we investigate which layers in the model learn to recognise words in the input. We find that deeper network layers are better at encoding word presence, although the final layer has slightly lower performance. This shows that our visually grounded sentence encoder learns to recognise words from the input even though it is not explicitly trained for word recognition.",0.0909090909],["the K3 quantity is studied for the neutrino oscillations in matter. it","The Leggett--Garg K3 quantity discriminates Dirac and Majorana neutrinos","summarize: The K3 quantity, introduced in a context of the Leggett--Garg inequality violation, is studied for the neutrino oscillations in matter with phenomenologically modelled dissipative environment. It is shown that the K3 function acquires different values depending on whether neutrino is Dirac or Majorana particle, provided that there is a dissipative interaction between matter and neutrinos. The difference occurs for various matter densities and can serve as a potential quantifier verifying the neutrino nature. Moreover, working within phenomenological model one can suggest the values of the matter density and dissipation for which the difference is the most visible. There exist also special conditions in which the violation of the Leggett--Garg inequality, to a different extent for both kinds of neutrino, is observed.",0.4375],["quantizers are constrained to use power-of-2 scale-factors and per-","Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks","summarize: We propose a method of training quantization thresholds for uniform symmetric quantizers using standard backpropagation and gradient descent. Contrary to prior work, we show that a careful analysis of the straight-through estimator for threshold gradients allows for a natural range-precision trade-off leading to better optima. Our quantizers are constrained to use power-of-2 scale-factors and per-tensor scaling of weights and activations to make it amenable for hardware implementations. We present analytical support for the general robustness of our methods and empirically validate them on various CNNs for ImageNet classification. We are able to achieve near-floating-point accuracy on traditionally difficult networks such as MobileNets with less than 5 epochs of quantized retraining. Finally, we present Graffitist, a framework that enables automatic quantization of TensorFlow graphs for TQT .",0.0641180388],["black holes are minimally coupled to a cloud of strings. the cosmological constant","Effects of a cloud of strings on the extended phase space of Einstein-Gauss-Bonnet AdS black holes","summarize: In this paper we study the thermodynamics of Einstein-Gauss-Bonnet -AdS black holes minimally coupled to a cloud of strings in an extended phase space where the cosmological constant is treated as pressure of the black holes and its conjugate variable is the thermodynamical volume of the black holes. To investigate the analogy between EGB black holes surrounded by a cloud of strings and liquid-gas system we derive the analytical solutions of the critical points and probe the effects of a cloud of strings on ",0.4851749576],["online attack\/anomaly detection problem proposed as a partially observable Markov","Online Cyber-Attack Detection in Smart Grid: A Reinforcement Learning Approach","summarize: Early detection of cyber-attacks is crucial for a safe and reliable operation of the smart grid. In the literature, outlier detection schemes making sample-by-sample decisions and online detection schemes requiring perfect attack models have been proposed. In this paper, we formulate the online attack\/anomaly detection problem as a partially observable Markov decision process problem and propose a universal robust online detection algorithm using the framework of model-free reinforcement learning for POMDPs. Numerical studies illustrate the effectiveness of the proposed RL-based algorithm in timely and accurate detection of cyber-attacks targeting the smart grid.",0.2105263158],["MCTS is one of the most widely used methods for planning. it is used to","Static and Dynamic Values of Computation in MCTS","summarize: Monte-Carlo Tree Search is one of the most-widely used methods for planning, and has powered many recent advances in artificial intelligence. In MCTS, one typically performs computations to collect statistics about the possible future consequences of actions, and then chooses accordingly. Many popular MCTS methods such as UCT and its variants decide which computations to perform by trading-off exploration and exploitation. In this work, we take a more direct approach, and explicitly quantify the value of a computation based on its expected impact on the quality of the action eventually chosen. Our approach goes beyond the myopic limitations of existing computation-value-based methods in two senses: we are able to account for the impact of non-immediate computations on non-immediate actions. We show that policies that greedily optimize computation values are optimal under certain assumptions and obtain results that are competitive with the state-of-the-art.",0.1052631579],["drone launches rockets at a release altitude of up to 20 feet. a","Drone Launched Short Range Rockets","summarize: A concept of drone launched short range rockets is presented. A drone or an aircraft rises DLSRR to a release altitude of up to 20 ",0.1739130435],["a deep learning algorithm with self-supervision is proposed in this paper: SAR2","SAR2SAR: a semi-supervised despeckling algorithm for SAR images","summarize: Speckle reduction is a key step in many remote sensing applications. By strongly affecting synthetic aperture radar images, it makes them difficult to analyse. Due to the difficulty to model the spatial correlation of speckle, a deep learning algorithm with self-supervision is proposed in this paper: SAR2SAR. Multi-temporal time series are leveraged and the neural network learns to restore SAR images by only looking at noisy acquisitions. To this purpose, the recently proposed noise2noise framework has been employed. The strategy to adapt it to SAR despeckling is presented, based on a compensation of temporal changes and a loss function adapted to the statistics of speckle. A study with synthetic speckle noise is presented to compare the performances of the proposed method with other state-of-the-art filters. Then, results on real images are discussed, to show the potential of the proposed algorithm. The code is made available to allow testing and reproducible research in this field.",0.3888888889],["in this paper we show the existence of the generalized Eberlein decomposition for","On the Fourier Analysis of Measures with Meyer Set Support","summarize: In this paper we show the existence of the generalized Eberlein decomposition for Fourier transformable measures with Meyer set support. We prove that each of the three components is also Fourier transformable and has Meyer set support. We obtain that each of the pure point, absolutely continuous and singular continuous components of the Fourier transform is a strong almost periodic measure, and hence is either trivial or has relatively dense support. We next prove that the Fourier transform of a measure with Meyer set support is norm almost periodic, and hence so is each of the pure point, absolutely continuous and singular continuous components. We show that a measure with Meyer set support is Fourier transformable if and only if it is a linear combination of positive definite measures, which can be chosen with Meyer set support, solving a particular case of an open problem. We complete the paper by discussing some applications to the diffraction of weighted Dirac combs with Meyer set support.",0.1875],["compositional nonparametric method is proposed in this paper. model is expressed as a","On the Statistical Efficiency of Compositional Nonparametric Prediction","summarize: In this paper, we propose a compositional nonparametric method in which a model is expressed as a labeled binary tree of ",0.3181818182],["this paper proposes a fully decentralized and recursive approach to online identification of","Decentralized and Recursive Identification for Cooperative Manipulation of Unknown Rigid Body with Local Measurements","summarize: This paper proposes a fully decentralized and recursive approach to online identification of unknown kinematic and dynamic parameters for cooperative manipulation of a rigid body based on commonly used local measurements. To the best of our knowledge, this is the first paper addressing the identification problem for 3D rigid body cooperative manipulation, though the approach proposed here applies to the 2D case as well. In this work, we derive truly linear observation models for kinematic and dynamic unknowns whose state-dependent uncertainties can be exactly evaluated. Dynamic consensus in different coordinates and a filter for dual quaternion are developed with which the identification problem can be solved in a distributed way. It can be seen that in our approach all unknowns to be identified are time-invariant constants. Finally, we provide numerical simulation results to illustrate the efficacy of our approach indicating that it can be used for online identification and adaptive control of rigid body cooperative manipulation.",0.4386131425],["electrochemical deposition of palladium between graphene oxide sheets result in a self","Self-limiting growth of two-dimensional palladium between graphene oxide layers","summarize: The ability of different materials to display self-limiting growth has recently attracted enormous attention due to the importance of nanoscale materials in applications for catalysis, energy conversion, electronics, etc. Here, we show that electrochemical deposition of palladium between graphene oxide sheets result in a self-limiting growth of 5 nm thin Pd nanosheets. The self-limiting growth is found to be a consequence of strong interaction of Pd with the confining GO sheets which results in bulk growth of Pd being energetically unfavourable for larger thicknesses. Furthermore, we have successfully carried out liquid exfoliation of the resulting Pd-GO laminates to isolate Pd nanosheets and demonstrated their high efficiency in continuous flow catalysis and electrocatalysis.",0.6875],["document ontology captures general purpose semantic structure and domain specific semantic concepts from large, structured","Understanding and representing the semantics of large structured documents","summarize: Understanding large, structured documents like scholarly articles, requests for proposals or business reports is a complex and difficult task. It involves discovering a document's overall purpose and subject, understanding the function and meaning of its sections and subsections, and extracting low level entities and facts about them. In this research, we present a deep learning based document ontology to capture the general purpose semantic structure and domain specific semantic concepts from a large number of academic articles and business documents. The ontology is able to describe different functional parts of a document, which can be used to enhance semantic indexing for a better understanding by human beings and machines. We evaluate our models through extensive experiments on datasets of scholarly articles from arXiv and Request for Proposal documents.",0.3333333333],["the category of strictly zero-dimensional biframes was introduced by Banaschewski and","Strictly zero-dimensional biframes and a characterisation of congruence frames","summarize: Strictly zero-dimensional biframes were introduced by Banaschewski and Br\\mmer as a class of strongly zero-dimensional biframes including the congruence biframes. We consider the category of strictly zero-dimensional biframes and show it is both complete and cocomplete. We characterise the extremal monomorphisms in this category and explore the special position that congruence biframes hold in it. Finally, we provide an internal characterisation of congruence biframes, and hence, of congruence frames.",0.3636363636],["the detailed observation of the distribution of redshifts and chirp masses of binary black hole","Effect of gravitational lensing on the distribution of gravitational waves from distant binary black hole mergers","summarize: The detailed observation of the distribution of redshifts and chirp masses of binary black hole mergers is expected to provide a clue to their origin. In this paper, we develop a hybrid model of the probability distribution function of gravitational lensing magnification taking account of both strong and weak gravitational lensing, and use it to study the effect of gravitational lensing magnification on the distribution of gravitational waves from distant binary black hole mergers detected in ongoing and future gravitational wave observations. We find that the effect of gravitational lensing magnification is significant at high ends of observed chirp mass and redshift distributions. While a high mass tail in the observed chirp mass distribution is produced by highly magnified gravitational lensing events, we find that highly demagnified images of strong lensing events produce a high redshift tail in the observed redshift distribution, which can easily be observed in the third-generation gravitational wave observatories. Such a demagnified, apparently high redshift event is expected to be accompanied by a magnified image that is observed typically ",0.4880906009],["existing studies have used approximation techniques. the method improves efficiency by reducing","Efficient Network Reliability Computation in Uncertain Graphs","summarize: Network reliability is an important metric to evaluate the connectivity among given vertices in uncertain graphs. Since the network reliability problem is known as #P-complete, existing studies have used approximation techniques. In this paper, we propose a new sampling-based approach that efficiently and accurately approximates network reliability. Our approach improves efficiency by reducing the number of samples based on stratified sampling. We theoretically guarantee that our approach improves the accuracy of approximation by using lower and upper bounds of network reliability, even though it reduces the number of samples. To efficiently compute the bounds, we develop an extended BDD, called S2BDD. During constructing the S2BDD, our approach employs dynamic programming for efficiently sampling possible graphs. Our experiment with real datasets demonstrates that our approach is up to 51.2 times faster than the existing sampling-based approach with higher accuracy.",0.0],["generative models are deep generative latent variable models. the learned generative model capture","Characterizing and Avoiding Problematic Global Optima of Variational Autoencoders","summarize: Variational Auto-encoders are deep generative latent variable models consisting of two components: a generative model that captures a data distribution p by transforming a distribution p over latent space, and an inference model that infers likely latent codes for each data point . Recent work shows that traditional training methods tend to yield solutions that violate modeling desiderata: the learned generative model captures the observed data distribution but does so while ignoring the latent codes, resulting in codes that do not represent the data ; Kim et al. ); the aggregate of the learned latent codes does not match the prior p. This mismatch means that the learned generative model will be unable to generate realistic data with samples from p; Tomczak and Welling ). In this paper, we demonstrate that both issues stem from the fact that the global optima of the VAE training objective often correspond to undesirable solutions. Our analysis builds on two observations: the generative model is unidentifiable - there exist many generative models that explain the data equally well, each with different properties and bias in the VAE objective - the VAE objective may prefer generative models that explain the data poorly but have posteriors that are easy to approximate. We present a novel inference method, LiBI, mitigating the problems identified in our analysis. On synthetic datasets, we show that LiBI can learn generative models that capture the data distribution and inference models that better satisfy modeling assumptions when traditional methods struggle to do so.",0.0],["SR technique is implemented in a sensitive magnetometer. it is used to detect nuclear","High Resolution Magnetic Resonance Spectroscopy Using Solid-State Spins","summarize: We demonstrate a synchronized readout technique for spectrally selective detection of oscillating magnetic fields with sub-millihertz resolution, using coherent manipulation of solid state spins. The SR technique is implemented in a sensitive magnetometer ) based on nitrogen vacancy centers in diamond, and used to detect nuclear magnetic resonance signals from liquid-state samples. We obtain NMR spectral resolution ~3 Hz, which is nearly two orders of magnitude narrower than previously demonstrated with NV based techniques, using a sample volume of ~1 picoliter. This is the first application of NV-detected NMR to sense Boltzmann-polarized nuclear spin magnetization, and the first to observe chemical shifts and J-couplings.",0.2],["we tune the wave dispersion and the level of nonlinearity by modifying the","Transition from Weak Wave Turbulence to Soliton-Gas","summarize: We report an experimental investigation of the effect of finite depth on the statistical properties of wave turbulence at the surface of water in the gravity-capillary range. We tune the wave dispersion and the level of nonlinearity by modifying the depth of water and the forcing respectively. We use space-time resolved profilometry to reconstruct the deformed surface of water. When decreasing the water depth, we observe a drastic transition between weak turbulence at the weakest forcing and a solitonic regime at stronger forcing. We characterize the transition between both states by studying their Fourier Spectra. We also study the efficiency of energy transfer in the weak turbulence regime. We report a loss of efficiency of angular transfer as the dispersion of the wave is reduced until the system bifurcates into the solitonic regime.",0.0],["a quantum dot is modeled using an infinite potential well and a two-dimensional","Energy levels in a single-electron quantum dot with hydrostatic pressure","summarize: In this article we present a study of the effects of hydrostatic pressure on the energy levels of a quantum dot with an electron. A quantum dot is modeled using an infinite potential well and a two-dimensional harmonic oscillator and solved through the formalism of second quantization. A scheme for the implementation of a quantum NOT gate controlled with hydrostatic pressure is proposed.",0.3333333333],["a large number of people interact with each other by means of online chatting. there","Development of Security Detection Model for the Security of Social Blogs and Chatting from Hostile Users","summarize: Worldwide, a large number of people interact with each other by means of online chatting. There has been a significant rise in the number of platforms, both social and professional, such as WhatsApp, Facebook,and Twitter, which allow people to share their experiences, views and knowledge with others. Sadly enough, with online communication getting embedded into our daily communication, incivility and misbehaviour has taken on many nuances from professional misbehaviour to professional decay. Generally flaming starts with the exchange of rude messages and comments, which in turn triggers to higher scale of flaming. To prevent online communication from getting downgraded, it is essential to keep away the hostile users from communication platforms. This paper presents a Security Detection Model and a tool which checks and prevents online flaming. It detects the presence of flaming while chatting or posting blogs, and censors swear words as well as blocks the users from flaming.",0.2976613134],["logit bandit model is a dynamic assortment planning problem. results close an estimated","A Note on a Tight Lower Bound for MNL-Bandit Assortment Selection Models","summarize: In this short note we consider a dynamic assortment planning problem under the capacitated multinomial logit bandit model. We prove a tight lower bound on the accumulated regret that matches existing regret upper bounds for all parameters up to logarithmic factors. Our results close an ",0.1428571429],["a manipulation planner needs to generate a trajectory of the manipulator arm. the manipul","Manipulation Trajectory Optimization with Online Grasp Synthesis and Selection","summarize: In robot manipulation, planning the motion of a robot manipulator to grasp an object is a fundamental problem. A manipulation planner needs to generate a trajectory of the manipulator arm to avoid obstacles in the environment and plan an end-effector pose for grasping. While trajectory planning and grasp planning are often tackled separately, how to efficiently integrate the two planning problems remains a challenge. In this work, we present a novel method for joint motion and grasp planning. Our method integrates manipulation trajectory optimization with online grasp synthesis and selection, where we apply online learning techniques to select goal configurations for grasping, and introduce a new grasp synthesis algorithm to generate grasps online. We evaluate our planning approach and demonstrate that our method generates robust and efficient motion plans for grasping in cluttered scenes. Our video can be found at https:\/\/www.youtube.com\/watch?v=LIcACf8YkGU .",0.375],["the burst radio emission analysis was carried out on the basis of an improved methodology for","Alternative Models of Zebra Patterns in the Event on June 21, 2011","summarize: The analysis of the spectral characteristics of the burst radio emission on June 21, 2011 was carried out on the basis of an improved methodology for determining harmonic numbers for the corresponding stripes of the zebra structure. By using the parameters of the zebra structure in the time frequency spectrum and basing on the double plasma resonance model, the magnetic field and its dynamics, electron density, and the time variation of the distance between the stripes with harmonics s = 55 and 56 and adjacent stripes near the frequency 183 MHz have been determined in the burst generation region. The relationships between the scale characteristics of the field and the density along and across the axis of the power tube and their dependence on time have been also determined. The field obtained turned out to be so small that, firstly, it fails to explain the dynamic features of the spectrum based on MHD waves, and secondly, it results in large values for plasma betta . Other possible difficulties of the generation mechanism of bursts with zebra pattern based on the double plasma resonance are also noted. Another possible mechanism, with whistlers explains qualitatively the main observational characteristics of this zebra. The magnetic field required in this case is about 4.5 G, and the plasma betta is 0.14, which fully corresponds to the coronal conditions.",0.2],["simulated data for the planned Deep Underground Neutrino Experiment. we study","Sensitivities to charged-current nonstandard neutrino interactions at DUNE","summarize: We investigate the effects of charged-current nonstandard neutrino interactions at the source and at the detector in the simulated data for the planned Deep Underground Neutrino Experiment , while neglecting the neutral-current NSIs at the propagation due to the fact that several solutions have been proposed to resolve the degeneracies posed by neutral-current NSIs while no solution exists for the degeneracies due to the CC NSIs. We study the effects of CC NSIs on the simultaneous measurements of ",0.0],["a convolutional point cloud decoder\/generator makes use of recent advances","A Convolutional Decoder for Point Clouds using Adaptive Instance Normalization","summarize: Automatic synthesis of high quality 3D shapes is an ongoing and challenging area of research. While several data-driven methods have been proposed that make use of neural networks to generate 3D shapes, none of them reach the level of quality that deep learning synthesis approaches for images provide. In this work we present a method for a convolutional point cloud decoder\/generator that makes use of recent advances in the domain of image synthesis. Namely, we use Adaptive Instance Normalization and offer an intuition on why it can improve training. Furthermore, we propose extensions to the minimization of the commonly used Chamfer distance for auto-encoding point clouds. In addition, we show that careful sampling is important both for the input geometry and in our point cloud generation process to improve results. The results are evaluated in an auto-encoding setup to offer both qualitative and quantitative analysis. The proposed decoder is validated by an extensive ablation study and is able to outperform current state of the art results in a number of experiments. We show the applicability of our method in the fields of point cloud upsampling, single view reconstruction, and shape synthesis.",0.3333333333],["bow shocks are created by a magnetized shock model. the molecules formed in the","H","summarize: When a fast moving star or a protostellar jet hits an interstellar cloud, the surrounding gas gets heated and illuminated: a bow shock is born which delineates the wake of the impact. In such a process, the new molecules that are formed and excited in the gas phase become accessible to observations. In this article, we revisit models of H2 emission in these bow shocks. We approximate the bow shock by a statistical distribution of planar shocks computed with a magnetized shock model. We improve on previous works by considering arbitrary bow shapes, a finite irradiation field, and by including the age effect of non-stationary C-type shocks on the excitation diagram and line profiles of H2. We also examine the dependence of the line profiles on the shock velocity and on the viewing angle: we suggest that spectrally resolved observations may greatly help to probe the dynamics inside the bow shock. For reasonable bow shapes, our analysis shows that low velocity shocks largely contribute to H2 excitation diagram. This can result in an observational bias towards low velocities when planar shocks are used to interpret H2 emission from an unresolved bow. We also report a large magnetization bias when the velocity of the planar model is set independently. Our 3D models reproduce excitation diagrams in BHR71 and Orion bow shocks better than previous 1D models. Our 3D model is also able to reproduce the shape and width of the broad H2 1-0S line profile in an Orion bow shock.",0.0],["DMRD condition implies log-concavity of expected profits per unit of output","On the Equilibrium Uniqueness in Cournot Competition with Demand Uncertainty","summarize: We revisit the linear Cournot model with uncertain demand that is studied in Lagerl\\of * and provide sufficient conditions for equilibrium uniqueness that complement the existing results. We show that if the distribution of the demand intercept has the decreasing mean residual demand or the increasing generalized failure rate property, then uniqueness of equilibrium is guaranteed. The DMRD condition implies log-concavity of the expected profits per unit of output without additional assumptions on the existence or the shape of the density of the demand intercept and, hence, answers in the affirmative the conjecture of Lagerl\\of that such conditions may not be necessary. *Johan Lagerl\\of, Equilibrium uniqueness in a Cournot model with demand uncertainty. The B.E. Journal in Theoretical Economics, Vol. 6: Iss 1. , Article 19:1--6, 2006.",0.0],["this article presents a theory for constructing hierarchical networks. we present a","Provably scale-covariant continuous hierarchical networks based on scale-normalized differential expressions coupled in cascade","summarize: This article presents a theory for constructing hierarchical networks in such a way that the networks are guaranteed to be provably scale covariant. We first present a general sufficiency argument for obtaining scale covariance, which holds for a wide class of networks defined from linear and non-linear differential expressions expressed in terms of scale-normalized scale-space derivatives. Then, we present a more detailed development of one example of such a network constructed from a combination of mathematically derived models of receptive fields and biologically inspired computations. Based on a functional model of complex cells in terms of an oriented quasi quadrature combination of first- and second-order directional Gaussian derivatives, we couple such primitive computations in cascade over combinatorial expansions over image orientations. Scale-space properties of the computational primitives are analysed and we give explicit proofs of how the resulting representation allows for scale and rotation covariance. A prototype application to texture analysis is developed and it is demonstrated that a simplified mean-reduced representation of the resulting QuasiQuadNet leads to promising experimental results on three texture datasets.",0.5257396655],["n tasks and m resources have random sizes X_j. each task","Stochastic Makespan Minimization in Structured Set Systems","summarize: We study stochastic combinatorial optimization problems where the objective is to minimize the expected maximum load . In this framework, we have a set of n tasks and m resources, where each task j uses some subset of the resources. Tasks have random sizes X_j, and our goal is to non-adaptively select t tasks to minimize the expected maximum load over all resources, where the load on any resource i is the total size of all selected tasks that use i. For example, given a set of intervals in time, with each interval j having random load X_j, how do we choose t intervals to minimize the expected maximum load at any time? Our technique is also applicable to other problems with some geometric structure in the relation between tasks and resources; e.g., packing paths, rectangles, and fat objects. Specifically, we give an O-approximation algorithm for all these problems. Our approach uses a strong LP relaxation using the cumulant generating functions of the random variables. We also show that this LP has an \\Omega integrality gap even for the problem of selecting intervals on a line. Moreover, we show logarithmic gaps for problems without geometric structure, showing that some structure is needed to get good results using these techniques.",0.3125],["discretization reduces the'space' in which adversarial examples exist.","Discretization based Solutions for Secure Machine Learning against Adversarial Attacks","summarize: Adversarial examples are perturbed inputs that are designed parameter gradients) to mislead the DLN during test time. Intuitively, constraining the dimensionality of inputs or parameters of a network reduces the 'space' in which adversarial examples exist. Guided by this intuition, we demonstrate that discretization greatly improves the robustness of DLNs against adversarial attacks. Specifically, discretizing the input space extensively improves the adversarial robustness of DLNs for a substantial range of perturbations for minimal loss in test accuracy. Furthermore, we find that Binary Neural Networks and related variants are intrinsically more robust than their full precision counterparts in adversarial scenarios. Combining input discretization with BNNs furthers the robustness even waiving the need for adversarial training for certain magnitude of perturbation values. We evaluate the effect of discretization on MNIST, CIFAR10, CIFAR100 and Imagenet datasets. Across all datasets, we observe maximal adversarial resistance with 2-bit input discretization that incurs an adversarial accuracy loss of just ~1-2% as compared to clean test accuracy.",0.0973500979],["we investigate the existence and uniqueness of solutions and derive the Ulam--Hy","On the Impulsive Implicit ","summarize: In this paper, we investigate the existence and uniqueness of solutions and derive the Ulam--Hyers--Mittag--Leffler stability results for impulsive implicit ",0.125],["the linearity for the equation is first established. the backward Euler convolution quad","Numerical algorithm for the model describing anomalous diffusion in expanding media","summarize: We provide a numerical algorithm for the model characterizing anomalous diffusion in expanding media, which is derived in . The Sobolev regularity for the equation is first established. Then we use the finite element method to discretize the Laplace operator and present error estimate of the spatial semi-discrete scheme based on the regularity of the solution; the backward Euler convolution quadrature is developed to approximate Riemann-Liouville fractional derivative and error estimates for the fully discrete scheme are established by using the continuity of solution. Finally, the numerical experiments verify the effectiveness of the algorithm.",0.2],["three sample patients' MRI images were selected to calculate the femoral neck angles","Femoral Neck Angle Impacts Hip Disorder and Surgical Intervention: A Patient-Specific 3D Printed Analysis","summarize: The purpose of this study is to investigate the femoral neck angulation for prediction of the complication associated with dynamic hip screw surgery and hip deformity. Three sample patients' MRI images were selected to calculate the femoral neck angles. A total of six femur head geometries were reconstructed and three dimensional models printed. The calculation of neck angles was done in both computer models and 3D-printed models. Our results showed that 3D-printed models achieved high accuracy and provided the physical measurements, when compared to the computer models could not confirm. Neck angulations related to uncomplicated DHS surgery ranged between 129-139, and non-deformity of normal neck angles ranged between 120-135. Our study indicated that patient-specific 3D printed femoral head models provide useful information for medical education and assist DHS surgery. Further research based on a large sample size is necessary.",0.0],["quantum logic gates can be highly faulty and introduce errors. the results of the circuit can","Finding Broken Gates in Quantum Circuits---Exploiting Hybrid Machine Learning","summarize: Current implementations of quantum logic gates can be highly faulty and introduce errors. In order to correct these errors, it is necessary to first identify the faulty gates. We demonstrate a procedure to diagnose where gate faults occur in a circuit by using a hybridized quantum-and-classical K-Nearest-Neighbors machine-learning technique. We accomplish this task using a diagnostic circuit and selected input qubits to obtain the fidelity between a set of output states and reference states. The outcomes of the circuit can then be stored to be used for a classical KNN algorithm. We numerically demonstrate an ability to locate a faulty gate in circuits with over 30 gates and up to nine qubits with over 90% accuracy.",0.0],["we consider dispatching a fleet of distributed energy reserve devices to meet a sequence of power","Robustly Maximal Utilisation of Energy-Constrained Distributed Resources","summarize: We consider the problem of dispatching a fleet of distributed energy reserve devices to collectively meet a sequence of power requests over time. Under the restriction that reserves cannot be replenished, we aim to maximise the survival time of an energy-constrained islanded electrical system; and we discuss realistic scenarios in which this might be the ultimate goal of the grid operator. We present a policy that achieves this optimality, and generalise this into a set-theoretic result that implies there is no better policy available, regardless of the realised energy requirement scenario.",0.347826087],["fractional Fokker-Planck system with multiple internal states is derived in.","Numerical algorithm for the space-time fractional Fokker-Planck system with two internal states","summarize: The fractional Fokker-Planck system with multiple internal states is derived in , where the space derivative is Laplace operator. If the jump length distribution of the particles is power law instead of Gaussian, the space derivative should be replaced with fractional Laplacian. This paper focuses on solving the two state Fokker-Planck system with fractional Laplacian. We first provide a priori estimate for this system under different regularity assumptions on the initial data. Then we use ",0.5210104792],["a slotted-ALOHA overlay on LoRaWAN networks is proposed.","Slotted ALOHA on LoRaWAN - Design, Analysis, and Deployment","summarize: LoRaWAN is one of the most promising standards for long-range sensing applications. However, the high number of end devices expected in at-scale deployment, combined with the absence of an effective synchronization scheme, challenge the scalability of this standard. In this paper, we present an approach to increase network throughput through a Slotted-ALOHA overlay on LoRaWAN networks. To increase the single channel capacity, we propose to regulate the communication of LoRaWAN networks using a Slotted-ALOHA variant on the top of the Pure-ALOHA approach used by the standard; thus, no modification in pre-existing libraries is necessary. Our method is based on an innovative synchronization service that is suitable for low-cost wireless sensor nodes. We modelled the LoRaWAN channel with extensive measurement on hardware platforms, and we quantified the impact of tuning parameters on physical and medium access control layers, as well as the packet collision rate. Results show that Slotted-ALOHA supported by our synchronization service significantly improves the performance of traditional LoRaWAN networks regarding packet loss rate and network throughput.",0.5294981416],["double negative acoustic metamaterials offer the promising ability of superlens","Systematic design and realization of double-negative acoustic metamaterials by topology optimization","summarize: Double-negative acoustic metamaterials offer the promising ability of superlensing for applications in ultrasonography, biomedical sensing and nondestructive evaluation. Here, under the simultaneous increasing or non-increasing mechanisms, we develop a unified topology optimization framework considering the different microstructure symmetries, minimal structural feature sizes and dispersion extents of effective parameters. Then we apply the optimization framework to furnish the heuristic resonance-cavity-based and space-coiling metamaterials with broadband double negativity. Meanwhile, we demonstrate the essences of double negativity derived from the novel artificial multipolar LC and Mie resonances which can be induced by controlling mechanisms in optimization. Furthermore, abundant numerical simulations validate the double negativity, negative refraction, enhancements of evanescent waves and subwavelengh imaging for the optimized AMMs. Finally, we experimentally show the desired broadband subwavelengh imaging using the 3D-printed optimized space-coiling metamaterial. The present methodology and broadband metamaterials provide the ideal strategy of constructing AMMs for subwavelengh imaging technology.",0.4112897355],["Let us know what you think about it!","Closed range of ","summarize: Let ",0.0],["photometric observations of a transit event of an extrasolar planet were carried out using the","Near-Infrared Transit Photometry of Extra-Solar Planet HAT-P-54b","summarize: The results of near-infrared photometric observations of a transit event of an extrasolar planet HAT-P-54b are presented herein. Precise near-infrared photometry was carried out using the Nayuta 2 m telescope at Nishi-Harima Astronomical Observatory, Japan and Nishi-harima Infrared Camera . 170 J-, H-, and Ks-band images were taken in each band in 196 minutes. The flux of the planetary system was observed to decrease during the transit event. While the the Ks-band transit depth is similar to that in the r-band, the J- and H-band transits are deeper than those in the Ks-band. We constructed simple models of the planetary atmosphere and found that the observed transit depths are well reproduced by inflated atmosphere containing H2S molecule.",0.3846153846],["a computationally efficient harmonic block uses Discrete Cosine Transform filters in CNN","Harmonic Networks with Limited Training Samples","summarize: Convolutional neural networks are very popular nowadays for image processing. CNNs allow one to learn optimal filters in a supervised machine learning context. However this typically requires abundant labelled training data to estimate the filter parameters. Alternative strategies have been deployed for reducing the number of parameters and \/ or filters to be learned and thus decrease overfitting. In the context of reverting to preset filters, we propose here a computationally efficient harmonic block that uses Discrete Cosine Transform filters in CNNs. In this work we examine the performance of harmonic networks in limited training data scenario. We validate experimentally that its performance compares well against scattering networks that use wavelets as preset filters.",0.2941176471],["a hypothesis has been proposed that mean motion resonances between Planet Nine and distant objects of the","Feasibility of a resonance-based Planet Nine search","summarize: It has been proposed that mean motion resonances between Planet Nine and distant objects of the scattered disk might inform the semimajor axis and instantaneous position of Planet Nine. Within the context of this hypothesis, the specific distribution of occupied MMRs largely determines the available constraints. Here we characterize the behavior of scattered Kuiper Belt objects arising in the presence of an eccentric Planet Nine , focusing on relative sizes of populations occupying particular commensurabilities. Highlighting the challenge of predicting the exact MMR of a given object, we find that the majority of resonant test particles have period ratios with Planet Nine other than those of the form ",0.36],["we prove improved inapproximability results for hypergraph coloring using the low-degree poly","Super-polylogarithmic hypergraph coloring hardness via low-degree long codes","summarize: We prove improved inapproximability results for hypergraph coloring using the low-degree polynomial code and the techniques proposed by Dinur and Guruswami to incorporate this code for inapproximability results. In particular, we prove quasi-NP-hardness of the following problems on ",0.3076923077],["youla parameterization relies on a doubly-coprime factorization of the","On the Equivalence of Youla, System-level and Input-output Parameterizations","summarize: A convex parameterization of internally stabilizing controllers is fundamental for many controller synthesis procedures. The celebrated Youla parameterization relies on a doubly-coprime factorization of the system, while the recent system-level and input-output characterizations require no doubly-coprime factorization but a set of equality constraints for achievable closed-loop responses. In this paper, we present explicit affine mappings among Youla, system-level and input-output parameterizations. Two direct implications of the affine mappings are 1) any convex problem in Youla, system level, or input-output parameters can be equivalently and convexly formulated in any other one of these frameworks, including the convex system-level synthesis ; 2) the condition of quadratic invariance is sufficient and necessary for the classical distributed control problem to admit an equivalent convex reformulation in terms of Youla, system-level, or input-output parameters.",0.5294117647],["CS methods always recover the scene images in pixel level. this causes the smoothness","Perceptual Compressive Sensing","summarize: Compressive sensing works to acquire measurements at sub-Nyquist rate and recover the scene images. Existing CS methods always recover the scene images in pixel level. This causes the smoothness of recovered images and lack of structure information, especially at a low measurement rate. To overcome this drawback, in this paper, we propose perceptual CS to obtain high-level structured recovery. Our task no longer focuses on pixel level. Instead, we work to make a better visual effect. In detail, we employ perceptual loss, defined on feature level, to enhance the structure information of the recovered images. Experiments show that our method achieves better visual results with stronger structure information than existing CS methods at the same measurement rate.",0.0625],["bubbleView is a mouse-contingent, moving-window interface. participants","BubbleView: an interface for crowdsourcing image importance maps and tracking visual attention","summarize: In this paper, we present BubbleView, an alternative methodology for eye tracking using discrete mouse clicks to measure which information people consciously choose to examine. BubbleView is a mouse-contingent, moving-window interface in which participants are presented with a series of blurred images and click to reveal bubbles - small, circular areas of the image at original resolution, similar to having a confined area of focus like the eye fovea. Across 10 experiments with 28 different parameter combinations, we evaluated BubbleView on a variety of image types: information visualizations, natural images, static webpages, and graphic designs, and compared the clicks to eye fixations collected with eye-trackers in controlled lab settings. We found that BubbleView clicks can both successfully approximate eye fixations on different images, and be used to rank image and design elements by importance. BubbleView is designed to collect clicks on static images, and works best for defined tasks such as describing the content of an information visualization or measuring image importance. BubbleView data is cleaner and more consistent than related methodologies that use continuous mouse movements. Our analyses validate the use of mouse-contingent, moving-window methodologies as approximating eye fixations for different image and task types.",0.2937249957],["restricted Boltzmann machine is a generative probabilistic graphic network. the distribution","Entropy, Free Energy, and Work of Restricted Boltzmann Machines","summarize: A restricted Boltzmann machine is a generative probabilistic graphic network. A probability of finding the network in a certain configuration is given by the Boltzmann distribution. Given training data, its learning is done by optimizing parameters of the energy function of the network. In this paper, we analyze the training process of the restricted Boltzmann machine in the context of statistical physics. As an illustration, for small size Bar-and-Stripe patterns, we calculate thermodynamic quantities such as entropy, free energy, and internal energy as a function of training epoch. We demonstrate the growth of the correlation between the visible and hidden layers via the subadditivity of entropies as the training proceeds. Using the Monte-Carlo simulation of trajectories of the visible and hidden vectors in configuration space, we also calculate the distribution of the work done on the restricted Boltzmann machine by switching the parameters of the energy function. We discuss the Jarzynski equality which connects the path average of the exponential function of the work and the difference in free energies before and after training.",0.2222222222],["gradient-based optimization methods are combined with finite-element modeling. the optimization procedure","Robust Shape Optimization of Electric Devices Based on Deterministic Optimization Methods and Finite Element Analysis With Affine Decomposition and Design Elements","summarize: In this paper, gradient-based optimization methods are combined with finite-element modeling for improving electric devices. Geometric design parameters are considered by affine decomposition of the geometry or by the design element approach, both of which avoid remeshing. Furthermore, it is shown how to robustify the optimization procedure, i.e., how to deal with uncertainties on the design parameters. The overall procedure is illustrated by an academic example and by the example of a permanent-magnet synchronous machine. The examples show the advantages of deterministic optimization compared to standard and popular stochastic optimization procedures such as, e.g., particle swarm optimization.",0.0],["heterointerface of spinel\/perovskite heterointerface.","Microscopic origin of the mobility enhancement at a spinel\/perovskite oxide heterointerface revealed by photoemission spectroscopy","summarize: The spinel\/perovskite heterointerface ",0.0426185741],["we quantify the redistribution of energy in the drop and the surrounding fluid during the impact","Local velocity variations for a drop moving through an orifice: effects of edge geometry and surface wettability","summarize: We investigate velocity variations inside of and surrounding a gravity driven drop impacting on and moving through a confining orifice, wherein the effects of edge geometry and surface wettability of the orifice are considered. Using refractive index matching and time-resolved PIV, we quantify the redistribution of energy in the drop and the surrounding fluid during the drop's impact and motion through a round-edged orifice. The measurements show the importance of a) drop kinetic energy transferred to and dissipated within the surrounding liquid, and b) the drop kinetic energy due to internal deformation and rotation during impact and passage through the orifice. While a rounded orifice edge prevents contact between the drop and orifice surface, a sharp edge promotes contact immediately upon impact, changing the near surface flow field as well as the drop passage dynamics. For a sharp-edged hydrophobic orifice, the contact lines remain localized near the orifice edge, but slipping and pinning strongly affect the drop propagation and outcome. For a sharp-edged hydrophilic orifice, on the other hand, the contact lines propagate away from the orifice edge, and their motion is coupled with the global velocity fields in the drop and the surrounding fluid. By examining the contact line propagation over a hydrophilic orifice surface with minimal drop penetration, we characterize two stages of drop spreading that exhibit power-law dependence with variable exponent. In the first stage, the contact line propagates under the influence of impact inertia and gravity. In the second stage, inertial influence subsides, and the contact line propagates mainly due to wettability.",0.1565688438],["annealing and temperature variation on properties of e-beam evaporated InS","Unveiling the electrical and thermoelectric properties of highly degenerate indium selenide thin films: Indication of In3Se4 phase","summarize: The effects of annealing and variation of temperature on the electrical and thermoelectric properties of e-beam evaporated InSe thin films has been investigated in details. The XRD study demonstrates that the as-deposited InSe thin films are amorphous while they become polycrystalline with the presence of In3Se4 phase after annealing. The SEM micrographs reveal that the surfaces of as-deposited films are smooth whereas they become non-uniform due to annealing. The heating and cooling cycles of the as-deposited films exhibit that the resistivity of the films shows an irreversible phase-transition and become stable after 3-4 successive heat-treatment operations in air. The electrical conductivity of annealed InSe thin films shows a highly degenerate semiconducting behavior. The thermopower of the annealed films indicates that InSe thin film is a highly degenerate n-type semiconductor i.e. metallic. Thickness dependence thermopower obeys the Fuchs-Sondheimer theory. The optical band gap of the annealed films increases as compared to the as-deposited films. These results indicate that InSe thin films encounter a phase-transformation from In2Se3 to a new In3Se4 metallic phase with an optical band gap of ~1.8 eV due to heat-treatment.",0.1805764741],["testing team should be creative and innovative. the experience and intuition of Tester matters a","A Reliabel and an efficient web testing system","summarize: To improve the reliability and efficiency of Web Software, the Testing Team should be creative and innovative. The experience and intuition of Tester also matters a lot and most often the destructive nature of Tester brings reliable software to the user. Actually, Testing is the responsibility of everybody who is involved in the Project.",0.3043478261],["the method can be used to classify benchmark datasets. it can also work in batch","A New Oscillating-Error Technique for Classifiers","summarize: This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made.",0.0555555556],["the learning algorithm is to be used a significant number of times during the design of a","Mise en abyme with artificial intelligence: how to predict the accuracy of NN, applied to hyper-parameter tuning","summarize: In the context of deep learning, the costliest phase from a computational point of view is the full training of the learning algorithm. However, this process is to be used a significant number of times during the design of a new artificial neural network, leading therefore to extremely expensive operations. Here, we propose a low-cost strategy to predict the accuracy of the algorithm, based only on its initial behaviour. To do so, we train the network of interest up to convergence several times, modifying its characteristics at each training. The initial and final accuracies observed during this beforehand process are stored in a database. We then make use of both curve fitting and Support Vector Machines techniques, the latter being trained on the created database, to predict the accuracy of the network, given its accuracy on the primary iterations of its learning. This approach can be of particular interest when the space of the characteristics of the network is notably large or when its full training is highly time-consuming. The results we obtained are promising and encouraged us to apply this strategy to a topical issue: hyper-parameter optimisation . In particular, we focused on the HO of a convolutional neural network for the classification of the databases MNIST and CIFAR-10. By using our method of prediction, and an algorithm implemented by us for a probabilistic exploration of the hyper-parameter space, we were able to find the hyper-parameter settings corresponding to the optimal accuracies already known in literature, at a quite low-cost.",0.5333333333],["axion Bose stars can engender bursts when undergoing conversion","May axion clusters be sources of fast radio bursts?","summarize: Fast radio bursts can be caused by some phenomena related to 'new physics'.One of the most prominent candidates of the kind are axion Bose stars which can engender bursts when undergoing conversion into photons in magnetospheres of neutron stars or during their collapse. In this short research note an importance of three observational criteria is outlined, namely total energetic, ",0.2222222222],["reverse convertible notes are representative of the broad spectrum of reverse convertible notes. we deduce two","A Probabilistic Analysis of Autocallable Optimization Securities","summarize: We consider in this paper some structured financial products, known as reverse convertible notes, that resulted in substantial losses to certain buyers of these notes in recent years. We shall focus on specific reverse convertible notes known as Autocallable Optimization Securities with Contingent Protection Linked to the S\\&P 500 Financial Index, because these notes are representative of the broad spectrum of reverse convertibles notes. Therefore, the analysis provided in this paper is applicable to many other reverse convertible notes. We begin by describing the notes in detail and identifying potential areas of confusion in the pricing supplement to the prospectus for the notes. We deduce two possible interpretations of the payment procedure for the notes and apply the Law of Total Expectation to develop a probabilistic analysis for each interpretation. We also determine the corresponding expected net payments to note-holders under various scenarios for the financial markets and show that, under a broad range of scenarios, note-holders were likely to suffer substantial losses. As a consequence, we infer that the prospectus is sufficiently complex that financial advisers generally lacked the mathematical knowledge and expertise to understand the prospectus completely. Therefore, financial advisers who recommended purchases of the notes did not have the knowledge and expertise that is required by a fiduciary relationship, hence were unable to exercise fiduciary duty, and ultimately misguided their clients. We conclude that these reverse convertibles notes were designed by financial institutions to insure themselves, against significant declines in the equities markets, at the expense of note-holders.",0.0833333333],["late M dwarf system TRAPPIST-1 has seven known transiting planets. the late","TRAPPIST-1 Habitable Atmosphere Intercomparison . Motivations and protocol version 1.0","summarize: Upcoming telescopes such as the James Webb Space Telescope , or the Extremely Large Telescope , may soon be able to characterize, through transmission, emission or reflection spectroscopy, the atmospheres of rocky exoplanets orbiting nearby M dwarfs. One of the most promising candidates is the late M dwarf system TRAPPIST-1 which has seven known transiting planets for which Transit Timing Variation measurements suggest that they are terrestrial in nature, with a possible enrichment in volatiles. Among these seven planets, TRAPPIST-1e seems to be the most promising candidate to have habitable surface conditions, receiving ~66 % of the Earth's incident radiation, and thus needing only modest greenhouse gas inventories to raise surface temperatures to allow surface liquid water to exist. TRAPPIST-1e is therefore one of the prime targets for JWST atmospheric characterization. In this context, the modeling of its potential atmosphere is an essential step prior to observation. Global Climate Models offer the most detailed way to simulate planetary atmospheres. However, intrinsic differences exist between GCMs which can lead to different climate prediction and thus observability of gas and\/or cloud features in transmission and thermal emission spectra. Such differences should preferably be known prior to observations. In this paper we present a protocol to inter-compare planetary GCMs. Four testing cases are considered for TRAPPIST-1e but the methodology is applicable to other rocky exoplanets in the Habitable Zone. The four test cases included two land planets composed with a modern Earth and pure CO2 atmospheres, respectively, and two aqua planets with the same atmospheric compositions. Currently, there are four participating models , however this protocol is intended to let other teams participate as well.",0.1428571429],["coloured noise excitation is a non-markovian response. the coloured","A systematic path to non-Markovian dynamics: New response pdf evolution equations under Gaussian coloured noise excitation","summarize: Determining evolution equations governing the probability density function of non-Markovian responses to random differential equations excited by coloured noise, is an important issue arising in various problems of stochastic dynamics, advanced statistical physics and uncertainty quantification of macroscopic systems. In the present work, such equations are derived for a scalar, nonlinear RDE under additive coloured Gaussian noise excitation, through the stochastic Liouville equation. The latter is an exact, yet non-closed equation, involving aver-ages over the time history of the non-Markovian response. This nonlocality is treated by applying an extension of the Novikov-Furutsu theorem and a novel approximation, employing a stochastic Volterra-Taylor functional expansion around instantaneous response moments, leading to efficient, closed, approximate equations for the response pdf. These equations retain a tractable amount of nonlocality and nonlinearity, and they are valid in both the transient and long-time regimes for any correlation function of the excitation. Also, they include as special cases various existing relevant models, and generalize Hanggi's ansatz in a rational way. Numerical results for a bistable nonlinear RDE confirm the accuracy and the efficiency of the new equations. Extension to the multidimensional case is feasible, yet laborious.",0.3062838827],["the results of the Mulliken spin densities and the spin polarization will be","On the geometric and magnetic properties of the monomer, dimer and trimer of NiFe2O4","summarize: In this work, by employing Density Functional Theory, we compute and discuss some geometric and magnetic properties of the monomer, dimer and trimer of NiFe2 O4 . The calculations are performed at the UDFT\/ B3LYP level of calculation, by employing the LANL2DZ effective pseudo potential. The results of the Mulliken spin densities and the spin polarization will be presented. Finally the outcome of the system density of states is considered.",0.3527470776],["a cylindrical specimen and a cylindrical container are analyzed for three boundary conditions. the","Analytical Solution of a Gas Release Problem Considering Permeation with Time-Dependent Boundary Conditions","summarize: In this paper the determination of material properties such as Sieverts' constant and diffusivity via so-called gas release experiments is discussed. In order to simulate the time-dependent hydrogen fluxes and concentration profiles efficiently, we make use of an analytical method, namely we provide an analytical solution for the corresponding diffusion equations on a cylindrical specimen and a cylindrical container for three boundary conditions. These conditions occur in three phases -- loading phase, evacuation phase and gas release phase. In the loading phase the specimen is charged with hydrogen assuring a constant partial pressure of hydrogen. Then the gas will be quickly removed by a vacuum pump in the second phase, and finally in the third time interval, the hydrogen is released from the specimen to the gaseous phase, where the pressure increase will be measured by an equipment which is attached to the cylindrical container. The investigated diffusion equation in each phase is a simple homogeneous equation, but due to the complex time-dependent boundary conditions which include the Sieverts' constant and the pressure, we transform the homogeneous equations to the non-homogeneous ones with a zero Dirichlet boundary condition. Compared with the time consuming numerical methods our analytical approach has an advantage that the flux of desorbed hydrogen can be explicitly given and therefore can be evaluated efficiently. Our analytical solution also assures that the time-dependent boundary conditions are exactly satisfied and furthermore that the interaction between specimen and container is correctly taken into account.",0.4117647059],["signal-aligned network coding scheme for multiple-input multiple-output interference","Signal-Aligned Network Coding in K-User MIMO Interference Channels with Limited Receiver Cooperation","summarize: In this paper, we propose a signal-aligned network coding scheme for K-user time-varying multiple-input multiple-output interference channels with limited receiver cooperation. We assume that the receivers are connected to a central processor via wired cooperation links with individual limited capacities. Our SNC scheme determines the precoding matrices of the transmitters so that the transmitted signals are aligned at each receiver. The aligned signals are then decoded into noiseless integer combinations of messages, also known as network-coded messages, by physical-layer network coding. The key idea of our scheme is to ensure that independent integer combinations of messages can be decoded at the receivers. Hence the central processor can recover the original messages of the transmitters by solving the linearly independent equations. We prove that our SNC scheme achieves full degrees of freedom by utilizing signal alignment and physical-layer network coding. Simulation results show that our SNC scheme outperforms the compute-and-forward scheme in the finite SNR regime of the two-user and the three-user cases. The performance improvement of our SNC scheme mainly comes from efficient utilization of the signal subspaces for conveying independent linear equations of messages to the central processor.",0.0],["we have constructed dark energy models in an anisotropic Bianchi-V space-","Dark Energy Cosmological Models with General forms of Scale Factor","summarize: In this paper, we have constructed dark energy models in an anisotropic Bianchi-V space-time and studied the role of anisotropy in the evolution of dark energy. We have considered anisotropic dark energy fluid with different pressure gradients along different spatial directions. In order to obtain a deterministic solution, we have considered three general forms of scale factor. The different forms of scale factors considered here produce time varying deceleration parameters in all the cases that simulates the cosmic transition. The variable equation of state parameter, skewness parameters for all the models are obtained and analyzed. The physical properties of the models are also discussed.",0.0],["3D convolutional neural networks are difficult to train because they are parameter-expensive","Temporal Factorization of 3D Convolutional Kernels","summarize: 3D convolutional neural networks are difficult to train because they are parameter-expensive and data-hungry. To solve these problems we propose a simple technique for learning 3D convolutional kernels efficiently requiring less training data. We achieve this by factorizing the 3D kernel along the temporal dimension, reducing the number of parameters and making training from data more efficient. Additionally we introduce a novel dataset called Video-MNIST to demonstrate the performance of our method. Our method significantly outperforms the conventional 3D convolution in the low data regime . Finally, our model achieves competitive results in the high data regime using up to 45% fewer parameters.",0.1428571429],["the woods-Saxon potential and the schematic separable-type interaction are employed as","Rotational motion of triaxially deformed nuclei studied by microscopic angular-momentum-projection method II: Chiral doublet band","summarize: In the sequel of the present study, we have investigated the rotational motion of triaxially deformed nucleus by using the microscopic framework of angular-momentum projection. The Woods-Saxon potential and the schematic separable-type interaction are employed as a microscopic Hamiltonian. As the first example nuclear wobbling motion was studied in detail in the part~I of the series. This second part reports on another interesting rotational mode, chiral doublet bands: two prototype examples, ",0.0534726099],["","ETH Hardness of SVP","summarize: ",0.0497870684],["Let us know what you think about it!","Dimensional lower bounds for Falconer type incidence and point configuration theorems","summarize: Let ",0.0],["topological photonics provides a new paradigm in studying cavity quantum electrodynamics with robust","Cavity Quantum Electrodynamics with Second-Order Topological Corner State","summarize: Topological photonics provides a new paradigm in studying cavity quantum electrodynamics with robustness to disorder. In this work, we demonstrate the coupling between single quantum dots and the second-order topological corner state. Based on the second-order topological corner state, a topological photonic crystal cavity is designed and fabricated into GaAs slabs with quantum dots embedded. The coexistence of corner state and edge state with high quality factor close to 2000 is observed. The enhancement of photoluminescence intensity and emission rate are both observed when the quantum dot is on resonance with the corner state. This result enables the application of topology into cavity quantum electrodynamics, offering an approach to topological devices for quantum information processing.",0.3],["emph is a novel context-assisted single shot face detector.","PyramidBox: A Context-assisted Single Shot Face Detector","summarize: Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named \\emph to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: \\href}.",0.3333333333],["a substantial gap exists between the requirements of this community and the solutions currently available. the","Predicting Malicious Insider Threat Scenarios Using Organizational Data and a Heterogeneous Stack-Classifier","summarize: Insider threats continue to present a major challenge for the information security community. Despite constant research taking place in this area; a substantial gap still exists between the requirements of this community and the solutions that are currently available. This paper uses the CERT dataset r4.2 along with a series of machine learning classifiers to predict the occurrence of a particular malicious insider threat scenario - the uploading sensitive information to wiki leaks before leaving the organization. These algorithms are aggregated into a meta-classifier which has a stronger predictive performance than its constituent models. It also defines a methodology for performing pre-processing on organizational log data into daily user summaries for classification, and is used to train multiple classifiers. Boosting is also applied to optimise classifier accuracy. Overall the models are evaluated through analysis of their associated confusion matrix and Receiver Operating Characteristic curve, and the best performing classifiers are aggregated into an ensemble classifier. This meta-classifier has an accuracy of \\textbf with an area under the ROC curve of \\textbf.",0.3103448276],["the regular hyperbranched polymers have attracted a wide spread attention. in this","Mean trapping time for an arbitrary node on regular hyperbranched polymers","summarize: The regular hyperbranched polymers , also known as Vicsek fractals, are an important family of hyperbranched structures which have attracted a wide spread attention during the past several years. In this paper, we study the first-passage properties for random walks on the RHPs. Firstly, we propose a way to label all the different nodes of the RHPs and derive exact formulas to calculate the mean first-passage time between any two nodes and the mean trapping time for any trap node. Then, we compare the trapping efficiency between any two nodes of the RHPs by using the MTT as the measures of trapping efficiency. We find that the central node of the RHPs is the best trapping site and the nodes which are the farthest nodes from the central node are the worst trapping sites. Furthermore, we find that the maximum of the MTT is about ",0.5789473684],["the flow preserves the flow of hypersurfaces in hyperbolic space. this flow","Locally constrained curvature flows and geometric inequalities in hyperbolic space","summarize: In this paper, we first study the locally constrained curvature flow of hypersurfaces in hyperbolic space, which was introduced by Brendle, Guan and Li . This flow preserves the ",0.25],["large balls on unbounded model hypersurfaces in carnot-carath'","On Uniform Large-Scale Volume Growth for the Carnot-Carath\\'eodory Metric on Unbounded Model Hypersurfaces in ","summarize: We consider the rate of volume growth of large Carnot-Carath\\'eodory metric balls on a class of unbounded model hypersurfaces in ",0.1042155049],["a new chronocoulometric sensor was fabricated via electrodeposition. the sensor","Sensitive Chronocoulometric Detection of miRNA at Screen-printed Electrodes modified by Gold decorated MoS2 Nanosheets","summarize: Here a new chronocoulometric sensor, based on semiconducting 2H MoS2 nanosheets decorated with a controlled density of monodispersed small gold nanoparticles, was fabricated via electrodeposition, for the highly sensitive detection of miRNA-21. The size and interparticle spacing of AuNPs was optimized by controlling nucleation and growth rates through tuning of deposition-potential and Au-precursor concentration and by getting simultaneous feedback from morphological and electrochemical activity studies. The sensing strategy, involved the selective immobilization of thiolated capture probe DNA at AuNPs and hybridization of CP to a part of miRNA target, whereas the remaining part of the target was complementary to a signaling non-labelled DNA sequence. Chronocoulometry provided precise quantification of nucleic acids at each step of the sensor assay by interrogating 3+ electrostatically bound to phosphate backbones of oligonucleotides. A detailed and systematic optimization study demonstrated that the thinnest and smallest MoS2 NSs improved the sensitivity of the AuNP@MoS2 sensor achieving an impressive detection limit of 100 aM, which is 2 orders of magnitude lower than that of bare Au electrode and also enhanced the DNA-miRNA hybridization efficiency by 25%. Such improved performance can be attributed to the controlled packing density of CPs achieved by their self -assembly on AuNPs, large interparticle density, small size and the intimate coupling between AuNPs and MoS2. Alongside the outstanding sensitivity, the sensor exhibited excellent selectivity down to femtomolar concentrations, for discriminating complementary miRNA-21 target in a complex system composed of different foreign targets including mismatched and non-complementary miRNA-155. These advantages make our sensor a promising contender in the point of care miRNA sensor family for medical diagnostics.",0.1256850086],["the existence of a counterexample to the infinite-dimensional Carleson embedd","Two more counterexamples to the infinite dimensional Carleson embedding theorem","summarize: The existence of a counterexample to the infinite-dimensional Carleson embedding theorem has been established by Nazarov, Pisier, Treil, and Volberg. We provide an explicit construction of such an example. We also obtain a non-constructive example of particularly simple form; the density function of the measure is the tensor-square of a Hilbert space-valued analytic function. This special structure of the measure has implications for Hankel-like operators appearing in control theory.",0.7333333333],["properties of persistent spectral holes relevant for frequency metrology have been investigated in the system","Characteristics of long-lived persistent spectral holes in ","summarize: Properties of persistent spectral holes relevant for frequency metrology have been investigated in the system ",0.3125],["locally Optimal Block Preconditioned Conjugate Gradient is widely used to compute","A robust and efficient implementation of LOBPCG","summarize: Locally Optimal Block Preconditioned Conjugate Gradient is widely used to compute eigenvalues of large sparse symmetric matrices. The algorithm can suffer from numerical instability if it is not implemented with care. This is especially problematic when the number of eigenpairs to be computed is relatively large. In this paper we propose an improved basis selection strategy based on earlier work by Hetmaniuk and Lehoucq as well as a robust convergence criterion which is backward stable to enhance the robustness. We also suggest several algorithmic optimizations that improve performance of practical LOBPCG implementations. Numerical examples confirm that our approach consistently and significantly outperforms previous competing approaches in both stability and speed.",0.0],["the most frequently discussed approaches are focused on large and even very large scientific experiments. we offer","AstroDS -- A Distributed Storage for Astrophysics of Cosmic Rays. Current Status","summarize: Currently, the processing of scientific data in astroparticle physics is based on various distributed technologies, the most common of which are Grid and cloud computing. The most frequently discussed approaches are focused on large and even very large scientific experiments, such as Cherenkov Telescope Array. We, by contrast, offer a solution designed for small to medium experiments such as TAIGA. In such experiments, as a rule, historically developed specific data processing methods and specialized software are used. We have specifically designed a distributed data storage for astroparticle physics data collaboration in medium-sized experiments. In this article, we discuss the current state of our work using the example of the TAIGA and CASCADE experiments. A feature of our approach is that we provide our users with scientific data in the form to which they are accustomed to in everyday work on local resources.",0.0],["the present and future of evolutionary algorithms depends on the proper use of modern parallel and distributed computing infrastructure","It is Time for New Perspectives on How to Fight Bloat in GP","summarize: The present and future of evolutionary algorithms depends on the proper use of modern parallel and distributed computing infrastructures. Although still sequential approaches dominate the landscape, available multi-core, many-core and distributed systems will make users and researchers to more frequently deploy parallel version of the algorithms. In such a scenario, new possibilities arise regarding the time saved when parallel evaluation of individuals are performed. And this time saving is particularly relevant in Genetic Programming. This paper studies how evaluation time influences not only time to solution in parallel\/distributed systems, but may also affect size evolution of individuals in the population, and eventually will reduce the bloat phenomenon GP features. This paper considers time and space as two sides of a single coin when devising a more natural method for fighting bloat. This new perspective allows us to understand that new methods for bloat control can be derived, and the first of such a method is described and tested. Experimental data confirms the strength of the approach: using computing time as a measure of individuals' complexity allows to control the growth in size of genetic programming individuals.",0.0384615385],["the Wiener-Hopf equation for this case is derived. it involves two","Diffraction by a quarter-plane. Analytical continuation of spectral functions","summarize: The problem of diffraction by a Dirichlet quarter-plane in a 3D space is studied. The Wiener-Hopf equation for this case is derived and involves two unknown functions depending on two complex variables. The aim of the present work is to build an analytical continuation of these functions onto a well-described Riemann manifold and to study their behaviour and singularities on this manifold. In order to do so, integral formulae for analytical continuation of the spectral functions are derived and used. It is shown that the Wiener-Hopf problem can be reformulated using the concept of additive crossing of branch lines introduced in the paper. Both the integral formulae and the additive crossing reformulation are novel and represent the main results of this work.",0.0],["the unknown mean of the data generating process is modelled as a piecewise linear function","Bayesian detection of piecewise linear trends in replicated time-series with application to growth data modelling","summarize: We consider the situation where a temporal process is composed of contiguous segments with differing slopes and replicated noise-corrupted time series measurements are observed. The unknown mean of the data generating process is modelled as a piecewise linear function of time with an unknown number of change-points. We develop a Bayesian approach to infer the joint posterior distribution of the number and position of change-points as well as the unknown mean parameters. A-priori, the proposed model uses an overfitting number of mean parameters but, conditionally on a set of change-points, only a subset of them influences the likelihood. An exponentially decreasing prior distribution on the number of change-points gives rise to a posterior distribution concentrating on sparse representations of the underlying sequence. A Metropolis-Hastings Markov chain Monte Carlo sampler is constructed for approximating the posterior distribution. Our method is benchmarked using simulated data and is applied to uncover differences in the dynamics of fungal growth from imaging time course data collected from different strains. The source code is available on CRAN.",0.5],["quartz resonator is designed to trap phonons in a way that they","Inducing Strong Non-Linearities in a Phonon Trapping Quartz Bulk Acoustic Wave Resonator Coupled to a Superconducting Quantum Interference Device","summarize: A quartz Bulk Acoustic Wave resonator is designed to coherently trap phonons in a way that they are well confined and immune to suspension losses so they exhibit extremely high acoustic ",0.2790175729],["a complete interbank loan contract dataset is used to investigate if maturity details are informative of","Loan maturity aggregation in interbank lending networks obscures mesoscale structure and economic functions","summarize: Since the 2007-2009 financial crisis, substantial academic effort has been dedicated to improving our understanding of interbank lending networks . Because of data limitations or by choice, the literature largely lacks multiple loan maturities. We employ a complete interbank loan contract dataset to investigate whether maturity details are informative of the network structure. Applying the layered stochastic block model of Peixoto and other tools from network science on a time series of bilateral loans with multiple maturity layers in the Russian ILN, we find that collapsing all such layers consistently obscures mesoscale structure. The optimal maturity granularity lies between completely collapsing and completely separating the maturity layers and depends on the development phase of the interbank market, with a more developed market requiring more layers for optimal description. Closer inspection of the inferred maturity bins associated with the optimal maturity granularity reveals specific economic functions, from liquidity intermediation to financing. Collapsing a network with multiple underlying maturity layers or extracting one such layer, common in economic research, is therefore not only an incomplete representation of the ILN's mesoscale structure, but also conceals existing economic functions. This holds important insights and opportunities for theoretical and empirical studies on interbank market functioning, contagion, stability, and on the desirable level of regulatory data disclosure.",0.3461538462],["many works focus on 3D reconstruction from images. we focus on 3D shape reconstruction and","Implicit Functions in Feature Space for 3D Shape Reconstruction and Completion","summarize: While many works focus on 3D reconstruction from images, in this paper, we focus on 3D shape reconstruction and completion from a variety of 3D inputs, which are deficient in some respect: low and high resolution voxels, sparse and dense point clouds, complete or incomplete. Processing of such 3D inputs is an increasingly important problem as they are the output of 3D scanners, which are becoming more accessible, and are the intermediate output of 3D computer vision algorithms. Recently, learned implicit functions have shown great promise as they produce continuous reconstructions. However, we identified two limitations in reconstruction from 3D inputs: 1) details present in the input data are not retained, and 2) poor reconstruction of articulated humans. To solve this, we propose Implicit Feature Networks , which deliver continuous outputs, can handle multiple topologies, and complete shapes for missing or sparse input data retaining the nice properties of recent learned implicit functions, but critically they can also retain detail when it is present in the input data, and can reconstruct articulated humans. Our work differs from prior work in two crucial aspects. First, instead of using a single vector to encode a 3D shape, we extract a learnable 3-dimensional multi-scale tensor of deep features, which is aligned with the original Euclidean space embedding the shape. Second, instead of classifying x-y-z point coordinates directly, we classify deep features extracted from the tensor at a continuous query point. We show that this forces our model to make decisions based on global and local shape structure, as opposed to point coordinates, which are arbitrary under Euclidean transformations. Experiments demonstrate that IF-Nets clearly outperform prior work in 3D object reconstruction in ShapeNet, and obtain significantly more accurate 3D human reconstructions.",0.3548387097],["the string lies on the Winkler foundation with a point inhomogeneity","Non-stationary localized oscillations of an infinite string, with time-varying tension, lying on the Winkler foundation with a point elastic inhomogeneity","summarize: We consider non-stationary oscillations of an infinite string with time-varying tension. The string lies on the Winkler foundation with a point inhomogeneity . In such a system with constant parameters , under certain conditions a trapped mode of oscillation exists and is unique. Therefore, applying a non-stationary external excitation to this system can lead to the emergence of the string oscillations localized near the inhomogeneity. We provide an analytical description of non-stationary localized oscillations of the string with slowly time-varying tension using the asymptotic procedure based on successive application of two asymptotic methods, namely the method of stationary phase and the method of multiple scales. The obtained analytical results were verified by independent numerical calculations based on the finite difference method. The applicability of the analytical formulas was demonstrated for various types of external excitation and laws governing the varying tension. In particular, we have shown that in the case when the trapped mode frequency approaches zero, localized low-frequency oscillations with increasing amplitude precede the localized string buckling. The dependence of the amplitude of such oscillations on its frequency is more complicated in comparison with the case of a one degree of freedom system with time-varying stiffness.",0.3529865342],["computational fluid Dynamics simulations are a very important tool for many industrial applications. the output","PREPRINT: Comparison of deep learning and hand crafted features for mining simulation data","summarize: Computational Fluid Dynamics simulations are a very important tool for many industrial applications, such as aerodynamic optimization of engineering designs like cars shapes, airplanes parts etc. The output of such simulations, in particular the calculated flow fields, are usually very complex and hard to interpret for realistic three-dimensional real-world applications, especially if time-dependent simulations are investigated. Automated data analysis methods are warranted but a non-trivial obstacle is given by the very large dimensionality of the data. A flow field typically consists of six measurement values for each point of the computational grid in 3D space and time . In this paper we address the task of extracting meaningful results in an automated manner from such high dimensional data sets. We propose deep learning methods which are capable of processing such data and which can be trained to solve relevant tasks on simulation data, i.e. predicting drag and lift forces applied on an airfoil. We also propose an adaptation of the classical hand crafted features known from computer vision to address the same problem and compare a large variety of descriptors and detectors. Finally, we compile a large dataset of 2D simulations of the flow field around airfoils which contains 16000 flow fields with which we tested and compared approaches. Our results show that the deep learning-based methods, as well as hand crafted feature based approaches, are well-capable to accurately describe the content of the CFD simulation output on the proposed dataset.",0.4],["a small-signal equivalent circuit of 2D-material based field-effect transistor","Small-signal model for 2D-material based field-effect transistors targeting radio-frequency applications: the importance of considering non-reciprocal capacitances","summarize: A small-signal equivalent circuit of 2D-material based field-effect transistors is presented. Charge conservation and non-reciprocal capacitances have been assumed so the model can be used to make reliable predictions at both device and circuit levels. In this context, explicit and exact analytical expressions of the main radio-frequency figures of merit of these devices are given. Moreover, a direct parameter extraction methodology is provided based on S-parameter measurements. In addition to the intrinsic capacitances, transconductance and output conductance, our approach allows extracting the series combination of drain\/source metal contact and access resistances. Accounting for these extrinsic resistances is of upmost importance when dealing with low dimensional field-effect transistors.",0.373283482],["flare was triggered by emerging magnetic bipolar region. the magnetic flux was ob","Super Penumbral Chromospheric Flare","summarize: We observed a C-class flare at the outer boundary of the super-penumbra of a sunspot. The flare was triggered by an emerging magnetic bipolar region that was obliquely oriented with respect to the super-penumbral fibrils. The flare started due to the low height magnetic reconnection of emerging magnetic flux with super-penumbral field resulting hot multi-temperature plasma flows in the inverse Evershed flow channel and its overlying atmosphere. The inverse Evershed flows in the chromosphere start from super penumbra towards sunspot that end at the outer boundary of the penumbra. The hot plasma flow towards the sunspot in the inverse Evershed channels show about 10 km s",0.0],["study uses questionnaire surveys with sparse samples and non-individual level statistical data.","CD-CNN: A Partially Supervised Cross-Domain Deep Learning Model for Urban Resident Recognition","summarize: Driven by the wave of urbanization in recent decades, the research topic about migrant behavior analysis draws great attention from both academia and the government. Nevertheless, subject to the cost of data collection and the lack of modeling methods, most of existing studies use only questionnaire surveys with sparse samples and non-individual level statistical data to achieve coarse-grained studies of migrant behaviors. In this paper, a partially supervised cross-domain deep learning model named CD-CNN is proposed for migrant\/native recognition using mobile phone signaling data as behavioral features and questionnaire survey data as incomplete labels. Specifically, CD-CNN features in decomposing the mobile data into location domain and communication domain, and adopts a joint learning framework that combines two convolutional neural networks with a feature balancing scheme. Moreover, CD-CNN employs a three-step algorithm for training, in which the co-training step is of great value to partially supervised cross-domain learning. Comparative experiments on the city Wuxi demonstrate the high predictive power of CD-CNN. Two interesting applications further highlight the ability of CD-CNN for in-depth migrant behavioral analysis.",0.0],["microbial growth model predicts rich population dynamics when there are tradeoffs between phases","Tradeoffs between microbial growth phases lead to frequency-dependent and non-transitive selection","summarize: Mutations in a microbial population can increase the frequency of a genotype not only by increasing its exponential growth rate, but also by decreasing its lag time or adjusting the yield . The contribution of multiple life-history traits to selection is a critical question for evolutionary biology as we seek to predict the evolutionary fates of mutations. Here we use a model of microbial growth to show there are two distinct components of selection corresponding to the growth and lag phases, while the yield modulates their relative importance. The model predicts rich population dynamics when there are tradeoffs between phases: multiple strains can coexist or exhibit bistability due to frequency-dependent selection, and strains can engage in rock-paper-scissors interactions due to non-transitive selection. We characterize the environmental conditions and patterns of traits necessary to realize these phenomena, which we show to be readily accessible to experiments. Our results provide a theoretical framework for analyzing high-throughput measurements of microbial growth traits, especially interpreting the pleiotropy and correlations between traits across mutants. This work also highlights the need for more comprehensive measurements of selection in simple microbial systems, where the concept of an ordinary fitness landscape breaks down.",0.3076923077],["we identify emergent topological phenomena such as dynamic Chern numbers and dynamic quantum phase transition","Fixed points and emergent topological phenomena in a parity-time-symmetric quantum quench","summarize: We identify emergent topological phenomena such as dynamic Chern numbers and dynamic quantum phase transitions in quantum quenches of the non-Hermitian Su-Schrieffer-Heeger Hamiltonian with parity-time symmetry. Their occurrence in the non-unitary dynamics are intimately connected with fixed points in the Brillouin zone, where the states do not evolve in time. We construct a theoretical formalism for characterizing topological properties in non-unitary dynamics within the framework of biorthogonal quantum mechanics, and prove the existence of fixed points for quenches between distinct static topological phases in the ",0.2777777778],["a new approach is developed with the help of a domain expert. we train Con","Prior Information Guided Regularized Deep Learning for Cell Nucleus Detection","summarize: Cell nuclei detection is a challenging research topic because of limitations in cellular image quality and diversity of nuclear morphology, i.e. varying nuclei shapes, sizes, and overlaps between multiple cell nuclei. This has been a topic of enduring interest with promising recent success shown by deep learning methods. These methods train Convolutional Neural Networks with a training set of input images and known, labeled nuclei locations. Many such methods are supplemented by spatial or morphological processing. Using a set of canonical cell nuclei shapes, prepared with the help of a domain expert, we develop a new approach that we call Shape Priors with Convolutional Neural Networks . We further extend the network to introduce a shape prior layer and then allowing it to become trainable . We call this network tunable SP-CNN . In summary, we present new network structures that can incorporate 'expected behavior' of nucleus shapes via two components: learnable layers that perform the nucleus detection and a fixed processing part that guides the learning with prior information. Analytically, we formulate two new regularization terms that are targeted at: 1) learning the shapes, 2) reducing false positives while simultaneously encouraging detection inside the cell nucleus boundary. Experimental results on two challenging datasets reveal that the proposed SP-CNN and TSP-CNN can outperform state-of-the-art alternatives.",0.24],["algorithms for strain tomography from energy-resolved neutron transmission measurements have been","Neutron Transmission Strain Tomography for Non-Constant Stress-Free Lattice Spacing","summarize: Recently, several algorithms for strain tomography from energy-resolved neutron transmission measurements have been proposed. These methods assume that the stress-free lattice spacing ",0.0909090909],["carsharing is a model of renting vehicles for short periods of time. payment is made","Proposal of a Carsharing System to Improve Urban Mobility","summarize: Carsharing is a model of renting vehicles for short periods of time, where the payment is made according to the time and distance effectively traveled. Carsharing offers a simple, economical and smart alternative to urban mobility, that is already being adopted in the major cities in the world. The proposed methodology consisted in the development of a decision support system that simplifies the process of choosing carsharing services. Adopting the AHP method, the user can indicate their preferences in the choice of vehicles, and the system returns an ordered list of the most suitable available vehicles based on their geographic location. The findings of the project indicate that the use of this system encourage and simplify the use of carsharing services, which will allow to enhance the financial, mobility and environment advantages inherent to their use.",0.3043478261],["a new population of close dual active galactic nuclei is being investigated. the","Hubble Space Telescope Wide Field Camera 3 Identifies an ","summarize: Kiloparsec-scale dual active galactic nuclei are active supermassive black hole pairs co-rotating in galaxies with separations of less than a few kpc. Expected to be a generic outcome of hierarchical galaxy formation, their frequency and demographics remain uncertain. We have carried out an imaging survey with the Hubble Space Telescope Wide Field Camera 3 of AGNs with double-peaked narrow emission lines. HST\/WFC3 offers high image quality in the near-infrared to resolve the two stellar nuclei, and in the optical to resolve from ionized gas in the narrow-line regions. This combination has proven to be key in sorting out alternative scenarios. With HST\/WFC3 we are able to explore a new population of close dual AGNs at more advanced merger stages than can be probed from the ground. Here we show that the AGN SDSS J0924+0510, which had previously shown two stellar bulges, contains two spatially distinct regions consistent with a dual AGN. While we cannot completely exclude cross-ionization from a single central engine, the nearly equal ratios of strongly suggest a dual AGN with a projected angular separation of 0.4, corresponding to a projected physical separation of ",0.2105263158],["an interference graph is employed to illustrate the partially-coupled cost functions. an algorithm is","A Distributed Nash Equilibrium Seeking in Networked Graphical Games","summarize: This paper considers a distributed gossip approach for finding a Nash equilibrium in networked games on graphs. In such games a player's cost function may be affected by the actions of any subset of players. An interference graph is employed to illustrate the partially-coupled cost functions and the asymmetric information requirements. For a given interference graph, network communication between players is considered to be limited. A generalized communication graph is designed so that players exchange only their required information. An algorithm is designed whereby players, with possibly partially-coupled cost functions, make decisions based on the estimates of other players' actions obtained from local neighbors. It is shown that this choice of communication graph guarantees that all players' information is exchanged after sufficiently many iterations. Using a set of standard assumptions on the cost functions, the interference and the communication graphs, almost sure convergence to a Nash equilibrium is proved for diminishing step sizes. Moreover, the case when the cost functions are not known by the players is investigated and a convergence proof is presented for diminishing step sizes. The effect of the second largest eigenvalue of the expected communication matrix on the convergence rate is quantified. The trade-off between parameters associated with the communication graph and the ones associated with the interference graph is illustrated. Numerical results are presented for a large-scale networked game.",0.1111111111],["voltage phase angle measurements are used in distribution-level microgrid interconnections. these","Distributed Mixed Voltage Angle and Frequency Droop Control of Microgrid Interconnections with Loss of Distribution-PMU Measurements","summarize: Recent advances in distribution-level phasor measurement unit technology have enabled the use of voltage phase angle measurements for direct load sharing control in distribution-level microgrid interconnections with high penetration of renewable distributed energy resources . In particular, D-PMU enabled voltage angle droop control has the potential to enhance stability and transient performance in such microgrid interconnections. However, these angle droop control designs are vulnerable to D-PMU angle measurement losses that frequently occur due to the unavailability of a GPS signal for synchronization. In the event of such measurement losses, angle droop controlled microgrid interconnections may suffer from poor performance and potentially lose stability. In this paper, we propose a novel distributed mixed voltage angle and frequency droop control framework to improve the reliability of angle droop controlled microgrid interconnections. In this framework, when the D-PMU phase angle measurement is lost at a microgrid, conventional frequency droop control is temporarily used for primary control in place of angle droop control to guarantee stability. We model the microgrid interconnection with this primary control architecture as a nonlinear switched system and design distributed secondary controllers to guarantee transient stability of the network. Further, we incorporate performance specifications such as robustness to generation-load mismatch and network topology changes in the distributed control design. We demonstrate the performance of this control framework by simulation on a test 123-feeder distribution network.",0.0],["the OB hydrodynamical system is a viscous source term in the heat con","Self-similar analysis of a viscous heated Oberbeck-Boussinesq flow system","summarize: The simplest model to couple the heat conduction and Navier-Stokes equations together is the Oberbeck-Boussinesqsystem which were investigated by E.N. Lorenz and opened the paradigm of chaos. In our former studies - Chaos, Solitons and Fractals 78, 249 , ibid, 103, 336 - we derived analytic solutions for the velocity, pressure and temperature fields. Additionally, we gave a possible explanation of the Rayleigh-B\\`enard convection cells with the help of the self-similar Ansatz. Now we generalize the OB hydrodynamical system, including a viscous source term in the heat conduction equation. Our results may attract the interest of various fields like micro or nanofluidics or climate studies.",0.5263157895],["a reranker trained with global features is expensive to obtain. a method","Entity-Aware Language Model as an Unsupervised Reranker","summarize: In language modeling, it is difficult to incorporate entity relationships from a knowledge-base. One solution is to use a reranker trained with global features, in which global features are derived from n-best lists. However, training such a reranker requires manually annotated n-best lists, which is expensive to obtain. We propose a method based on the contrastive estimation method that alleviates the need for such data. Experiments in the music domain demonstrate that global features, as well as features extracted from an external knowledge-base, can be incorporated into our reranker. Our final model, a simple ensemble of a language model and reranker, achieves a 0.44\\% absolute word error rate improvement over an LSTM language model on the blind test data.",0.5416666667],["NER for Myanmar language is treated as a sequence tagging problem. the effectiveness","Syllable-based Neural Named Entity Recognition for Myanmar Language","summarize: Named Entity Recognition for Myanmar Language is essential to Myanmar natural language processing research work. In this work, NER for Myanmar language is treated as a sequence tagging problem and the effectiveness of deep neural networks on NER for Myanmar language has been investigated. Experiments are performed by applying deep neural network architectures on syllable level Myanmar contexts. Very first manually annotated NER corpus for Myanmar language is also constructed and proposed. In developing our in-house NER corpus, sentences from online news website and also sentences supported from ALT-Parallel-Corpus are also used. This ALT corpus is one part of the Asian Language Treebank project under ASEAN IVO. This paper contributes the first evaluation of neural network models on NER task for Myanmar language. The experimental results show that those neural sequence models can produce promising results compared to the baseline CRF model. Among those neural architectures, bidirectional LSTM network added CRF layer above gives the highest F-score value. This work also aims to discover the effectiveness of neural network approaches to Myanmar textual processing as well as to promote further researches on this understudied language.",0.55],["a method is proposed for obtaining diffuse field measurements in untreated environments. a","A Novel Method for Obtaining Diffuse Field Measurements for Microphone Calibration","summarize: We propose a straightforward and cost-effective method to perform diffuse soundfield measurements for calibrating the magnitude response of a microphone array. Typically, such calibration is performed in a diffuse soundfield created in reverberation chambers, an expensive and time-consuming process. A method is proposed for obtaining diffuse field measurements in untreated environments. First, a closed-form expression for the spatial correlation of a wideband signal in a diffuse field is derived. Next, we describe a practical procedure for obtaining the diffuse field response of a microphone array in the presence of a non-diffuse soundfield by the introduction of random perturbations in the microphone location. Experimental spatial correlation data obtained is compared with the theoretical model, confirming that it is possible to obtain diffuse field measurements in untreated environments with relatively few loudspeakers. A 30 second test signal played from 4-8 loudspeakers is shown to be sufficient in obtaining a diffuse field measurement using the proposed method. An Eigenmike is then successfully calibrated at two different geographical locations.",0.4782608696],["embeddings fail to precisely represent users\/items with uncertainty. unified","Convolutional Gaussian Embeddings for Personalized Recommendation with Uncertainty","summarize: Most of existing embedding based recommendation models use embeddings corresponding to a single fixed point in low-dimensional space, to represent users and items. Such embeddings fail to precisely represent the users\/items with uncertainty often observed in recommender systems. Addressing this problem, we propose a unified deep recommendation framework employing Gaussian embeddings, which are proven adaptive to uncertain preferences exhibited by some users, resulting in better user representations and recommendation performance. Furthermore, our framework adopts Monte-Carlo sampling and convolutional neural networks to compute the correlation between the objective user and the candidate item, based on which precise recommendations are achieved. Our extensive experiments on two benchmark datasets not only justify that our proposed Gaussian embeddings capture the uncertainty of users very well, but also demonstrate its superior performance over the state-of-the-art recommendation models.",0.1111111111],["sketch-and-solve methods randomly project the data first, and do regression after.","Asymptotics for Sketching in Least Squares Regression","summarize: We consider a least squares regression problem where the data has been generated from a linear model, and we are interested to learn the unknown regression parameters. We consider sketch-and-solve methods that randomly project the data first, and do regression after. Previous works have analyzed the statistical and computational performance of such methods. However, the existing analysis is not fine-grained enough to show the fundamental differences between various methods, such as the Subsampled Randomized Hadamard Transform and Gaussian projections. In this paper, we make progress on this problem, working in an asymptotic framework where the number of datapoints and dimension of features goes to infinity. We find the limits of the accuracy loss incurred by popular sketching methods. We show separation between different methods, so that SRHT is better than Gaussian projections. Our theoretical results are verified on both real and synthetic data. The analysis of SRHT relies on novel methods from random matrix theory that may be of independent interest.",0.0],["we consider continuous-time stochastic optimal control problems. the problem is a bi","Optimal Control of Conditional Value-at-Risk in Continuous Time","summarize: We consider continuous-time stochastic optimal control problems featuring Conditional Value-at-Risk in the objective. The major difficulty in these problems arises from time-inconsistency, which prevents us from directly using dynamic programming. To resolve this challenge, we convert to an equivalent bilevel optimization problem in which the inner optimization problem is standard stochastic control. Furthermore, we provide conditions under which the outer objective function is convex and differentiable. We compute the outer objective's value via a Hamilton-Jacobi-Bellman equation and its gradient via the viscosity solution of a linear parabolic equation, which allows us to perform gradient descent. The significance of this result is that we provide an efficient dynamic programming-based algorithm for optimal control of CVaR without lifting the state-space. To broaden the applicability of the proposed algorithm, we propose convergent approximation schemes in cases where our key assumptions do not hold and characterize relevant suboptimality bounds. In addition, we extend our method to a more general class of risk metrics, which includes mean-variance and median-deviation. We also demonstrate a concrete application to portfolio optimization under CVaR constraints. Our results contribute an efficient framework for solving time-inconsistent CVaR-based sequential optimization.",0.2666666667],["high quality upsampling of sparse 3D point clouds is critically useful for","Data-driven Upsampling of Point Clouds","summarize: High quality upsampling of sparse 3D point clouds is critically useful for a wide range of geometric operations such as reconstruction, rendering, meshing, and analysis. In this paper, we propose a data-driven algorithm that enables an upsampling of 3D point clouds without the need for hard-coded rules. Our approach uses a deep network with Chamfer distance as the loss function, capable of learning the latent features in point clouds belonging to different object categories. We evaluate our algorithm across different amplification factors, with upsampling learned and performed on objects belonging to the same category as well as different categories. We also explore the desirable characteristics of input point clouds as a function of the distribution of the point samples. Finally, we demonstrate the performance of our algorithm in single-category training versus multi-category training scenarios. The final proposed model is compared against a baseline, optimization-based upsampling method. Results indicate that our algorithm is capable of generating more uniform and accurate upsamplings.",0.0833333333],["automatic colorization is the process of adding color to greyscale images. we condition this process","Learning to Color from Language","summarize: Automatic colorization is the process of adding color to greyscale images. We condition this process on language, allowing end users to manipulate a colorized image by feeding in different captions. We present two different architectures for language-conditioned colorization, both of which produce more accurate and plausible colorizations than a language-agnostic version. Through this language-based framework, we can dramatically alter colorizations by manipulating descriptive color words in captions.",0.05],["paper proposes the Seamless Internetwork Flow Mobility architecture. the architecture uses this","SIFM: A network architecture for seamless flow mobility between LTE and WiFi networks - Analysis and Testbed Implementation","summarize: This paper deals with cellular networks that selectively offload the mobile data traffic onto WiFi networks to improve network performance. We propose the Seamless Internetwork Flow Mobility architecture that provides seamless flow-mobility support using concepts of Software Defined Networking . The SDN paradigm decouples the control and data plane, leading to a centralized network intelligence and state. The SIFM architecture utilizes this aspect of SDN and moves the mobility decisions to a centralized Flow Controller . This provides a global network view while making mobility decisions and also reduces the complexity at the PGW. We implement and evaluate both basic PMIPv6 and the SIFM architectures by incorporating salient LTE and WiFi network features in the ns-3 simulator. Performance experiments validate that seamless mobility is achieved. Also, the SIFM architecture shows an improved network performance when compared to the base PMIPv6 architecture. A proof-of-concept prototype of the SIFM architecture has been implemented on an experimental testbed. The LTE network is emulated by integrating USRP B210x with the OpenLTE eNodeB and OpenLTE EPC. The WiFi network is emulated using hostapd and dnsmasq daemons running on Ubuntu 12.04. An off-the-shelf LG G2 mobile phone running Android 4.2.2 is used as the user equipment. We demonstrate seamless mobility between the LTE network and the WiFi network with the help of ICMP ping and a TCP chat application.",0.0758163325],["parallel processes are actually interleaved according to some interleaving strategy.","Process algebra with strategic interleaving","summarize: In process algebras such as ACP , parallel processes are considered to be interleaved in an arbitrary way. In the case of multi-threading as found in contemporary programming languages, parallel processes are actually interleaved according to some interleaving strategy. An interleaving strategy is what is called a process-scheduling policy in the field of operating systems. In many systems, for instance hardware\/software systems, we have to do with both parallel processes that may best be considered to be interleaved in an arbitrary way and parallel processes that may best be considered to be interleaved according to some interleaving strategy. Therefore, we extend ACP in this paper with the latter form of interleaving. The established properties of the extension concerned include an elimination property, a conservative extension property, and a unique expansion property.",0.1],["experimental evidence for probabilistic contextuality in psychology. previous attempts to find contextuality in","Snow Queen is Evil and Beautiful: Experimental Evidence for Probabilistic Contextuality in Human Choices","summarize: We present unambiguous experimental evidence for probabilistic contextuality in psychology. All previous attempts to find contextuality in a psychological experiment were unsuccessful because of the gross violations of marginal selectivity in behavioral data, making the traditional mathematical tests developed in quantum mechanics inapplicable. In our crowdsourcing experiment respondents were making two simple choices: of one of two characters in a story , and of one of two characteristics, such as Kind and Evil, so that the character and the characteristic chosen matched the story line. The formal structure of the experiment imitated that of the Einstein-Podolsky-Rosen paradigm in the Bohm-Bell version. Marginal selectivity was violated, indicating that the two choices were directly influencing each other, but the application of a mathematical test developed in the Contextuality-by-Default theory, extending the traditional quantum-mechanical test, indicated a strong presence of contextuality proper, not reducible to direct influences.",0.1462043808],["a series of 'half-histories' are a series of '","The concept of velocity in the history of Brownian motion -- From physics to mathematics and vice versa","summarize: Brownian motion is a complex object shared by different communities: first observed by the botanist Robert Brown in 1827, then theorised by physicists in the 1900s, and eventually modelled by mathematicians from the 1920s. Consequently, it is now ambiguously referring to the natural phenomenon but also to the theories accounting for it. There is no published work telling its entire history from its discovery until today, but rather partial histories either from 1827 to Perrin's experiments in the late 1900s, from a physicist's point of view; or from the 1920s from a mathematician's point of view. In this article, we tackle a period straddling the two `half-histories' just mentioned, in order to highlight its continuity, to question the relationship between physics and mathematics, and to remove the ambiguities mentioned above. We study the works of Einstein, Smoluchowski, Langevin, Wiener, Ornstein and Uhlenbeck from 1905 to 1934 as well as experimental results, through the concept of Brownian velocity. We show how Brownian motion became a research topic for the mathematician Wiener in the 1920s, why his model was an idealization of physical reality, what Ornstein and Uhlenbeck added to Einstein's results and how Wiener, Ornstein and Uhlenbeck developed in parallel contradictory theories concerning Brownian velocity.",0.2102168235],["Symbolic musical phrases were generated and tagged with emotional information by human musicians.","Establishing Human-Robot Trust through Music-Driven Robotic Emotion Prosody and Gesture","summarize: As human-robot collaboration opportunities continue to expand, trust becomes ever more important for full engagement and utilization of robots. Affective trust, built on emotional relationship and interpersonal bonds is particularly critical as it is more resilient to mistakes and increases the willingness to collaborate. In this paper we present a novel model built on music-driven emotional prosody and gestures that encourages the perception of a robotic identity, designed to avoid uncanny valley. Symbolic musical phrases were generated and tagged with emotional information by human musicians. These phrases controlled a synthesis engine playing back pre-rendered audio samples generated through interpolation of phonemes and electronic instruments. Gestures were also driven by the symbolic phrases, encoding the emotion from the musical phrase to low degree-of-freedom movements. Through a user study we showed that our system was able to accurately portray a range of emotions to the user. We also showed with a significant result that our non-linguistic audio generation achieved an 8% higher mean of average trust than using a state-of-the-art text-to-speech system.",0.0769230769],["in cooperation, the agents must know how co-workers behave. but the policy is","Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents","summarize: In cooperation, the workers must know how co-workers behave. However, an agent's policy, which is embedded in a statistical machine learning model, is hard to understand, and requires much time and knowledge to comprehend. Therefore, it is difficult for people to predict the behavior of machine learning robots, which makes Human Robot Cooperation challenging. In this paper, we propose Instruction-based Behavior Explanation , a method to explain an autonomous agent's future behavior. In IBE, an agent can autonomously acquire the expressions to explain its own behavior by reusing the instructions given by a human expert to accelerate the learning of the agent's policy. IBE also enables a developmental agent, whose policy may change during the cooperation, to explain its own behavior with sufficient time granularity.",0.0666666667],["autophagosome formation involves dynamic morphological changes. disk-shaped membrane c","Modeling membrane morphological change during autophagosome formation","summarize: Autophagy is an intracellular degradation process that is mediated by de novo formation of autophagosomes. Autophagosome formation involves dynamic morphological changes; a disk-shaped membrane cisterna grows, bends to become a cup-shaped structure, and finally develops into a spherical autophagosome. We have constructed a theoretical model that integrates the membrane morphological change and entropic partitioning of putative curvature generators, which we have used to investigate the autophagosome formation process quantitatively. We show that the membrane curvature and the distribution of the curvature generators stabilize disk- and cup-shaped intermediate structures during autophagosome formation, which is quantitatively consistent with in vivo observations. These results suggest that various autophagy proteins with membrane curvature-sensing properties control morphological change by stabilizing these intermediate structures. Our model provides a framework for understanding autophagosome formation.",0.5],["monadic second-order Logic defines a class of languages that has algebraic","Monadic Second-Order Logic with Arbitrary Monadic Predicates","summarize: We study Monadic Second-Order Logic over finite words, extended with monadic predicates. We show that it defines a class of languages that has algebraic, automata-theoretic and machine-independent characterizations. We consider the regularity question: given a language in this class, when is it regular? To answer this, we show a substitution property and the existence of a syntactical predicate. We give three applications. The first two are to give very simple proofs that the Straubing Conjecture holds for all fragments of MSO with monadic predicates, and that the Crane Beach Conjecture holds for MSO with monadic predicates. The third is to show that it is decidable whether a language defined by an MSO formula with morphic predicates is regular.",0.2631578947],["conventional methods for forecasting forward citations cast the problem as analysis of temporal point","Patent Citation Dynamics Modeling via Multi-Attention Recurrent Networks","summarize: Modeling and forecasting forward citations to a patent is a central task for the discovery of emerging technologies and for measuring the pulse of inventive progress. Conventional methods for forecasting these forward citations cast the problem as analysis of temporal point processes which rely on the conditional intensity of previously received citations. Recent approaches model the conditional intensity as a chain of recurrent neural networks to capture memory dependency in hopes of reducing the restrictions of the parametric form of the intensity function. For the problem of patent citations, we observe that forecasting a patent's chain of citations benefits from not only the patent's history itself but also from the historical citations of assignees and inventors associated with that patent. In this paper, we propose a sequence-to-sequence model which employs an attention-of-attention mechanism to capture the dependencies of these multiple time sequences. Furthermore, the proposed model is able to forecast both the timestamp and the category of a patent's next citation. Extensive experiments on a large patent citation dataset collected from USPTO demonstrate that the proposed model outperforms state-of-the-art models at forward citation forecasting.",0.0],["two-way relay is potentially an effective approach to spectrum sharing and aggregation.","On the DoF of Two-way ","summarize: Two-way relay is potentially an effective approach to spectrum sharing and aggregation by allowing simultaneous bidirectional transmissions between source-destinations pairs. This paper studies the two-way ",0.0],["the orbits in Rindler space are quite different from that of Schwarzschild case.","Motion of Massive Particles in Rindler Space and the Problem of Fall at the Centre","summarize: The motion of a massive particle in Rindler space has been studied and obtained the geodesics of motion. The orbits in Rindler space are found to be quite different from that of Schwarzschild case. The paths are not like the Perihelion Precession type. Further we have set up the non-relativistic Schrodinger equation for the particle in the quantum mechanical scenario in presence of background constant gravitational field and investigated the problem of fall of the particle at the center. This problem is also treated classically. Unlike the conventional scenario, here the fall occurs at the surface of a sphere of unit radius.",0.3062156854],["the 5th international workshop on rewriting techniques for program transformations and evaluation held on 8","Proceedings Fifth International Workshop on Rewriting Techniques for Program Transformations and Evaluation","summarize: This volume contains the formal proceedings of the 5th International Workshop on Rewriting Techniques for Program Transformations and Evaluation , held on 8th of Juli 2018 in Oxford, United Kingdom, and affiliated with FLoC 2018 and FSCD 2018. Scope of WPTE: Rewriting techniques are of great help for studying correctness of program transformations, translations and evaluation, and the aim of WPTE is to bring together the researchers working on program transformations, evaluation, and operationally-based programming language semantics, using rewriting methods, in order to share the techniques and recent developments and to exchange ideas to encourage further activation of research in this area. Topics in the scope of WPTE include the correctness of program transformations, optimisations and translations; program transformations for proving termination, confluence and other properties; correctness of evaluation strategies; operational semantics of programs, operationally-based program equivalences such as contextual equivalences and bisimulations; cost-models for reasoning about the optimizing power of transformations and the costs of evaluation; program transformations for verification and theorem proving purposes; translation, simulation, equivalence of programs with different formalisms, and evaluation strategies; program transformations for applying rewriting techniques to programs in specific programming languages; program transformations for program inversions and program synthesis; program transformation and evaluation for Haskell and rewriting. Research Paper Selection: At the workshop six research papers were presented of which five were accepted for the postproceedings. Each submission was reviewed by three or four members of the Program Committee in two to three rounds, one round for workshop presentation and at most two rounds for publication to the postproceedings. The program also included one invited talk by Jean-Pierre Jouannaud on a framework for graph rewriting; the abstract of this talk is included in this volume.",0.4583333333],["integro-differential methods are applied to Neumann Homogenization problems","Neumann Homogenization via Integro-Differential Operators, Part 2: singular gradient dependence","summarize: We continue the program initiated in a previous work, of applying integro-differential methods to Neumann Homogenization problems. We target the case of linear periodic equations with a singular drift, which includes divergence equations with \\emph oscillatory Neumann conditions. Our analysis focuses on an induced integro-differential homogenization problem on the boundary of the domain. Also, we use homogenization results for regular Dirichlet problems to build barriers for the oscillatory Neumann problem with the singular gradient term. We note that our method allows to recast some existing results for fully nonlinear Neumann homogenization into this same framework. This version is the journal version.",0.2920502937],["we propose a scalable, efficient and accurate approach to retrieve 3D models. we","3D Pose Estimation and 3D Model Retrieval for Objects in the Wild","summarize: We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.",0.2272727273],["image-based 3D reconstruction using convolutional neural networks has attracted increasing interest.","Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the Deep Learning Era","summarize: 3D reconstruction is a longstanding ill-posed problem, which has been explored for decades by the computer vision, computer graphics, and machine learning communities. Since 2015, image-based 3D reconstruction using convolutional neural networks has attracted increasing interest and demonstrated an impressive performance. Given this new era of rapid evolution, this article provides a comprehensive survey of the recent developments in this field. We focus on the works which use deep learning techniques to estimate the 3D shape of generic objects either from a single or multiple RGB images. We organize the literature based on the shape representations, the network architectures, and the training mechanisms they use. While this survey is intended for methods which reconstruct generic objects, we also review some of the recent works which focus on specific object classes such as human body shapes and faces. We provide an analysis and comparison of the performance of some key papers, summarize some of the open problems in this field, and discuss promising directions for future research.",0.083009156],["this paper defines a positive and unlabeled classification problem for standard GANs","On Positive-Unlabeled Classification in GAN","summarize: This paper defines a positive and unlabeled classification problem for standard GANs, which then leads to a novel technique to stabilize the training of the discriminator in GANs. Traditionally, real data are taken as positive while generated data are negative. This positive-negative classification criterion was kept fixed all through the learning process of the discriminator without considering the gradually improved quality of generated data, even if they could be more realistic than real data at times. In contrast, it is more reasonable to treat the generated data as unlabeled, which could be positive or negative according to their quality. The discriminator is thus a classifier for this positive and unlabeled classification problem, and we derive a new Positive-Unlabeled GAN . We theoretically discuss the global optimality the proposed model will achieve and the equivalent optimization goal. Empirically, we find that PUGAN can achieve comparable or even better performance than those sophisticated discriminator stabilization methods.",0.15],["knowledge leakage poses a critical risk to the competitive advantage of knowledge-intensive organisations.","Exploring Knowledge Leakage Risk in Knowledge-Intensive Organisations: Behavioural aspects and Key controls","summarize: Knowledge leakage poses a critical risk to the competitive advantage of knowledge-intensive organisations. Although knowledge leakage is a human-centric security issue, little is known about leakage resulting from individual behaviour and the protective strategies and controls that could be effective in mitigating leakage risk. Therefore, this research explores the perspectives of security practitioners on the key factors that influence knowledge leakage risk in the context of knowledge-intensive organisations. We conduct two focus groups to explore these perspectives. The research highlights three types of behavioural controls that mitigate the risk of knowledge leakage: human resource management practices, knowledge security training and awareness practices, and compartmentalisation practices.",0.3636363636],["the relation between input-to-state stability and integral input-to-state stability is studied","Infinite-dimensional input-to-state stability and Orlicz spaces","summarize: In this work, the relation between input-to-state stability and integral input-to-state stability is studied for linear infinite-dimensional systems with an unbounded control operator. Although a special focus is laid on the case ",0.3333333333],["proposed solution uses weightless neural network known as Wisard. it is used to decide","Weightless Neural Network with Transfer Learning to Detect Distress in Asphalt","summarize: The present paper shows a solution to the problem of automatic distress detection, more precisely the detection of holes in paved roads. To do so, the proposed solution uses a weightless neural network known as Wisard to decide whether an image of a road has any kind of cracks. In addition, the proposed architecture also shows how the use of transfer learning was able to improve the overall accuracy of the decision system. As a verification step of the research, an experiment was carried out using images from the streets at the Federal University of Tocantins, Brazil. The architecture of the developed solution presents a result of 85.71% accuracy in the dataset, proving to be superior to approaches of the state-of-the-art.",0.2],["a key problem for addressing goal-action gaps in human endeavors is coordination.","Coordination Technology for Active Support Networks: Context, Needfinding, and Design","summarize: Coordination is a key problem for addressing goal-action gaps in many human endeavors. We define interpersonal coordination as a type of communicative action characterized by low interpersonal belief and goal conflict. Such situations are particularly well described as having collectively intelligent, common good solutions, viz., ones that almost everyone would agree constitute social improvements. Coordination is useful across the spectrum of interpersonal communication -- from isolated individuals to organizational teams. Much attention has been paid to coordination in teams and organizations. In this paper we focus on the looser interpersonal structures we call active support networks , and on technology that meets their needs. We describe two needfinding investigations focused on social support, which examined four application areas for improving coordination in ASNs: academic coaching, vocational training, early learning intervention, and volunteer coordination; and existing technology relevant to ASNs. We find a thus-far unmet need for personal task management software that allows smooth integration with an individual's active support network. Based on identified needs, we then describe an open architecture for coordination that has been developed into working software. The design includes a set of capabilities we call social prompting, as well as templates for accomplishing multi-task goals, and an engine that controls coordination in the network. The resulting tool is currently available and in continuing development. We explain its use in ASNs with an example. Follow-up studies are underway in which the technology is being applied in existing support networks.",0.2857142857],["the Illumination Conjecture is a longstanding open problem in discrete geometry","The geometry of homothetic covering and illumination","summarize: At a first glance, the problem of illuminating the boundary of a convex body by external light sources and the problem of covering a convex body by its smaller positive homothetic copies appear to be quite different. They are in fact two sides of the same coin and give rise to one of the important longstanding open problems in discrete geometry, namely, the Illumination Conjecture. In this paper, we survey the activity in the areas of discrete geometry, computational geometry and geometric analysis motivated by this conjecture. Special care is taken to include the recent advances that are not covered by the existing surveys. We also include some of our recent results related to these problems and describe two new approaches -- one conventional and the other computer-assisted -- to make progress on the illumination problem. Some open problems and conjectures are also presented.",0.375],["the present and future of evolutionary algorithms depends on the proper use of modern parallel and distributed computing infrastructure","It is Time for New Perspectives on How to Fight Bloat in GP","summarize: The present and future of evolutionary algorithms depends on the proper use of modern parallel and distributed computing infrastructures. Although still sequential approaches dominate the landscape, available multi-core, many-core and distributed systems will make users and researchers to more frequently deploy parallel version of the algorithms. In such a scenario, new possibilities arise regarding the time saved when parallel evaluation of individuals are performed. And this time saving is particularly relevant in Genetic Programming. This paper studies how evaluation time influences not only time to solution in parallel\/distributed systems, but may also affect size evolution of individuals in the population, and eventually will reduce the bloat phenomenon GP features. This paper considers time and space as two sides of a single coin when devising a more natural method for fighting bloat. This new perspective allows us to understand that new methods for bloat control can be derived, and the first of such a method is described and tested. Experimental data confirms the strength of the approach: using computing time as a measure of individuals' complexity allows to control the growth in size of genetic programming individuals.",0.0384615385],["maximum coordinates are formulated in maximum coordinates. the algorithm does not suffer from constrain","Linear-Time Variational Integrators in Maximal Coordinates","summarize: Most dynamic simulation tools parameterize the configuration of multi-body robotic systems using minimal coordinates, also called generalized or joint coordinates. However, maximal-coordinate approaches have several advantages over minimal-coordinate parameterizations, including native handling of closed kinematic loops and nonholonomic constraints. This paper describes a linear-time variational integrator that is formulated in maximal coordinates. Due to its variational formulation, the algorithm does not suffer from constraint drift and has favorable energy and momentum conservation properties. A sparse matrix factorization technique allows the dynamics of a loop-free articulated mechanism with ",0.1428571429],["the double descent risk curve was proposed to qualitatively describe the out-of-sample","Two models of double descent for weak features","summarize: The double descent risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares\/least norm predictor. Specifically, it is shown that the risk peaks when the number of features ",0.1428571429],["the DM is connected to the visible sector via a gauged U portal. the","Minimally Extended Left-Right Symmetric Model for Dark Matter with U Portal","summarize: A minimal extension of the left-right symmetric model for neutrino masses that includes a vector-like singlet fermion dark matter is presented with the DM connected to the visible sector via a gauged U portal. We discuss the symmetry breaking in this model and calculate the mass and mixings of the extra heavy neutral gauge boson at the TeV scale. The extra gauge boson can decay to both standard model particles as well to dark matter. We calculate the relic density of the singlet fermion dark matter and its direct detection cross section and use these constraints to obtain the allowed parameter range for the new gauge coupling and the dark matter mass.",0.2],["a recent study shows that some dynamical scenarii can be relevant from 0 to 2","Extreme events in forced oscillatory media in 0, 1 and 2 dimensions","summarize: One of the open questions in the field of optical rogue waves is the relevance of the number of spatial dimensions in which waves propagate. Here we review recent results on extreme events obtained in 0, 1 and 2 spatial dimensions in the specific context of forced oscillatory media. We show that some dynamical scenarii can be relevant from 0 to 2D while others can take place only in sufficiently large number of spatial dimensions.",0.2857142857],["apnea episodes result in a rise in beat-to-beat blood","Mathematical Modeling of Arterial Blood Pressure Using Photo-Plethysmography Signal in Breath-hold Maneuver","summarize: Recent research has shown that each apnea episode results in a significant rise in the beat-to-beat blood pressure and by a drop to the pre-episode levels when patient resumes normal breathing. While the physiological implications of these repetitive and significant oscillations are still unknown, it is of interest to quantify them. Since current array of instruments deployed for polysomnography studies does not include beat-to-beat measurement of blood pressure, but includes oximetry, it is both of clinical interest to estimate the magnitude of BP oscillations from the photoplethysmography signal that is readily available from sleep lab oximeters. We have investigated a new method for continuous estimation of systolic , diastolic , and mean blood pressure waveforms from PPG. Peaks and troughs of PPG waveform are used as input to a 5th order autoregressive moving average model to construct estimates of SBP, DBP, and MBP waveforms. Since breath hold maneuvers are shown to simulate apnea episodes faithfully, we evaluated the performance of the proposed method in 7 subjects in supine position doing 5 breath maneuvers with 90s of normal breathing between them. The modeling error ranges were -0.88+-4.87 to -2.19+-5.73 ; 0.29+-2.39 to -0.97+-3.83 ; and -0.42+-2.64 to -1.17+-3.82 . The cross validation error ranges were 0.28+-6.45 to -1.74+-6.55 ; 0.09+-3.37 to -0.97+-3.67 ; and 0.33+-4.34 to -0.87+-4.42 . The level of estimation error in, as measured by the root mean squared of the model residuals, was less than 7 mmHg",0.4299187863],["conformal field theories have a SL algebra of local bosonic constraints. they","Space-time CFTs from the Riemann sphere","summarize: We consider two-dimensional chiral, first-order conformal field theories governing maps from the Riemann sphere to the projective light cone inside Minkowski space -- the natural setting for describing conformal field theories in two fewer dimensions. These theories have a SL algebra of local bosonic constraints which can be supplemented by additional fermionic constraints depending on the matter content of the theory. By computing the BRST charge associated with gauge fixing these constraints, we find anomalies which vanish for specific target space dimensions. These critical dimensions coincide precisely with those for which cubic scalar theory, gauge theory and gravity are classically conformally invariant. Furthermore, the BRST cohomology of each theory contains vertex operators for the full conformal multiplets of single field insertions in each of these space-time CFTs. We give a prescription for the computation of three-point functions, and compare our formalism with the scattering equations approach to on-shell amplitudes.",0.1111111111],["centralized secondary voltage control in a power system has been replaced by the distributed controller.","Fully Distributed Secondary Voltage Control in Inverter-Based Microgrids","summarize: Centralized secondary voltage control in a power system has been replaced by the distributed controller in the recent literature due to its high dependency on extensive communication messages. Although in the new method each distributed generator only communicate with its neighbors to control the voltage, yet the messages are circulating among the whole system. In this paper, we have utilized distributed controller locally so that it will work as a fully distributed control system. This controller has been justified by being studied within a case study including 6 distributed generators.",0.1904761905],["a system that is not properly able to perform a computer-science lab is","Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure","summarize: As machine learning systems move from computer-science laboratories into the open world, their accountability becomes a high priority problem. Accountability requires deep understanding of system behavior and its failures. Current evaluation methods such as single-score error metrics and confusion matrices provide aggregate views of system performance that hide important shortcomings. Understanding details about failures is important for identifying pathways for refinement, communicating the reliability of systems in different settings, and for specifying appropriate human oversight and engagement. Characterization of failures and shortcomings is particularly complex for systems composed of multiple machine learned components. For such systems, existing evaluation methods have limited expressiveness in describing and explaining the relationship among input content, the internal states of system components, and final output quality. We present Pandora, a set of hybrid human-machine methods and tools for describing and explaining system failures. Pandora leverages both human and system-generated observations to summarize conditions of system malfunction with respect to the input content and system architecture. We share results of a case study with a machine learning pipeline for image captioning that show how detailed performance views can be beneficial for analysis and debugging.",0.4782608696],["we consider the steady fractional Schrodinger equation. we consider the steady","The fractional Schr\\odinger equation with singular potential and measure data","summarize: We consider the steady fractional Schr\\odinger equation ",0.1052631579],["a new algorithm is based on a gradient-based markov chain Monte Carlo","Hamiltonian Monte Carlo using an adjoint-differentiated Laplace approximation: Bayesian inference for latent Gaussian models and beyond","summarize: Gaussian latent variable models are a key class of Bayesian hierarchical models with applications in many fields. Performing Bayesian inference on such models can be challenging as Markov chain Monte Carlo algorithms struggle with the geometry of the resulting posterior distribution and can be prohibitively slow. An alternative is to use a Laplace approximation to marginalize out the latent Gaussian variables and then integrate out the remaining hyperparameters using dynamic Hamiltonian Monte Carlo, a gradient-based Markov chain Monte Carlo sampler. To implement this scheme efficiently, we derive a novel adjoint method that propagates the minimal information needed to construct the gradient of the approximate marginal likelihood. This strategy yields a scalable differentiation method that is orders of magnitude faster than state of the art differentiation techniques when the hyperparameters are high dimensional. We prototype the method in the probabilistic programming framework Stan and test the utility of the embedded Laplace approximation on several models, including one where the dimension of the hyperparameter is ",0.5254562944],["eight-dimensional theory explores whether Yang-Mills instantons formed in extra dimensions","Expanding Universe and Dynamical Compactification Using Yang-Mills Instantons","summarize: We consider an eight-dimensional Einstein-Yang-Mills theory to explore whether Yang-Mills instantons formed in extra dimensions can induce the dynamical instability of our four-dimensional spacetime. We show that the Yang-Mills instantons in extra dimensions can trigger the expansion of our universe in four-dimensional spacetime as well as the dynamical compactification of extra dimensions. We also discuss a possibility to realize a reheating mechanism via the quantum back-reaction from the contracting tiny internal space with a smeared instanton.",0.2727272727],["a better understanding of the mechanisms which generate EZs would help with understanding the possible importance","Exclusion zone phenomena in water -- a critical review of experimental findings and theories","summarize: The existence of the exclusion zone , a layer of water in which plastic microspheres are repelled from hydrophilic surfaces, has now been independently demonstrated by several groups. A better understanding of the mechanisms which generate EZs would help with understanding the possible importance of EZs in biology and in engineering applications such as filtration and microfluidics. Here we review the experimental evidence for EZ phenomena in water and the major theories that have been proposed. We review experimental results from birefringence, neutron radiography, nuclear magnetic resonance, and other studies. Pollack and others have theorized that water in the EZ exists has a different structure than bulk water, and that this accounts for the EZ. We present several alternative explanations for EZs and argue that Schurr's theory based on diffusiophoresis presents a compelling alternative explanation for the core EZ phenomenon. Among other things, Schurr's theory makes predictions about the growth of the EZ with time which have been confirmed by Florea et al. and others. We also touch on several possible confounding factors that make experimentation on EZs difficult, such as charged surface groups, dissolved solutes, and adsorbed nanobubbles.",0.36],["a loss term encourages the network to capture the composability of visual sequences","Learning what you can do before doing anything","summarize: Intelligent agents can learn to represent the action spaces of other agents simply by observing them act. Such representations help agents quickly learn to predict the effects of their own actions on the environment and to plan complex action sequences. In this work, we address the problem of learning an agent's action space purely from visual observation. We use stochastic video prediction to learn a latent variable that captures the scene's dynamics while being minimally sensitive to the scene's static content. We introduce a loss term that encourages the network to capture the composability of visual sequences and show that it leads to representations that disentangle the structure of actions. We call the full model with composable action representations Composable Learned Action Space Predictor . We show the applicability of our method to synthetic settings and its potential to capture action spaces in complex, realistic visual settings. When used in a semi-supervised setting, our learned representations perform comparably to existing fully supervised methods on tasks such as action-conditioned video prediction and planning in the learned action space, while requiring orders of magnitude fewer action labels. Project website: https:\/\/daniilidis-group.github.io\/learned_action_spaces",0.2105263158],["the HDLSS context is a gender classification problem of face images. the dimension of","Distance weighted discrimination of face images for gender classification","summarize: We illustrate the advantages of distance weighted discrimination for classification and feature extraction in a High Dimension Low Sample Size situation. The HDLSS context is a gender classification problem of face images in which the dimension of the data is several orders of magnitude larger than the sample size. We compare distance weighted discrimination with Fisher's linear discriminant, support vector machines, and principal component analysis by exploring their classification interpretation through insightful visuanimations and by examining the classifiers' discriminant errors. This analysis enables us to make new contributions to the understanding of the drivers of human discrimination between males and females.",0.5454545455],["generalized motivic cohomology and generalized motivic cohomology are computed","The Generalized Slices of Hermitian K-Theory","summarize: We compute the generalized slices of the motivic spectrum KO in terms of motivic cohomology and generalized motivic cohomology, obtaining good agreement with the situation in classical topology and the results predicted by Markett-Schlichting. As an application, we compute the homotopy sheaves of generalized motivic cohomology, which establishes a version of a conjecture of Morel.",0.0],["approach for semi-automatic annotation of object instances mimics how most current datasets have been","Annotating Object Instances with a Polygon-RNN","summarize: We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We further show generalization capabilities of our approach to unseen datasets.",0.0],["dynamical array consists of a family of functions.","The Lindeberg theorem for Gibbs-Markov dynamics","summarize: A dynamical array consists of a family of functions ",0.1333333333],["the anisotropy in the solar neighbourhood due to the motion of the sun through the","Are the planetary orbital effects of the Solar dark matter wake detectable?","summarize: Recently, a discussion about the effects of the anisotropy in the spatial density of Dark Matter in the Solar neighbourhood due to the motion of the Sun through the Galactic halo on the orbital motion of the solar system's planets and their ability to be effectively constrained by the radiotechnical observations collected by the Cassini spacecraft appeared in the literature. We show that the semilatus rectum ",0.3142857143],["only lens spaces and prism manifolds admit several Seifert fibrations","On the diffeomorphism type of Seifert fibered spherical 3-orbifolds","summarize: It is well known that, among closed spherical Seifert three-manifolds, only lens spaces and prism manifolds admit several Seifert fibrations which are not equivalent up to diffeomorphism. Moreover the former admit infinitely many fibrations, and the latter exactly two. In this work, we analyse the non-uniqueness phenomenon for orbifold Seifert fibrations. For any closed spherical Seifert three-orbifold, we determine the number of its inequivalent fibrations. When these are in a finite number we provide a complete list. In case of infinitely many fibrations, we describe instead an algorithmic procedure to determine whether two closed spherical Seifert orbifolds are diffeomorphic.",0.1],["the stability of the DPG method gives a norm equivalence. this","A scalable preconditioner for a DPG method","summarize: We show how a scalable preconditioner for the primal discontinuous Petrov-Galerkin method can be developed using existing algebraic multigrid preconditioning techniques. The stability of the DPG method gives a norm equivalence which allows us to exploit existing AMG algorithms and software. We show how these algebraic preconditioners can be applied directly to a Schur complement system of interface unknowns arising from the DPG method. To the best of our knowledge, this is the first massively scalable algebraic preconditioner for DPG problems.",0.3333333333],["Gamma-ray bursts were confirmed to be of extragalactic origin","Testing the anisotropy in the angular distribution of ","summarize: Gamma-ray bursts were confirmed to be of extragalactic origin due to their isotropic angular distribution, combined with the fact that they exhibited an intensity distribution that deviated strongly from the ",0.1111111111],["paper studies differences between different types of newspapers in expressing temporal information. the paper is","A Pattern-mining Driven Study on Differences of Newspapers in Expressing Temporal Information","summarize: This paper studies the differences between different types of newspapers in expressing temporal information, which is a topic that has not received much attention. Techniques from the fields of temporal processing and pattern mining are employed to investigate this topic. First, a corpus annotated with temporal information is created by the author. Then, sequences of temporal information tags mixed with part-of-speech tags are extracted from the corpus. The TKS algorithm is used to mine skip-gram patterns from the sequences. With these patterns, the signatures of the four newspapers are obtained. In order to make the signatures uniquely characterize the newspapers, we revise the signatures by removing reference patterns. Through examining the number of patterns in the signatures and revised signatures, the proportion of patterns containing temporal information tags and the specific patterns containing temporal information tags, it is found that newspapers differ in ways of expressing temporal information.",0.2857142857],["Graphs arise naturally in many real-world applications including social networks, recommender systems,","Representation Learning for Dynamic Graphs: A Survey","summarize: Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.",0.1428571429],["the main focus of this work is the reconstruction of the signals.","Disjoint sparsity for signal separation and applications to hybrid inverse problems in medical imaging","summarize: The main focus of this work is the reconstruction of the signals ",0.0403086536],["the paper presents standard and handcrafted features. the influence of various objective factors on the subjective","Study on the Assessment of the Quality of Experience of Streaming Video","summarize: Dynamic adaptive streaming over HTTP provides the work of most multimedia services, however, the nature of this technology further complicates the assessment of the QoE . In this paper, the influence of various objective factors on the subjective estimation of the QoE of streaming video is studied. The paper presents standard and handcrafted features, shows their correlation and p-Value of significance. VQA models based on regression and gradient boosting with SRCC reaching up to 0.9647 on the validation subsample are proposed. The proposed regression models are adapted for applied applications ; the Gradient Boosting Regressor model is perspective for further improvement of the quality estimation model. We take SQoE-III database, so far the largest and most realistic of its kind. The VQA models are available at https:\/\/github.com\/AleksandrIvchenko\/QoE-assesment",0.3333333333],["rowland et al. analysed the C51 algorithm in terms of the C","Distributional reinforcement learning with linear function approximation","summarize: Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. 's analysis of the C51 algorithm in terms of the Cram\\'er distance, but their results only apply to the tabular setting and ignore C51's use of a softmax to produce normalized distributions. In this paper we adapt the Cram\\'er distance to deal with arbitrary vectors. From it we derive a new distributional algorithm which is fully Cram\\'er-based and can be combined to linear function approximation, with formal guarantees in the context of policy evaluation. In allowing the model's prediction to be any real vector, we lose the probabilistic interpretation behind the method, but otherwise maintain the appealing properties of distributional approaches. To the best of our knowledge, ours is the first proof of convergence of a distributional algorithm combined with function approximation. Perhaps surprisingly, our results provide evidence that Cram\\'er-based distributional methods may perform worse than directly approximating the value function.",0.1176470588],["we propose a scalable, efficient and accurate approach to retrieve 3D models. we","3D Pose Estimation and 3D Model Retrieval for Objects in the Wild","summarize: We propose a scalable, efficient and accurate approach to retrieve 3D models for objects in the wild. Our contribution is twofold. We first present a 3D pose estimation approach for object categories which significantly outperforms the state-of-the-art on Pascal3D+. Second, we use the estimated pose as a prior to retrieve 3D models which accurately represent the geometry of objects in RGB images. For this purpose, we render depth images from 3D models under our predicted pose and match learned image descriptors of RGB images against those of rendered depth images using a CNN-based multi-view metric learning approach. In this way, we are the first to report quantitative results for 3D model retrieval on Pascal3D+, where our method chooses the same models as human annotators for 50% of the validation images on average. In addition, we show that our method, which was trained purely on Pascal3D+, retrieves rich and accurate 3D models from ShapeNet given RGB images of objects in the wild.",0.2272727273],["upscaled model is presented for complex networks with highly clustered regions. upscale","The Interaction Between PDE and Graphs in Multiscale Modeling","summarize: In this article an upscaled model is presented, for complex networks with highly clustered regions exchanging some abstract quantities in both, microscale and macroscale level. Such an intricate system is approximated by a partitioned open map in ",0.0769230769],["the rotating chemically peculiar star V473,Tau has been observed for more than","The Period Evolution of the Chemically Peculiar Star V473 Tau","summarize: In this paper, the period evolution of the rotating chemically peculiar star V473\\,Tau is investigated. Even though the star has been observed for more than fifty years, for the first time four consecutive years of space-based data covering between 2007 and 2010 is presented. The data is from the satellite, and is combined with the archival results. The analysis shows that the rotation period of V473\\,Tau is ",0.0833333333]]}