{"columns":["predict_title","actual_title","actual_abstract","bleu"],"index":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999],"data":[["textbf combines cost variances in different views with small extra memory consumption","Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation","summarize: n this paper, we propose an effective and efficient pyramid multi-view stereo net with self-adaptive view aggregation for accurate and complete dense point cloud reconstruction. Different from using mean square variance to generate cost volume in previous deep-learning based MVS methods, our \\textbf incorporates the cost variances in different views with small extra memory consumption by introducing two novel self-adaptive view aggregations: pixel-wise view aggregation and voxel-wise view aggregation. To further boost the robustness and completeness of 3D point cloud reconstruction, we extend VA-MVSNet with pyramid multi-scale images input as \\textbf, where multi-metric constraints are leveraged to aggregate the reliable depth estimation at the coarser scale to fill in the mismatched regions at the finer scale. Experimental results show that our approach establishes a new state-of-the-art on the \\textsl} dataset with significant improvements in the completeness and overall quality, and has strong generalization by achieving a comparable performance as the state-of-the-art methods on the \\textsl} benchmark. Our codebase is at \\hyperlink",0.0769230769],["a hidden semi-Markov model represents a state duration inside a model.","State Duration and Interval Modeling in Hidden Semi-Markov Model for Sequential Data Analysis","summarize: Sequential data modeling and analysis have become indispensable tools for analyzing sequential data, such as time-series data, because larger amounts of sensed event data have become available. These methods capture the sequential structure of data of interest, such as input-output relations and correlation among datasets. However, because most studies in this area are specialized or limited to their respective applications, rigorous requirement analysis of such models has not been undertaken from a general perspective. Therefore, we particularly examine the structure of sequential data, and extract the necessity of `state duration' and `state interval' of events for efficient and rich representation of sequential data. Specifically addressing the hidden semi-Markov model that represents such state duration inside a model, we attempt to add representational capability of a state interval of events onto HSMM. To this end, we propose two extended models: an interval state hidden semi-Markov model to express the length of a state interval with a special state node designated as interval state node; and an interval length probability hidden semi-Markov model which represents the length of the state interval with a new probabilistic parameter interval length probability. Exhaustive simulations have revealed superior performance of the proposed models in comparison with HSMM. These proposed models are the first reported extensions of HMM to support state interval representation as well as state duration representation.",0.5359840188],["quantum computers speed up classical Markov chain algorithms. quantum algorithms are faster than classical Markov","Adaptive Quantum Simulated Annealing for Bayesian Inference and Estimating Partition Functions","summarize: Markov chain Monte Carlo algorithms have important applications in counting problems and in machine learning problems, settings that involve estimating quantities that are difficult to compute exactly. How much can quantum computers speed up classical Markov chain algorithms? In this work we consider the problem of speeding up simulated annealing algorithms, where the stationary distributions of the Markov chains are Gibbs distributions at temperatures specified according to an annealing schedule. We construct a quantum algorithm that both adaptively constructs an annealing schedule and quantum samples at each temperature. Our adaptive annealing schedule roughly matches the length of the best classical adaptive annealing schedules and improves on nonadaptive temperature schedules by roughly a quadratic factor. Our dependence on the Markov chain gap matches other quantum algorithms and is quadratically better than what classical Markov chains achieve. Our algorithm is the first to combine both of these quadratic improvements. Like other quantum walk algorithms, it also improves on classical algorithms by producing qsamples instead of classical samples. This means preparing quantum states whose amplitudes are the square roots of the target probability distribution. In constructing the annealing schedule we make use of amplitude estimation, and we introduce a method for making amplitude estimation nondestructive at almost no additional cost, a result that may have independent interest. Finally we demonstrate how this quantum simulated annealing algorithm can be applied to the problems of estimating partition functions and Bayesian inference.",0.0],["tidal forces with orbital and rotational centrifugal forces can partially","Hot Super-Earths with Hydrogen Atmospheres: A Model Explaining Their Paradoxical Existence","summarize: In this paper we propose a new mechanism that could explain the survival of hydrogen atmospheres on some hot super-Earths. We argue that on close-orbiting tidally-locked super-Earths the tidal forces with the orbital and rotational centrifugal forces can partially confine the atmosphere on the nightside. Assuming a super terran body with an atmosphere dominated by volcanic species and a large hydrogen component, the heavier molecules can be shown to be confined within latitudes of ",0.0754031182],["quantum key distribution and entanglement distribution are determined using a single-sender multiple","Unconstrained Capacities of Quantum Key Distribution and Entanglement Distillation for Pure-Loss Bosonic Broadcast Channels","summarize: We consider quantum key distribution and entanglement distribution using a single-sender multiple-receiver pure-loss bosonic broadcast channel. We determine the unconstrained capacity region for the distillation of bipartite entanglement and secret key between the sender and each receiver, whenever they are allowed arbitrary public classical communication. A practical implication of our result is that the capacity region demonstrated drastically improves upon rates achievable using a naive time-sharing strategy, which has been employed in previously demonstrated network QKD systems. We show a simple example of the broadcast QKD protocol overcoming the limit of the point-to-point strategy. Our result is thus an important step toward opening a new framework of network channel-based quantum communication technology.",0.282160575],["we investigate the propagating profiles of a degenerate chemotaxis model","Propagating Profiles of a Chemotaxis Model with Degenerate Diffusion: Initial Shrinking, Eventual Smoothness and Expanding","summarize: We investigate the propagating profiles of a degenerate chemotaxis model describing the bacteria chemotaxis and consumption of oxygen by aerobic bacteria, in particular, the effect of the initial attractant distribution on bacterial clustering. We prove that the compact support of solutions may shrink if the signal concentration satisfies a special structure, and show the finite speed propagating property without assuming the special structure on attractant concentration, and obtain an explicit formula of the population spreading speed in terms of model parameters. The presented results suggest that bacterial cluster formation can be affected by chemotactic attractants and density-dependent dispersal.",0.3032653299],["the present study aims at investigating the impact of resting HR on the hemodynamic response to","A Computational Study on the Relation between Resting Heart Rate and Atrial Fibrillation Hemodynamics under Exercise","summarize: Aims. Clinical data indicating a heart rate target during rate control therapy for permanent atrial fibrillation and assessing its eventual relationship with reduced exercise tolerance are lacking. The present study aims at investigating the impact of resting HR on the hemodynamic response to exercise in permanent AF patients by means of a computational cardiovascular model. Methods. The AF lumped-parameter model was run to simulate resting and various exercise conditions , considering different resting HR . To compare relative variations of cardiovascular variables upon exertion, the variation comparative index - the absolute variation between the exercise and the resting values in SHR simulations referred to the absolute variation in HHR simulations -was calculated at each exercise grade . Results. Pulmonary venous pressure underwent a greater increase in HHR compared to SHR simulations , while for systemic arterial pressure the opposite is true . Conclusions. The computational findings suggest that a slower, with respect to a higher resting HR, might be preferable in permanent AF patients, since during exercise pulmonary venous pressure undergoes a slighter increase and systemic blood pressure reveals a more appropriate increase.",0.2916666667],["we propose a multi-player extension of the minimum cost flow problem. we associate one","On seeking efficient Pareto optimal points in multi-player minimum cost flow problems with application to transportation systems","summarize: In this paper, we propose a multi-player extension of the minimum cost flow problem inspired by a transportation problem that arises in modern transportation industry. We associate one player with each arc of a directed network, each trying to minimize its cost function subject to the network flow constraints. In our model, the cost function can be any general nonlinear function, and the flow through each arc is an integer. We present algorithms to compute efficient Pareto optimal point, where the maximum possible number of players minimize their cost functions simultaneously. The computed Pareto optimal points are Nash equilibriums if the problem is transformed into a finite static game in normal form.",0.3823189328],["human cooperation does not require sheer computational power, but rather relies on intuition, cultural norm","Cooperating with Machines","summarize: Since Alan Turing envisioned Artificial Intelligence , a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks , or defeating humans in strategic zero-sum encounters . In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition , cultural norms , emotions and signals , and pre-evolved dispositions toward cooperation , common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",0.0],["network virtualization allows multiple virtual networks to share physical resources of single substrate network. but sharing","A Collective Neurodynamic Approach to Survivable Virtual Network Embedding","summarize: Network virtualization has attracted significant amount of attention in the last few years as one of the key features of cloud computing. Network virtualization allows multiple virtual networks to share physical resources of single substrate network. However, sharing substrate network resources increases impact of single substrate resource failure. One of the commonly applied mechanisms to protect against such failures is provisioning redundant substrate resources for each virtual network to be used to recover affected virtual resources. However, redundant resources decreases cloud revenue by increasing virtual network embedding cost. In this paper, a collective neurodynamic approach has been proposed to reduce amount of provisioned redundant resources and reduce cost of embedding virtual networks. The proposed approach has been evaluated by using simulation and compared against some existing survivable virtual network embedding techniques.",0.0476190476],["we present results of high-resolution numerical simulations of compressible 2D turbul","Energy Transfer and Spectra in Simulations of Two-dimensional Compressible Turbulence","summarize: We present results of high-resolution numerical simulations of compressible 2D turbulence forced at intermediate spatial scales with a solenoidal white-in-time external acceleration. A case with an isothermal equation of state, low energy injection rate, and turbulent Mach number ",0.1538461538],["a range-based and three range-free localization algorithms are used to analyse the distance","Measurement Errors in Range-Based Localization Algorithms for UAVs: Analysis and Experimentation","summarize: Localizing ground devices is an important requirement for a wide variety of applications, such as infrastructure monitoring, precision agriculture, search and rescue operations, to name a few. To this end, unmanned aerial vehicles or drones offer a promising technology due to their flexibility. However, the distance measurements performed using a drone, an integral part of a localization procedure, incur several errors that affect the localization accuracy. In this paper, we provide analytical expressions for the impact of different kinds of measurement errors on the ground distance between the UAV and GDs. We review three range-based and three range-free localization algorithms, identify their source of errors, and analytically derive the error bounds resulting from aggregating multiple inaccurate measurements. We then extend the range-free algorithms for improved accuracy. We validate our theoretical analysis and compare the observed localization error of the algorithms after collecting data from a testbed using ten GDs and one drone, equipped with ultra wide band antennas and operating in an open field. Results show that our analysis closely matches with experimental localization errors. Moreover, compared to their original counterparts, the extended range-free algorithms significantly improve the accuracy.",0.375],["a long elastic filament confined to a spherical container is confined","Spontaneous Domain Formation in Spherically-Confined Elastic Filaments","summarize: Although the free energy of a genome packing into a virus is dominated by DNA-DNA interactions, ordering of the DNA inside the capsid is elasticity-driven, suggesting general solutions with DNA organized into spool-like domains. Using analytical calculations and computer simulations of a long elastic filament confined to a spherical container, we show that the ground state is not a single spool as assumed hitherto, but an ordering mosaic of multiple homogeneously-ordered domains. At low densities, we observe concentric spools, while at higher densities, other morphologies emerge, which resemble topological links. We discuss our results in the context of metallic wires, viral DNA, and flexible polymers.",0.5217391304],["graph product multilayer networks are a family of multilayer networks. the networks can be","Graph Product Multilayer Networks: Spectral Properties and Applications","summarize: This paper aims to establish theoretical foundations of graph product multilayer networks , a family of multilayer networks that can be obtained as a graph product of two or more factor networks. Cartesian, direct , and strong product operators are considered, and then generalized. We first describe mathematical relationships between GPMNs and their factor networks regarding their degree\/strength, adjacency, and Laplacian spectra, and then show that those relationships can still hold for nonsimple and generalized GPMNs. Applications of GPMNs are discussed in three areas: predicting epidemic thresholds, modeling propagation in nontrivial space and time, and analyzing higher-order properties of self-similar networks. Directions of future research are also discussed.",0.1785714286],["nonlinear oscillators are a key modelling tool in many applications. the influence","Quenched Noise and Nonlinear Oscillations in Bistable Multiscale Systems","summarize: Nonlinear oscillators are a key modelling tool in many applications. The influence of annealed noise on nonlinear oscillators has been studied intensively. It can induce effects in nonlinear oscillators not present in the deterministic setting. Yet, there is no theory regarding the quenched noise scenario of random parameters sampled on fixed time intervals, although this situation is often a lot more natural. Here we study a paradigmatic nonlinear oscillator of van-der-Pol\/FitzHugh-Nagumo type under quenched noise as a piecewise-deterministic Markov process. There are several interesting effects such as period shifts and new different trapped types of small-amplitude oscillations, which can be captured analytically. Furthermore, we numerically discover quenched resonance and show that it differs significantly from previous finite-noise optimality resonance effects. This demonstrates that quenched oscillatorscan be viewed as a new building block of nonlinear dynamics.",0.3333333333],["demand response provides an opportunity for load serving entities that operate retail electricity markets to strategically purchase energy and","Optimal Joint Bidding and Pricing of Profit-seeking Load Serving Entity","summarize: The demand response provides an opportunity for load serving entities that operate retail electricity markets to strategically purchase energy and provide reserves in wholesale electricity markets . This paper concerns with the problem of simultaneously determining the optimal energy bids and reserve offers an LSE submits to the WEM as well as the optimal energy and reserve prices it sets in the REM so as to maximize its profit. To this end, we explicitly model the tri-layer market structure that consists of a WEM, a REM, and a set of end user customers, so as to capture the coupling between the bidding problem and the pricing problem. Based on the tri-layer market model, we then formulate the joint bidding and pricing problem as a bi-level programming problem and further transform it into a single-level mixed integer linear programming problem, which can be solved efficiently. Numerical studies using the IEEE test cases are presented to illustrate the application of the proposed methodology as well as to reveal several interesting characteristics of the LSE's profit-seeking behavior.",0.0909090909],["a novel deep fusion algorithm is based on the representations from an end-to","Dense Feature Aggregation and Pruning for RGBT Tracking","summarize: How to perform effective information fusion of different modalities is a core factor in boosting the performance of RGBT tracking. This paper presents a novel deep fusion algorithm based on the representations from an end-to-end trained convolutional neural network. To deploy the complementarity of features of all layers, we propose a recursive strategy to densely aggregate these features that yield robust representations of target objects in each modality. In different modalities, we propose to prune the densely aggregated features of all modalities in a collaborative way. In a specific, we employ the operations of global average pooling and weighted random selection to perform channel scoring and selection, which could remove redundant and noisy features to achieve more robust feature representation. Experimental results on two RGBT tracking benchmark datasets suggest that our tracker achieves clear state-of-the-art against other RGB and RGBT tracking methods.",0.3157894737],["haptic device can provide kinesthetic feedback to the user's index finger","Effects of Different Hand-Grounding Locations on Haptic Performance With a Wearable Kinesthetic Haptic Device","summarize: Grounding of kinesthetic feedback against a user's hand can increase the portability and wearability of a haptic device. However, the effects of different hand-grounding locations on haptic perception of a user are unknown. In this letter, we investigate the effects of three different hand-grounding locations-back of the hand, proximal phalanx of the index finger, and middle phalanx of the index finger-on haptic perception using a newly designed wearable haptic device. The novel device can provide kinesthetic feedback to the user's index finger in two directions: along the finger-axis and in the finger's flexion-extension movement direction. We measure users' haptic perception for each grounding location through a psychophysical experiment for each of the two feedback directions. Results show that among the studied locations, grounding at proximal phalanx has a smaller average just noticeable difference for both feedback directions, indicating a more sensitive haptic perception. The realism of the haptic feedback, based on user ratings, was the highest with grounding at the middle phalanx for feedback along the finger axis, and at the proximal phalanx for feedback in the flexion-extension direction. Users identified the haptic feedback as most comfortable with grounding at the back of the hand for feedback along the finger axis and at the proximal phalanx for feedback in the flexion-extension direction. These findings show that the choice of grounding location has a significant impact on the user's haptic perception and qualitative experience. The results provide insights for designing next-generation wearable hand-grounded kinesthetic devices to achieve better haptic performance and user experience in virtual reality and teleoperated robotic applications.",0.0634416989],["van der Waals heterostructures explore the synergetic properties of two-dimensional materials when","Convergent beam electron holography for analysis of van der Waals heterostructures","summarize: Van der Waals heterostructures, which explore the synergetic properties of two-dimensional materials when assembled into three-dimensional stacks, have already brought to life a number of exciting new phenomena and novel electronic devices. Still, the interaction between the layers in such assembly, possible surface reconstruction, intrinsic and extrinsic defects are very difficult to characterise by any method, because of the single-atomic nature of the crystals involved. Here we present a convergent beam electron holographic technique which allows imaging of the stacking order in such heterostructures. Based on the interference of electron waves scattered on different crystals in the stack, this approach allows one to reconstruct the relative rotation, stretching, out-of-plane corrugation of the layers with atomic precision. Being holographic in nature, our approach allows extraction of quantitative information about the three-dimensional structure of the typical defects from a single image covering thousands of square nanometres. Furthermore, qualitative information about the defects in the stack can be extracted from the convergent diffraction patterns even without reconstruction - simply by comparing the patterns in different diffraction spots. We expect that convergent beam electron holography will be widely used to study the properties of van der Waals heterostructures.",0.4166666667],["the Brazilian Multipurpose Reactor will provide 3 thermal guides and 3 cold guides. the","Neutron Guide Building Instruments of the Brazilian Multipurpose Reactor Project","summarize: A growing community of scientists has been using neutrons in the most diverse areas of science. In order to meet the researchers demand in the areas of physics, chemistry, materials sciences, engineering, cultural heritage, biology and earth sciences, the Brazilian Multipurpose Reactor will provide 3 thermal guides and 3 cold guides, with the installation of several instruments for materials characterization. In this study, we present a standard design requirement of two primordial instruments, namely Sabi\\'a and Araponga. They are, respectively, cold and thermal neutron instruments and correspond to a Small-Angle Neutron Scattering and High-Resolution Powder Neutron Diffractometer to be installed in the Neutron Guide Building of RMB. To provide adequate flux for both instruments, we propose here an initial investigation of the use of simple and split guides to transport neutron beams to two different instruments on the same guide. For this purpose, we use Monte Carlo simulations utilizing McStas software to check the efficiency of thermal neutron transport for different basic configuration and sources. By considering these results, it is possible to conclude that the split guide configuration is, in most cases, more efficient than cases that use transmitted neutron beams independently of source. We also verify that the employment of different coating indexes for concave and convex surfaces on curved guides is crucial, at least on simulated cases, to optimise neutron flux and diminish facility installation cost.",0.2272727273],["inference method is based on a Markov chain Monte Carlo algorithm. the","Exact Bayesian inference in spatio-temporal Cox processes driven by multivariate Gaussian processes","summarize: In this paper we present a novel inference methodology to perform Bayesian inference for spatiotemporal Cox processes where the intensity function depends on a multivariate Gaussian process. Dynamic Gaussian processes are introduced to allow for evolution of the intensity function over discrete time. The novelty of the method lies on the fact that no discretisation error is involved despite the non-tractability of the likelihood function and infinite dimensionality of the problem. The method is based on a Markov chain Monte Carlo algorithm that samples from the joint posterior distribution of the parameters and latent variables of the model. The models are defined in a general and flexible way but they are amenable to direct sampling from the relevant distributions, due to careful characterisation of its components. The models also allow for the inclusion of regression covariates and\/or temporal components to explain the variability of the intensity function. These components may be subject to relevant interaction with space and\/or time. Real and simulated examples illustrate the methodology, followed by concluding remarks.",0.3888888889],["a user can define a model and customize the calibration. a user can define","A mechanism for balancing accuracy and scope in cross-machine black-box GPU performance modeling","summarize: The ability to model, analyze, and predict execution time of computations is an important building block supporting numerous efforts, such as load balancing, performance optimization, and automated performance tuning for high performance, parallel applications. In today's increasingly heterogeneous computing environment, this task must be accomplished efficiently across multiple architectures, including massively parallel coprocessors like GPUs. To address this challenge, we present an approach for constructing customizable, cross-machine performance models for GPU kernels, including a mechanism to automatically and symbolically gather performance-relevant kernel operation counts, a tool for formulating mathematical models using these counts, and a customizable parameterized collection of benchmark kernels used to calibrate models to GPUs in a black-box fashion. Our approach empowers a user to manage trade-offs between model accuracy, evaluation speed, and generalizability. A user can define a model and customize the calibration process, making it as simple or complex as desired, and as application-targeted or general as desired. To evaluate our approach, we demonstrate both linear and nonlinear models; each example models execution times for multiple variants of a particular computation: two matrix multiplication variants, four Discontinuous Galerkin differentiation operation variants, and two 2-D five-point finite difference stencil variants. For each variant, we present accuracy results on GPUs from multiple vendors and hardware generations. We view this customizable approach as a response to a central question in GPU performance modeling: how can we model GPU performance in a cost-explanatory fashion while maintaining accuracy, evaluation speed, portability, and ease of use, an attribute we believe precludes manual collection of kernel or hardware statistics.",0.6341463415],["the multi-agent Reinforcement Learning in MalmO competition proposes research in","The Multi-Agent Reinforcement Learning in Malm\\O Competition","summarize: Learning in multi-agent scenarios is a fruitful research direction, but current approaches still show scalability problems in multiple games with general reward settings and different opponent types. The Multi-Agent Reinforcement Learning in Malm\\O competition is a new challenge that proposes research in this domain using multiple 3D games. The goal of this contest is to foster research in general agents that can learn across different games and opponent types, proposing a challenge as a milestone in the direction of Artificial General Intelligence.",0.5],["we investigate the existence and uniqueness of solutions and derive the Ulam--Hy","On the Impulsive Implicit ","summarize: In this paper, we investigate the existence and uniqueness of solutions and derive the Ulam--Hyers--Mittag--Leffler stability results for impulsive implicit ",0.125],["the algorithm uses a variational Bayes based posterior approximation.","Skew-t Filter and Smoother with Improved Covariance Matrix Approximation","summarize: Filtering and smoothing algorithms for linear discrete-time state-space models with skew-t-distributed measurement noise are proposed. The algorithms use a variational Bayes based posterior approximation with coupled location and skewness variables to reduce the error caused by the variational approximation. Although the variational update is done suboptimally using an expectation propagation algorithm, our simulations show that the proposed method gives a more accurate approximation of the posterior covariance matrix than an earlier proposed variational algorithm. Consequently, the novel filter and smoother outperform the earlier proposed robust filter and smoother and other existing low-complexity alternatives in accuracy and speed. We present both simulations and tests based on real-world navigation data, in particular GPS data in an urban area, to demonstrate the performance of the novel methods. Moreover, the extension of the proposed algorithms to cover the case where the distribution of the measurement noise is multivariate skew-",0.3529411765],["Active Learning is concerned with the question of how to identify the most useful samples for a Machine","On the Robustness of Active Learning","summarize: Active Learning is concerned with the question of how to identify the most useful samples for a Machine Learning algorithm to be trained with. When applied correctly, it can be a very powerful tool to counteract the immense data requirements of Artificial Neural Networks. However, we find that it is often applied with not enough care and domain knowledge. As a consequence, unrealistic hopes are raised and transfer of the experimental results from one dataset to another becomes unnecessarily hard. In this work we analyse the robustness of different Active Learning methods with respect to classifier capacity, exchangeability and type, as well as hyperparameters and falsely labelled data. Experiments reveal possible biases towards the architecture used for sample selection, resulting in suboptimal performance for other classifiers. We further propose the new Sum of Squared Logits method based on the Simpson diversity index and investigate the effect of using the confusion matrix for balancing in sample selection.",0.2608695652],["cadmium arsenide is a newly discovered three-dimensional dirac semi","Widely Tunable Optical and Thermal Properties of Dirac Semimetal Cd","summarize: In this paper we report a detailed analysis of the temperature-dependent optical properties of epitaxially grown cadmium arsenide , a newly discovered three-dimensional Dirac semimetal. Dynamic Fermi level tuning -- instigated from Pauli-blocking in the linear Dirac cone -- and varying Drude response, generate large variations in the mid and far-infrared optical properties. We demonstrate thermo-optic shifts larger than those of traditional III-V semiconductors, which we attribute to the obtained large thermal expansion coefficient as revealed by first-principles calculations. Electron scattering rate, plasma frequency edge, Fermi level shift, optical conductivity, and electron effective mass analysis of Cd",0.3195854703],["we present the first observational evidence of the irregular surface of interplanetary shocks. we","First observations of irregular surface of interplanetary shocks at ion scales by Cluster","summarize: We present the first observational evidence of the irregular surface of interplanetary shocks by using multi-spacecraft observations of the Cluster mission. In total we discuss observations of four IP shocks that exhibit moderate Alfv\\'enic Mach numbers . Three of them are high-",0.4],["previous joint sGGM estimators fail to use existing knowledge. sGGM","A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models","summarize: We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks under a high-dimensional situation. In this paper, we propose a novel \\underlineoint \\underlinelementary \\underlinestimator incorporating additional \\underlinenowledge to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledge-specific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art ",0.0345637665],["citizens and emergency managers need to distinguish ''fake'' news posts from real","Real or Fake? User Behavior and Attitudes Related to Determining the Veracity of Social Media Posts","summarize: Citizens and Emergency Managers need to be able to distinguish ''fake'' news posts from real news posts on social media during disasters. This paper is based on an online survey conducted in 2018 that produced 341 responses from invitations distributed via email and through Facebook. It explores to what extent and how citizens generally assess whether postings are ''true'' or ''fake,'' and describes indicators of the trustworthiness of content that users would like. The mean response on a semantic differential scale measuring how frequently users attempt to verify the news trustworthiness was 3.37. The most frequent message characteristics citizens' use are grammar and the trustworthiness of the sender. Most respondents would find an indicator of trustworthiness helpful, with the most popular choice being a colored graphic. Limitations and implications for assessments of trustworthiness during disasters are discussed.",0.1194218851],["open educational resource introduces classical molecular dynamics simulations. the topic covered in this","An introduction to classical molecular dynamics simulation for experimental scattering users","summarize: Classical molecular dynamics simulations are a common component of multi-modal analyses from scattering measurements, such as small-angle scattering and diffraction. Users of these experimental techniques often have no formal training in the theory and practice of molecular dynamics simulation, leading to the possibility of these simulations being treated as a black box analysis technique. In this article, we describe an open educational resource designed to introduce classical molecular dynamics to users of scattering methods. This resource is available as a series of interactive web pages, which can be easily accessed by students, and as an open source software repository, which can be freely copied, modified, and redistributed by educators. The topic covered in this OER includes classical atomistic modelling, parameterising interatomic potentials, molecular dynamics simulations, typical sources of error, and some of the approaches to using simulations in the analysis of scattering data.",0.3571428571],["kinematic limit analysis is based on adaptive finite elements over conforming quadtre","An adaptive strategy based on conforming quadtree meshes for kinematic limit analysis","summarize: We propose a simple and efficient scheme based on adaptive finite elements over conforming quadtree meshes for collapse plastic analysis of structures. Our main interest in kinematic limit analysis is concerned with both purely cohesive-frictional and cohesive materials. It is shown that the most computational efficiency for collapse plastic problems is to employ an adaptive mesh strategy on quadtree meshes. However, a major difficulty in finite element formulations is the appearance of hanging nodes during adaptive process. This can be resolved by a definition of conforming quadtree meshes in the context of polygonal elements. Piecewise-linear shape functions enhanced with generalized bubble functions in barycentric coordinates are used to approximate the velocity field. Numerical results prove the reliability and benefit of the present approach.",0.7142857143],["we introduce the notion of a skeleton that defines a directed graph. the","A Graph Isomorphism Condition and Equivalence of Reaction Systems","summarize: We consider global dynamics of reaction systems as introduced by Ehrenfeucht and Rozenberg. The dynamics is represented by a directed graph, the so-called transition graph, and two reaction systems are considered equivalent if their corresponding transition graphs are isomorphic. We introduce the notion of a skeleton that uniquely defines a directed graph. We provide the necessary and sufficient conditions for two skeletons to define isomorphic graphs. This provides a necessary and sufficient condition for two reactions systems to be equivalent, as well as a characterization of the directed graphs that correspond to the global dynamics of reaction systems.",0.4285714286],["we examine linear perturbation theory to evaluate the contribution of viscosity coefficient in the growing","The Integrated Sachs-Wolfe Effect in the Bulk Viscous Dark Energy Model","summarize: We examine linear perturbation theory to evaluate the contribution of viscosity coefficient in the growing of dark matter perturbations in the context of the bulk viscous dark energy model inspired by thermodynamical dissipative phenomena proposed by . As the cosmological implementations, we investigate the auto-power spectrum, the ISW-galaxy cross-power spectrum and derive limits on ",0.1363636364],["Given a continuous function given a continuous function, the function is a continuous function.","Structure and Stability of the 1-Dimensional Mapper","summarize: Given a continuous function ",0.2857142857],["this article presents advances in resource allocation for downlink non-orthogonal multiple access systems","Resource Allocation for Downlink NOMA Systems: Key Techniques and Open Issues","summarize: This article presents advances in resource allocation for downlink non-orthogonal multiple access systems, focusing on user pairing and power allocation algorithms. The former pairs the users to obtain the high capacity gain by exploiting the channel gain difference between the users, while the later allocates power to users in each cluster to balance system throughput and user fairness. Additionally, the article introduces the concept of cluster fairness and proposes the divideand- next largest difference-based UP algorithm to distribute the capacity gain among the NOMA clusters in a controlled manner. Furthermore, performance comparison between multiple-input multiple-output NOMA and MIMO-OMA is conducted when users have pre-defined quality of service. Simulation results are presented, which validate the advantages of NOMA over OMA. Finally, the article provides avenues for further research on RA for downlink NOMA.",0.1428571429],["a substantial percentage of hard X-ray sources discovered with the BAT instrument onboard","Optical spectroscopic classification of 35 hard X-ray sources from the Swift-BAT 70-month catalogue","summarize: The nature of a substantial percentage of hard X-ray sources discovered with the BAT instrument onboard the Neil Gehrels Swift Observatory is unknown because of the lack of an identified longer-wavelength counterpart. Without such follow-up, an X-ray catalogue is of limited astrophysical value: we therefore embarked, since 2009, on a long-term project to uncover the optical properties of sources identified by Swift by using a large suite of ground-based telescopes and instruments. In this work, we continue our programme of characterization of unidentified or poorly studied hard X-ray sources by presenting the results of an optical spectroscopic campaign aimed at pinpointing and classifying the optical counterparts of 35 hard X-ray sources taken from the 70-month BAT catalogue. With the use of optical spectra taken at six different telescopes we were able to identify the main spectral characteristics of the observed objects, and determined their nature. We identify and characterize a total of 41 optical candidate counterparts corresponding to 35 hard X-ray sources given that, because of positional uncertainties, multiple lower energy counterparts can sometimes be associated with higher energy detections. We discuss which ones are the actual counterparts based on our observational results. In particular, 31 sources in our sample are active galactic nuclei: 16 are classified as Type 1 and 13 are classified as Type 2 ; two more are BL Lac-type objects. We also identify one LINER, one starburst, and 3 elliptical galaxies. The remaining 5 objects are galactic sources: we identify 4 of them as cataclysmic variables, whereas one is a low mass X-ray binary.",0.6842105263],["online tutoring agents are transforming into customizable, on-demand services driven by the learner","Intelligent Tutoring Systems for Generation Z's Addiction","summarize: As generation Z's big data is flooding the Internet through social nets, neural network based data processing is turning an important cornerstone, showing significant potential for fast extraction of data patterns. Online course delivery and associated tutoring are transforming into customizable, on-demand services driven by the learner. Besides automated grading, strong potential exists for the development and deployment of next generation intelligent tutoring software agents. Self-adaptive, online tutoring agents exhibiting intelligent-like behavior, being capable to learn from the learner, will become the next educational superstars. Over the past decade, computer-based tutoring agents were deployed in a variety of extended reality environments, from patient rehabilitation to psychological trauma healing. Most of these agents are driven by a set of conditional control statements and a large answers\/questions pairs dataset. This article provides a brief introduction on Generation Z's addiction to digital information, highlights important efforts for the development of intelligent dialogue systems, and explains the main components and important design decisions for Intelligent Tutoring System.",0.0],["the theory of exact completions is used to study categorical properties of small setoids in","Exact completion and constructive theories of sets","summarize: In the present paper we use the theory of exact completions to study categorical properties of small setoids in Martin-L\\of type theory and, more generally, of models of the Constructive Elementary Theory of the Category of Sets, in terms of properties of their subcategories of choice objects . Because of these intended applications, we deal with categories that lack equalisers and just have weak ones, but whose objects can be regarded as collections of global elements.In this context, we study the internal logic of the categories involved, and employ this analysis to give a sufficient condition for the local cartesian closure of an exact completion. Finally, we apply this result to show when an exact completion produces a model of CETCS.",0.1578947368],["this paper provides a pricing-hedging duality for the model-independent superhe","Pathwise superhedging on prediction sets","summarize: In this paper we provide a pricing-hedging duality for the model-independent superhedging price with respect to a prediction set ",0.1666666667],["kinematic limit analysis is based on adaptive finite elements over conforming quadtre","An adaptive strategy based on conforming quadtree meshes for kinematic limit analysis","summarize: We propose a simple and efficient scheme based on adaptive finite elements over conforming quadtree meshes for collapse plastic analysis of structures. Our main interest in kinematic limit analysis is concerned with both purely cohesive-frictional and cohesive materials. It is shown that the most computational efficiency for collapse plastic problems is to employ an adaptive mesh strategy on quadtree meshes. However, a major difficulty in finite element formulations is the appearance of hanging nodes during adaptive process. This can be resolved by a definition of conforming quadtree meshes in the context of polygonal elements. Piecewise-linear shape functions enhanced with generalized bubble functions in barycentric coordinates are used to approximate the velocity field. Numerical results prove the reliability and benefit of the present approach.",0.7142857143],["gravitomagnetic equations are similar to the Maxwell's EM equations","Quaternion algebra on 4D superfluid quantum space-time. Gravitomagnetism","summarize: Gravitomagnetic equations result from applying quaternionic differential operators to the energy-momentum tensor. These equations are similar to the Maxwell's EM equations. Both sets of the equations are isomorphic after changing orientation of either the gravitomagnetic orbital force or the magnetic induction. The gravitomagnetic equations turn out to be parent equations generating the following set of equations: the vorticity equation giving solutions of vortices with nonzero vortex cores and with infinite lifetime; the Hamilton-Jacobi equation loaded by the quantum potential. This equation in pair with the continuity equation leads to getting the \\Schrodinger equation describing a state of the superfluid quantum medium ; gravitomagnetic wave equations loaded by forces acting on the outer space. These waves obey to the Planck's law of radiation.",0.0833333333],["we introduce the first quantum Ansatze for the statistical relational learning on knowledge graph","Variational Quantum Circuit Model for Knowledge Graphs Embedding","summarize: In this work, we propose the first quantum Ans\\atze for the statistical relational learning on knowledge graphs using parametric quantum circuits. We introduce two types of variational quantum circuits for knowledge graph embedding. Inspired by the classical representation learning, we first consider latent features for entities as coefficients of quantum states, while predicates are characterized by parametric gates acting on the quantum states. For the first model, the quantum advantages disappear when it comes to the optimization of this model. Therefore, we introduce a second quantum circuit model where embeddings of entities are generated from parameterized quantum gates acting on the pure quantum state. The benefit of the second method is that the quantum embeddings can be trained efficiently meanwhile preserving the quantum advantages. We show the proposed methods can achieve comparable results to the state-of-the-art classical models, e.g., RESCAL, DistMult. Furthermore, after optimizing the models, the complexity of inductive inference on the knowledge graphs might be reduced with respect to the number of entities.",0.1176470588],["novel algorithm for 802.15.4 based wireless sensor networks. algorithm is in agreement with ideal","Accurate Localization in Wireless Sensor Networks in the Presence of Cross Technology Interference","summarize: Localization of mobile nodes in a wireless sensor networks is an active area of research. In this paper, we present a novel RSSI based localization algorithm for 802.15.4 based WSNs. We propose and implement a novel range based localization algorithm to minimize cross technology interference operating in the same band. The goal is to minimize the mean square error of the localization algorithm. Hardware implementation of the algorithm is in agreement with ideal simulation results where an accuracy of less than 0.5m has been achieved.",0.0625],["graph model modeled as two graphs. each node in one graph is supported by","Connectivity in Interdependent Networks","summarize: We propose and analyze a graph model to study the connectivity of interdependent networks. Two interdependent networks of arbitrary topologies are modeled as two graphs, where every node in one graph is supported by supply nodes in the other graph, and a node fails if all of its supply nodes fail. Such interdependence arises in cyber-physical systems and layered network architectures. We study the \\emph of a network: namely, the minimum number of supply node removals that would disconnect the network. We develop algorithms to evaluate the supply node connectivity given arbitrary network topologies and interdependence between two networks. Moreover, we develop interdependence assignment algorithms that maximize the supply node connectivity. We prove that a random assignment algorithm yields a supply node connectivity within a constant factor from the optimal for most networks.",0.1052631579],["acyclic networks are optimal structures when assuming time-independent in- and out","Cyclic structure induced by load fluctuations in adaptive transportation networks","summarize: Transport networks are crucial to the functioning of natural systems and technological infrastructures. For flow networks in many scenarios, such as rivers or blood vessels, acyclic networks are optimal structures when assuming time-independent in- and outflow. Dropping this assumption, fluctuations of net flow at source and\/or sink nodes may render the pure tree solutions unstable even under a simple local adaptation rule for conductances. Here, we consider tree-like networks under the influence of spatially heterogeneous distribution of fluctuations, where the root of the tree is supplied by a constant source and the leaves at the bottom are equipped with sinks with fluctuating loads. We find that the network divides into two regions characterized by tree-like motifs and stable cycles. The cycles emerge through transcritical bifurcations at a critical amplitude of fluctuation. For a simple network structure, depending on parameters defining the local adaptation, cycles first appear close to the leaves and then appear closer towards the root . The interaction between topology and dynamics gives rise to complex feedback mechanisms with many open questions in the theory of network dynamics. A general understanding of the dynamics in adaptive transport networks is essential in the study of mammalian vasculature, and adaptive transport networks may find technological applications in self-organizing piping systems.",0.0909090909],["two-way relay is potentially an effective approach to spectrum sharing and aggregation.","On the DoF of Two-way ","summarize: Two-way relay is potentially an effective approach to spectrum sharing and aggregation by allowing simultaneous bidirectional transmissions between source-destinations pairs. This paper studies the two-way ",0.0],["the cluster is currently the most distant cluster detected in the south pole Telescope 2500 square degree","Deep XMM-Newton Observations of the Most Distant SPT-SZ Galaxy Cluster","summarize: We present results from a 577 ks XMM-Newton observation of SPT-CL J0459-4947, the most distant cluster detected in the South Pole Telescope 2500 square degree survey, and currently the most distant cluster discovered through its Sunyaev-Zel'dovich effect. The data confirm the cluster's high redshift, ",0.1538461538],["homogenized model has been proposed by Sugimoto. resonators of different","A two-way model for nonlinear acoustic waves in a non-uniform lattice of Helmholtz resonators","summarize: Propagation of high amplitude acoustic pulses is studied in a 1D waveguide connected to a lattice of Helmholtz resonators. An homogenized model has been proposed by Sugimoto ), taking into account both the nonlinear wave propagation and various mechanisms of dissipation. This model is extended here to take into account two important features: resonators of different strengths and back-scattering effects. An energy balance is obtained, and a numerical method is developed. A closer agreement is reached between numerical and experimental results. Numerical experiments are also proposed to highlight the effect of defects and of disorder.",0.2010960138],["short-period stars orbiting our Galactic Center can probe gravitational theory in","Testing General Relativity with stellar orbits around the supermassive black hole in our Galactic center","summarize: In this Letter, we demonstrate that short-period stars orbiting around the supermassive black hole in our Galactic Center can successfully be used to probe the gravitational theory in a strong regime. We use 19 years of observations of the two best measured short-period stars orbiting our Galactic Center to constrain a hypothetical fifth force that arises in various scenarios motivated by the development of a unification theory or in some models of dark matter and dark energy. No deviation from General Relativity is reported and the fifth force strength is restricted to an upper 95% confidence limit of ",0.2317146428],["hydraulic conductivity is a nonlinear function of pressure head. a vadose","A numerical method for efficient 3D inversions using Richards equation","summarize: Fluid flow in the vadose zone is governed by Richards equation; it is parameterized by hydraulic conductivity, which is a nonlinear function of pressure head. Investigations in the vadose zone typically require characterizing distributed hydraulic properties. Saturation or pressure head data may include direct measurements made from boreholes. Increasingly, proxy measurements from hydrogeophysics are being used to supply more spatially and temporally dense data sets. Inferring hydraulic parameters from such datasets requires the ability to efficiently solve and deterministically optimize the nonlinear time domain Richards equation. This is particularly important as the number of parameters to be estimated in a vadose zone inversion continues to grow. In this paper, we describe an efficient technique to invert for distributed hydraulic properties in 1D, 2D, and 3D. Our algorithm does not store the Jacobian, but rather computes the product with a vector, which allows the size of the inversion problem to become much larger than methods such as finite difference or automatic differentiation; which are constrained by computation and memory, respectively. We show our algorithm in practice for a 3D inversion of saturated hydraulic conductivity using saturation data through time. The code to run our examples is open source and the algorithm presented allows this inversion process to run on modest computational resources.",0.2857142857],["the IA self-calibration method reduces the GI contamination by up to","Effects of Self-Calibration of Intrinsic Alignment on Cosmological Parameter Constraints from Future Cosmic Shear Surveys","summarize: Intrinsic alignments of galaxies have been recognized as one of the most serious contaminants to weak lensing. These systematics need to be isolated and mitigated in order for ongoing and future lensing surveys to reach their full potential. The IA self-calibration method was shown in previous studies to be able to reduce the GI contamination by up to a factor of 10 for the 2-point and 3-point correlations. The SC method does not require to assume an IA model in its working and can extract the GI signal from the same photo-z survey offering the possibility to test and understand structure formation scenarios and their relationship to IA models. In this paper, we study the effects of the IA SC mitigation method on the precision and accuracy of cosmological parameter constraints from future cosmic shear surveys LSST, WFIRST and Euclid. We perform analytical and numerical calculations to estimate the loss of precision and the residual bias in the best fit cosmological parameters after the self-calibration is performed. We take into account uncertainties from photometric redshifts and the galaxy bias. We find that the confidence contours are slightly inflated from applying the SC method itself while a significant increase is due to the inclusion of the photo-z uncertainties. The bias of cosmological parameters is reduced from several-",0.0],["the so-called mean squared derivative cost function plays a crucial role in both the fundamental","On the fundamental solution and a variational formulation of a degenerate diffusion of Kolmogorov type","summarize: In this paper, we construct the fundamental solution to a degenerate diffusion of Kolmogorov type and develop a time-discrete variational scheme for its adjoint equation. The so-called mean squared derivative cost function plays a crucial role occurring in both the fundamental solution and the variational scheme. The latter is implemented by minimizing a free energy functional with respect to the Kantorovich optimal transport cost functional associated with the mean squared derivative cost. We establish the convergence of the scheme to the solution of the adjoint equation generalizing previously known results for the Fokker-Planck equation and the Kramers equation.",0.48],["the approach is validated with respect to compression rate and storage requirement. it is observed that","On identification of self-similar characteristics using the Tensor Train decomposition method with application to channel turbulence flow","summarize: A study on the application of the Tensor Train decomposition method to 3D direct numerical simulation data of channel turbulence flow is presented. The approach is validated with respect to compression rate and storage requirement. In tests with synthetic data, it is found that grid-aligned self-similar patterns are well captured, and also the application to non grid-aligned self-similarity yields satisfying results. It is observed that the shape of the input Tensor significantly affects the compression rate. Applied to data of channel turbulent flow, the Tensor Train format allows for surprisingly high compression rates whilst ensuring low relative errors.",0.328794572],["collisional ring galaxies make up only 0.01% of galaxie","A giant galaxy in the young Universe with a massive ring","summarize: In the local Universe, collisional ring galaxies make up only ~0.01% of galaxies and are formed by head-on galactic collisions that trigger radially propagating density waves. These striking systems provide key snapshots for dissecting galactic disks and are studied extensively in the local Universe. However, not much is known about distant collisional rings. Here we present a detailed study of a ring galaxy at a look-back time of 10.8 Gyr . Compared with our Milky Way, this galaxy has a similar stellar mass, but has a stellar half-light radius that is 1.5-2.2 times larger and is forming stars 50 times faster. The large, diffuse stellar light outside the star-forming ring, combined with a radial velocity on the ring and an intruder galaxy nearby, provides evidence for this galaxy hosting a collisional ring. If the ring is secularly evolved, the implied large bar in a giant disk would be inconsistent with the current understanding of the earliest formation of barred spirals. Contrary to previous predictions, this work suggests that massive collisional rings were as rare 11 Gyr ago as they are today. Our discovery offers a unique pathway for studying density waves in young galaxies, as well as constraining the cosmic evolution of spiral disks and galaxy groups.",0.0800737403],["algencan is a well established safeguarded Augmented Lagrangian algorithm introduced in","Complexity and performance of an Augmented Lagrangian algorithm","summarize: Algencan is a well established safeguarded Augmented Lagrangian algorithm introduced in . Complexity results that report its worst-case behavior in terms of iterations and evaluations of functions and derivatives that are necessary to obtain suitable stopping criteria are presented in this work. In addition, the computational performance of a new version of the method is presented, which shows that the updated software is a useful tool for solving large-scale constrained optimization problems.",0.4545454545],["multiple use cases can be mapped to a single type of network slice instance. the","How Should Network Slice Instances be Provided to Multiple Use Cases of a Single Vertical Industry?","summarize: There are a large number of vertical industries implementing multiple use cases, each use case characterized by diverging service, network, and connectivity requirements such as automobile, manufacturing, power grid, etc. Such heterogeneity cannot be effectively managed and efficiently mapped onto a single type of network slice instance . Thus the tailored provisioning of an end-to-end network slicing solution to a vertical industry that consists of multiple use cases is a critical issue, which motivates this article, aimed at exploring this never-addressed and challenging research problem by proposing the Use-case Specific Network Slicing and the SubNetwork Slicing concepts that enable the provisioning of Use-case-specific NSI and GeNeric NSI, respectively. Both approaches tackle the same technical issue of provisioning, management, and orchestration of per vertical per use case NSIs in order to improve resource allocation and enhance network performance. The article also presents the architectural frameworks for managing US-NSI and GN-NSI, which extend the service deployment concept and system architecture of network slicing for vertical customers of fifth-generation mobile networks.",0.3446604682],["causal parameters may not be point identified in the presence of unobserved confounding","Deriving Bounds and Inequality Constraints Using LogicalRelations Among Counterfactuals","summarize: Causal parameters may not be point identified in the presence of unobserved confounding. However, information about non-identified parameters, in the form of bounds, may still be recovered from the observed data in some cases. We develop a new general method for obtaining bounds on causal parameters using rules of probability and restrictions on counterfactuals implied by causal graphical models. We additionally provide inequality constraints on functionals of the observed data law implied by such causal models. Our approach is motivated by the observation that logical relations between identified and non-identified counterfactual events often yield information about non-identified events. We show that this approach is powerful enough to recover known sharp bounds and tight inequality constraints, and to derive novel bounds and constraints.",0.2],["in vivo wireless body area networks are shaping the future of healthcare. it is necessary","Anatomical Region-Specific In Vivo Wireless Communication Channel Characterization","summarize: In vivo wireless body area networks and their associated technologies are shaping the future of healthcare by providing continuous health monitoring and noninvasive surgical capabilities, in addition to remote diagnostic and treatment of diseases. To fully exploit the potential of such devices, it is necessary to characterize the communication channel which will help to build reliable and high-performance communication systems. This paper presents an in vivo wireless communication channel characterization for male torso both numerically and experimentally considering various organs at 915 MHz and 2.4 GHz. A statistical path loss model is introduced, and the anatomical region-specific parameters are provided. It is found that the mean PL in dB scale exhibits a linear decaying characteristic rather than an exponential decaying profile inside the body, and the power decay rate is approximately twice at 2.4 GHz as compared to 915 MHz. Moreover, the variance of shadowing increases significantly as the in vivo antenna is placed deeper inside the body since the main scatterers are present in the vicinity of the antenna. Multipath propagation characteristics are also investigated to facilitate proper waveform designs in the future wireless healthcare systems, and a root-mean-square delay spread of 2.76 ns is observed at 5 cm depth. Results show that the in vivo channel exhibit different characteristics than the classical communication channels, and location dependency is very critical for accurate, reliable, and energy-efficient link budget calculations.",0.0],["this tutorial provides an intuitive and concrete description of the phenomena of electromagnetic nonreciprocity that will","Tutorial on Electromagnetic Nonreciprocity and Its Origins","summarize: This tutorial provides an intuitive and concrete description of the phenomena of electromagnetic nonreciprocity that will be useful for readers with engineering or physics backgrounds. The notion of time reversal and its different definitions are discussed with special emphasis to its relationship with the reciprocity concept. Starting from the Onsager reciprocal relations generally applicable to many physical processes, we present the derivation of the Lorentz theorem and discuss other implications of reciprocity for electromagnetic systems. Next, we identify all possible routes towards engineering nonreciprocal devices and analyze in detail three of them: Based on external bias, based on nonlinear and time-variant systems. The principles of the operation of different nonreciprocal devices are explained. We address the similarity and fundamental difference between nonreciprocal effects and asymmetric transmission in reciprocal systems. In addition to the tutorial description of the topic, the manuscript also contains original findings. In particular, general classification of reciprocal and nonreciprocal phenomena in linear bianisotropic media based on the space- and time-reversal symmetries is presented. This classification serves as a powerful tool for drawing analogies between seemingly distinct effects having the same physical origin and can be used for predicting novel electromagnetic phenomena. Furthermore, electromagnetic reciprocity theorem for time-varying systems is derived and its applicability is discussed.",0.1052631579],["VPA recognize a robust and algorithmically tractable fragment of context-free languages.","Minimization of Visibly Pushdown Automata Using Partial Max-SAT","summarize: We consider the problem of state-space reduction for nondeterministic weakly-hierarchical visibly pushdown automata . VPA recognize a robust and algorithmically tractable fragment of context-free languages that is natural for modeling programs. We define an equivalence relation that is sufficient for language-preserving quotienting of VPA. Our definition allows to merge states that have different behavior, as long as they show the same behavior for reachable equivalent stacks. We encode the existence of such a relation as a Boolean partial maximum satisfiability problem and present an algorithm that quickly finds satisfying assignments. These assignments are sub-optimal solutions to the PMax-SAT problem but can still lead to a significant reduction of states. We integrated our method in the automata-based software verifier Ultimate Automizer and show performance improvements on benchmarks from the software verification competition SV-COMP.",0.3684210526],["NTMA applications require real-time and scalable approaches. traffic classification and poli","A Survey on Big Data for Network Traffic Monitoring and Analysis","summarize: Network Traffic Monitoring and Analysis represents a key component for network management, especially to guarantee the correct operation of large-scale networks such as the Internet. As the complexity of Internet services and the volume of traffic continue to increase, it becomes difficult to design scalable NTMA applications. Applications such as traffic classification and policing require real-time and scalable approaches. Anomaly detection and security mechanisms require to quickly identify and react to unpredictable events while processing millions of heterogeneous events. At last, the system has to collect, store, and process massive sets of historical data for post-mortem analysis. Those are precisely the challenges faced by general big data approaches: Volume, Velocity, Variety, and Veracity. This survey brings together NTMA and big data. We catalog previous work on NTMA that adopt big data approaches to understand to what extent the potential of big data is being explored in NTMA. This survey mainly focuses on approaches and technologies to manage the big NTMA data, additionally briefly discussing big data analytics for the sake of NTMA. Finally, we provide guidelines for future work, discussing lessons learned, and research directions.",0.1538461538],["convolutional networks are very effective at dealing with rotational invariant classification problems.","Convolutional Networks for Spherical Signals","summarize: The success of convolutional networks in learning problems involving planar signals such as images is due to their ability to exploit the translation symmetry of the data distribution through weight sharing. Many areas of science and egineering deal with signals with other symmetries, such as rotation invariant data on the sphere. Examples include climate and weather science, astrophysics, and chemistry. In this paper we present spherical convolutional networks. These networks use convolutions on the sphere and rotation group, which results in rotational weight sharing and rotation equivariance. Using a synthetic spherical MNIST dataset, we show that spherical convolutional networks are very effective at dealing with rotationally invariant classification problems.",0.0],["digital democracy has yet to realize its potential for deliberative transformation. the undemocratic","Digital Democracy: Episode IV -- A New Hope, How a Corporation for Public Software Could Transform Digital Engagement for Government and Civil Society","summarize: Though successive generations of digital technology have become increasingly powerful in the past twenty years, digital democracy has yet to realize its potential for deliberative transformation. The undemocratic exploitation of massive social media systems continued this trend, but it only worsened an existing problem of modern democracies, which were already struggling to develop deliberative infrastructure independent of digital technologies. There have been many creative conceptions of civic tech, but implementation has lagged behind innovation. This essay argues for implementing one such vision of digital democracy through the establishment of a public corporation. Modeled on the Corporation for Public Broadcasting in the U.S., this entity would foster the creation of new digital technology by providing a stable source of funding to nonprofit technologists, interest groups, civic organizations, government, researchers, private companies, and the public. Funded entities would produce and maintain software infrastructure for public benefit. The concluding sections identify what circumstances might create and sustain such an entity.",0.0661956242],["the paper presents the description of several key RAN enablers. the five-generation","RAN Enablers for 5G Radio Resource Management","summarize: This paper presents the description of several key RAN enablers for the radio resource management framework of the fifth generation radio access network , referred to as building blocks of the 5G RRM. In particular, the following key RAN enablers are discussed: i) interference management techniques for dense and dynamic deployments, focusing on cell-edge performance enhancement; ii) dynamic traffic steering mechanisms that aim to attain the optimum mapping of 5G services to any available resources when and where needed by considering the peculiarities of different air interface variants ; iii) resource management strategies that deal with network slices; and iv) tight interworking between novel 5G AIVs and evolved legacy AIVs such as Long-term Evolution . Evaluation results for each of these key RAN enablers are also presented.",0.0555555556],["model of inflation is based on phantom field which exhibits an initial transient","Cosmological perturbations in transient phantom inflation scenarios","summarize: We present a model of inflation where the inflaton is accommodated as a phantom field which exhibits an initial transient pole behavior and then decays into a quintessence field which is responsible for a radiation era. We must stress that the present unified model only deals with a single field and that the transition between two eras is achieved in a smooth way, so the model does not suffer from the eternal inflation issue. We explore the conditions for the crossing of the phantom divide line within the inflationary era along with the structural stability of several critical points. We study the behavior of the phantom field within the slow climb approximation along with the necessary conditions to have sufficient inflation. We also examine the model at the level of classical perturbations within the Newtonian gauge and determine the behavior of the gravitational potential, contrast density and perturbed field near the inflation stage and the subsequent radiation era.",0.4375],["we prove a sharp analogue of Minkowski's inhomogeneous app","Inhomogeneous Diophantine approximation over fields of formal power series","summarize: We prove a sharp analogue of Minkowski's inhomogeneous approximation theorem over fields of power series ",0.5384615385],["a self-contained study of metastability properties of quasi-linear par","Metastability for parabolic equations with drift: part II. The quasilinear case","summarize: This is the second part of our series of papers on metastability results for parabolic equations with drift. The aim is to present a self-contained study, using partial differential equations methods, of the metastability properties of quasi-linear parabolic equations with a drift and to obtain results similar to those in Freidlin and Koralov .",0.3768176014],["high-level features are often abstract, subjective, and hard to quantify. we present","Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling","summarize: High-level musical qualities are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate sliding faders through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders . Using arousal as an example of a high-level feature, we show that the faders of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes , with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.",0.0],["the maximum inequalities for diffusion processes have drawn increasing attention in recent years. the","Moderate maximal inequalities for the Ornstein-Uhlenbeck process","summarize: The maximal inequalities for diffusion processes have drawn increasing attention in recent years. However, the existing proof of the ",0.3157894737],["smart world concept and smart world concept addressed in the fourth industrial revolution. new challenges in distributed","Smart systems, the fourth industrial revolution and new challenges in distributed computing","summarize: Smart systems and the smart world concept are addressed in the framework of the fourth industrial revolution. New challenges in distributed autonomous robots and computing are considered. An illustration of a new kind of smart and reconfigurable distributed modular robot system is given. A prototype is also presented as well as the associated distributed algorithm.",0.5185185185],["ionization is a cosmology reionization. it is","Spectroscopic confirmation of an ultra-faint galaxy at the epoch of reionization","summarize: Within one billion years of the Big Bang, intergalactic hydrogen was ionized by sources emitting ultraviolet and higher energy photons. This was the final phenomenon to globally affect all the baryons in the Universe. It is referred to as cosmic reionization and is an integral component of cosmology. It is broadly expected that intrinsically faint galaxies were the primary ionizing sources due to their abundance in this epoch. However, at the highest redshifts , all galaxies with spectroscopic confirmations to date are intrinsically bright and, therefore, not necessarily representative of the general population. Here, we report the unequivocal spectroscopic detection of a low luminosity galaxy at ",0.188239374],["recent methods have proposed deep learning techniques to predict the next event from a process state.","Decay Replay Mining to Predict Next Process Events","summarize: In complex processes, various events can happen in different sequences. The prediction of the next event given an a-priori process state is of importance in such processes. Recent methods have proposed deep learning techniques such as recurrent neural networks, developed on raw event logs, to predict the next event from a process state. However, such deep learning models by themselves lack a clear representation of the process states. At the same time, recent methods have neglected the time feature of event instances. In this paper, we take advantage of Petri nets as a powerful tool in modeling complex process behaviors considering time as an elemental variable. We propose an approach which starts from a Petri net process model constructed by a process mining algorithm. We enhance the Petri net model with time decay functions to create continuous process state samples. Finally, we use these samples in combination with discrete token movement counters and Petri net markings to train a deep learning model that predicts the next event. We demonstrate significant performance improvements and outperform the state-of-the-art methods on nine real-world benchmark event logs.",0.1578947368],["Let us know what you think about it!","Dimensional lower bounds for Falconer type incidence and point configuration theorems","summarize: Let ",0.0],["a physical meaningful electrical model was employed to fit the electrochemical impedance spectros","Insights into the enhancement of oxygen mass transport properties of strontium doped lanthanum manganite interface-dominated thin films","summarize: Strontium doped lanthanum manganite thin films were deposited by pulsed laser deposition on yttria stabilized zirconia single crystals for a comprehensive electrochemical characterization of the material acting as a cathode. A physically meaningful electrical model was employed to fit the electrochemical impedance spectroscopy results in order to extract the main oxygen mass transport parameters as a function of the temperature and oxygen partial pressure. The oxygen diffusion and surface exchange coefficients extracted from the analysis showed several orders of magnitude of enhancement with respect to the bulk values reported in the literature and an unexpectedly low dependence with the oxygen partial pressure. Different observations were combined to propose a mechanism for the enhanced incorporation of oxygen in interface dominated thin films mainly based on the high concentration of oxygen vacancies expected in the grain boundaries.",0.3482249119],["the SM is realized as a 4-dimensional effective theory without any compactification for the 5","Domain-Wall Standard Model in non-compact 5D and LHC phenomenology","summarize: We propose a framework to construct Domain-Wall Standard Model in a non compact 5-dimensional space-time, where all the Standard Model fields are localized in certain domains of the 5th dimension and the SM is realized as a 4-dimensional effective theory without any compactification for the 5th dimension. In this context, we investigate the collider phenomenology of the Kaluza-Klein modes of the SM gauge bosons and the current constraints from the search for a new gauge boson resonance at the LHC Run-2. The couplings of the SM fermions with the KK-mode gauge bosons depend on the configuration of the SM fermions in the 5-dimensional bulk. This geometry of the model can be tested at the future Large Hadron Collider experiment, once a KK-mode of the SM gauge boson is discovered.",0.28],["the symmetrized bidisc  has interesting geometric properties. it","A Geometric Characterization of the Symmetrized Bidisc","summarize: The symmetrized bidisc \\ has interesting geometric properties. While it has a plentiful supply of complex geodesics and of automorphisms, there is nevertheless a unique complex geodesic ",0.7671232877],["a quantum dot is modeled using an infinite potential well and a two-dimensional","Energy levels in a single-electron quantum dot with hydrostatic pressure","summarize: In this article we present a study of the effects of hydrostatic pressure on the energy levels of a quantum dot with an electron. A quantum dot is modeled using an infinite potential well and a two-dimensional harmonic oscillator and solved through the formalism of second quantization. A scheme for the implementation of a quantum NOT gate controlled with hydrostatic pressure is proposed.",0.3333333333],["integro-differential methods are applied to Neumann Homogenization problems","Neumann Homogenization via Integro-Differential Operators, Part 2: singular gradient dependence","summarize: We continue the program initiated in a previous work, of applying integro-differential methods to Neumann Homogenization problems. We target the case of linear periodic equations with a singular drift, which includes divergence equations with \\emph oscillatory Neumann conditions. Our analysis focuses on an induced integro-differential homogenization problem on the boundary of the domain. Also, we use homogenization results for regular Dirichlet problems to build barriers for the oscillatory Neumann problem with the singular gradient term. We note that our method allows to recast some existing results for fully nonlinear Neumann homogenization into this same framework. This version is the journal version.",0.2920502937],["we propose a flexible notion of characteristic functions defined on graph vertices. we","Characteristic Functions on Graphs: Birds of a Feather, from Statistical Descriptors to Parametric Models","summarize: In this paper, we propose a flexible notion of characteristic functions defined on graph vertices to describe the distribution of vertex features at multiple scales. We introduce FEATHER, a computationally efficient algorithm to calculate a specific variant of these characteristic functions where the probability weights of the characteristic function are defined as the transition probabilities of random walks. We argue that features extracted by this procedure are useful for node level machine learning tasks. We discuss the pooling of these node representations, resulting in compact descriptors of graphs that can serve as features for graph classification algorithms. We analytically prove that FEATHER describes isomorphic graphs with the same representation and exhibits robustness to data corruption. Using the node feature characteristic functions we define parametric models where evaluation points of the functions are learned parameters of supervised classifiers. Experiments on real world large datasets show that our proposed algorithm creates high quality representations, performs transfer learning efficiently, exhibits robustness to hyperparameter changes, and scales linearly with the input size.",0.3240863775],["nonconforming P1 finite element discretizations are thought to be less sensitive to the","Nonconforming P1 elements on distorted triangulations: Lower bounds for the discrete energy norm error","summarize: Compared to conforming P1 finite elements, nonconforming P1 finite element discretizations are thought to be less sensitive to the appearance of distorted triangulations. E.g., optimal-order discrete ",0.3086536929],["we developed a new kind of inorganic coating material based on ce-","Formation and characterization of ceramic coating from alumino silicate mineral powders in the matrix of cement composite on the concrete wall","summarize: Enhancement of thermal performance of concrete wall is nowadays of great importance in reducing the operational energy demand of buildings. We developed a new kind of inorganic coating material based on \\ce-\\ce-rich minerals and Portland cement powder. The finely pulverized mineral powder with the particle size distribution of 0.4-40 ",0.2500764103],["the classical approaches have reached their limits in regards to achievable charge carrier density, as well as mobility","Defect Modulation Doping","summarize: The doping of semiconductor materials is a fundamental part of modern technology, but the classical approaches have in many cases reached their limits both in regard to achievable charge carrier density, as well as mobility. Modulation doping, a mechanism that exploits the energy band alignment at an interface between two materials to induce free charge carriers in one of them, has been shown to circumvent the mobility restriction. Due to an alignment of doping limits by intrinsic defects, however, the carrier density limit cannot be lifted using this approach. Here we present a novel doping strategy using defects in a wide band gap material to dope the surface of a second semiconductor layer of dissimilar nature. We show that by depositing an insulator on a semiconductor material, the conductivity of the layer stack can be increased by seven orders of magnitude, without the necessity of high temperature processes or epitaxial growth. This approach has the potential to circumvent limits to both carrier mobility and density, opening up new possibilities in semiconductor device fabrication, particularly for the emerging field of oxide thin film electronics.",0.0434782609],["driftless swimmers include swimmers in a 3D Stokes flow or swimmers in","Optimal Strokes for Driftless Swimmers: A General Geometric Approach","summarize: Swimming consists by definition in propelling through a fluid by means of bodily movements. Thus, from a mathematical point of view, swimming turns into a control problem for which the controls are the deformations of the swimmer. The aim of this paper is to present a unified geometric approach for the optimization of the body deformations of so-called driftless swimmers. The class of driftless swimmers includes, among other, swimmers in a 3D Stokes flow or swimmers in a 2D or 3D potential flow. A general framework is introduced, allowing the complete analysis of five usual nonlinear optimization problems to be carried out. The results are illustrated with examples coming from the literature and with an in-depth study of a swimmer in a 2D potential flow. Numerical tests are also provided.",0.0909090909],["the paper describes the application of soft computing methods. the method is used to solve the problem","Soft computing methods for multiobjective location of garbage accumulation points in smart cities","summarize: This article describes the application of soft computing methods for solving the problem of locating garbage accumulation points in urban scenarios. This is a relevant problem in modern smart cities, in order to reduce negative environmental and social impacts in the waste management process, and also to optimize the available budget from the city administration to install waste bins. A specific problem model is presented, which accounts for reducing the investment costs, enhance the number of citizens served by the installed bins, and the accessibility to the system. A family of single- and multi-objective heuristics based on the PageRank method and two mutiobjective evolutionary algorithms are proposed. Experimental evaluation performed on real scenarios on the cities of Montevideo and Bahia Blanca demonstrates the effectiveness of the proposed approaches. The methods allow computing plannings with different trade-off between the problem objectives. The computed results improve over the current planning in Montevideo and provide a reasonable budget cost and quality of service for Bahia Blanca.",0.1612903226],["the quantum amplitude estimation task is an important problem which has various applications in fields such as","Faster Amplitude Estimation","summarize: In this paper, we introduce an efficient algorithm for the quantum amplitude estimation task which works in noisy intermediate-scale quantum devices. The quantum amplitude estimation is an important problem which has various applications in fields such as quantum chemistry, machine learning, and finance. Because the well-known algorithm for the quantum amplitude estimation using the phase estimation cannot be executed in NISQ devices, alternative approaches have been proposed in recent literature. Some of them provide a proof of the upper bound which almost achieves the Heisenberg scaling. However, the constant factor is large and thus the bound is loose. Our contribution in this paper is to provide the algorithm such that the upper bound of query complexity almost achieves the Heisenberg scaling and the constant factor is small.",0.0476190476],["role of participants in group interactions can produce different groups. roles have distinct patterns of behavioral engagement","Group Communication Analysis: A Computational Linguistics Approach for Detecting Sociocognitive Roles in Multi-Party Interactions","summarize: Roles are one of the most important concepts in understanding human sociocognitive behavior. During group interactions, members take on different roles within the discussion. Roles have distinct patterns of behavioral engagement , contribution characteristics , and social orientation . Different combinations of these roles can produce characteristically different group outcomes, being either less or more productive towards collective goals. In online collaborative learning environments, this can lead to better or worse learning outcomes for the individual participants. In this study, we propose and validate a novel approach for detecting emergent roles from the participants' contributions and patterns of interaction. Specifically, we developed a group communication analysis by combining automated computational linguistic techniques with analyses of the sequential interactions of online group communication. The GCA was applied to three large collaborative interaction datasets . Cluster analyses and linear mixed-effects modeling were used to assess the validity of the GCA approach and the influence of learner roles on student and group performance. The results indicate that participants' patterns in linguistic coordination and cohesion are representative of the roles that individuals play in collaborative discussions. More broadly, GCA provides a framework for researchers to explore the micro intra- and interpersonal patterns associated with the participants' roles and the sociocognitive processes related to successful collaboration.",0.1304347826],["the verification of beacon signatures in Vehicular Communication systems ensure awareness among neighboring vehicles","DoS-resilient Cooperative Beacon Verification for Vehicular Communication Systems","summarize: Authenticated safety beacons in Vehicular Communication systems ensure awareness among neighboring vehicles. However, the verification of beacon signatures introduces significant processing overhead for resource-constrained vehicular On-Board Units . Even worse in dense neighborhood or when a clogging Denial of Service attack is mounted. The OBU would fail to verify for all received beacons. This could significantly delay the verifications of authentic beacons or even affect the awareness of neighboring vehicle status. In this paper, we propose an efficient cooperative beacon verification scheme leveraging efficient symmetric key based authentication on top of pseudonymous authentication , providing efficient discovery of authentic beacons among a pool of received authentic and fictitious beacons, and can significantly decrease waiting times of beacons in queue before their validations. We show with simulation results that our scheme can guarantee low waiting times for received beacons even in high neighbor density situations and under DoS attacks, under which a traditional scheme would not be workable.",0.1333333333],["the sex of the u.s.","Optimal ","summarize: The ",0.0],["sputtered grown Ti films on Si3N4\/Si substrate have been reported","Interface study of thermally driven chemical kinetics involved in Ti\/Si3N4 based metal-substrate assembly by X-ray photoelectron spectroscopy","summarize: Diffusion mediated interaction in metal-substrate assembly during high temperature annealing leads to possible formation of new composite materials. Here, sputtered grown Ti films on Si3N4\/Si substrate has been reported to produce titanium nitride and silicide based binary composites while undergoing high vacuum annealing process at temperatures 650C and above. Diffusion of thermally decomposed Si and N atoms from Si3N4 and their subsequent chemical reaction with Ti have been probed by X-ray photo electron spectroscopy. For annealing at 800C and above, most of the Si atoms show preferences to stay in elemental form rather than developing silicide phase with Ti. Whereas at lower annealing temperature, silicide becomes the dominant phase for decomposed Si atoms. However, N atoms react promptly with Ti and form TiN which appears as the majority phase for each of the studied annealing temperature. Further, the nitride and silicide phases across the films have been compared quantitatively for various annealing temperature and the maximum silicide formation is observed for the sample annealed at 780C. Finally, the thermally anchored metal-substrate interaction mechanism can be exploited to fabricate disordered superconducting TiN films where TiSi2 and Si can be used to tune the level of disorder by altering the annealing temperature.",0.1489755911],["the curvature estimates for immersed stable free boundary minimal hypersurfaces satisfy a uniform","Curvature estimates for stable free boundary minimal hypersurfaces","summarize: In this paper, we prove uniform curvature estimates for immersed stable free boundary minimal hypersurfaces which satisfy a uniform area bound. Our result is a natural generalization of the celebrated Schoen-Simon-Yau interior curvature estimates up to the free boundary. A direct corollary of our curvature estimates is a smooth compactness theorem which is an essential ingredient in the min-max theory of free boundary minimal hypersurfaces developed by the last two authors. We also prove a monotonicity formula for free boundary minimal submanifolds in Riemannian manifolds for any dimension and codimension. For the case of ",0.619047619],["proposed algorithm outperforms existing methods in distinguishing extremely close DoAs for the case","Gridless Angular Domain Channel Estimation for mmWave Massive MIMO System With One-Bit Quantization Via Approximate Message Passing","summarize: We develop a direction of arrival and channel estimation algorithm for the one-bit quantized millimeter-wave massive multiple-input multiple-output system. By formulating the estimation problem as a noisy one-bit compressed sensing problem, we propose a computationally efficient gridless solution based on the expectation-maximization generalized approximate message passing approach. The proposed algorithm does not need the prior knowledge about the number of DoAs and outperforms the existing methods in distinguishing extremely close DoAs for the case of one-bit quantization. Both the DoAs and the channel coefficients are estimated for the case of one-bit quantization. The simulation results show that the proposed algorithm has effective estimation performances when the DoAs are very close to each other.",0.1297308495],["127 targeted flybys of Titan have been analyzed by spectrometer.","The Cassini VIMS archive of Titan: from browse products to global infrared color maps","summarize: We have analyzed the complete Visual and Infrared Mapping Spectrometer data archive of Titan. Our objective is to build global surface cartographic products, by combining all the data gathered during the 127 targeted flybys of Titan into synthetic global maps interpolated on a grid at 32 pixels per degree , in seven infrared spectral atmospheric windows. Multispectral summary images have been computed for each single VIMS cube in order to rapidly identify their scientific content and assess their quality. These summary images are made available to the community on a public website . The global mapping work faced several challenges due to the strong absorbing and scattering effects of the atmosphere coupled to the changing observing conditions linked to the orbital tour of the Cassini mission. We determined a surface photometric function which accounts for variations in incidence, emergence and phase angles, and which is able to mitigate brightness variations linked to the viewing geometry of the flybys. The atmospheric contribution has been reduced using the subtraction of the methane absorption band wings, considered as proxies for atmospheric haze scattering. We present a new global three color composite map of band ratios , which has also been empirically corrected from an airmass dependence. This map provides a detailed global color view of Titan's surface partially corrected from the atmosphere and gives a global insight of the spectral variability, with the equatorial dunes fields appearing in brownish tones, and several occurrences of bluish tones localized in areas such as Sinlap, Menvra and Selk craters. ",0.121876372],["medical first responders are trained to deal with emergencies more effectively. this would require real-","Effects of Voice-Based Synthetic Assistant on Performance of Emergency Care Provider in Training","summarize: As part of a perennial project, our team is actively engaged in developing new synthetic assistant technologies to assist in training combat medics and medical first responders. It is critical that medical first responders are well trained to deal with emergencies more effectively. This would require real-time monitoring and feedback for each trainee. Therefore, we introduced a voice-based SA to augment the training process of medical first responders and enhance their performance in the field. The potential benefits of SAs include a reduction in training costs and enhanced monitoring mechanisms. Despite the increased usage of voice-based personal assistants in day-to-day life, the associated effects are commonly neglected for a study of human factors. Therefore, this paper focuses on performance analysis of the developed voice-based SA in emergency care provider training for a selected emergency treatment scenario. The research discussed in this paper follows design science in developing proposed technology; at length, we discussed architecture and development and presented working results of voice-based SA. The empirical testing was conducted on two groups as user studies using statistical analysis tools, one trained with conventional methods and the other with the help of SA. The statistical results demonstrated the amplification in training efficacy and performance of medical responders powered by SA. Furthermore, the paper also discusses the accuracy and time of task execution and concludes with the guidelines for resolving the identified problems.",0.0666666667],["surrogate models are usually continuous and smooth, which is beneficial for continuous optimization problems.","Black-box Combinatorial Optimization using Models with Integer-valued Minima","summarize: When a black-box optimization objective can only be evaluated with costly or noisy measurements, most standard optimization algorithms are unsuited to find the optimal solution. Specialized algorithms that deal with exactly this situation make use of surrogate models. These models are usually continuous and smooth, which is beneficial for continuous optimization problems, but not necessarily for combinatorial problems. However, by choosing the basis functions of the surrogate model in a certain way, we show that it can be guaranteed that the optimal solution of the surrogate model is integer. This approach outperforms random search, simulated annealing and one Bayesian optimization algorithm on the problem of finding robust routes for a noise-perturbed traveling salesman benchmark problem, with similar performance as another Bayesian optimization algorithm, and outperforms all compared algorithms on a convex binary optimization problem with a large number of variables.",0.0],["the Dicke state is the largest state in the state.","Deterministic Preparation of Dicke States","summarize: The Dicke state ",0.1363636364],["the resulting SYZ mirror coincides with the one written down via physical means.","Quasimap SYZ for toric Calabi-Yau manifolds","summarize: In this note, we study the SYZ mirror construction for a toric Calabi-Yau manifold using instanton corrections coming from Woodward's quasimap Floer theory instead of Fukaya-Oh-Ohta-Ono's Lagrangian Floer theory. We show that the resulting SYZ mirror coincides with the one written down via physical means .",0.0666666667],["large-scale training for semantic segmentation is challenging. a standard CNN-based segment","Learning random-walk label propagation for weakly-supervised semantic segmentation","summarize: Large-scale training for semantic segmentation is challenging due to the expense of obtaining training data for this task relative to other vision tasks. We propose a novel training approach to address this difficulty. Given cheaply-obtained sparse image labelings, we propagate the sparse labels to produce guessed dense labelings. A standard CNN-based segmentation network is trained to mimic these labelings. The label-propagation process is defined via random-walk hitting probabilities, which leads to a differentiable parameterization with uncertainty estimates that are incorporated into our loss. We show that by learning the label-propagator jointly with the segmentation predictor, we are able to effectively learn semantic edges given no direct edge supervision. Experiments also show that training a segmentation network in this way outperforms the naive approach.",0.6666666667],["the sex of the u.s.","Distance-generalized Core Decomposition","summarize: The ",0.0],["the travelling thief problem involves two interdependent NP-hard components. the","Surrogate Assisted Optimisation for Travelling Thief Problems","summarize: The travelling thief problem is a multi-component optimisation problem involving two interdependent NP-hard components: the travelling salesman problem and the knapsack problem . Recent state-of-the-art TTP solvers modify the underlying TSP and KP solutions in an iterative and interleaved fashion. The TSP solution is typically changed in a deterministic way, while changes to the KP solution typically involve a random search, effectively resulting in a quasi-meandering exploration of the TTP solution space. Once a plateau is reached, the iterative search of the TTP solution space is restarted by using a new initial TSP tour. We propose to make the search more efficient through an adaptive surrogate model that learns the characteristics of initial TSP tours that lead to good TTP solutions. The model is used to filter out non-promising initial TSP tours, in effect reducing the amount of time spent to find a good TTP solution. Experiments on a broad range of benchmark TTP instances indicate that the proposed approach filters out a considerable number of non-promising initial tours, at the cost of omitting only a small number of the best TTP solutions.",0.0],["use of video or wearable gloves has raised privacy concerns. RF sensors are used to","American Sign Language Recognition Using RF Sensing","summarize: Many technologies for human-computer interaction have been designed for hearing individuals and depend upon vocalized speech, precluding users of American Sign Language in the Deaf community from benefiting from these advancements. While great strides have been made in ASL recognition with video or wearable gloves, the use of video in homes has raised privacy concerns, while wearable gloves severely restrict movement and infringe on daily life. Methods: This paper proposes the use of RF sensors for HCI applications serving the Deaf community. A multi-frequency RF sensor network is used to acquire non-invasive, non-contact measurements of ASL signing irrespective of lighting conditions. The unique patterns of motion present in the RF data due to the micro-Doppler effect are revealed using time-frequency analysis with the Short-Time Fourier Transform. Linguistic properties of RF ASL data are investigated using machine learning . Results: The information content, measured by fractal complexity, of ASL signing is shown to be greater than that of other upper body activities encountered in daily living. This can be used to differentiate daily activities from signing, while features from RF data show that imitation signing by non-signers is 99\\% differentiable from native ASL signing. Feature-level fusion of RF sensor network data is used to achieve 72.5\\% accuracy in classification of 20 native ASL signs. Implications: RF sensing can be used to study dynamic linguistic properties of ASL and design Deaf-centric smart environments for non-invasive, remote recognition of ASL. ML algorithms should be benchmarked on native, not imitation, ASL data.",0.0588235294],["study used an objective health indicator, namely antidepressant prescription rates. current studies","More green space is related to less antidepressant prescription rates in the Netherlands: A Bayesian geoadditive quantile regression approach","summarize: Exposure to green space seems to be beneficial for self-reported mental health. In this study we used an objective health indicator, namely antidepressant prescription rates. Current studies rely exclusively upon mean regression models assuming linear associations. It is, however, plausible that the presence of green space is non-linearly related with different quantiles of the outcome antidepressant prescription rates. These restrictions may contribute to inconsistent findings. Our aim was to assess antidepressant prescription rates in relation to green space, and to analyze how the relationship varies non-linearly across different quantiles of antidepressant prescription rates. We used cross-sectional data for the year 2014 at a municipality level in the Netherlands. Ecological Bayesian geoadditive quantile regressions were fitted for the 15, 50, and 85 percent quantiles to estimate green space-prescription rate correlations, controlling for confounders. The results suggested that green space was overall inversely and non-linearly associated with antidepressant prescription rates. More important, the associations differed across the quantiles, although the variation was modest. Significant non-linearities were apparent: The associations were slightly positive in the lower quantile and strongly negative in the upper one. Our findings imply that an increased availability of green space within a municipality may contribute to a reduction in the number of antidepressant prescriptions dispensed. Green space is thus a central health and community asset, whilst a minimum level of 28 percent needs to be established for health gains. The highest effectiveness occurred at a municipality surface percentage higher than 79 percent. This inverse dose-dependent relation has important implications for setting future community-level health and planning policies.",0.2391579196],["involution switches two sets of statistics known as the rises and the contacts. this is","The Rise-Contact involution on Tamari intervals","summarize: We describe an involution on Tamari intervals and m-Tamari intervals. This involution switches two sets of statistics known as the rises and the contacts and so proves an open conjecture from Pr\\'eville-Ratelle on intervals of the m-Tamari lattice.",0.1],["given an initial convex polygon with convex polygon with convex poly","A universal result for consecutive random subdivision of polygons","summarize: We consider consecutive random subdivision of polygons described as follows. Given an initial convex polygon with ",0.1739130435],["self-assembly is the autonomous organization of components into patterns or structures.","Targeted Assembly and Synchronization of Self-Spinning Microgears","summarize: Self-assembly is the autonomous organization of components into patterns or structures: an essential ingredient of biology and a desired route to complex organization. At equilibrium, the structure is encoded through specific interactions, at an unfavorable entropic cost for the system. An alternative approach, widely used by Nature, uses energy input to bypass the entropy bottleneck and develop features otherwise impossible at equilibrium. Dissipative building blocks that inject energy locally were made available by recent advance in colloidal science but have not been used to control self-assembly. Here we show the robust formation of self-powered rotors and dynamical superstructures from active particles and harness non-equilibrium phoretic phenomena to tailor interactions and direct self-assembly. We use a photoactive component that consumes fuel, hematite, to devise phototactic microswimmers that form self-spinning microgears following spatiotemporal light patterns. The gears are coupled via their chemical clouds and constitute the elementary bricks of synchronized superstructures, which autonomously regulate their dynamics. The results are quantitatively rationalized on the basis of a stochastic description of diffusio-phoretic oscillators dynamically coupled by chemical gradients to form directional interactions. Our findings demonstrate that non-equilibrium phenomena can be harnessed to shape interactions and program hierarchical constructions. It lays the groundwork for the self-assembly of dynamical architectures and synchronized micro-machinery.",0.0833333333],["the procedure relies on the formalism and mimetic properties of diagonal-norm","Conservative and entropy stable solid wall boundary conditions for the compressible Navier-Stokes equations: Adiabatic wall and heat entropy transfer","summarize: We present a novel technique for the imposition of non-linear entropy conservative and entropy stable solid wall boundary conditions for the compressible Navier-Stokes equations in the presence of an adiabatic wall, or a wall with a prescribed heat entropy flow. The procedure relies on the formalism and mimetic properties of diagonal-norm, summation-by-parts, and simultaneous-approximation-term operators, and is a generalization of previous works on discontinuous interface coupling and solid wall boundary conditions . Using the method of lines, a semi-discrete entropy estimate for the entire domain is obtained when the proposed numerical imposition of boundary conditions are coupled with an entropy-conservative or entropy-stable discrete interior operator. The resulting estimate mimics the global entropy estimate obtained at the continuous level. The boundary data at the wall are weakly imposed using a penalty flux approach and a simultaneous-approximation-term technique for both the conservative variables and the gradient of the entropy variables. Discontinuous spectral collocation operators , on high-order unstructured grids, are used for the purpose of demonstrating the robustness and efficacy of the new procedure for weakly enforcing boundary conditions. Numerical simulations confirm the non-linear stability of the proposed technique, with applications to three-dimensional subsonic and supersonic flows. The procedure described is compatible with any diagonal-norm summation-by-parts spatial operator, including finite element, finite difference, finite volume, discontinuous Galerkin, and flux reconstruction schemes.",0.1725803861],["the ongoing pandemic threatens the health of humans and causes great economic losses. we","A Data-Driven Network Model for the Emerging COVID-19 Epidemics in Wuhan, Toronto and Italy","summarize: The ongoing Coronavirus Disease 2019 pandemic threatens the health of humans and causes great economic losses. Predictive modelling and forecasting the epidemic trends are essential for developing countermeasures to mitigate this pandemic. We develop a network model, where each node represents an individual and the edges represent contacts between individuals where the infection can spread. The individuals are classified based on the number of contacts they have each day and their infection status. The transmission network model was respectively fitted to the reported data for the COVID-19 epidemic in Wuhan , Toronto , and the Italian Republic using a Markov Chain Monte Carlo optimization algorithm. Our model fits all three regions well with narrow confidence intervals and could be adapted to simulate other megacities or regions. The model projections on the role of containment strategies can help inform public health authorities to plan control measures.",0.1764705882],["deep learning-based VO can learn effective representations from data without explicit computation. this","Approaches, Challenges, and Applications for Deep Visual Odometry: Toward to Complicated and Emerging Areas","summarize: Visual odometry is a prevalent way to deal with the relative localization problem, which is becoming increasingly mature and accurate, but it tends to be fragile under challenging environments. Comparing with classical geometry-based methods, deep learning-based methods can automatically learn effective and robust representations, such as depth, optical flow, feature, ego-motion, etc., from data without explicit computation. Nevertheless, there still lacks a thorough review of the recent advances of deep learning-based VO . Therefore, this paper aims to gain a deep insight on how deep learning can profit and optimize the VO systems. We first screen out a number of qualifications including accuracy, efficiency, scalability, dynamicity, practicability, and extensibility, and employ them as the criteria. Then, using the offered criteria as the uniform measurements, we detailedly evaluate and discuss how deep learning improves the performance of VO from the aspects of depth estimation, feature extraction and matching, pose estimation. We also summarize the complicated and emerging areas of Deep VO, such as mobile robots, medical robots, augmented reality and virtual reality, etc. Through the literature decomposition, analysis, and comparison, we finally put forward a number of open issues and raise some future research directions in this field.",0.0],["heterointerface of spinel\/perovskite heterointerface.","Microscopic origin of the mobility enhancement at a spinel\/perovskite oxide heterointerface revealed by photoemission spectroscopy","summarize: The spinel\/perovskite heterointerface ",0.0426185741],["ab initio methods are used to study the absorption energy of atomic hydrogen at rotated","Selective Hydrogen Adsoprtion in Graphene Rotated Bilayers","summarize: The absorption energy of atomic hydrogen at rotated graphene bilayers is studied using ab initio methods based on the density functional theory including van der Waals interactions. We find that, due to the surface corrugation induced by the underneath rotated layer and the perturbation of the electronic density of states near the Fermi energy, the atoms with an almost AA stacking are the preferential ones for hydrogen chemisorption. The adsorption energy difference between different atoms can be as large as 80 meV. In addition, we find that, due to the logarithmic van Hove singularities in the electronic density of states at energies close to the Dirac point, the adsorption energy of either electron or hole doped samples is substantially increased. We also find that the adsorption energy increases with the decrease of the rotated angle between the layers. Finally, the large zero point energy of the C-H bond suggests adsorption and desorption of atomic hydrogen and deuterium should behave differently.",0.0526315789],["fighting fish is a new family of combinatorial objects. the generating functions are","Fighting fish and two-stack sortable permutations","summarize: In 2017, Duchi, Guerrini, Rinaldi and Schaeffer proposed a new family of combinatorial objects called fighting fish, which are counted by the same formula as more classical objects, such as two-stack sortable permutations and non-separable planar maps. In this article, we explore the bijective aspect of fighting fish by establishing a bijection to two-stack sortable permutations, using a new recursive decomposition of these permutations. With our bijection, we give combinatorial explanations of several results on fighting fish proved previously with generating functions. Using the decomposition of two-stack sortable permutations, we also prove the algebraicity of their generating function, extending a result of Bousquet-M\\'elou .",0.3157894737],["students used audience response systems and guided inquiry worksheets differently. students used each of the tools","The role of pedagogical tools in active learning: a case for sense-making","summarize: Evidence from the research literature indicates that both audience response systems and guided inquiry worksheets can lead to greater student engagement, learning, and equity in the STEM classroom. We compare the use of these two tools in large enrollment STEM courses delivered in different contexts, one in biology and one in engineering. The instructors studied utilized each of the active learning tools differently. In the biology course, ARS questions were used mainly to check in with students and assess if they were correctly interpreting and understanding worksheet questions. The engineering course presented ARS questions that afforded students the opportunity to apply learned concepts to new scenarios towards improving students conceptual understanding. In the biology course, the GIWs were primarily used in stand-alone activities, and most of the information necessary for students to answer the questions was contained within the worksheet in a context that aligned with a disciplinary model. In the engineering course, the instructor intended for students to reference their lecture notes and rely on their conceptual knowledge of fundamental principles from the previous ARS class session in order to successfully answer the GIW questions. However, while their specific implementation structures and practices differed, both instructors used these tools to build towards the same basic disciplinary thinking and sense-making processes of conceptual reasoning, quantitative reasoning, and metacognitive thinking.",0.1],["non-free data types are data types whose data have no canonical forms.","Non-linear Pattern Matching with Backtracking for Non-free Data Types","summarize: Non-free data types are data types whose data have no canonical forms. For example, multisets are non-free data types because the multiset ",0.0],["two filters have been demonstrated at 4.5 GHz with sharp roll-off, flat in-","4.5 GHz Lithium Niobate MEMS Filters with 10% Fractional Bandwidth for 5G Front-ends","summarize: This paper presents a new class of micro-electro-mechanical system C-band filters for 5G front-ends. The filter is comprised of resonators based on the first-order asymmetric Lamb wave mode in thin film lithium niobate. Two filters have been demonstrated at 4.5 GHz with sharp roll-off, flat in-band group delay, and spurious-free response over a wide frequency range. The first design shows a fractional bandwidth of 10%, an insertion loss of 1.7 dB, an out-of-band rejection of -13 dB, and a compact footprint of 0.36 mm2, while the second design shows an FBW of 8.5%, an IL of 2.7 dB, an OoB rejection of -25 dB, and a footprint of 0.9 mm^2. The demonstrations herein mark the largest fractional bandwidth achieved for acoustic-only filters at 5G frequencies.",0.2666666667],["two experiments on the impact of post hoc explanations by example and error rates on peoples","Play MNIST For Me! User Studies on the Effects of Post-Hoc, Example-Based Explanations & Error Rates on Debugging a Deep Learning, Black-Box Classifier","summarize: This paper reports two experiments on the impact of post hoc explanations by example and error rates on peoples perceptions of a black box classifier. Both experiments show that when people are given case based explanations, from an implemented ANN CBR twin system, they perceive miss classifications to be more correct. They also show that as error rates increase above 4%, people trust the classifier less and view it as being less correct, less reasonable and less trustworthy. The implications of these results for XAI are discussed.",0.2582594106],["combinatorics of combinatorics of the u.s.","","summarize: Using the combinatorics of ",0.0],["records and drawings for the 806 event have been philologically traced back to mid","Provenance of the Cross Sign of 806 in the Anglo-Saxon Chronicle: A possible Lunar Halo over Continental Europe?","summarize: While graphical records of astronomical\/meteorological events before telescopic observations are of particular interest, they have frequently undergone multiple copying and may have been modified from the original. Here, we analyze a graphical record of the cross-sign of 806 CE in the Anglo-Saxon Chronicle, which has been considered one of the earliest datable halo drawings in British records, whereas another cross-sign in 776 CE has been associated with the aurora. However, philological studies have revealed the later 806 event is derived from Continental annals. Here, records and drawings for the 806 event have been philologically traced back to mid-9th Century Continental manuscripts and the probable observational site identified as the area of Sens in northern France. The possible lunar halos at that time have been comprehensively examined by numerical ray tracing. Combined with calculations of twilight sky brightness, they identify a visibility window supporting monastic observation. Cruciform halos are shown to be fainter and rarer than brighter and more commonplace lunar halos. Physically credible cloud ice crystal variations can reproduce all the manuscript renditions. The manuscript records prove less than desirable detail but what is presented is fully consistent with a lunar halo interpretation. Finally, the possible societal impacts of such celestial events have been mentioned in the context of contemporary coins in Anglo-Saxon England and the Carolingian Empire. These analyses show that we need to trace their provenance back as far as possible, to best reconstruct the original event, even if graphical records are available for given astronomical\/meteorological events.",0.107353899],["facial-parity edge-coloring of a facial-parity edge-coloring","Improved bounds for some facially constrained colorings","summarize: A facial-parity edge-coloring of a ",0.1587153234],["complex structures are integrable on the total space of a smooth principal bundle. the","Group actions, non-K\\ahler complex manifolds and SKT structures","summarize: We give a construction of integrable complex structures on the total space of a smooth principal bundle over a complex manifold, with an even dimensional compact Lie group as structure group, under certain conditions. This generalizes the constructions of complex structure on compact Lie groups by Samelson and Wang, and on principal torus bundles by Calabi-Eckmann and others. It also yields large classes of new examples of non-K\\ahler compact complex manifolds. Moreover, under suitable restrictions on the base manifold, the structure group, and characteristic classes, the total space of the principal bundle admits SKT metrics. This generalizes recent results of Grantcharov et al. We study the Picard group and the algebraic dimension of the total space in some cases. We also use a slightly generalized version of the construction to obtain complex structures on tangential frame bundles of complex orbifolds.",0.3333333333],["structured prediction can be considered as a generalization of many standard supervised learning tasks","Minimax bounds for structured prediction","summarize: Structured prediction can be considered as a generalization of many standard supervised learning tasks, and is usually thought as a simultaneous prediction of multiple labels. One standard approach is to maximize a score function on the space of labels, which decomposes as a sum of unary and pairwise potentials, each depending on one or two specific labels, respectively. For this approach, several learning and inference algorithms have been proposed over the years, ranging from exact to approximate methods while balancing the computational complexity. However, in contrast to binary and multiclass classification, results on the necessary number of samples for achieving learning is still limited, even for a specific family of predictors such as factor graphs. In this work, we provide minimax bounds for a class of factor-graph inference models for structured prediction. That is, we characterize the necessary sample complexity for any conceivable algorithm to achieve learning of factor-graph predictors.",0.125],["we have implemented a platform that integrates DLTs with a monitoring system based","Trusted Wireless Monitoring based on Blockchain over NB-IoT Connectivity","summarize: The data collected from Internet of Things devices on various emissions or pollution, can have a significant economic value for the stakeholders. This makes it prone to abuse or tampering and brings forward the need to integrate IoT with a Distributed Ledger Technology to collect, store, and protect the IoT data. However, DLT brings an additional overhead to the frugal IoT connectivity and symmetrizes the IoT traffic, thus changing the usual assumption that IoT is uplink-oriented. We have implemented a platform that integrates DLTs with a monitoring system based on narrowband IoT . We evaluate the performance and discuss the tradeoffs in two use cases: data authorization and real-time monitoring.",0.2],["the learning algorithm is to be used a significant number of times during the design of a","Mise en abyme with artificial intelligence: how to predict the accuracy of NN, applied to hyper-parameter tuning","summarize: In the context of deep learning, the costliest phase from a computational point of view is the full training of the learning algorithm. However, this process is to be used a significant number of times during the design of a new artificial neural network, leading therefore to extremely expensive operations. Here, we propose a low-cost strategy to predict the accuracy of the algorithm, based only on its initial behaviour. To do so, we train the network of interest up to convergence several times, modifying its characteristics at each training. The initial and final accuracies observed during this beforehand process are stored in a database. We then make use of both curve fitting and Support Vector Machines techniques, the latter being trained on the created database, to predict the accuracy of the network, given its accuracy on the primary iterations of its learning. This approach can be of particular interest when the space of the characteristics of the network is notably large or when its full training is highly time-consuming. The results we obtained are promising and encouraged us to apply this strategy to a topical issue: hyper-parameter optimisation . In particular, we focused on the HO of a convolutional neural network for the classification of the databases MNIST and CIFAR-10. By using our method of prediction, and an algorithm implemented by us for a probabilistic exploration of the hyper-parameter space, we were able to find the hyper-parameter settings corresponding to the optimal accuracies already known in literature, at a quite low-cost.",0.5333333333],["unsupervised near-duplicate detection has many practical applications. it involves running a","Benchmarking unsupervised near-duplicate image detection","summarize: Unsupervised near-duplicate detection has many practical applications ranging from social media analysis and web-scale retrieval, to digital image forensics. It entails running a threshold-limited query on a set of descriptors extracted from the images, with the goal of identifying all possible near-duplicates, while limiting the false positives due to visually similar images. Since the rate of false alarms grows with the dataset size, a very high specificity is thus required, up to ",0.3684210526],["layered transition-metal trichalcogenides are a new frontier as two","Pressure-Induced Structural Phase Transition and a Special Amorphization Phase of Two-Dimensional Ferromagnetic Semiconductor Cr2Ge2Te6","summarize: Layered transition-metal trichalcogenides have become one of the research frontiers as two-dimensional magnets and candidate materials used for phase-change memory devices. Herein we report the high-pressure synchrotron X-ray diffraction and resistivity measurements on Cr2Ge2Te6 single crystal by using diamond anvil cell techniques, which reveal a mixture of crystalline-to-crystalline and crystalline-to-amorphous transitions taking place concurrently at 18.3-29.2 GPa. The polymorphic transition could be interpreted by atomic layer reconstruction and the amorphization could be understood in connection with randomly flipping atoms into van der Waals gaps. The amorphous phase is quenchable to ambient conditions. The electrical resistance of CGT shows a bouncing point at ~ 18 GPa, consistent with the polymorphism phase transition. Interestingly, the high-pressure AM phase exhibits metallic resistance with the magnitude comparable to that of high-pressure crystalline phases, whereas the resistance of the AM phase at ambient pressure fails to exceed that of the crystalline phase, indicating that the AM phase of CGT appeared under high pressure is quite unique and similar behavior has never been observed in other phase-change materials. The results definitely would have significant implications for the design of new functional materials.",0.3060018244],["a framework based on automatic image analysis works on two different training systems. the images","Counting of Grapevine Berries in Images via Semantic Segmentation using Convolutional Neural Networks","summarize: The extraction of phenotypic traits is often very time and labour intensive. Especially the investigation in viticulture is restricted to an on-site analysis due to the perennial nature of grapevine. Traditionally skilled experts examine small samples and extrapolate the results to a whole plot. Thereby different grapevine varieties and training systems, e.g. vertical shoot positioning and semi minimal pruned hedges pose different challenges. In this paper we present an objective framework based on automatic image analysis which works on two different training systems. The images are collected semi automatic by a camera system which is installed in a modified grape harvester. The system produces overlapping images from the sides of the plants. Our framework uses a convolutional neural network to detect single berries in images by performing a semantic segmentation. Each berry is then counted with a connected component algorithm. We compare our results with the Mask-RCNN, a state-of-the-art network for instance segmentation and with a regression approach for counting. The experiments presented in this paper show that we are able to detect green berries in images despite of different training systems. We achieve an accuracy for the berry detection of 94.0% in the VSP and 85.6% in the SMPH.",0.4814814815],["glottal vocoders generate the glottal excitation waveform by","Generative adversarial network-based glottal waveform model for statistical parametric speech synthesis","summarize: Recent studies have shown that text-to-speech synthesis quality can be improved by using glottal vocoding. This refers to vocoders that parameterize speech into two parts, the glottal excitation and vocal tract, that occur in the human speech production apparatus. Current glottal vocoders generate the glottal excitation waveform by using deep neural networks . However, the squared error-based training of the present glottal excitation models is limited to generating conditional average waveforms, which fails to capture the stochastic variation of the waveforms. As a result, shaped noise is added as post-processing. In this study, we propose a new method for predicting glottal waveforms by generative adversarial networks . GANs are generative models that aim to embed the data distribution in a latent space, enabling generation of new instances very similar to the original by randomly sampling the latent distribution. The glottal pulses generated by GANs show a stochastic component similar to natural glottal pulses. In our experiments, we compare synthetic speech generated using glottal waveforms produced by both DNNs and GANs. The results show that the newly proposed GANs achieve synthesis quality comparable to that of widely-used DNNs, without using an additive noise component.",0.2749157115],["BRST and supervariable approaches derive nilpotent charges for a","Massive Spinning Relativistic Particle: Revisited Under BRST and Supervariable Approaches","summarize: We discuss the continuous and infinitesimal gauge, supergauge, reparameterization, nilpotent Becchi-Rouet-Stora-Tyutin and anti-BRST symmetries and derive corresponding nilpotent charges for the one -dimensional massive model of a spinning relativistic particle. We exploit the theoretical potential and power of the BRST and supervariable approaches to derive the BRST symmetries and coupled Lagrangians for this system. In particular, we capture the off-shell nilpotency and absolute anticommutatvity of the conserved BRST charges within the framework of the newly proposed chiral supervariable approach to BRST formalism where only the chiral supervariables ). One of the novel observations of our present investigation is the derivation of the Curci-Ferrari -type restriction by the requirement of the absolute anticommutatvity of the BRST charges in the ordinary space. We obtain the same restriction within the framework of ACSA to BRST formalism by the symmetry invariance of the coupled Lagrangians, and the proof of the absolute anticommutatvity of the conserved and nilpotent BRST charges. The observation of the anticommutativity property of the BRST charges is a novel result in view of the fact that we have taken into account only the chiral super expansions.",0.5369035901],["this paper studies the possibility of detecting and isolating topology failures of","Generic Detectability and Isolability of Topology Failures in Networked Linear Systems","summarize: This paper studies the possibility of detecting and isolating topology failures of a networked system from subsystem measurements, in which subsystems are of fixed high-order linear dynamics, and the exact interaction weights among them are unknown. We prove that in such class of networked systems with the same network topologies, the detectability and isolability of a given topology failure are generic properties, indicating that it is the network topology that dominates the property of being detectable or isolable for a failure . We first give algebraic conditions for detectability and isolability of arbitrary parameter perturbations for a lumped plant, and then derive graph-theoretical necessary and sufficient conditions for generic detectability and isolability of topology failures for the networked systems. On the basis of these results, we consider the problems of deploying the smallest set of sensors for generic detectability and isolability. We reduce the associated sensor placement problems to the hitting set problems, which can be effectively solved by greedy algorithms with guaranteed approximation performances.",0.2142857143],["we look at the classical properties of Tremblay-Turbiner-Wintern","Lobachevsky geometry in TTW and PW systems","summarize: We review the classical properties of Tremblay-Turbiner-Winternitz and Post-Wintenitz systems and their relation with N-dimensional rational Calogero model with oscillator and Coulomb potentials, paying special attention to their hidden symmetries. Then we show that combining the radial coordinate and momentum in a single complex coordinate in proper way, we get an elegant description for the hidden and dynamical symmetries in these systems related with action-angle variables.",0.0],["the IBM Quantum Experience Platform makes it possible for every person around the world to get acquainted with","Experimental Realization of Controlled Square Root of Z Gate Using IBM's Cloud Quantum Experience Platform","summarize: Quantum computers form a technological cluster with huge growth in the last few years. Although this technology is of still very limited size: perhaps the reason it is not seen as a technology which may be mass produced or of public use in the near future: it is one of the most promising developments with a potential to change the world. The IBM Quantum Experience Platform makes it possible for every person around the world, without limitation as to geographical location, to get acquainted with the technology of quantum computing. It is a resource for both researchers and enthusiasts entering the quantum world. With the development of the platform, IBM has proven that the programming and writing code executable on a quantum computer can be easy and accessible even to people lacking any deep knowledge of quantum mechanics. The construction of the Controlled Square Root of Z gate is achieved using only existing predefined gates in the Composer tool. This newly created gate could be used for further work on quantum algorithms and opens a new feasible way to write quantum code.",0.2272727273],["a partial differential equations model for ELISPOT and Fluorospot immunoa","Cell detection on image-based immunoassays","summarize: Cell detection and counting in the image-based ELISPOT and Fluorospot immunoassays is considered a bottleneck. The task has remained hard to automatize, and biomedical researchers often have to rely on results that are not accurate. Previously proposed solutions are heuristic, and data-based solutions are subject to a lack of objective ground truth data. In this paper, we analyze a partial differential equations model for ELISPOT, Fluorospot, and assays of similar design. This leads us to a mathematical observation model for the images generated by these assays. We use this model to motivate a methodology for cell detection. Finally, we provide a real-data example that suggests that this cell detection methodology and a human expert perform comparably.",0.3125],["we analyze the fastest way to process graphs: pushing the updates to a shared state or","To Push or To Pull: On Reducing Communication and Synchronization in Graph Computations","summarize: We reduce the cost of communication and synchronization in graph processing by analyzing the fastest way to process graphs: pushing the updates to a shared state or pulling the updates to a private state.We investigate the applicability of this push-pull dichotomy to various algorithms and its impact on complexity, performance, and the amount of used locks, atomics, and reads\/writes. We consider 11 graph algorithms, 3 programming models, 2 graph abstractions, and various families of graphs. The conducted analysis illustrates surprising differences between push and pull variants of different algorithms in performance, speed of convergence, and code complexity; the insights are backed up by performance data from hardware counters.We use these findings to illustrate which variant is faster for each algorithm and to develop generic strategies that enable even higher speedups. Our insights can be used to accelerate graph processing engines or libraries on both massively-parallel shared-memory machines as well as distributed-memory systems.",0.2142857143],["videography provides easy methods for the observation and recording of animal behavior in diverse settings. but","Markerless tracking of user-defined features with deep learning","summarize: Quantifying behavior is crucial for many applications in neuroscience. Videography provides easy methods for the observation and recording of animal behavior in diverse settings, yet extracting particular aspects of a behavior for further analysis can be highly time consuming. In motor control studies, humans or other animals are often marked with reflective markers to assist with computer-based tracking, yet markers are intrusive , and the number and location of the markers must be determined a priori. Here, we present a highly efficient method for markerless tracking based on transfer learning with deep neural networks that achieves excellent results with minimal training data. We demonstrate the versatility of this framework by tracking various body parts in a broad collection of experimental settings: mice odor trail-tracking, egg-laying behavior in drosophila, and mouse hand articulation in a skilled forelimb task. For example, during the skilled reaching behavior, individual joints can be automatically tracked . Remarkably, even when a small number of frames are labeled , the algorithm achieves excellent tracking performance on test frames that is comparable to human accuracy.",0.2222222222],["resetting to the origin was assumed to take zero time or a time decoupled","Invariants of motion with stochastic resetting and space-time coupled returns","summarize: Motion under stochastic resetting serves to model a myriad of processes in physics and beyond, but in most cases studied to date resetting to the origin was assumed to take zero time or a time decoupled from the spatial position at the resetting moment. However, in our world, getting from one place to another always takes time and places that are further away take more time to be reached. We thus set off to extend the theory of stochastic resetting such that it would account for this inherent spatio-temporal coupling. We consider a particle that starts at the origin and follows a certain law of stochastic motion until it is interrupted at some random time. The particle then returns to the origin via a prescribed protocol. We study this model and surprisingly discover that the shape of the steady-state distribution which governs the stochastic motion phase does not depend on the return protocol. This shape invariance then gives rise to a simple, and generic, recipe for the computation of the full steady-state distribution. Several case studies are analyzed and a class of processes whose steady-state is completely invariant with respect to the speed of return is highlighted. For processes in this class we recover the same steady-state obtained for resetting with instantaneous returns---irrespective of whether the actual return speed is high or low. Our work significantly extends previous results on motion with stochastic resetting and is expected to find various applications in statistical, chemical, and biological physics.",0.4090909091],["a simulated model of matrices is a powerful tool for predicting missing","Distributed Bayesian Matrix Factorization with Limited Communication","summarize: Bayesian matrix factorization is a powerful tool for producing low-rank representations of matrices and for predicting missing values and providing confidence intervals. Scaling up the posterior inference for massive-scale matrices is challenging and requires distributing both data and computation over many workers, making communication the main computational bottleneck. Embarrassingly parallel inference would remove the communication needed, by using completely independent computations on different data subsets, but it suffers from the inherent unidentifiability of BMF solutions. We introduce a hierarchical decomposition of the joint posterior distribution, which couples the subset inferences, allowing for embarrassingly parallel computations in a sequence of at most three stages. Using an efficient approximate implementation, we show improvements empirically on both real and simulated data. Our distributed approach is able to achieve a speed-up of almost an order of magnitude over the full posterior, with a negligible effect on predictive accuracy. Our method outperforms state-of-the-art embarrassingly parallel MCMC methods in accuracy, and achieves results competitive to other available distributed and parallel implementations of BMF.",0.4736842105],["the discussion session was held at the sixth international genetic improvement workshop, GI-2019 @","The State and Future of Genetic Improvement","summarize: We report the discussion session at the sixth international Genetic Improvement workshop, GI-2019 @ ICSE, which was held as part of the 41st ACM\/IEEE International Conference on Software Engineering on Tuesday 28th May 2019. Topics included GI representations, the maintainability of evolved code, automated software testing, future areas of GI research, such as co-evolution, and existing GI tools and benchmarks.",0.0588235294],["new estimator induces two adaptive procedures. the adaptive BH procedure is conservative non-","Multiple testing with discrete data: proportion of true null hypotheses and two adaptive FDR procedures","summarize: We consider multiple testing with false discovery rate control when p-values have discrete and heterogeneous null distributions. We propose a new estimator of the proportion of true null hypotheses and demonstrate that it is less upwardly biased than Storey's estimator and two other estimators. The new estimator induces two adaptive procedures, i.e., an adaptive Benjamini-Hochberg procedure and an adaptive Benjamini-Hochberg-Heyse procedure. We prove that the the adaptive BH procedure is conservative non-asymptotically. Through simulation studies, we show that these procedures are usually more powerful than their non-adaptive counterparts and that the adaptive BHH procedure is usually more powerful than the adaptive BH procedure and a procedure based on randomized p-value. The adaptive procedures are applied to a study of HIV vaccine efficacy, where they identify more differentially polymorphic positions than the BH procedure at the same FDR level.",0.3215264697],["PPSP protocol is inherently insecure and should not be used. we describe specific","A Comment on Privacy-Preserving Scalar Product Protocols as proposed in SPOC","summarize: Privacy-preserving scalar product protocols are an important building block for secure computation tasks in various applications. Lu et al. introduced a PPSP protocol that does not rely on cryptographic assumptions and that is used in a wide range of publications to date. In this comment paper, we show that Lu et al.'s protocol is insecure and should not be used. We describe specific attacks against it and, using impossibility results of Impagliazzo and Rudich , show that it is inherently insecure and cannot be fixed without relying on at least some cryptographic assumptions.",0.0],["we consider the H'enon problem in the unit disc with Dirichlet boundary conditions","The H\\'enon problem with large exponent in the disc","summarize: In this paper we consider the H\\'enon problem in the unit disc with Dirichlet boundary conditions. We study the asymptotic profile of least energy and nodal least energy radial solutions and then deduce the exact computation of their Morse index for large values of the exponent p. As a consequence of this computation a multiplicity result for positive and nodal solutions is obtained.",0.4],["spectral action for a model of noncommutative geometry is a product of","On almost commutative Friedmann-Lema\\^itre-Robertson-Walker geometries","summarize: We analyze the leading terms of the spectral action for a model of noncommutative geometry, which is a product of ",0.4545454545],["the METI risk problem refers to the uncertain outcome of sending transmissions into space with the","Policy options for the radio detectability of Earth","summarize: The METI risk problem refers to the uncertain outcome of sending transmissions into space with the intention of messaging to extraterrestrial intelligence . Here, I demonstrate that this uncertainty is undecidable by proving that that the METI risk problem reduces to the halting problem. This implies that any proposed moratorium on METI activities cannot be based solely on the requirement for new information. I discuss three policy resolutions to deal with this risk ambiguity. Precautionary malevolence assumes that contact with ETI is likely to cause net harm to humanity, which remains consistent with the call for a METI moratorium, while assumed benevolence states that METI is likely to yield net benefits to humanity. I also propose a policy of preliminary neutrality, which suggests that humanity should engage in both SETI and METI until either one achieves its first success.",0.1739130435],["we consider an extension of the standard model based on the group.","A ","summarize: We consider an extension of the standard model based on the group ",0.0],["the forecasting module is a plug-and-play structured module. it can be","ForecastTB An R Package as a Test-Bench for Time Series Forecasting Application of Wind Speed and Solar Radiation Modeling","summarize: This paper introduces an R package ForecastTB that can be used to compare the accuracy of different forecasting methods as related to the characteristics of a time series dataset. The ForecastTB is a plug-and-play structured module, and several forecasting methods can be included with simple instructions. The proposed test-bench is not limited to the default forecasting and error metric functions, and users are able to append, remove, or choose the desired methods as per requirements. Besides, several plotting functions and statistical performance metrics are provided to visualize the comparative performance and accuracy of different forecasting methods. Furthermore, this paper presents real application examples with natural time series datasets to exhibit the features of the ForecastTB package to evaluate forecasting comparison analysis as affected by the characteristics of a dataset. Modeling results indicated the applicability and robustness of the proposed R package ForecastTB for time series forecasting.",0.1421250239],["graph neural network transforms features in each vertex's neighborhood into a vector representation of","Residual Correlation in Graph Neural Network Regression","summarize: A graph neural network transforms features in each vertex's neighborhood into a vector representation of the vertex. Afterward, each vertex's representation is used independently for predicting its label. This standard pipeline implicitly assumes that vertex labels are conditionally independent given their neighborhood features. However, this is a strong assumption, and we show that it is far from true on many real-world graph datasets. Focusing on regression tasks, we find that this conditional independence assumption severely limits predictive power. This should not be that surprising, given that traditional graph-based semi-supervised learning methods such as label propagation work in the opposite fashion by explicitly modeling the correlation in predicted outcomes. Here, we address this problem with an interpretable and efficient framework that can improve any graph neural network architecture simply by exploiting correlation structure in the regression residuals. In particular, we model the joint distribution of residuals on vertices with a parameterized multivariate Gaussian, and estimate the parameters by maximizing the marginal likelihood of the observed labels. Our framework achieves substantially higher accuracy than competing baselines, and the learned parameters can be interpreted as the strength of correlation among connected vertices. Furthermore, we develop linear time algorithms for low-variance, unbiased model parameter estimates, allowing us to scale to large networks. We also provide a basic version of our method that makes stronger assumptions on correlation structure but is painless to implement, often leading to great practical performance with minimal overhead.",0.2380952381],["this year, we reran the four subtasks from semEval-2016","SemEval-2017 Task 3: Community Question Answering","summarize: We describe SemEval-2017 Task 3 on Community Question Answering. This year, we reran the four subtasks from SemEval-2016: Question-Comment Similarity, Question-Question Similarity, Question-External Comment Similarity, and Rerank the correct answers for a new question in Arabic, providing all the data from 2015 and 2016 for training, and fresh data for testing. Additionally, we added a new subtask E in order to enable experimentation with Multi-domain Question Duplicate Detection in a larger-scale scenario, using StackExchange subforums. A total of 23 teams participated in the task, and submitted a total of 85 runs for subtasks A-D. Unfortunately, no teams participated in subtask E. A variety of approaches and features were used by the participating systems to address the different subtasks. The best systems achieved an official score of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D, respectively. These scores are better than the baselines, especially for subtasks A-C.",0.1111111111],["the scheme is based on higher order interference between quantum down-converted light and classical coherent","HIgh-Noon States with High Flux of Photons Using coherent Beam Stimulated Non-Collinear Parametric Down Conversion","summarize: We show how to reach high fidelity NOON states with a high count rate inside optical interferometers. Recently it has been shown that by mixing squeezed and coherent light at a beamsplitter it is possible to generate NOON states of arbitrary N with a fidelity as high as 94%. ). The scheme is based on higher order interference between quantum down-converted light and classical coherent light. However, this requires optimizing the amplitude ratio of classical to quantum light thereby limiting the overall count rate for the interferometric super-resolution signal. We propose using coherent-beam-stimulated non-collinear down converted light as input to the interferometer. Our scheme is based on stimulation of non-collinear parametric down conversion by two-mode coherent light. We have somehow a better flexibility of choosing the amplitude ratio in generating NOON states. This enables super-resolution intensity exceeding the previous scheme by many orders of magnitude. Therefore we hope to improve the magnitude of N-fold super-resolution in quantum interferometry for arbitrary N by using bright light sources. We give some results for N=4 and 5.",0.1875],["proposed model incorporates a gating network which assigns noisy speech signals to an appropriate","Sparse Mixture of Local Experts for Efficient Speech Enhancement","summarize: In this paper, we investigate a deep learning approach for speech denoising through an efficient ensemble of specialist neural networks. By splitting up the speech denoising task into non-overlapping subproblems and introducing a classifier, we are able to improve denoising performance while also reducing computational complexity. More specifically, the proposed model incorporates a gating network which assigns noisy speech signals to an appropriate specialist network based on either speech degradation level or speaker gender. In our experiments, a baseline recurrent network is compared against an ensemble of similarly-designed smaller recurrent networks regulated by the auxiliary gating network. Using stochastically generated batches from a large noisy speech corpus, the proposed model learns to estimate a time-frequency masking matrix based on the magnitude spectrogram of an input mixture signal. Both baseline and specialist networks are trained to estimate the ideal ratio mask, while the gating network is trained to perform subproblem classification. Our findings demonstrate that a fine-tuned ensemble network is able to exceed the speech denoising capabilities of a generalist network, doing so with fewer model parameters.",0.1904761905],["symmetry in the pairing interaction of nuclei is a key factor. seniority","Goodness of Generalized Seniority in Semi-magic Nuclei","summarize: Symmetry plays an important role in understanding the nuclear structure properties from the rotation of a nucleus to the spin, parity and isospin of nuclear states. This simplifies the complexity of the nuclear problems in one way or the other. Seniority is also a well known quantum number which arises due to the symmetry in the pairing interaction of nuclei. We present empirical as well as theoretical evidences based on decay rates which support the goodness of seniority at higher spins as well as in nrich or, n-deficient nuclei. We find that the generalized seniority governs the identical trends of high-spin isomers in different semi-magic chains, where different set of nucleon orbitals from different valence spaces are involved.",0.2352941176],["we divide the receivers into two sets: the decoding set and the malicious set.","Entanglement-assisted private communication over quantum broadcast channels","summarize: We consider entanglement-assisted private communication over a quantum broadcast channel, in which there is a single sender and multiple receivers. We divide the receivers into two sets: the decoding set and the malicious set. The decoding set and the malicious set can either be disjoint or can have a finite intersection. For simplicity, we say that a single party Bob has access to the decoding set and another party Eve has access to the malicious set, and both Eve and Bob have access to the pre-shared entanglement with Alice. The goal of the task is for Alice to communicate classical information reliably to Bob and securely against Eve, and Bob can take advantage of pre-shared entanglement with Alice. In this framework, we establish a lower bound on the one-shot EA private capacity. When there exists a quantum channel mapping the state of the decoding set to the state of the malicious set, such a broadcast channel is said to be degraded. We establish an upper bound on the one-shot EA private capacity in terms of smoothed min- and max-entropies for such channels. In the limit of a large number of independent channel uses, we prove that the EA private capacity of a degraded quantum broadcast channel is given by a single-letter formula. Finally, we consider two specific examples of degraded broadcast channels and find their capacities. In the first example, we consider the scenario in which one part of Bob's laboratory is compromised by Eve. We show that the capacity for this protocol is given by the conditional quantum mutual information of a quantum broadcast channel, and so we thus provide an operational interpretation to the dynamic counterpart of the conditional quantum mutual information. In the second example, Eve and Bob have access to mutually exclusive sets of outputs of a broadcast channel.",0.0],["the transport coefficients are calculated from first principles. we compare our results to recent observations","Perpendicular Diffusion of Solar Energetic Particles: Model Results and Implications for Electrons","summarize: The processes responsible for the effective longitudinal transport of solar energetic particles are still not completely understood. We address this issue by simulating SEP electron propagation using a spatially 2D transport model that includes perpendicular diffusion. By implementing, as far as possible, the most reasonable estimates of the transport coefficients, we compare our results, in a qualitative manner, to recent observations , focusing on the longitudinal distribution of the peak intensity, the maximum anisotropy and the onset time. By using transport coefficients which are derived from first principles, we limit the number of free parameters in the model to: the probability of SEPs following diffusing magnetic field lines, quantified by ",0.0],["ordinal distance information is used to study machine learning problems. it is more common to use","Lens depth function and k-relative neighborhood graph: versatile tools for ordinal data analysis","summarize: In recent years it has become popular to study machine learning problems in a setting of ordinal distance information rather than numerical distance measurements. By ordinal distance information we refer to binary answers to distance comparisons such as ",0.2173913043],["software testing is becoming a critical part of the development cycle of embedded devices. the goal","Side-Channel Aware Fuzzing","summarize: Software testing is becoming a critical part of the development cycle of embedded devices, enabling vulnerability detection. A well-studied approach of software testing is fuzz-testing , during which mutated input is sent to an input-processing software while its behavior is monitored. The goal is to identify faulty states in the program, triggered by malformed inputs. Even though this technique is widely performed, fuzzing cannot be applied to embedded devices to its full extent. Due to the lack of adequately powerful I\/O capabilities or an operating system the feedback needed for fuzzing cannot be acquired. In this paper we present and evaluate a new approach to extract feedback for fuzzing on embedded devices using information the power consumption leaks. Side-channel aware fuzzing is a threefold process that is initiated by sending an input to a target device and measuring its power consumption. First, we extract features from the power traces of the target device using machine learning algorithms. Subsequently, we use the features to reconstruct the code structure of the analyzed firmware. In the final step we calculate a score for the input, which is proportional to the code coverage. We carry out our proof of concept by fuzzing synthetic software and a light-weight AES implementation running on an ARM Cortex-M4 microcontroller. Our results show that the power side-channel carries information relevant for fuzzing.",0.0769230769],["ab initio methods are used to study the absorption energy of atomic hydrogen at rotated","Selective Hydrogen Adsoprtion in Graphene Rotated Bilayers","summarize: The absorption energy of atomic hydrogen at rotated graphene bilayers is studied using ab initio methods based on the density functional theory including van der Waals interactions. We find that, due to the surface corrugation induced by the underneath rotated layer and the perturbation of the electronic density of states near the Fermi energy, the atoms with an almost AA stacking are the preferential ones for hydrogen chemisorption. The adsorption energy difference between different atoms can be as large as 80 meV. In addition, we find that, due to the logarithmic van Hove singularities in the electronic density of states at energies close to the Dirac point, the adsorption energy of either electron or hole doped samples is substantially increased. We also find that the adsorption energy increases with the decrease of the rotated angle between the layers. Finally, the large zero point energy of the C-H bond suggests adsorption and desorption of atomic hydrogen and deuterium should behave differently.",0.0526315789],["the orientation of Greek temples has been the subject of several debates since the end of the","Understanding the meaning of Greek temples' orientations. Akragas Valley of the Temples as a case study","summarize: The issue of the orientation of Greek Temples has been the subject of several debates since the end of the 19 century. In fact, although a general tendency to orientation within the arc of the rising sun is undeniable, specific patterns and true meaning remain obscure. With the aim of shedding light on this problem we present here a new complete, high precision survey of the temples of Akragas, the so called Valley of the Temples UNESCO site. Our results include a important temple which was essentially yet unpublished, and most of all show that very different reasons influenced the orientation choices, some symbolical, but others by far more practical, besides the general rule of orienting to the rising sun. In particular, the existence of temples orientated in accordance with the towns grid, as well as to the cardinal points irrespectively from the sun's declination associated to true east at the uneven horizon, is evidenced. Finally, for two temples having anomalous orientations a stellar and a lunar proposal are made respectively",0.4857142857],["5G will meet the growing demand of new, sustainable, and more accessible services. the","Performance Requirements of Advanced Healthcare Services over Future Cellular Systems","summarize: The fifth generation of communication systems has ambitious targets of data rate, end-to-end latency, and connection availability, while the deployment of a new flexible network architecture will spur new applications. E-health and mobile health solutions will meet the increasing demand of new, sustainable, and more accessible services beneficial to both practitioners and the rapidly aging population. This paper aims at defining the technical requirements of future cellular networks to support a variety of advanced healthcare services . While 5G will be able to satisfy these requirements, it will also pave the way for future e- and m-health in the sixth-generation cellular networks.",0.0588235294],["the algorithm is based on the interpretation of such a realization as a point in the","Counting realizations of Laman graphs on the sphere","summarize: We present an algorithm that computes the number of realizations of a Laman graph on a sphere for a general choice of the angles between the vertices. The algorithm is based on the interpretation of such a realization as a point in the moduli space of stable curves of genus zero with marked points, and on the explicit description, due to Keel, of the Chow ring of this space.",0.4390243902],["the game is a game titled The Cursed Prince. the main character in this","Game of the Cursed Prince based on Android","summarize: Nowadays Games become an entertainment alternative for various circles, industry and game development business is also a profitable industry. In Indonesia the amount of game consumption is very high, especially the console game type RPG . The task of this research is developing game software using Unity 3D to create an Android-based RPG game app. The story is packed with RPG genres so the player can feel the main role of the storys imagination. The game to be built is a game titled The Cursed Prince. Users will get the sensation of royal adventure. Multiplayer game system, graphics in 3D game, The main character in this game is Prince, enemies in this game are wizards and monsters, Game is not limited time to complete. And the game can be saved, so it can be reopened. The game of The Cursed Prince can be part of Indonesian Industry Gaming development.",0.2692307692],["we propose a statistical non-linear model based on the photomultiplier tube","Statistical Non-linear Model, Achievable Rates and Signal Detection for Photon-level Photomultiplier Receiver","summarize: We characterize the practical receiver in a wide range of signal intensity for optical wireless communication, from discrete pulse regime to continuous waveform regime. We first propose a statistical non-linear model based on the photomultiplier tube multi-stage amplification and Poisson channel, and then derive the optimal and tractable suboptimal duty cycle with peak-power and average-power constraints for on-off key modulation in linear regime. Subsequently, a threshold-based classifier is proposed to distinguish the PMT working regimes based on the non-linear model. Moreover, we derive the approximate performance of mean power detection with infinite sampling rate and finite over-sampling rate in the linear regime based on small dead time and central-limit theorem. We also fomulate a signal model in the non-linar regime. Furthermore, the performance of mean power detection and photon counting detection with maximum likelihood detection for different sampling rates is evaluated from both theoretical and numerical perspectives. We can conclude that the sample interval equivalent to dead time is a good choice, and lower sampling rate would significantly degrade the performance.",0.3994815634],["driftless swimmers include swimmers in a 3D Stokes flow or swimmers in","Optimal Strokes for Driftless Swimmers: A General Geometric Approach","summarize: Swimming consists by definition in propelling through a fluid by means of bodily movements. Thus, from a mathematical point of view, swimming turns into a control problem for which the controls are the deformations of the swimmer. The aim of this paper is to present a unified geometric approach for the optimization of the body deformations of so-called driftless swimmers. The class of driftless swimmers includes, among other, swimmers in a 3D Stokes flow or swimmers in a 2D or 3D potential flow. A general framework is introduced, allowing the complete analysis of five usual nonlinear optimization problems to be carried out. The results are illustrated with examples coming from the literature and with an in-depth study of a swimmer in a 2D potential flow. Numerical tests are also provided.",0.0909090909],["we show that when restricted to smooth manifolds equipped with tangential structure, this","Central extensions of mapping class groups from characteristic classes","summarize: We characterize, for every higher smooth stack equipped with tangential structure, the induced higher group extension of the geometric realization of its higher automorphism stack. We show that when restricted to smooth manifolds equipped with higher degree topological structures, this produces higher extensions of homotopy types of diffeomorphism groups. Passing to the groups of connected components, we obtain abelian extensions of mapping class groups and we derive sufficient conditions for these being central. We show as a special case that this provides an elegant re-construction of Segal's approach to ",0.0],["super resolution is the problem of recovering a high-resolution image from a single or","ImagePairs: Realistic Super Resolution Dataset via Beam Splitter Camera Rig","summarize: Super Resolution is the problem of recovering a high-resolution image from a single or multiple low-resolution images of the same scene. It is an ill-posed problem since high frequency visual details of the scene are completely lost in low-resolution images. To overcome this, many machine learning approaches have been proposed aiming at training a model to recover the lost details in the new scenes. Such approaches include the recent successful effort in utilizing deep learning techniques to solve super resolution problem. As proven, data itself plays a significant role in the machine learning process especially deep learning approaches which are data hungry. Therefore, to solve the problem, the process of gathering data and its formation could be equally as vital as the machine learning technique used. Herein, we are proposing a new data acquisition technique for gathering real image data set which could be used as an input for super resolution, noise cancellation and quality enhancement techniques. We use a beam-splitter to capture the same scene by a low resolution camera and a high resolution camera. Since we also release the raw images, this large-scale dataset could be used for other tasks such as ISP generation. Unlike current small-scale dataset used for these tasks, our proposed dataset includes 11,421 pairs of low-resolution high-resolution images of diverse scenes. To our knowledge this is the most complete dataset for super resolution, ISP and image quality enhancement. The benchmarking result shows how the new dataset can be successfully used to significantly improve the quality of real-world image super resolution.",0.3684210526],["key-policy RS-ABE scheme links each key to an access structure.","A proof of security for a key-policy RS-ABE scheme","summarize: A revocable-storage attribute-based encryption scheme is an encryption scheme which extends attribute-based encryption by intro- ducing user revocation. A key-policy RS-ABE scheme links each key to an access structure. We propose a new key-policy RS-ABE scheme whose security we prove in term of indistinguishability under a chosen-plaintext attack .",0.3636363636],["flowrate is determined directly using a gravimetric method. the flowrate is determined","Acoustic prediction of flowrate: varying liquid jet stream onto a free surface","summarize: Information on liquid jet stream flow is crucial in many real world applications. In a large number of cases, these flows fall directly onto free surfaces , creating a splash with accompanying splashing sounds. The sound produced is supplied by energy interactions between the liquid jet stream and the passive free surface. In this investigation, we collect the sound of a water jet of varying flowrate falling into a pool of water, and use this sound to predict the flowrate and flowrate trajectory involved. Two approaches are employed: one uses machine-learning models trained using audio features extracted from the collected sound to predict the flowrate . In contrast, the second method directly uses acoustic parameters related to the spectral energy of the liquid-liquid interaction to estimate the flowrate trajectory. The actual flowrate, however, is determined directly using a gravimetric method: tracking the change in mass of the pooling liquid over time. We show here that the two methods agree well with the actual flowrate and offer comparable performance in accurately predicting the flowrate trajectory, and accordingly offer insights for potential real-life applications using sound.",0.2857142857],["double samplers are samplers with additional combinatorial properties. they give","List Decoding with Double Samplers","summarize: We develop the notion of double samplers, first introduced by Dinur and Kaufman , which are samplers with additional combinatorial properties, and whose existence we prove using high dimensional expanders. We show how double samplers give a generic way of amplifying distance in a way that enables efficient list-decoding. There are many error correcting code constructions that achieve large distance by starting with a base code ",0.0833333333],["the optimal open-loop solution passes by the optimal steady-state for consecutive time instants","Stability and performance in transient average constrained economic MPC without terminal constraints","summarize: In this paper, we investigate system theoretic properties of transient average constrained economic model predictive control without terminal constraints. We show that the optimal open-loop solution passes by the optimal steady-state for consecutive time instants. Using this turnpike property and suitable controllability conditions, we provide closed-loop performance bounds. Furthermore, stability is proved by combining the rotated value function with an input-to-state Lyapunov function of an extended state related to the transient average constraints. The results are illustrated with a numerical example.",0.0588235294],["the stability of the DPG method gives a norm equivalence. this","A scalable preconditioner for a DPG method","summarize: We show how a scalable preconditioner for the primal discontinuous Petrov-Galerkin method can be developed using existing algebraic multigrid preconditioning techniques. The stability of the DPG method gives a norm equivalence which allows us to exploit existing AMG algorithms and software. We show how these algebraic preconditioners can be applied directly to a Schur complement system of interface unknowns arising from the DPG method. To the best of our knowledge, this is the first massively scalable algebraic preconditioner for DPG problems.",0.3333333333],["ASP is a well-established declarative paradigm. state-of-the-","Constraints, Lazy Constraints, or Propagators in ASP Solving: An Empirical Analysis","summarize: Answer Set Programming is a well-established declarative paradigm. One of the successes of ASP is the availability of efficient systems. State-of-the-art systems are based on the ground+solve approach. In some applications this approach is infeasible because the grounding of one or few constraints is expensive. In this paper, we systematically compare alternative strategies to avoid the instantiation of problematic constraints, that are based on custom extensions of the solver. Results on real and synthetic benchmarks highlight some strengths and weaknesses of the different strategies. ",0.3630330784],["proposed determinant approximation allows us to construct a voltage stability index.","Hierarchical and Distributed Monitoring of Voltage Stability in Distribution Networks","summarize: We consider the problem of quantifying and assessing the steady-state voltage stability in radial distribution networks. Our approach to the voltage stability problem is based on a local, approximate, and yet highly accurate characterization of the determinant of the Jacobian of the power flow equations parameterized according to the branch-flow model. The proposed determinant approximation allows us to construct a voltage stability index that can be computed in a fully distributed or in a hierarchical fashion, resulting in a scalable approach to the assessment of steady-state voltage stability. Finally, we provide upper bounds for the approximation error and we numerically validate the quality and the robustness of the proposed approximation with the IEEE 123-bus test feeder.",0.3529411765],["a new squeeze-excitation block is designed to improve the accuracy of a CNN-","Acoustic Scene Classification with Squeeze-Excitation Residual Networks","summarize: Acoustic scene classification is a problem related to the field of machine listening whose objective is to classify\/tag an audio clip in a predefined label describing a scene location . Many state-of-the-art solutions to ASC incorporate data augmentation techniques and model ensembles. However, considerable improvements can also be achieved only by modifying the architecture of convolutional neural networks . In this work we propose two novel squeeze-excitation blocks to improve the accuracy of a CNN-based ASC framework based on residual learning. The main idea of squeeze-excitation blocks is to learn spatial and channel-wise feature maps independently instead of jointly as standard CNNs do. This is usually achieved by some global grouping operators, linear operators and a final calibration between the input of the block and its obtained relationships. The behavior of the block that implements such operators and, therefore, the entire neural network, can be modified depending on the input to the block, the established residual configurations and the selected non-linear activations. The analysis has been carried out using the TAU Urban Acoustic Scenes 2019 dataset presented in the 2019 edition of the DCASE challenge. All configurations discussed in this document exceed the performance of the baseline proposed by the DCASE organization by 13\\% percentage points. In turn, the novel configurations proposed in this paper outperform the residual configurations proposed in previous works.",0.380952381],["model has a linearized rotational symmetry, which is broken by boundary conditions.","Dislocation lines in three-dimensional solids at low temperature","summarize: We propose a model for three-dimensional solids on a mesoscopic scale with a statistical mechanical description of dislocation lines in thermal equilibrium. The model has a linearized rotational symmetry, which is broken by boundary conditions. We show that this symmetry is spontaneously broken in the thermodynamic limit at small positive temperatures.",0.2941176471],["the method subdivides a large problem in two smaller ones. the solution's","Analysis of Divide & Conquer strategies for the 0-1 Minimization Knapsack Problem","summarize: We introduce and asses several Divide \\& Conquer heuristic strategies aimed to solve large instances of the 0-1 Minimization Knapsack Problem. The method subdivides a large problem in two smaller ones , to lower down the global computational complexity of the original problem, at the expense of a moderate loss of quality in the solution. Theoretical mathematical results are presented in order to guarantee an algorithmically successful application of the method and to suggest the potential strategies for its implementation. In contrast, due to the lack of theoretical results, the solution's quality deterioration is measured empirically by means of Monte Carlo simulations for several types and values of the chosen strategies. Finally, introducing parameters of efficiency we suggest the best strategies depending on the data input.",0.375],["aApplying the Born-Infeld Anti de Sitter charged black hole","Holographic entanglement entropy for small subregions and thermalization of Born-Infeld AdS black holes","summarize: AApplying the Born-Infeld Anti de Sitter charged black hole metric we calculate holographic entanglement entropy by regarding the proposal of Ryu and Takanayagi. To do so we assume that time dependence of the black hole mass and charge to be as step function. Our work is restricted to small subregions where a collapsing null shell dose not penetrate the black holes horizon. To calculate time dependent HEE we use perturbation method for small subregions where turning point is much smaller than local equilibrium point of black hole. We choose two shape functions for entangled regions on the boundary which are the strip and the ball regions. There is a saturation time at which the null shell grazes the turning point and the HEE reaches to its maximum value. In general, this work satisfies result of the works presented by Camelio et al and Zeng et al. We must point out that they used equal time two-point correlation functions and Wilson loops instead of the entanglement entropy as non-local observable to study this thermalization by applying the numerical method.",0.2849690615],["a paper by Grochenig retraces the steps of a","Almost Diagonalization of Pseudodifferential Operators","summarize: In this review we focus on the almost diagonalization of pseudodifferential operators and highlight the advantages that time-frequency techniques provide here. In particular, we retrace the steps of an insightful paper by Gr\\ochenig, who succeeded in characterizing a class of symbols previously investigated by Se\\ostrand by noticing that Gabor frames almost diagonalize the corresponding Weyl operators. This approach also allows to give new and more natural proofs of related results such as boundedness of operators or algebra and Wiener properties of the symbol class. Then, we discuss some recent developments on the theme, namely an extension of these results to a more general family of pseudodifferential operators and similar outcomes for a symbol class closely related to Sj\\ostrand's one.",0.6],["we consider the Caputo fractional derivative and say that a function is Caputo","Local density of Caputo-stationary functions in the space of smooth functions","summarize: We consider the Caputo fractional derivative and say that a function is Caputo-stationary if its Caputo derivative is zero. We then prove that any ",0.3913043478],["a correlation between the atmospheric perturbations at Paranal Observatory and the tid","Detection of a 14-days atmospheric perturbation peak at Paranal associated with lunar cycles","summarize: In this paper we investigate the correlation between the atmospheric perturbations at Paranal Observatory and the Chilean coast tides, which are mostly modulated by the 14-day syzygy solar-lunar tidal cycle. To this aim, we downloaded 15 years of cloud coverage data from the AQUA satellite, in a matrix that includes also Armazones, the site of the European Extremely Large Telescope. By applying the Fast Fourier Transform to these data we detected a periodicity peak of about 14 days. We studied the tide cycle at Chanaral De Las Animas, on the ocean coast, for the year 2017, and we correlated it with the atmospheric perturbations at Paranal and the lunar phases. We found a significant correlation between the phenomena of short duration and intensity and the tidal cycle at Chanaral. We then show that an atmospheric perturbation occurs at Paranal in concomitance with the low tide, which anticipates the full moon by 3-4 days. This result allows to improve current weather forecasting models for astronomical observatories by introducing a lunar variable.",0.545211505],["quantum parametric oscillator is a system which has been extensively used to model quantum heat","Minimum-Time Transitions between Thermal and Fixed Average Energy States of the Quantum Parametric Oscillator","summarize: In this article we use geometric optimal control to completely solve the problem of minimum-time transitions between thermal equilibrium and fixed average energy states of the quantum parametric oscillator, a system which has been extensively used to model quantum heat engines and refrigerators. We subsequently use the obtained results to find the minimum driving time for a quantum refrigerator and the quantum finite-time availability of the parametric oscillator, i.e. the potential work which can be extracted from this system by a very short finite-time process.",0.36],["phase-field methods have long been used to model the flow of immiscible fluid","Phase-field simulation of core-annular pipe flow","summarize: Phase-field methods have long been used to model the flow of immiscible fluids. Their ability to naturally capture interface topological changes is widely recognized, but their accuracy in simulating flows of real fluids in practical geometries is not established. We here quantitatively investigate the convergence of the phase-field method to the sharp-interface limit with simulations of two-phase pipe flow. We focus on core-annular flows, in which a highly viscous fluid is lubricated by a less viscous fluid, and validate our simulations with an analytic laminar solution, a formal linear stability analysis and also in the fully nonlinear regime. We demonstrate the ability of the phase-field method to accurately deal with non-rectangular geometry, strong advection, unsteady fluctuations and large viscosity contrast. We argue that phase-field methods are very promising for quantitatively studying moderately turbulent flows, especially at high concentrations of the disperse phase.",0.1538461538],["we find a sharp, non-linear transformation between segregated and mixed-we","Wealth and Identity: The dynamics of dual segregation","summarize: We extend our model of wealth segregation to incorporate migration and study the tendencies towards dual segregation - segregation due to identity and segregation due to wealth. We find a sharp, non-linear transformation between segregated and mixed-wealth states as neighborhood wealth thresholds become less stringent, allowing agents to move into neighborhoods they cannot afford. The number of such moves required for the onset of this transformation varies inversely with the likelihood of agents willing to move into less wealthy neighborhoods. We also find that this sharp transformation from segregated to mixed wealth states is simultaneously accompanied by a corresponding non-linear transformation from a less identity-segregated to a highly identity-segregated state. We argue that the decrease in wealth segregation does not merely accompany, but in fact drives the increase in identity-based segregation. This implies that lowering wealth segregation necessarily exacerbates identity segregation and that therefore, this trade-off could to be an important consideration in designing urban policy. Additionally, we find that the time taken for the tolerance levels of residents to be breached is significantly lesser under rapid migration. This rapidity of breach of tolerance could potentially underpin the intensity of resident responses to sharp bursts of migration.",0.3333333333],["graphs are constructed row by row from either a square grid or a hexagonal la","Limit shape and height fluctuations of random perfect matchings on square-hexagon lattices","summarize: We study asymptotics of perfect matchings on a large class of graphs called the contracting square-hexagon lattice, which is constructed row by row from either a row of a square grid or a row of a hexagonal lattice. We assign the graph periodic edge weights with period ",0.59375],["the user history is often modeled by various RNN structures. the user history is often","Hierarchical Context enabled Recurrent Neural Network for Recommendation","summarize: A long user history inevitably reflects the transitions of personal interests over time. The analyses on the user history require the robust sequential model to anticipate the transitions and the decays of user interests. The user history is often modeled by various RNN structures, but the RNN structures in the recommendation system still suffer from the long-term dependency and the interest drifts. To resolve these challenges, we suggest HCRNN with three hierarchical contexts of the global, the local, and the temporary interests. This structure is designed to withhold the global long-term interest of users, to reflect the local sub-sequence interests, and to attend the temporary interests of each transition. Besides, we propose a hierarchical context-based gate structure to incorporate our \\textit. As we suggest a new RNN structure, we support HCRNN with a complementary \\textit structure to utilize hierarchical context. We experimented the suggested structure on the sequential recommendation tasks with CiteULike, MovieLens, and LastFM, and our model showed the best performances in the sequential recommendations.",0.0],["metamaterials can manipulate wavefront of an electromagnetic wave through just the subwavelength propagation","Transformation optics based on metasurfaces","summarize: Recently, new artificial material has been proposed to control an electromagnetic wave-metasurface, a two-dimensional metamaterial. Compared with a three-dimensional bulky metamaterial, this artificial plane material with sub-wavelength thickness greatly reduces fabrication time and mitigates fabrication complexity. Additionally, traditional metamaterials usually control the wavefront of an electromagnetic wave by accumulating the phase through propagating at a distance far larger than the wavelength. However, a metasurface can efficiently manipulate the wavefront of an incident electromagnetic wave through just the subwavelength propagation distance. Therefore, this can largely alleviate the propagation loss. Given the fact that a metasurface has high manipulation efficiency for an electromagnetic wave in the near filed regime, our group investigated experimental work on the analogy of gravity using a metasurface.",0.0588235294],["we present in detail Thomas Royen's proof of the correlation inequality which states that that the","Royen's proof of the Gaussian correlation inequality","summarize: We present in detail Thomas Royen's proof of the Gaussian correlation inequality which states that ",0.4090909091],["the programmatic content of the Knowledge of nature area in the curriculum currently in force in Uruguay is","Un enfoque de ense\\~nanza de la Astronom\\'ia: Algunas consideraciones epistemol\\'ogicas y did\\'acticas","summarize: In order to guide teaching practices in Initial and Primary Education level, it seems fundamental to start with an epistemological and didactic analysis of the programmatic content of the Knowledge of Nature area in the curriculum currently in force in Uruguay. In this work we briefly discuss the astronomical contents prescribed in the Program of IPE, year 2008, in order to provide teachers with some reading keys that might translate into possible guidelines for the design, implementation and evaluation of teaching sequences that address these contents.",0.03125],["ultrathin insulating oxide films have been studied extensively for decades. atomic-","Electronic Reconstruction Enhanced Tunneling Conductance at Terrace Edges of Ultrathin Oxide Films","summarize: Quantum mechanical tunneling of electrons across ultrathin insulating oxide barriers has been studied extensively for decades due to its great potential in electronic device applications. In the few-nanometer-thick epitaxial oxide films, atomic-scale structural imperfections, such as the ubiquitously existed one-unit-cell-high terrace edges, can dramatically affect the tunneling probability and device performance. However, the underlying physics has not been investigated adequately. Here, taking ultrathin BaTiO3 films as a model system, we report an intrinsic tunneling conductance enhancement near the terrace edges. Scanning probe microscopy results demonstrate the existence of highly-conductive regions near the terrace edges. First-principles calculations suggest that the terrace edge geometry can trigger an electronic reconstruction, which reduces the effective tunneling barrier width locally. Furthermore, such tunneling conductance enhancement can be discovered in other transition-metal-oxides and controlled by surface termination engineering. The controllable electronic reconstruction could facilitate the implementation of oxide electronic devices and discovery of exotic low-dimensional quantum phases.",0.0],["formulas express exponential generating functions counting permutations by peak number, valley number, double","Hopping from Chebyshev polynomials to permutation statistics","summarize: We prove various formulas which express exponential generating functions counting permutations by the peak number, valley number, double ascent number, and double descent number statistics in terms of the exponential generating function for Chebyshev polynomials, as well as cyclic analogues of these formulas for derangements. We give several applications of these results, including formulas for the ",0.0666666667],["formulas express exponential generating functions counting permutations by peak number, valley number, double","Hopping from Chebyshev polynomials to permutation statistics","summarize: We prove various formulas which express exponential generating functions counting permutations by the peak number, valley number, double ascent number, and double descent number statistics in terms of the exponential generating function for Chebyshev polynomials, as well as cyclic analogues of these formulas for derangements. We give several applications of these results, including formulas for the ",0.0666666667],["proposed plans can reflect an incident plane wave into a prescribed angle. these devices can be","Analytical Design of Printed-Circuit-Board Metagratings for Perfect Anomalous Reflection","summarize: We present an analytical scheme for the design of realistic metagratings for wide-angle engineered reflection. These recently proposed planar structures can reflect an incident plane wave into a prescribed angle with very high efficiencies, using only a single meta-atom per period. Such devices offer a means to overcome the implementation difficulties associated with standard metasurfaces and the relatively low efficiencies of gradient metasurfaces. In contrast to previous work, in which accurate systematic design was limited to metagratings unrealistically suspended in free space, we derive herein a closed-form formalism allowing realization of printed-circuit-board metagrating perfect reflectors, comprised of loaded conducting strips defined on standard metal-backed dielectric substrate. The derivation yields a detailed procedure for the determination of the substrate thickness and conductor geometry required to achieve unitary coupling efficiencies, without requiring even a single full-wave simulation. Our methodology, verified via commercial solvers, ultimately allows one to proceed from a theoretical design to synthesis of a full physical structure, avoiding the time-consuming numerical optimizations typically involved in standard metasurface design.",0.1935483871],["NP-completed.","Turing Kernelization for Finding Long Paths and Cycles in Restricted Graph Classes","summarize: The NP-complete ",0.0],["theorem is a new type of the darbo fixed point theore","A New Type of Darbo's Fixed Point Theorem Defined by The Sequences of Functions","summarize: In this paper, we introduce a new type of Darbo's fixed point theorem by using concept of function sequences with shifting distance property. Afterward, we investigate existence of fixed point under this the theorem. Also we are going to give interesting example held the conditions of sequences of functions",0.1015067182],["we calculate the shape and velocity of a bubble rising in an infinitely large and closed He","Theoretical analysis for flattening of a rising bubble in a Hele-Shaw cell","summarize: We calculate the shape and the velocity of a bubble rising in an infinitely large and closed Hele-Shaw cell using Park and Homsy's boundary condition which accounts for the change of the three dimensional structure in the perimeter zone. We first formulate the problem in the form of a variational problem, and discuss the shape change assuming that the bubble takes elliptic shape. We calculate the shape and the velocity of the bubble as a function of the bubble size, gap distance and the inclination angle of the cell. We show that the bubble is flattened as it rises. This result is in agreement with experiments for large Hele-Shaw cells.",0.4838709677],["textbf combines cost variances in different views with small extra memory consumption","Pyramid Multi-view Stereo Net with Self-adaptive View Aggregation","summarize: n this paper, we propose an effective and efficient pyramid multi-view stereo net with self-adaptive view aggregation for accurate and complete dense point cloud reconstruction. Different from using mean square variance to generate cost volume in previous deep-learning based MVS methods, our \\textbf incorporates the cost variances in different views with small extra memory consumption by introducing two novel self-adaptive view aggregations: pixel-wise view aggregation and voxel-wise view aggregation. To further boost the robustness and completeness of 3D point cloud reconstruction, we extend VA-MVSNet with pyramid multi-scale images input as \\textbf, where multi-metric constraints are leveraged to aggregate the reliable depth estimation at the coarser scale to fill in the mismatched regions at the finer scale. Experimental results show that our approach establishes a new state-of-the-art on the \\textsl} dataset with significant improvements in the completeness and overall quality, and has strong generalization by achieving a comparable performance as the state-of-the-art methods on the \\textsl} benchmark. Our codebase is at \\hyperlink",0.0769230769],["citizens and emergency managers need to distinguish ''fake'' news posts from real","Real or Fake? User Behavior and Attitudes Related to Determining the Veracity of Social Media Posts","summarize: Citizens and Emergency Managers need to be able to distinguish ''fake'' news posts from real news posts on social media during disasters. This paper is based on an online survey conducted in 2018 that produced 341 responses from invitations distributed via email and through Facebook. It explores to what extent and how citizens generally assess whether postings are ''true'' or ''fake,'' and describes indicators of the trustworthiness of content that users would like. The mean response on a semantic differential scale measuring how frequently users attempt to verify the news trustworthiness was 3.37. The most frequent message characteristics citizens' use are grammar and the trustworthiness of the sender. Most respondents would find an indicator of trustworthiness helpful, with the most popular choice being a colored graphic. Limitations and implications for assessments of trustworthiness during disasters are discussed.",0.1194218851],["random dot product graph is an independent-edge random graph that is analytically tractable","Statistical inference on random dot product graphs: a survey","summarize: The random dot product graph is an independent-edge random graph that is analytically tractable and, simultaneously, either encompasses or can successfully approximate a wide range of random graphs, from relatively simple stochastic block models to complex latent position graphs. In this survey paper, we describe a comprehensive paradigm for statistical inference on random dot product graphs, a paradigm centered on spectral embeddings of adjacency and Laplacian matrices. We examine the analogues, in graph inference, of several canonical tenets of classical Euclidean inference: in particular, we summarize a body of existing results on the consistency and asymptotic normality of the adjacency and Laplacian spectral embeddings, and the role these spectral embeddings can play in the construction of single- and multi-sample hypothesis tests for graph data. We investigate several real-world applications, including community detection and classification in large social networks and the determination of functional and biologically relevant network properties from an exploratory data analysis of the Drosophila connectome. We outline requisite background and current open problems in spectral graph inference.",0.4090909091],["parallel ac-dc reconfigurable link technology can find interesting applications in medium voltage","Boundaries of Operation for Refurbished Parallel AC-DC Reconfigurable Links in Distribution Grids","summarize: Parallel ac-dc reconfigurable link technology can find interesting applications in medium voltage power distribution. A given system can operate in different configurations while maintaining equivalent capacity during contingencies. It is proved that within the defined operating boundaries, a parallel ac-dc configuration has higher efficiency as compared to pure ac or pure dc power delivery. Using sensitivity analysis, the variations in these efficiency boundaries with power demand, power factor, grid voltages, link lengths, conductor areas and converter efficiency is described. It is shown that parallel ac-dc system can have smaller payback time as compared to a purely dc power transmission for the same capacity due to lower investment cost in converter station and superior efficiency. As compared to a purely ac system, the payback of a refurbished parallel acdc configuration can be less than 5 years for a 10 km, 10 kV distribution link within the specified assumptions and operating conditions.",0.125],["the Large Magellanic Cloud has been spotted by the large magellanic cloud.","X-Ray Luminosity and Size Relationship of Supernova Remnants in the LMC","summarize: The Large Magellanic Cloud has ",0.1428571429],["traditional human computer interaction based systems ignore bulk of information communicated through those affective states","A novel database of Children's Spontaneous Facial Expressions ","summarize: Computing environment is moving towards human-centered designs instead of computer centered designs and human's tend to communicate wealth of information through affective states or expressions. Traditional Human Computer Interaction based systems ignores bulk of information communicated through those affective states and just caters for user's intentional input. Generally, for evaluating and benchmarking different facial expression analysis algorithms, standardized databases are needed to enable a meaningful comparison. In the absence of comparative tests on such standardized databases it is difficult to find relative strengths and weaknesses of different facial expression recognition algorithms. In this article we present a novel video database for Children's Spontaneous facial Expressions . Proposed video database contains six basic spontaneous facial expressions shown by 12 ethnically diverse children between the ages of 6 and 12 years with mean age of 7.3 years. To the best of our knowledge, this database is first of its kind as it records and shows spontaneous facial expressions of children. Previously there were few database of children expressions and all of them show posed or exaggerated expressions which are different from spontaneous or natural expressions. Thus, this database will be a milestone for human behavior researchers. This database will be a excellent resource for vision community for benchmarking and comparing results. In this article, we have also proposed framework for automatic expression recognition based on convolutional neural network architecture with transfer learning approach. Proposed architecture achieved average classification accuracy of 75% on our proposed database i.e. LIRIS-CSE.",0.0666666667],["Viscous hydrodynamics gives a satisfactory description of the transverse momentum spectra","Hydrodynamic models of particle production","summarize: Viscous hydrodynamics gives a satisfactory description of the transverse momentum spectra, of the elliptic and triangular flow, and of the femtoscopic correlations for particles produced in relativistic heavy-ion collisions. On general grounds, a similar collective behavior has been predicted for proton-lead collisions at the LHC. We present results of the hydrodynamics calculation of the elliptic and triangular flow in p-Pb. We discuss the mass dependence of flow coefficients and of the average transverse momentum for identified particles.",0.1875],["Lie algebras were introduced by Kaplan as a class of real Lie algebras","On complex H-type Lie algebras","summarize: H-type Lie algebras were introduced by Kaplan as a class of real Lie algebras generalizing the familiar Heisenberg Lie algebra ",0.2413793103],["a treatment for a complicated disease may be helpful for some but not all patients.","Individual Treatment Effect Prediction for ALS Patients","summarize: A treatment for a complicated disease may be helpful for some but not all patients, which makes predicting the treatment effect for new patients important yet challenging. Here we develop a method for predicting the treatment effect based on patient char- acteristics and use it for predicting the effect of the only drug approved for treating Amyotrophic Lateral Sclerosis . Our proposed method of model-based ran- dom forests detects similarities in the treatment effect among patients and on this basis computes personalised models for new patients. The entire procedure focuses on a base model, which usually contains the treatment indicator as a single covariate and takes the survival time or a health or treatment success measurement as primary outcome. This base model is used both to grow the model-based trees within the forest, in which the patient characteristics that interact with the treatment are split variables, and to com- pute the personalised models, in which the similarity measurements enter as weights. We applied the personalised models using data from several clinical trials for ALS from the PRO-ACT database. Our results indicate that some ALS patients benefit more from the drug Riluzole than others. Our method allows shifting from stratified medicine to person- alised medicine and can also be used in assessing the treatment effect for other diseases studied in a clinical trial.",0.2580645161],["this paper aims to provide a thorough study on the effectiveness of the transformation-based ensemble","Where Does the Robustness Come from? A Study of the Transformation-based Ensemble Defence","summarize: This paper aims to provide a thorough study on the effectiveness of the transformation-based ensemble defence for image classification and its reasons. It has been empirically shown that they can enhance the robustness against evasion attacks, while there is little analysis on the reasons. In particular, it is not clear whether the robustness improvement is a result of transformation or ensemble. In this paper, we design two adaptive attacks to better evaluate the transformation-based ensemble defence. We conduct experiments to show that 1) the transferability of adversarial examples exists among the models trained on data records after different reversible transformations; 2) the robustness gained through transformation-based ensemble is limited; 3) this limited robustness is mainly from the irreversible transformations rather than the ensemble of a number of models; and 4) blindly increasing the number of sub-models in a transformation-based ensemble does not bring extra robustness gain.",0.3913043478],["enabling CMOS technology scaling has faced significant challenges of device uncertainties. over-pe","Cross-Layer Optimization for Power-Efficient and Robust Digital Circuits and Systems","summarize: With the increasing digital services demand, performance and power-efficiency become vital requirements for digital circuits and systems. However, the enabling CMOS technology scaling has been facing significant challenges of device uncertainties, such as process, voltage, and temperature variations. To ensure system reliability, worst-case corner assumptions are usually made in each design level. However, the over-pessimistic worst-case margin leads to unnecessary power waste and performance loss as high as 2.2x. Since optimizations are traditionally confined to each specific level, those safe margins can hardly be properly exploited. To tackle the challenge, it is therefore advised in this Ph.D. thesis to perform a cross-layer optimization for digital signal processing circuits and systems, to achieve a global balance of power consumption and output quality. To conclude, the traditional over-pessimistic worst-case approach leads to huge power waste. In contrast, the adaptive voltage scaling approach saves power by providing a just-needed supply voltage. The power saving is maximized when a more aggressive voltage over-scaling scheme is applied. These sparsely occurred circuit errors produced by aggressive voltage over-scaling are mitigated by higher level error resilient designs. For functions like FFT and CORDIC, smart error mitigation schemes were proposed to enhance reliability . Applications like Massive MIMO systems are robust against lower level errors, thanks to the intrinsically redundant antennas. This property makes it applicable to embrace digital hardware that trades quality for power savings.",0.0],["matrices represent genuine homographies associated with multiple planes. we also show","Full explicit consistency constraints in uncalibrated multiple homography estimation","summarize: We reveal a complete set of constraints that need to be imposed on a set of 3-by-3 matrices to ensure that the matrices represent genuine homographies associated with multiple planes between two views. We also show how to exploit the constraints to obtain more accurate estimates of homography matrices between two views. Our study resolves a long-standing research question and provides a fresh perspective and a more in-depth understanding of the multiple homography estimation task.",0.0909090909],["in this paper, we give a criterion on instability of an equilibrium of non","An instability theorem for nonlinear fractional differential systems","summarize: In this paper, we give a criterion on instability of an equilibrium of nonlinear Caputo fractional differential systems. More precisely, we prove that if the spectrum of the linearization has at least one eigenvalue in the sector ",0.4545454545],["global spherical and planar mapping techniques simplify complex geometry. but these techniques","LMap: Shape-Preserving Local Mappings for Biomedical Visualization","summarize: Visualization of medical organs and biological structures is a challenging task because of their complex geometry and the resultant occlusions. Global spherical and planar mapping techniques simplify the complex geometry and resolve the occlusions to aid in visualization. However, while resolving the occlusions these techniques do not preserve the geometric context, making them less suitable for mission-critical biomedical visualization tasks. In this paper, we present a shape-preserving local mapping technique for resolving occlusions locally while preserving the overall geometric context. More specifically, we present a novel visualization algorithm, LMap, for conformally parameterizing and deforming a selected local region-of-interest on an arbitrary surface. The resultant shape-preserving local mappings help to visualize complex surfaces while preserving the overall geometric context. The algorithm is based on the robust and efficient extrinsic Ricci flow technique, and uses the dynamic Ricci flow algorithm to guarantee the existence of a local map for a selected ROI on an arbitrary surface. We show the effectiveness and efficacy of our method in three challenging use cases: multimodal brain visualization, optimal coverage of virtual colonoscopy centerline flythrough, and molecular surface visualization.",0.0],["v=2\/3 edge mode is based on a carefully designed double-quantum","Synthesizing a Fractional v=2\/3 State from Particle and Hole States","summarize: Topological edge-reconstruction occurs in hole-conjugate states of the fractional quantum Hall effect. The frequently studied polarized state of filling factor v=2\/3 was originally proposed to harbor two counter-propagating edge modes: a downstream v=1 and an upstream v=1\/3. However, charge equilibration between these two modes always led to an observed downstream v=2\/3 charge mode accompanied by an upstream neutral mode . Here, we present a new approach to synthetize the v=2\/3 edge mode from its basic counter-propagating charged constituents, allowing a controlled equilibration between the two counter-propagating charge modes. This novel platform is based on a carefully designed double-quantum-well, which hosts two populated electronic sub-bands , with corresponding filling factors, vl & vu. By separating the 2D plane to two gated intersecting halves, each with different fillings, counter-propagating chiral modes can be formed along the intersection line. Equilibration between these modes can be controlled with the top gates' voltage and the magnetic field. Our measurements of the two-terminal conductance G2T and the presence of a neutral mode allowed following the transition from the non-equilibrated charged modes, manifested by G2T=e2\/h, to the fully equilibrated modes, with a downstream charge mode with G2T=e2\/h accompanied by an upstream neutral mode.",0.4615384615],["theorem describes the convergence of an infinite array of variants of SGD.","SGD: General Analysis and Improved Rates","summarize: We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form mini-batches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.",0.1],["the Allen-Cahn equation inherits the energy decreasing property of the full order model","Energy stable model order reduction for the Allen-Cahn equation","summarize: The Allen-Cahn equation is a gradient system, where the free-energy functional decreases monotonically in time. We develop an energy stable reduced order model for a gradient system, which inherits the energy decreasing property of the full order model . For the space discretization we apply a discontinuous Galerkin method and for time discretization the energy stable average vector field method. We construct ROMs with proper orthogonal decomposition -greedy adaptive sampling of the snapshots in time and evaluating the nonlinear function with greedy discrete empirical interpolation method . The computational efficiency and accuracy of the reduced solutions are demonstrated numerically for the parametrized Allen-Cahn equation with Neumann and periodic boundary conditions.",0.3684210526],["a simple theory for the green photoluminescence of ZnO quantum dots allows us","Quantum-Size Effects in the Visible Photoluminescence of Colloidal ZnO Quantum Dots: A Theoretical Analysis","summarize: In this work we develop a simple theory for the green photoluminescence of ZnO quantum dots that allows us to understand and rationalize several experimental findings on fundamental grounds. We calculate the spectrum of light emitted in the radiative recombination of a conduction band electron with a deeply trapped hole and find that the experimental behavior of this emission band with particle size can be understood in terms of quantum size effects of the electronic states and their overlap with the deep hole.We focus the comparison of our results on detailed experiments performed for colloidal ZnO nanoparticles in ethanol and find that the experimental evolution of the luminescent signal with particle sizeat room temperature can be better reproduced by assuming the deep hole to be localized at the surface of the nanoparticles. However, the experimental behavior of the intensity and decay time of the signal with temperature can be rationalized in terms of holes predominantly trapped near the center of the nanoparticles at low temperatures being transferred to surface defects at room temperature. Furthermore, the calculated values of the radiative lifetimes are comparable to the experimental values of the decay time of the visible emission signal.We also study the visible emission band as a function of the number of electrons in the conduction band of the nanoparticle, finding a pronounced dependence of the radiative lifetime but a weak dependence of energetic position of the maximum intensity.",0.3472354045],["electrochemical properties of vapor-phase grown MoS_ have been investigated. electrochemical","Dielectric Properties and Ion Transport in Layered MoS_ Grown by Vapor-Phase Sulfurization","summarize: Electronic and dielectric properties of vapor-phase grown MoS_ have been investigated in metal\/MoS_\/silicon capacitor structures by capacitance-voltage and conductancevoltage techniques. Analytical methods confirm the MoS_ layered structure, the presence of interfacial silicon oxide and the composition of the films. Electrical characteristics in combination with theoretical considerations quantify the concentration of electron states at the interface between Si and a 2.5 - 3 nm thick silicon oxide interlayer between Si and MoS_. Measurements under electric field stress indicate the existence of mobile ions in MoS_ that interact with interface states. Based on time-offlight secondary ion mass spectrometry, we propose OH^ ions as probable candidates responsible for the observations. The dielectric constant of the vapor-phase grown MoS_ extracted from CV measurements at 100 KHz is in the range of 2.6 to 2.9. The present study advances the understanding of defects and interface states in MoS_. It also indicates opportunities for ion-based plasticity in 2D material devices for neuromorphic computing applications.",0.0682275628],["recent work on fairness in machine learning has primarily emphasized how to define, quantify,","On Consequentialism and Fairness","summarize: Recent work on fairness in machine learning has primarily emphasized how to define, quantify, and encourage fair outcomes. Less attention has been paid, however, to the ethical foundations which underlie such efforts. Among the ethical perspectives that should be taken into consideration is consequentialism, the position that, roughly speaking, outcomes are all that matter. Although consequentialism is not free from difficulties, and although it does not necessarily provide a tractable way of choosing actions , it nevertheless provides a powerful foundation from which to critique the existing literature on machine learning fairness. Moreover, it brings to the fore some of the tradeoffs involved, including the problem of who counts, the pros and cons of using a policy, and the relative value of the distant future. In this paper we provide a consequentialist critique of common definitions of fairness within machine learning, as well as a machine learning perspective on consequentialism. We conclude with a broader discussion of the issues of learning and randomization, which have important implications for the ethics of automated decision making systems.",0.0555555556],["cones are certain generalizations of hyperbolic spaces. cones are certain general","The area minimizing problem in conformal cones, II","summarize: In this paper we continue to study the connection among the area minimizing problem, certain area functional and the Dirichlet problem of minimal surface equations in a class of conformal cones with a similar motivation from \\cite. These cones are certain generalizations of hyperbolic spaces. We describe the structure of area minimizing ",0.2222222222],["we prove the existence of a Galois closure for towers of torsors under","Nori fundamental gerbe of essentially finite covers and Galois closure of towers of torsors","summarize: We prove the existence of a Galois closure for towers of torsors under finite group schemes over a proper, geometrically connected and geometrically reduced algebraic stack ",0.5991512862],["the workshop was held in Eindhoven, The Netherlands, on July 17th, 2019","Proceedings Tenth International Workshop on Graph Computation Models","summarize: This volume contains the post-proceedings of the Tenth International Workshop on Graph Computation Models . The workshop was held in Eindhoven, The Netherlands, on July 17th, 2019, as part of STAF 2019 . Graphs are common mathematical structures that are visual and intuitive. They constitute a natural and seamless way for system modelling in science, engineering and beyond, including computer science, biology, business process modelling, etc. Graph computation models constitute a class of very high-level models where graphs are first-class citizens. The aim of the International GCM Workshop series is to bring together researchers interested in all aspects of computation models based on graphs and graph transformation. It promotes the cross-fertilizing exchange of ideas and experiences among senior and young researchers from the different communities interested in the foundations, applications, and implementations of graph computation models and related areas. These post-proceedings contain four selected papers from GCM2019 proceedings and an invited presentation that gives an account of the very successful panel discussion dedicated to the Analysis of Graph Transformation Systems, which took place during the workshop and was animated by Reiko Heckel, Leen Lambers and Maryam Ghaffari Saadat. All submissions were subject to careful refereeing. The topics of accepted papers include theoretical aspects of graph transformation and parsing techniques as well as an application to model-driven engineering.",0.1428571429],["the center for Planetary Science conducted 200 observations in the radio spectrum. the results of this","Hydrogen Line Observations of Cometary Spectra at 1420 Mhz","summarize: In 2016, the Center for Planetary Science proposed a hypothesis arguing a comet and\/or its hydrogen cloud were a strong candidate for the source of the Wow! Signal. From 27 November 2016 to 24 February 2017, the Center for Planetary Science conducted 200 observations in the radio spectrum to validate the hypothesis. The investigation discovered that comet 266\/P Christensen emitted a radio signal at 1420.25 MHz. The results of this investigation, therefore, conclude that cometary spectra are detectable at 1420 MHz and, more importantly, that the 1977 Wow! Signal was a natural phenomenon from a Solar System body.",0.0909090909],["the Los Humeros Volcanic Complex is a large silicic caldera complex","The structural architecture of the Los Humeros volcanic complex and geothermal field","summarize: The Los Humeros Volcanic Complex is a large silicic caldera complex in the Trans-Mexican Volcanic Belt , hosting a geothermal field currently in exploitation by the Comision Federal de Electricidad of Mexico, with an installed capacity of ca. 95 MW of electric power. Understanding the structural architecture of LHVC is important to get insights into the interplay between the volcano-tectonic setting and the characteristics of the geothermal resources in the area. The analysis of volcanotectonic interplay in LHVC benefits from the availability of subsurface data obtained during the exploration of the geothermal reservoir that allows the achievement of a 3D structural view of the volcano system. The LHVC thus represents an important natural laboratory for the development of general models of volcano-tectonic interaction in calderas.",0.5478604298],["the widespread use of wireless technologies leads to an ever-increasing number of users","Results and Tools for Evaluating the Effectiveness of Focusing Systems to Improve Accessibility in Wireless Networks","summarize: The widespread use of wireless technologies leads to an ever-increasing number of users and permanently functioning devices. However, the growth of the number of wireless users in a limited space and a limited frequency range leads to an increase in their mutual influence, which ultimately affects the throughput of wireless channels and even the performance of the system as a whole. The article presents the statistics and tendencies of the distribution of wireless networks of the IEEE 802.11 standard systems, as well as analyzes the main problems that arise during the expansion of their use. Substantiation and choice of ways to overcome these difficulties largely depends on the objective control of radiation parameters of access points and subscriber funds in a particular environment. The review of the state control facilities provided by the developers of the equipment is presented, and author's variants of experimental measuring complexes are offered, allowing to control signal and information parameters of Wi-Fi systems. The experimental results obtained with the use of the indicated means, obtained using the accelerating metal-plate lens as an additional autonomous element for focusing the field, including for MIMO systems, the effect of the accelerating metal-plate lens on the spatial distribution of the field, on the spectral structure of the signal are presented. In addition, polarization effects were investigated. Possible ways to further increase the availability, integrity of information and energy efficiency of wireless access systems are discussed. The authors propose simpler and less costly options for increasing the direction of radiation on the basis of an accelerating metal-plate lens, experimentally tested, as well as the use of zone zoning on the path of the computer.",0.2481008306],["the construction was generalized by Matsui et al. to a larger family","Interlacing Ehrhart Polynomials of Reflexive Polytopes","summarize: It was observed by Bump et al. that Ehrhart polynomials in a special family exhibit properties similar to the Riemann function. The construction was generalized by Matsui et al. to a larger family of reflexive polytopes coming from graphs. We prove several conjectures confirming when such polynomials have zeros on a certain line in the complex plane. Our main new method is to prove a stronger property called interlacing.",0.2631578947],["the faa is considering remote ID systems for unmanned aerial vehicles. the systems","Experiments with a LoRaWAN-Based Remote ID System for Locating Unmanned Aerial Vehicles ","summarize: Federal Aviation Administration of the United States is considering Remote ID systems for unmanned aerial vehicles . These systems act as license plates used on automobiles, but they transmit information using radio waves. To be useful, the transmissions in such systems need to reach long distances to minimize the number of ground stations to capture these transmissions. LoRaWAN is designed as a cheap long-range technology to be used for long-range communication for the Internet of Things. Several manufacturers make LoRaWAN modules, which are readily available on the market and are, therefore, ideal for the UAVs Remote IDs at a low cost. In this paper, we present our experiences in using LoRaWAN technology as a communication technology. Our experiments to identify and locate the UAV systems uncovered several issues of using LoRaWAN in such systems that are documented in this paper. Using several ground stations, we can determine the location of a UAV equipped with a LoRaWAN module that transmits the UAV Remote ID. Hence, it can help identify UAVs that unintentionally, or intentionally, fly into restricted zones.",0.1176470588],["algorithm is based on the ODE model of distribution voltage. it is based on","Synthesis of Spatial Charging\/Discharging Patterns of In-Vehicle Batteries for Provision of Ancillary Service and Mitigation of Voltage Impact","summarize: We develop an algorithm for synthesizing a spatial pattern of charging\/discharging operations of in-vehicle batteries for provision of Ancillary Service in power distribution grids. The algorithm is based on the ODE model of distribution voltage that has been recently introduced. In this paper, firstly, we derive analytical solutions of the ODE model for a single straight-line feeder through a partial linearization, thereby providing a physical insight to the impact of spatial EV charging\/discharging to the distribution voltage. Second, based on the analytical solutions, we propose an algorithm for determining the values of charging\/discharging power by in-vehicle batteries in the single feeder grid, so that the power demanded as AS is provided by EVs, and the deviation of distribution voltage from a nominal value is reduced in the grid. Effectiveness of the algorithm is established with numerical simulations on the single feeder grid and on a realistic feeder grid with multiple bifurcations.",0.3907681924],["Graphene is a two-dimensional layer of carbon atoms arranged in","Tunable nonlinear and active THz devices based on hybrid graphene metasurfaces","summarize: Graphene is a two-dimensional layer of carbon atoms arranged in a honeycomb lattice, whose outstanding properties makes it an excellent material for future electronic and photonic terahertz devices. In this work, we design hybrid graphene metasurfaces by using a monolayer graphene placed over a metallic grating, operating in the THz frequency range. Perfect absorption can be achieved at the resonance, where the electric field is greatly enhanced due to the coupling between the graphene and the grating plasmonic responses. The enhancement of the electric field along the graphene monolayer, as well as the large nonlinear conductivity of graphene, can dramatically boost the nonlinear response of the proposed THz device. In addition, the presented enhanced nonlinear effects can be significantly tuned by varying the doping level of graphene. The proposed structure can be used in the design of THz-frequency generators and all-optical processors.",0.4790315743],["affine inequalities serve as cuts to get rid of points outside of the feasible","Least Square Estimation-Based SDP Cuts for SOCP Relaxation of AC OPF","summarize: This paper presents a method that generates affine inequalities to strengthen the second-order conic programming relaxation of an alternating current optimal power flow problem. The affine inequalities serve as cuts to get rid of points outside of the feasible region of AC OPF with semi-definite programming relaxation. Hence, the affine inequalities are names as SDP cuts. While AC OPF with SDP relaxation has a high computational complexity, AC OPF with SOCP has a much lower computational complexity. Recent research has found that the feasible region of SDP relaxation is contained inside the feasible region of the SOCP relaxation. Therefore, the integration of SDP cuts into SOCP relaxation provides better scalability compared to the SDP relaxation and a tighter gap compared to the SOCP relaxation. The SDP cuts are generated by solving least square estimation problems at cycle basis and further exploring the geometric characteristic of LSE. General feasibility cuts generating method is also employed for analysis. We found that the SDP cuts generated by LSE method are indeed feasibility cuts. The SDP cuts effectively reduce the search space. Case studies of systems with several buses to hundreds of buses have demonstrated the method is very effective in reducing the gaps.",0.2352941176],["memristive Murali-Lakshmanan-Chua circuit is","Discontinuity Induced Hopf and Neimark-Sacker Bifurcations in a Memristive Murali-Lakshmanan-Chua Circuit","summarize: We report using Clarke's concept of generalised differential and a modification of Floquet theory to non-smooth oscillations, the occurrence of discontinuity induced Hopf bifurcations and Neimark-Sacker bifurcations leading to quasiperiodic attractors in a memristive Murali-Lakshmanan-Chua circuit. The above bifurcations arise because of the fact that a memristive MLC circuit is basically a nonsmooth system by virtue of having a memristive element as its nonlinearity. The switching and modulating properties of the memristor which we have considered endow the circuit with two discontinuity boundaries and multiple equilibrium points as well. As the Jacobian matrices about these equilibrium points are non-invertible, they are non-hyperbolic, some of these admit local bifurcations as well. Consequently when these equilibrium points are perturbed, they lose their stability giving rise to quasiperiodic orbits. The numerical simulations carried out by incorporating proper discontinuity mappings , such as the Poincar\\' discontinuity map and zero time discontinuity map , are found to agree well with experimental observations.",0.1042643661],["the feasible region can be employed for the selection of feasible footholds and CoM tra","Feasible Region: an Actuation-Aware Extension of the Support Region","summarize: In legged locomotion the projection of the robot Center of Mass being inside the convex hull of the contact points is a commonly accepted sufficient condition to achieve static balancing. However, some of these configurations cannot be realized because the joint torques required to sustain them would be above their limits . In this manuscript we rule out such configurations and define the Feasible Region, a revisited support region that guarantees both global static stability in the sense of tipover and slippage avoidance and of existence of a set of joint-torques that are able to sustain the robot body weight. We show that the feasible region can be employed for the selection of feasible footholds and CoM trajectories to achieve static locomotion on rough terrains, also in presence of load intensive tasks. Key results of our approach include the efficiency in the computation of the feasible region thanks to an Iterative Projection algorithm. This allowed us to carry out successful experiments on the HyQ robot, that was able to negotiate obstacles of moderate dimensions while carrying an extra 10 kg payload.",0.1578947368],["we model the process of turning a full ranking into an incomplete one. we propose the","Statistical Inference for Incomplete Ranking Data: The Case of Rank-Dependent Coarsening","summarize: We consider the problem of statistical inference for ranking data, specifically rank aggregation, under the assumption that samples are incomplete in the sense of not comprising all choice alternatives. In contrast to most existing methods, we explicitly model the process of turning a full ranking into an incomplete one, which we call the coarsening process. To this end, we propose the concept of rank-dependent coarsening, which assumes that incomplete rankings are produced by projecting a full ranking to a random subset of ranks. For a concrete instantiation of our model, in which full rankings are drawn from a Plackett-Luce distribution and observations take the form of pairwise preferences, we study the performance of various rank aggregation methods. In addition to predictive accuracy in the finite sample setting, we address the theoretical question of consistency, by which we mean the ability to recover a target ranking when the sample size goes to infinity, despite a potential bias in the observations caused by the coarsening.",0.2608695652],["the benefits include reduced latency for communication, less network traffic and increased privacy for data processing","Checkpointing and Migration of IoT Edge Functions","summarize: The serverless and functions as a service paradigms are currently trending among cloud providers and are now increasingly being applied to the network edge, and to the Internet of Things devices. The benefits include reduced latency for communication, less network traffic and increased privacy for data processing. However, there are challenges as IoT devices have limited resources for running multiple simultaneous containerized functions, and also FaaS does not typically support long-running functions. Our implementation utilizes Docker and CRIU for checkpointing and suspending long-running blocking functions. The results show that checkpointing is slightly slower than regular Docker pause, but it saves memory and allows for more long-running functions to be run on an IoT device. Furthermore, the resulting checkpoint files are small, hence they are suitable for live migration and backing up stateful functions, therefore improving availability and reliability of the system.",0.0555555556],["An","Completely positive semidefinite rank","summarize: An ",0.0],["Izergin-Korepin analysis extends the recently developed Izergin","Izergin-Korepin analysis on the wavefunctions of the ","summarize: We extend the recently developed Izergin-Korepin analysis on the wavefunctions of the ",0.4334389499],["textit method is a parsimonious version of the classical multivariate","New Parsimonious Multivariate Spatial Model: Spatial Envelope","summarize: Dimension reduction provides a useful tool for analyzing high dimensional data. The recently developed \\textit method is a parsimonious version of the classical multivariate regression model through identifying a minimal reducing subspace of the responses. However, existing envelope methods assume an independent error structure in the model. While the assumption of independence is convenient, it does not address the additional complications associated with spatial or temporal correlations in the data. In this article, we introduce a \\textit method for dimension reduction in the presence of dependencies across space. We study the asymptotic properties of the proposed estimators and show that the asymptotic variance of the estimated regression coefficients under the spatial envelope model is smaller than that from the traditional maximum likelihood estimation. Furthermore, we present a computationally efficient approach for inference. The efficacy of the new approach is investigated through simulation studies and an analysis of an Air Quality Standard dataset from the Environmental Protection Agency .",0.4],["hash tables are ubiquitous in computer science for efficient access to large datasets. but there","A Faster Algorithm for Cuckoo Insertion and Bipartite Matching in Large Graphs","summarize: Hash tables are ubiquitous in computer science for efficient access to large datasets. However, there is always a need for approaches that offer compact memory utilisation without substantial degradation of lookup performance. Cuckoo hashing is an efficient technique of creating hash tables with high space utilisation and offer a guaranteed constant access time. We are given ",0.125],["this paper proposes an algorithm to minimize weighted service latency for different classes of tenants","Differentiated latency in data center networks with erasure coded files through traffic engineering","summarize: This paper proposes an algorithm to minimize weighted service latency for different classes of tenants in a data center network where erasure-coded files are stored on distributed disks\/racks and access requests are scattered across the network. Due to limited bandwidth available at both top-of-the-rack and aggregation switches and tenants in different service classes need differentiated services, network bandwidth must be apportioned among different intra- and inter-rack data flows for different service classes in line with their traffic statistics. We formulate this problem as weighted queuing and employ a class of probabilistic request scheduling policies to derive a closed-form upper-bound of service latency for erasure-coded storage with arbitrary file access patterns and service time distributions. The result enables us to propose a joint weighted latency optimization over three entangled control knobs: the bandwidth allocation at top-of-the-rack and aggregation switches for different service classes, dynamic scheduling of file requests, and the placement of encoded file chunks . The joint optimization is shown to be a mixed-integer problem. We develop an iterative algorithm which decouples and solves the joint optimization as 3 sub-problems, which are either convex or solvable via bipartite matching in polynomial time. The proposed algorithm is prototyped in an open-source, distributed file system, , and evaluated on a cloud testbed with 16 separate physical hosts in an Openstack cluster using 48-port Cisco Catalyst switches. Experiments validate our theoretical latency analysis and show significant latency reduction for diverse file access patterns. The results provide valuable insights on designing low-latency data center networks with erasure coded storage.",0.0625],["knowledge distillation is widely used for model compression. it is difficult to achieve comparable performance with","Light Multi-segment Activation for Model Compression","summarize: Model compression has become necessary when applying neural networks into many real application tasks that can accept slightly-reduced model accuracy with strict tolerance to model complexity. Recently, Knowledge Distillation, which distills the knowledge from well-trained and highly complex teacher model into a compact student model, has been widely used for model compression. However, under the strict requirement on the resource cost, it is quite challenging to achieve comparable performance with the teacher model, essentially due to the drastically-reduced expressiveness ability of the compact student model. Inspired by the nature of the expressiveness ability in Neural Networks, we propose to use multi-segment activation, which can significantly improve the expressiveness ability with very little cost, in the compact student model. Specifically, we propose a highly efficient multi-segment activation, called Light Multi-segment Activation , which can rapidly produce multiple linear regions with very few parameters by leveraging the statistical information. With using LMA, the compact student model is capable of achieving much better performance effectively and efficiently, than the ReLU-equipped one with same model scale. Furthermore, the proposed method is compatible with other model compression techniques, such as quantization, which means they can be used jointly for better compression performance. Experiments on state-of-the-art NN architectures over the real-world tasks demonstrate the effectiveness and extensibility of the LMA.",0.0454545455],["proposed obstacle avoidance scheme is tested on a lab-scale autonomous vehicle.","Embedded nonlinear model predictive control for obstacle avoidance using PANOC","summarize: We employ the proximal averaged Newton-type method for optimal control to solve obstacle avoidance problems in real time. We introduce a novel modeling framework for obstacle avoidance which allows us to easily account for generic, possibly nonconvex, obstacles involving polytopes, ellipsoids, semialgebraic sets and generic sets described by a set of nonlinear inequalities. PANOC is particularly well-suited for embedded applications as it involves simple steps, its implementation comes with a low memory footprint and its fast convergence meets the tight runtime requirements of fast dynamical systems one encounters in modern mechatronics and robotics. The proposed obstacle avoidance scheme is tested on a lab-scale autonomous vehicle.",0.4444444444],["Shannon's analysis of the fundamental capacity limits for memoryless communication channels has been refined over time","The Log-Volume of Optimal Codes for Memoryless Channels, Asymptotically Within A Few Nats","summarize: Shannon's analysis of the fundamental capacity limits for memoryless communication channels has been refined over time. In this paper, the maximum volume ",0.125],["proposed procedure makes use of energy measurements and is based on the application of a closed form","On profile reconstruction of Euler-Bernoulli beams by means of an energy based genetic algorithm","summarize: This paper studies the inverse problem related to the identification of the flexural stiffness of an Euler Bernoulli beam in order to reconstruct its profile starting from available response data. The proposed identification procedure makes use of energy measurements and is based on the application of a closed form solution for the static displacements of multi-stepped beams. This solution allows to easily calculate the energy related to beams modeled with arbitrary multi-step shapes subjected to a transversal roving force, and to compare it with the correspondent data obtained through direct measurements on real beams. The optimal solution which minimizes the difference between measured and calculated data is then sought by means of genetic algorithms. In the paper several different stepped beams are investigated showing that the proposed procedure allows in many cases to identify the exact beam profile. However it is shown that in some other cases different multi-step profiles may correspond to very similar static responses, and therefore to comparable minima in the optimization problem, thus complicating the profile identification problem.",0.5],["matrix factorization is a fundamental building block in machine learning. it is used to summarize","Supervised Quantile Normalization for Low-rank Matrix Approximation","summarize: Low rank matrix factorization is a fundamental building block in machine learning, used for instance to summarize gene expression profile data or word-document counts. To be robust to outliers and differences in scale across features, a matrix factorization step is usually preceded by ad-hoc feature normalization steps, such as \\texttt scaling or data whitening. We propose in this work to learn these normalization operators jointly with the factorization itself. More precisely, given a ",0.275862069],["the sex of the u.s.","On the p-regularized trust region subproblem","summarize: The ",0.2339230723],["a seqBDD can compress a set of sequences into a graphical","Extraction of Templates from Phrases Using Sequence Binary Decision Diagrams","summarize: The extraction of templates such as ``regard X as Y'' from a set of related phrases requires the identification of their internal structures. This paper presents an unsupervised approach for extracting templates on-the-fly from only tagged text by using a novel relaxed variant of the Sequence Binary Decision Diagram . A SeqBDD can compress a set of sequences into a graphical structure equivalent to a minimal DFA, but more compact and better suited to the task of template extraction. The main contribution of this paper is a relaxed form of the SeqBDD construction algorithm that enables it to form general representations from a small amount of data. The process of compression of shared structures in the text during Relaxed SeqBDD construction, naturally induces the templates we wish to extract. Experiments show that the method is capable of high-quality extraction on tasks based on verb+preposition templates from corpora and phrasal templates from short messages from social media.",0.7307692308],["graphs embeddable on fixed surface can be embedded on a fixed surface.","Defective colouring of graphs excluding a subgraph or minor","summarize: Archdeacon proved that graphs embeddable on a fixed surface can be ",0.16],["this paper offers a synthesis of the positions in the literature regarding controversies about","La pol\\'emica del multiverso","summarize: This paper offers a synthesis of the positions in the literature regarding controversies about the multiverse. After reviewing some simple elements of modern cosmology and its observational limitations, we will present the history of the ideas that led, first, to the proposal of the anthropic principle and, years later, to the possible existence of causally disconnected domains, eventually endowed with their own laws and fundamental constants, not all compatible with the existence of observers, which were globally baptized with the confusing name of parallel universes or multiverse.",0.0833333333],["silicon silicon absorber films are used to enhance light-matter interaction. a new","Film flip and transfer process to enhance light harvesting in ultrathin absorber films on specular back-reflectors","summarize: Optical interference is used to enhance light-matter interaction and harvest broadband light in ultrathin semiconductor absorber films on specular back-reflectors. However, the high-temperature processing in oxygen atmosphere required for oxide absorbers often degrades metallic back-reflectors and their specular reflectance. In order to overcome this problem, we present a newly developed film flip and transfer process that allows for high-temperature processing without degradation of the metallic back-reflector and without the need of passivation interlayers. The film flip and transfer process improves the performance of photoanodes for photoelectrochemical water splitting comprising ultrathin hematite films on silver-gold alloy back-reflectors. We obtain specular back-reflectors with high reflectance below hematite films, which is necessary for maximizing the productive light absorption in the hematite film and minimizing non-productive absorption in the back-reflector. Furthermore, the film flip and transfer process opens up a new route to attach thin film stacks onto a wide range of substrates including flexible or temperature sensitive materials.",0.3771217424],["model of a single stem is used to maximize the captured sunlight. the shape of each","Competition Models for Plant Stems","summarize: The models introduced in this paper describe a uniform distribution of plant stems competing for sunlight. The shape of each stem, and the density of leaves, are designed in order to maximize the captured sunlight, subject to a cost for transporting water and nutrients from the root to all the leaves. Given the intensity of light, depending on the height above ground, we first solve the optimization problem determining the best possible shape for a single stem. We then study a competitive equilibrium among a large number of similar plants, where the shape of each stem is optimal given the shade produced by all others. Uniqueness of equilibria is proved by analyzing the two-point boundary value problem for a system of ODEs derived from the necessary conditions for optimality.",0.0416666667],["the solar activity during the Maunder Minimum has been considered significantly different from the one captured in modern","Reanalyses of the Sunspot Observations of Fogelius and Siverus: Two Long-Term Observers during the Maunder Minimum","summarize: The solar activity during the Maunder Minimum has been considered significantly different from the one captured in modern observations, in terms of sunspot group number and sunspot positions, whereas its actual amplitudes and distributions is still under active discussions. In its core period , Martin Fogelius and Henrich Siverus have formed significant long-term series in the existing databases with numerous spotless days, as the 13th and 7th most active observers before the end of the MM. In this study, we have analysed their original archival records, revised their data, have removed significant contaminations of the apparent spotless days in the existing databases, and cast caveats on the potential underestimation of the solar-cycle amplitude in the core MM. Still, they reported at best one sunspot group throughout their observational period and confirm the significant suppressed the solar cycles during the MM, which is also supported from the contemporary observations of Hook and Willoughby. Based on the revised data, we have also derived positions of notable sunspot groups, which Siverus recorded in 1671 , in comparison with those of Cassini's drawings . Their coincidence in position and chronology in corrected dates indicates these sunspot groups were probably the same recurrent active region and its significantly long lifespan even during the MM.",0.4230769231],["for more information","Bandwidth Selection for the Wolverton-Wagner Estimator","summarize: For ",0.0919698603],["a group of experts met in Switzerland to discuss where our discipline stands today. a","Supporting Requirements Engineering Research that Industry Needs: The Naming the Pain in Requirements Engineering Initiative","summarize: In light of the 40th jubilee of Requirements Engineering , roughly 40 experts met in Switzerland to discuss where our discipline stands today. As of today, the common view is, indisputably, that RE as a discipline is stable and respected, as pointed out by Sarah Gregory when covering the seminar in her column to which articles like this one are invited to present ongoing research. However, it is also evident that after 40 years of promising research, conducting research that industry needs is still an ongoing challenge. Research that industry needs means research that solves industrial problems practitioners face; but do we really understand those problems? Here, I want to recapitulate on this research challenge and outline an initiative, the Naming the Pain in Requirements Engineering Initiative, that aims at tackling this problem.",0.48],["global multispectral land cover classification is a complex classification. the spectral information","CoSpace: Common Subspace Learning from Hyperspectral-Multispectral Correspondences","summarize: With a large amount of open satellite multispectral imagery , considerable attention has been paid to global multispectral land cover classification. However, its limited spectral information hinders further improving the classification performance. Hyperspectral imaging enables discrimination between spectrally similar classes but its swath width from space is narrow compared to multispectral ones. To achieve accurate land cover classification over a large coverage, we propose a cross-modality feature learning framework, called common subspace learning , by jointly considering subspace learning and supervised classification. By locally aligning the manifold structure of the two modalities, CoSpace linearly learns a shared latent subspace from hyperspectral-multispectral correspondences. The multispectral out-of-samples can be then projected into the subspace, which are expected to take advantages of rich spectral information of the corresponding hyperspectral data used for learning, and thus leads to a better classification. Extensive experiments on two simulated HSMS datasets , where HS-MS data sets have trade-offs between coverage and spectral resolution, are performed to demonstrate the superiority and effectiveness of the proposed method in comparison with previous state-of-the-art methods.",0.32],["surface alloying is a straightforward route to control and modify the structure and electronic properties of surfaces","Two-Dimensional Rare Earth -- Gold Intermetallic Compounds on Au by Surface Alloying","summarize: Surface alloying is a straightforward route to control and modify the structure and electronic properties of surfaces. Here, We present a systematical study on the structural and electronic properties of three novel rare earth-based intermetallic compounds, namely ReAu2 , on Au via directly depositing rare-earth metals onto the hot Au surface. Scanning tunneling microscopy\/spectroscopy measurements reveal the very similar atomic structures and electronic properties, e.g. electronic states, and surface work functions, for all these intermetallic compound systems due to the physical and chemical similarities between these rare earth elements. Further, these electronic properties are periodically modulated by the moir\\'e structures caused by the lattice mismatches between ReAu2 and Au. These periodically modulated surfaces could serve as templates for the self-assembly of nanostructures. Besides, these two-dimensional rare earth-based intermetallic compounds provide platforms to investigate the rare earth related catalysis, magnetisms, etc., in the lower dimensions.",0.1851851852],["the rotating chemically peculiar star V473,Tau has been observed for more than","The Period Evolution of the Chemically Peculiar Star V473 Tau","summarize: In this paper, the period evolution of the rotating chemically peculiar star V473\\,Tau is investigated. Even though the star has been observed for more than fifty years, for the first time four consecutive years of space-based data covering between 2007 and 2010 is presented. The data is from the satellite, and is combined with the archival results. The analysis shows that the rotation period of V473\\,Tau is ",0.0833333333],["a single molecule could be able to visualize the inner structure of a single","Visually Constructing the Chemical Structure of a Single Molecule by Scanning Raman Picoscopy","summarize: The strong spatial confinement of a nanocavity plasmonic field has made it possible to visualize the inner structure of a single molecule and even to distinguish its vibrational modes in real space. With such ever-improved spatial resolution, it is anticipated that full vibrational imaging of a molecule could be achieved to reveal molecular structural details. Here we demonstrate full Raman images of individual vibrational modes on the ngstr\\om level for a single Mg-porphine molecule, revealing distinct characteristics of each vibrational mode in real space. Furthermore, by exploiting the underlying interference effect and Raman fingerprint database, we propose a new methodology for structural determination, coined as scanning Raman picoscopy, to show how such ultrahigh-resolution spectromicroscopic vibrational images can be used to visually assemble the chemical structure of a single molecule through a simple Lego-like building process.",0.4545454545],["crystallographic T-duality is a crystallographic concept inspired by the appearance of","Crystallographic T-duality","summarize: We introduce the notion of crystallographic T-duality, inspired by the appearance of ",0.1904761905],["paper introduces the real image Super-Resolution challenge. challenge was part of the Advance","AIM 2020 Challenge on Real Image Super-Resolution: Methods and Results","summarize: This paper introduces the real image Super-Resolution challenge that was part of the Advances in Image Manipulation workshop, held in conjunction with ECCV 2020. This challenge involves three tracks to super-resolve an input image for ",0.0588235294],["the MNO's optimal multi-cap data plans with time flexibility are studied. the","Multi-Cap Optimization for Wireless Data Plans with Time Flexibility","summarize: An effective way for a Mobile network operator to improve its revenue is price discrimination, i.e., providing different combinations of data caps and subscription fees. Rollover data plan is an innovative data mechanism with time flexibility. In this paper, we study the MNO's optimal multi-cap data plans with time flexibility in a realistic asymmetric information scenario. Specifically, users are associated with multi-dimensional private information, and the MNO designs a contract to induce users to truthfully reveal their private information. This problem is quite challenging due to the multi-dimensional private information. We address the challenge in two aspects. First, we find that a feasible contract should allocate the data caps according to users' willingness-to-pay . Second, for the non-convex data cap allocation problem, we propose a Dynamic Quota Allocation Algorithm, which has a low complexity and guarantees the global optimality. Numerical results show that the time-flexible data mechanisms increase both the MNO's profit and users' payoffs under price discrimination.",0.0714285714],["we address the problem of studying those K manifolds satisfying the","A characterization of complex space forms via Laplace operators","summarize: Inspired by the work of Z. Lu and G. Tian \\cite, in this paper we address the problem of studying those \\K\\ manifolds satisfying the ",0.0769230769],["a non-perturbative formulation leads to a vacuum created gravitational pair","Emergent D-instanton as a source of Dark Energy","summarize: We revisit a non-perturbative formulation leading to a vacuum created gravitational pair of -brane by a Poincare dual higher form U gauge theory on a D4 -brane. In particular, the analysis has revealed a dynamical geometric torsion H 3 for an on-shell Neveu-Schwarz form on a fat 4-brane. We argue that a D-instanton can be a viable candidate to incorporate the quintessence correction hidden to an emergent -dimensional brane universe. It is shown that a dynamical non-perturbative correction may be realized with an axionic scalar QFT on an emergent anti 3-brane within a gravitational pair. The theoretical tool provokes thought to believe for an extra instantaneous dimension transverse to our classical brane-universe in an emergent scenario. Interestingly a D-instanton correction, sourced by an axion on an anti 3-brane, may serve as a potential candidate to explain the accelerated rate of expansion of our 3-brane universe and may provide a clue to the origin of dark energy.",0.3],["the presence or absence of one or more of these attributes contributes to the overall artistic value of","A Geometry-Sensitive Approach for Photographic Style Classification","summarize: Photographs are characterized by different compositional attributes like the Rule of Thirds, depth of field, vanishing-lines etc. The presence or absence of one or more of these attributes contributes to the overall artistic value of an image. In this work, we analyze the ability of deep learning based methods to learn such photographic style attributes. We observe that although a standard CNN learns the texture and appearance based features reasonably well, its understanding of global and geometric features is limited by two factors. First, the data-augmentation strategies distort the composition of a photograph and affect the performance. Secondly, the CNN features, in principle, are translation-invariant and appearance-dependent. But some geometric properties important for aesthetics, e.g. the Rule of Thirds , are position-dependent and appearance-invariant. Therefore, we propose a novel input representation which is geometry-sensitive, position-cognizant and appearance-invariant. We further introduce a two-column CNN architecture that performs better than the state-of-the-art in photographic style classification. From our results, we observe that the proposed network learns both the geometric and appearance-based attributes better than the SoA.",0.09375],["opinion mining is a subfield of opinion mining that deals with identifying and extracting information","Comparative Opinion Mining: A Review","summarize: Opinion mining refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in textual material. Opinion mining, also known as sentiment analysis, has received a lot of attention in recent times, as it provides a number of tools to analyse the public opinion on a number of different topics. Comparative opinion mining is a subfield of opinion mining that deals with identifying and extracting information that is expressed in a comparative form . Comparative opinion mining plays a very important role when ones tries to evaluate something, as it provides a reference point for the comparison. This paper provides a review of the area of comparative opinion mining. It is the first review that cover specifically this topic as all previous reviews dealt mostly with general opinion mining. This survey covers comparative opinion mining from two different angles. One from perspective of techniques and the other from perspective of comparative opinion elements. It also incorporates preprocessing tools as well as dataset that were used by the past researchers that can be useful to the future researchers in the field of comparative opinion mining.",0.0833333333],["a user can define a model and customize the calibration. a user can define","A mechanism for balancing accuracy and scope in cross-machine black-box GPU performance modeling","summarize: The ability to model, analyze, and predict execution time of computations is an important building block supporting numerous efforts, such as load balancing, performance optimization, and automated performance tuning for high performance, parallel applications. In today's increasingly heterogeneous computing environment, this task must be accomplished efficiently across multiple architectures, including massively parallel coprocessors like GPUs. To address this challenge, we present an approach for constructing customizable, cross-machine performance models for GPU kernels, including a mechanism to automatically and symbolically gather performance-relevant kernel operation counts, a tool for formulating mathematical models using these counts, and a customizable parameterized collection of benchmark kernels used to calibrate models to GPUs in a black-box fashion. Our approach empowers a user to manage trade-offs between model accuracy, evaluation speed, and generalizability. A user can define a model and customize the calibration process, making it as simple or complex as desired, and as application-targeted or general as desired. To evaluate our approach, we demonstrate both linear and nonlinear models; each example models execution times for multiple variants of a particular computation: two matrix multiplication variants, four Discontinuous Galerkin differentiation operation variants, and two 2-D five-point finite difference stencil variants. For each variant, we present accuracy results on GPUs from multiple vendors and hardware generations. We view this customizable approach as a response to a central question in GPU performance modeling: how can we model GPU performance in a cost-explanatory fashion while maintaining accuracy, evaluation speed, portability, and ease of use, an attribute we believe precludes manual collection of kernel or hardware statistics.",0.6341463415],["the research aims to examine areas of the Compton cross section of annihilation","Probing entanglement in Compton interactions","summarize: This theoretical research aims to examine areas of the Compton cross section of entangled annihilation photons for the purpose of testing for possible break down of theory, which could have consequences for predicted optimal capabilities of Compton PET systems.We provide maps of the cross section for entangled annihilation photons for experimental verification.We introduce a strategy to derive cross sections in a relatively straight forward manner for the Compton scattering of a hypothetical separable, mixed and entangled states. To understand the effect that entanglement has on the cross section for annihilation photons, we derive the cross section so that it is expressed in terms of the cross section of a hypothetical separable state and of a hypothetical forbidden maximally entangled state.We find lobe-like structures in the cross section which are regions where entanglement has the greatest effect.We also find that mixed states do not reproduce the cross section for annihilation photons, contrary to a recent investigation which reported otherwise.We review the motivation and method of the most precise Compton scattering experiment for annihilation photons, in order to resolve conflicting reports regarding the extent to which the cross section itself has been experimentally verified.",0.1111111111],["2DCCA is incapable of extracting sufficient discriminatory representations. this is a","A Complete Discriminative Tensor Representation Learning for Two-Dimensional Correlation Analysis","summarize: As an effective tool for two-dimensional data analysis, two-dimensional canonical correlation analysis is not only capable of preserving the intrinsic structural information of original two-dimensional data, but also reduces the computational complexity effectively. However, due to the unsupervised nature, 2DCCA is incapable of extracting sufficient discriminatory representations, resulting in an unsatisfying performance. In this letter, we propose a complete discriminative tensor representation learning method based on linear correlation analysis for analyzing 2D signals . This letter shows that the introduction of the complete discriminatory tensor representation strategy provides an effective vehicle for revealing, and extracting the discriminant representations across the 2D data sets, leading to improved results. Experimental results show that the proposed CDTRL outperforms state-of-the-art methods on the evaluated data sets.",0.4545454545],["comet is a member of the Theobalda asteroid family.","P\/2017 S5: Another Active Asteroid Associated with the Theobalda Family","summarize: In this note we have shown that a newly discovered comet P\/2017 S5 , that moves around the Sun in an asteroid-like orbit, is a member of the Theobalda asteroid family.",0.4130027616],["memory consistency models represent the behaviour of concurrent programs at the chip level. snippets","Memory Consistency Models using Constraints","summarize: Memory consistency models are at the heart of concurrent programming. They represent the behaviour of concurrent programs at the chip level. To test these models small program snippets called litmus test are generated, which show allowed or forbidden behaviour of different MCMs. This paper is showcasing the use of constraint programming to automate the generation and testing of litmus tests for memory consistency models. We produce a few exemplary case studies for two MCMs, namely Sequential Consistency and Total Store Order. These studies demonstrate the flexibility of constrains programming in this context and lay foundation to the direct verification of MCMs against the software facing cache coherence protocols.",0.0],["we investigate the relativistic photoionization processes of H, Nb, and Nb","Relativistic photoionization of H isoelectronic series including plasma shielding effects","summarize: With plasma shielding effects of the Debye-H\\uckel model, we investigate the relativistic photoionization processes of H, Nb",0.1666666667],["extraPush and Normalized extraPush can iterate with a fixed step size","ExtraPush for convex smooth decentralized optimization over directed networks","summarize: In this note, we extend the algorithms Extra and subgradient-push to a new algorithm ExtraPush for consensus optimization with convex differentiable objective functions over a directed network. When the stationary distribution of the network can be computed in advance}, we propose a simplified algorithm called Normalized ExtraPush. Just like Extra, both ExtraPush and Normalized ExtraPush can iterate with a fixed step size. But unlike Extra, they can take a column-stochastic mixing matrix, which is not necessarily doubly stochastic. Therefore, they remove the undirected-network restriction of Extra. Subgradient-push, while also works for directed networks, is slower on the same type of problem because it must use a sequence of diminishing step sizes. We present preliminary analysis for ExtraPush under a bounded sequence assumption. For Normalized ExtraPush, we show that it naturally produces a bounded, linearly convergent sequence provided that the objective function is strongly convex. In our numerical experiments, ExtraPush and Normalized ExtraPush performed similarly well. They are significantly faster than subgradient-push, even when we hand-optimize the step sizes for the latter.",0.1578947368],["umbral calculus and umbral image technique are based on the two types of","Operational Methods in the Study of Sobolev-Jacobi Polynomials","summarize: Inspired by ideas from umbral calculus and based on the two types of integrals occurring in the defining equations for the gamma and the reciprocal gamma functions, respectively, we develop a multi-variate version of umbral calculus and of the so-called umbral image technique. Besides providing a class of new formulae for generalized hypergeometric functions and an implementation of series manipulations for computing lacunary generating functions, our main application of these techniques is the study of Sobolev-Jacobi polynomials. Motivated by applications to theoretical chemistry, we moreover present a deep link between generalized normal-ordering techniques introduced by Gurappa and Panigrahi, two-variable Hermite polynomials and our integral-based series transforms. Notably, we thus calculate all K-tuple L-shifted lacunary exponential generating functions for a certain family of SJ polynomials explicitly.",0.2],["numerical analysis is carried out for a class of history-dependent inequalities arising","Numerical Analysis of History-dependent Variational-hemivariational Inequalities","summarize: In this paper, numerical analysis is carried out for a class of history-dependent variational-hemivariational inequalities arising in contact problems. Three different numerical treatments for temporal discretization are proposed to approximate the continuous model. Fixed-point iteration algorithms are employed to implement the implicit scheme and the convergence is proved with a convergence rate independent of the time step-size and mesh grid-size. A special temporal discretization is introduced for the history-dependent operator, leading to numerical schemes for which the unique solvability and error bounds for the temporally discrete systems can be proved without any restriction on the time step-size. As for spatial approximation, the finite element method is applied and an optimal order error estimate for the linear element solutions is provided under appropriate regularity assumptions. Numerical examples are presented to illustrate the theoretical results.",0.5],["we present new connections between quantum information and classical cryptography. examples consist of a quantum","Using Simon's Algorithm to Attack Symmetric-Key Cryptographic Primitives","summarize: We present new connections between quantum information and the field of classical cryptography. In particular, we provide examples where Simon's algorithm can be used to show insecurity of commonly used cryptographic symmetric-key primitives. Specifically, these examples consist of a quantum distinguisher for the 3-round Feistel network and a forgery attack on CBC-MAC which forges a tag for a chosen-prefix message querying only other messages . We assume that an adversary has quantum-oracle access to the respective classical primitives. Similar results have been achieved recently in independent work by Kaplan et al. Our findings shed new light on the post-quantum security of cryptographic schemes and underline that classical security proofs of cryptographic constructions need to be revisited in light of quantum attackers.",0.0769230769],["images capture generation has seen tremendous progress over the last few years. most methods use transfer learning","Comparative evaluation of CNN architectures for Image Caption Generation","summarize: Aided by recent advances in Deep Learning, Image Caption Generation has seen tremendous progress over the last few years. Most methods use transfer learning to extract visual information, in the form of image features, with the help of pre-trained Convolutional Neural Network models followed by transformation of the visual information using a Caption Generator module to generate the output sentences. Different methods have used different Convolutional Neural Network Architectures and, to the best of our knowledge, there is no systematic study which compares the relative efficacy of different Convolutional Neural Network architectures for extracting the visual information. In this work, we have evaluated 17 different Convolutional Neural Networks on two popular Image Caption Generation frameworks: the first based on Neural Image Caption generation model and the second based on Soft-Attention framework. We observe that model complexity of Convolutional Neural Network, as measured by number of parameters, and the accuracy of the model on Object Recognition task does not necessarily co-relate with its efficacy on feature extraction for Image Caption Generation task.",0.0],["we consider an extension of the standard model based on the group.","A ","summarize: We consider an extension of the standard model based on the group ",0.0],["a concurrent pandemic of information has spread across the world. the pandemic","The COVID19 infodemic. The role and place of academics in science communication","summarize: As the COVID19 pandemic has spread across the world, a concurrent pandemic of information has spread with it. Deemed an infodemic by the World Health Organization, and described as an overabundance of information, some accurate, some not, that occurs during an epidemic, this proliferation of data, research and opinions provides both opportunities and challenges for academics. Academics and scientists have a key role to play in the solutions to the infodemic challenge: as educators, influences and communicators, even where their expertise and experience does not align precisely with the SARS-Cov2 virus and its impacts. Successful communication requires a better understanding of how the public seeks, understands and processes scientific information, however, in order to maximise the ways in which experts engage with traditional and social media and to make sure that such engagement does not add to confusion and misinformation alongside efforts to counter or challenge it. This paper will outline the key advantages to be had from greater engagement with COVID19 discussions, the popular channels through which such discussions take place and through which information is disseminated. It also warns against the common pitfalls those who choose to engage might encounter, whilst stressing that the disadvantages of doing so are far outweighed by the advantages such engagement offers.",0.2727272727],["topological photonics provides a new paradigm in studying cavity quantum electrodynamics with robust","Cavity Quantum Electrodynamics with Second-Order Topological Corner State","summarize: Topological photonics provides a new paradigm in studying cavity quantum electrodynamics with robustness to disorder. In this work, we demonstrate the coupling between single quantum dots and the second-order topological corner state. Based on the second-order topological corner state, a topological photonic crystal cavity is designed and fabricated into GaAs slabs with quantum dots embedded. The coexistence of corner state and edge state with high quality factor close to 2000 is observed. The enhancement of photoluminescence intensity and emission rate are both observed when the quantum dot is on resonance with the corner state. This result enables the application of topology into cavity quantum electrodynamics, offering an approach to topological devices for quantum information processing.",0.3],["the secular behavior of an orbit under the gravitational perturbation due to a two-","Orbital Dynamics with the Gravitational Perturbation due to a Disk","summarize: The secular behavior of an orbit under the gravitational perturbation due to a two-dimensional uniform disk is studied in this paper, through analytical and numerical approaches. We develop the secular approximation of this problem and obtain the averaged Hamiltonian for this system first. We find that, when the ratio of the semimajor axes of the inner orbit and the disk radius takes very small values , and if the inclination between the inner orbit and the disk is greater than the critical value of ",0.4782608696],["in recent years the PC has been replaced by mobile devices. security mechanisms are deployed at various","SpotCheck: On-Device Anomaly Detection for Android","summarize: In recent years the PC has been replaced by mobile devices for many security sensitive operations, both from a privacy and a financial standpoint. While security mechanisms are deployed at various levels, these are frequently put under strain by previously unseen malware. An additional protection layer capable of novelty detection is therefore needed. In this work we propose SpotCheck, an anomaly detector intended to run on Android devices. It samples app executions and submits suspicious apps to more thorough processing by malware sandboxes. We compare Kernel Principal Component Analysis and Variational Autoencoders on app execution representations based on the well-known system call traces, as well as a novel approach based on memory dumps. Results show that when using VAE, SpotCheck attains a level of effectiveness comparable to what has been previously achieved for network anomaly detection. Interestingly this is also true for the memory dump approach, relinquishing the need for continuous app monitoring.",0.0],["image forensic plays a crucial role in both criminal investigations and civil litigation. there are","A Survey of Machine Learning Techniques in Adversarial Image Forensics","summarize: Image forensic plays a crucial role in both criminal investigations and civil litigation . Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial examples, with real-world consequences . Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.",0.32],["steam flood injection uses thermal and gravitational potential to mobilize and dilute heavy oil in","Machine Learning and the Internet of Things Enable Steam Flood Optimization for Improved Oil Production","summarize: Recently developed machine learning techniques, in association with the Internet of Things allow for the implementation of a method of increasing oil production from heavy-oil wells. Steam flood injection, a widely used enhanced oil recovery technique, uses thermal and gravitational potential to mobilize and dilute heavy oil in situ to increase oil production. In contrast to traditional steam flood simulations based on principles of classic physics, we introduce here an approach using cutting-edge machine learning techniques that have the potential to provide a better way to describe the performance of steam flood. We propose a workflow to address a category of time-series data that can be analyzed with supervised machine learning algorithms and IoT. We demonstrate the effectiveness of the technique for forecasting oil production in steam flood scenarios. Moreover, we build an optimization system that recommends an optimal steam allocation plan, and show that it leads to a 3% improvement in oil production. We develop a minimum viable product on a cloud platform that can implement real-time data collection, transfer, and storage, as well as the training and implementation of a cloud-based machine learning model. This workflow also offers an applicable solution to other problems with similar time-series data structures, like predictive maintenance.",0.2222222222],["the validity of the measurement has been double-checked in the well-mixed","Moran-evolution of cooperation: From well-mixed to heterogeneous complex networks","summarize: Configurational arrangement of network architecture and interaction character of individuals are two most influential factors on the mechanisms underlying the evolutionary outcome of cooperation, which is explained by the well-established framework of evolutionary game theory. In the current study, not only qualitatively but also quantitatively, we measure Moran-evolution of cooperation to support an analytical agreement based on the consequences of the replicator equation in a finite population. The validity of the measurement has been double-checked in the well-mixed network by the Langevin stochastic differential equation and the Gillespie-algorithmic version of Moran-evolution, while in a structured network, the measurement of accuracy is verified by the standard numerical simulation. Considering the Birth-Death and Death-Birth updating rules through diffusion of individuals, the investigation is carried out in the wide range of game environments those relate to the various social dilemmas where we are able to draw a new rigorous mathematical track to tackle the heterogeneity of complex networks. The set of modified criteria reveals the exact fact about the emergence and maintenance of cooperation in the structured population. We find that in general, nature promotes the environment of coexistent traits.",0.1176470588],["multiresolution decomposition is commonly understood as a procedure to capture scale-dependent features","Multiresolution Decomposition of Areal Count Data","summarize: Multiresolution decomposition is commonly understood as a procedure to capture scale-dependent features in random signals. Such methods were first established for image processing and typically rely on raster or regularly gridded data. In this article, we extend a particular multiresolution decomposition procedure to areal count data, i.e.~discrete irregularly gridded data. More specifically, we incorporate in a new model concept and distributions from the so-called Besag--York--Molli\\' model to include a priori demographical knowledge. These adaptions and subsequent changes in the computation schemes are carefully outlined below, whereas the main idea of the original multiresolution decomposition remains. Finally, we show the extension's feasibility by applying it to oral cavity cancer counts in Germany.",0.1764705882],["article presents method of organizing men-in-the-middle attack and penetration test on","Men-in-the-Middle Attack Simulation on Low Energy Wireless Devices using Software Define Radio","summarize: The article presents a method of organizing men-in-the-middle attack and penetration test on Bluetooth Low Energy devices and ZigBee packets using software define radio with sniffing and spoofing packets, capture and analysis techniques on wireless waves with the focus on Bluetooth. The paper contains the analysis of the latest scientific work in this area, provides a comparative analysis of SDRs and the rationale for the choice of hardware, gives the sequence of actions for collecting wireless data packets and data collection from ZigBee and BLE devices, and analyzes ways to improve captured wireless packet analysis techniques. For the study collected experimental setup, the results of which are analyzed in real time. The collected wireless data packets are compared with those sent. The result of the experiment shows the weaknesses of local wireless networks.",0.2282751791],["topological insulators are materials that have a gapped bulk energy spectrum. they","Quantum interference of topological states of light","summarize: Topological insulators are materials that have a gapped bulk energy spectrum, but contain protected in-gap states appearing at their surface. These states exhibit remarkable properties such as unidirectional propagation and robustness to noise that offer an opportunity to improve the performance and scalability of quantum technologies. For quantum applications, it is essential that the topological states are indistinguishable. Here we report high-visibility quantum interference of single photon topological states in an integrated photonic circuit. Two topological boundary-states, initially at opposite edges of a coupled waveguide array, are brought into proximity, where they interfere and undergo a beamsplitter operation. We observe ",0.2],["gait data are collected using color sensors, such as a CCD camera, depth sensors","Robust Gait Recognition by Integrating Inertial and RGBD Sensors","summarize: Gait has been considered as a promising and unique biometric for person identification. Traditionally, gait data are collected using either color sensors, such as a CCD camera, depth sensors, such as a Microsoft Kinect, or inertial sensors, such as an accelerometer. However, a single type of sensors may only capture part of the dynamic gait features and make the gait recognition sensitive to complex covariate conditions, leading to fragile gait-based person identification systems. In this paper, we propose to combine all three types of sensors for gait data collection and gait recognition, which can be used for important identification applications, such as identity recognition to access a restricted building or area. We propose two new algorithms, namely EigenGait and TrajGait, to extract gait features from the inertial data and the RGBD data, respectively. Specifically, EigenGait extracts general gait dynamics from the accelerometer readings in the eigenspace and TrajGait extracts more detailed sub-dynamics by analyzing 3D dense trajectories. Finally, both extracted features are fed into a supervised classifier for gait recognition and person identification. Experiments on 50 subjects, with comparisons to several other state-of-the-art gait-recognition approaches, show that the proposed approach can achieve higher recognition accuracy and robustness.",0.1818181818],["conformal field theories have a SL algebra of local bosonic constraints. they","Space-time CFTs from the Riemann sphere","summarize: We consider two-dimensional chiral, first-order conformal field theories governing maps from the Riemann sphere to the projective light cone inside Minkowski space -- the natural setting for describing conformal field theories in two fewer dimensions. These theories have a SL algebra of local bosonic constraints which can be supplemented by additional fermionic constraints depending on the matter content of the theory. By computing the BRST charge associated with gauge fixing these constraints, we find anomalies which vanish for specific target space dimensions. These critical dimensions coincide precisely with those for which cubic scalar theory, gauge theory and gravity are classically conformally invariant. Furthermore, the BRST cohomology of each theory contains vertex operators for the full conformal multiplets of single field insertions in each of these space-time CFTs. We give a prescription for the computation of three-point functions, and compare our formalism with the scattering equations approach to on-shell amplitudes.",0.1111111111],["the duality theory of the transport problem is investigated in an abstract measure framework. let'","A note on duality theorems in mass transportation","summarize: The duality theory of the Monge-Kantorovich transport problem is investigated in an abstract measure theoretic framework. Let ",0.2857142857],["the paper deals with the three-dimensional Dirichlet boundary value problem. it develops","Localized Boundary-Domain Singular Integral Equations of Dirichlet Problem for Self-adjoint Second Order Strongly Elliptic PDE Systems","summarize: The paper deals with the three-dimensional Dirichlet boundary value problem for a second order strongly elliptic self-adjoint system of partial differential equations in the divergence form with variable coefficients and develops the integral potential method based on a localized parametrix. Using Green's representation formula and properties of the localized layer and volume potentials, we reduce the Dirichlet BVP to a system of localized boundary-domain integral equations . The equivalence between the Dirichlet BVP and the corresponding LBDIE system is studied. We establish that the obtained localized boundary-domain integral operator belongs to the Boutet de Monvel algebra. With the help of the Wiener-Hopf factorization method we investigate corresponding Fredholm properties and prove invertibility of the localized operator in appropriate Sobolev spaces.",0.047768754],["heterointerface of spinel\/perovskite heterointerface.","Microscopic origin of the mobility enhancement at a spinel\/perovskite oxide heterointerface revealed by photoemission spectroscopy","summarize: The spinel\/perovskite heterointerface ",0.0426185741],["Let us know what you think about it!","Universality in Random Moment Problems","summarize: Let ",0.0],["medical research suggests the anterior-posterior -diameter of the inferior","Estimation and Tracking of AP-diameter of the Inferior Vena Cava in Ultrasound Images Using a Novel Active Circle Algorithm","summarize: Medical research suggests that the anterior-posterior -diameter of the inferior vena cava and its associated temporal variation as imaged by bedside ultrasound is useful in guiding fluid resuscitation of the critically-ill patient. Unfortunately, indistinct edges and gaps in vessel walls are frequently present which impede accurate estimation of the IVC AP-diameter for both human operators and segmentation algorithms. The majority of research involving use of the IVC to guide fluid resuscitation involves manual measurement of the maximum and minimum AP-diameter as it varies over time. This effort proposes using a time-varying circle fitted inside the typically ellipsoid IVC as an efficient, consistent and novel approach to tracking and approximating the AP-diameter even in the context of poor image quality. In this active-circle algorithm, a novel evolution functional is proposed and shown to be a useful tool for ultrasound image processing. The proposed algorithm is compared with an expert manual measurement, and state-of-the-art relevant algorithms. It is shown that the algorithm outperforms other techniques and performs very close to manual measurement.",0.119706541],["a novel DLCM is based on a unique combination of genetic algorithms and deep","A Novel Hybrid Scheme Using Genetic Algorithms and Deep Learning for the Reconstruction of Portuguese Tile Panels","summarize: This paper presents a novel scheme, based on a unique combination of genetic algorithms and deep learning , for the automatic reconstruction of Portuguese tile panels, a challenging real-world variant of the jigsaw puzzle problem with important national heritage implications. Specifically, we introduce an enhanced GA-based puzzle solver, whose integration with a novel DL-based compatibility measure yields state-of-the-art performance, regarding the above application. Current compatibility measures consider typically edge pixels , and help achieve high accuracy for the synthetic JPP variant. However, such measures exhibit rather poor performance when applied to the Portuguese tile panels, which are susceptible to various real-world effects, e.g., monochromatic panels, non-squared tiles, edge degradation, etc. To overcome such difficulties, we have developed a novel DLCM to extract high-level texture\/color statistics from the entire tile information. Integrating this measure with our enhanced GA-based puzzle solver, we have demonstrated, for the first time, how to deal most effectively with large-scale real-world problems, such as the Portuguese tile problem. Specifically, we have achieved 82% accuracy for the reconstruction of Portuguese tile panels with unknown piece rotation and puzzle dimension . The proposed method outperforms even human experts in several cases, correcting their mistakes in the manual tile assembly.",0.3228470988],["the team from the new technologies for the Information Society research center of the university of west boh","UWB-NTIS Speaker Diarization System for the DIHARD II 2019 Challenge","summarize: In this paper, we present our system developed by the team from the New Technologies for the Information Society research center of the University of West Bohemia in Pilsen, for the Second DIHARD Speech Diarization Challenge. The base of our system follows the currently-standard approach of segmentation, i\/x-vector extraction, clustering, and resegmentation. The hyperparameters for each of the subsystems were selected according to the domain classifier trained on the development set of DIHARD II. We compared our system with results from the Kaldi diarization and combined these systems. At the time of writing of this abstract, our best submission achieved a DER of 23.47% and a JER of 48.99% on the evaluation set .",0.1515151515],["reinforcement learning is essential in online systems. there are many practical challenges to implement deep reinforcement learning","MBCAL: Sample Efficient and Variance Reduced Reinforcement Learning for Recommender Systems","summarize: In recommender systems such as news feed stream, it is essential to optimize the long-term utilities in the continuous user-system interaction processes. Previous works have proved the capability of reinforcement learning in this problem. However, there are many practical challenges to implement deep reinforcement learning in online systems, including low sample efficiency, uncontrollable risks, and excessive variances. To address these issues, we propose a novel reinforcement learning method, namely model-based counterfactual advantage learning . The proposed method takes advantage of the characteristics of recommender systems and draws ideas from the model-based reinforcement learning method for higher sample efficiency. It has two components: an environment model that predicts the instant user behavior one-by-one in an auto-regressive form, and a future advantage model that predicts the future utility. To alleviate the impact of excessive variance when learning the future advantage model, we employ counterfactual comparisons derived from the environment model. In consequence, the proposed method possesses high sample efficiency and significantly lower variance; Also, it is able to use existing user logs to avoid the risks of starting from scratch. In contrast to its capability, its implementation cost is relatively low, which fits well with practical systems. Theoretical analysis and elaborate experiments are presented. Results show that the proposed method transcends the other supervised learning and RL-based methods in both sample efficiency and asymptotic performances.",0.0769230769],["side-effect modulation is a variation in the intensity of the light emitted by","Mitigation of Side-Effect Modulation in Optical OFDM VLC Systems","summarize: Side-effect modulation has the potential to be a significant source of interference in future visible light communication systems. SEM is a variation in the intensity of the light emitted by a luminaire and is usually a side-effect caused by the power supply used to drive the luminaires. For LED luminaires powered by a switched mode power supply, the SEM can be at much higher frequencies than that emitted by conventional incandescent or fluorescent lighting. It has been shown that the SEM caused by commercially available LED luminaires is often periodic and of low power. In this paper, we investigate the impact of typical forms of SEM on the performance of optical OFDM VLC systems; both ACO-OFDM and DCO-OFDM are considered. Our results show that even low levels of SEM power can significantly degrade the bit-error-rate performance. To solve this problem, an SEM mitigation scheme is described. The mitigation scheme is decision-directed and is based on estimating and subtracting the fundamental component of the SEM from the received signal. We describe two forms of the algorithm; one uses blind estimation while the other uses pilot-assisted estimation based on a training sequence. Decision errors, resulting in decision noise, limit the performance of the blind estimator even when estimation is based on very long signals. However, the pilot system can achieve more accurate estimations, thus better performance. Results are first presented for typical SEM waveforms for the case where the fundamental frequency of the SEM is known. The algorithms are then extended to include a frequency estimation step and the mitigation algorithm is shown also to be effective in this case.",0.2631578947],["a range of models have been proposed to explain and predict popularity. but there is","Will This Video Go Viral? Explaining and Predicting the Popularity of Youtube Videos","summarize: What makes content go viral? Which videos become popular and why others don't? Such questions have elicited significant attention from both researchers and industry, particularly in the context of online media. A range of models have been recently proposed to explain and predict popularity; however, there is a short supply of practical tools, accessible for regular users, that leverage these theoretical results. HIPie -- an interactive visualization system -- is created to fill this gap, by enabling users to reason about the virality and the popularity of online videos. It retrieves the metadata and the past popularity series of Youtube videos, it employs Hawkes Intensity Process, a state-of-the-art online popularity model for explaining and predicting video popularity, and it presents videos comparatively in a series of interactive plots. This system will help both content consumers and content producers in a range of data-driven inquiries, such as to comparatively analyze videos and channels, to explain and predict future popularity, to identify viral videos, and to estimate response to online promotion.",0.35],["krypton produces high-order harmonic generation based on a low-repet","Compact 200 kHz HHG source driven by a few-cycle OPCPA","summarize: We present efficient high-order harmonic generation based on a high-repetition rate, few-cycle, near infrared , carrier-envelope phase stable, optical parametric chirped pulse amplifier , emitting 6fs pulses with 9J pulse energy at 200kHz repetition rate. In krypton, we reach conversion efficiencies from the NIR to the extreme ultraviolet radiation pulse energy on the order of ~10^ with less than 3J driving pulse energy. This is achieved by optimizing the OPCPA for a spatially and temporally clean pulse and by a specially designed high-pressure gas target. In the future, the high efficiency of the HHG source will be beneficial for high-repetition rate two-colour pumpprobe experiments, where the available pulse energy from the laser has to be distributed economically between pump and probe pulses.",0.1193119089],["experimental evidence of macroscopic quantum tunneling and an incomplete 0-pi transition in tunnel","Electrodynamics of highly spin-polarized tunnel Josephson junctions","summarize: The continuous development of superconducting electronics is encouraging several studies on hybrid Josephson junctions based on superconductor\/ferromagnet\/superconductor heterostructures, as either spintronic devices or switchable elements in quantum and classical circuits. Recent experimental evidence of macroscopic quantum tunneling and of an incomplete 0-pi transition in tunnel-ferromagnetic spin-filter JJs could enhance the capabilities of SFS JJs also as active elements. Here, we provide a self-consistent electrodynamic characterization of NbN\/GdN\/NbN spin-filter JJs as a function of the barrier thickness, disentangling the high-frequency dissipation effects due to the environment from the intrinsic low-frequency dissipation processes. The fitting of the IV characteristics at 4.2K and at 300mK by using the Tunnel Junction Microscopic model allows us to determine the subgap resistance Rsg, the quality factor Q and the junction capacitance C. These results provide the scaling behavior of the electrodynamic parameters as a function of the barrier thickness, which represents a fundamental step for the feasibility of tunnel ferromagnetic JJs as active elements in classical and quantum circuits, and are of general interest for tunnel junctions other than conventional SIS JJs.",0.1578947368],["optical unit consists of a flat, circular acrylic plate coated with tetraphen","A Model for the Global Quantum Efficiency for a TPB-based Wavelength-Shifting System used with Photomultiplier Tubes in Liquid Argon in MicroBooNE","summarize: We present a model for the Global Quantum Efficiency of the MicroBooNE optical units. An optical unit consists of a flat, circular acrylic plate, coated with tetraphenyl butadiene , positioned near the photocathode of a 20.2-cm diameter photomultiplier tube. The plate converts the ultra-violet scintillation photons from liquid argon into visible-spectrum photons to which the cryogenic phototubes are sensitive. The GQE is the convolution of the efficiency of the plates that convert the 128 nm scintillation light from liquid argon to visible light, the efficiency of the shifted light to reach the photocathode, and the efficiency of the cryogenic photomultiplier tube. We develop a GEANT4-based model of the optical unit, based on first principles, and obtain the range of probable values for the expected number of detected photoelectrons given the known systematic errors on the simulation parameters. We compare results from four measurements of the ",0.1491683851],["the master equation is a very useful tool to study the decoherence of a","Alternative Derivation of the Hu-Paz-Zhang Master Equation for Quantum Brownian Motion","summarize: Hu, Paz and Zhang have derived an exact master equation for quantum Brownian motion in a general environment via path integral techniques. Their master equation provides a very useful tool to study the decoherence of a quantum system due to the interaction with its environment. In this paper, we give an alternative and elementary derivation of the Hu-Paz-Zhang master equation, which involves tracing the evolution equation for the Wigner function. We also discuss the master equation in some special cases.",0.4782608696],["quantum adiabatic algorithm of Farhi emph can find pareto-","Multiobjective Optimization in a Quantum Adiabatic Computer","summarize: In this work we present a quantum algorithm for multiobjective combinatorial optimization. We show how to map a convex combination of objective functions onto a Hamiltonian and then use that Hamiltonian to prove that the quantum adiabatic algorithm of Farhi \\emph can find Pareto-optimal solutions in finite time provided certain convex combinations of objectives are used and the underlying multiobjective problem meets certain restrictions.",0.0],["an Eulerian finite volume approach has been developed to simulate fully resolved hyperelastic","A conservative and non-dissipative Eulerian formulation for the simulation of soft solids in fluids","summarize: Soft solids in fluids find wide range of applications in science and engineering, especially in the study of biological tissues and membranes. In this study, an Eulerian finite volume approach has been developed to simulate fully resolved incompressible hyperelastic solids immersed in a fluid. We have adopted the recently developed reference map technique by Valkov et. al and assessed multiple improvements for this approach.These modifications maintain the numerical robustness of the solver and allow the simulations without any artificial viscosity in the solid regions . This has also resulted in eliminating the striations of the fluid-solid interface that was seen before and hence obviates the need for any additional routines to achieve a smooth interface. An approximate projection method has been used to project the velocity field onto a divergence free field. Cost and accuracy improvements of the modifications on the method have also been discussed.",0.1984202311],["the architecture is equipped with temporal-spatial feature extraction in encoder stage, feature","Sequential vessel segmentation via deep channel attention network","summarize: This paper develops a novel encoder-decoder deep network architecture which exploits the several contextual frames of 2D+t sequential images in a sliding window centered at current frame to segment 2D vessel masks from the current frame. The architecture is equipped with temporal-spatial feature extraction in encoder stage, feature fusion in skip connection layers and channel attention mechanism in decoder stage. In the encoder stage, a series of 3D convolutional layers are employed to hierarchically extract temporal-spatial features. Skip connection layers subsequently fuse the temporal-spatial feature maps and deliver them to the corresponding decoder stages. To efficiently discriminate vessel features from the complex and noisy backgrounds in the XCA images, the decoder stage effectively utilizes channel attention blocks to refine the intermediate feature maps from skip connection layers for subsequently decoding the refined features in 2D ways to produce the segmented vessel masks. Furthermore, Dice loss function is implemented to train the proposed deep network in order to tackle the class imbalance problem in the XCA data due to the wide distribution of complex background artifacts. Extensive experiments by comparing our method with other state-of-the-art algorithms demonstrate the proposed method's superior performance over other methods in terms of the quantitative metrics and visual validation. The source codes are at https:\/\/github.com\/Binjie-Qin\/SVS-net",0.0],["vertical bifacial panel reduces dust accumulation and provides two output peaks during the","Vertical Bifacial Solar Farms: Physics, Design, and Global Optimization","summarize: There have been sustained interest in bifacial solar cell technology since 1980s, with prospects of 30-50% increase in the output power from a stand-alone single panel. Moreover, a vertical bifacial panel reduces dust accumulation and provides two output peaks during the day, with the second peak aligned to the peak electricity demand. Recent commercialization and anticipated growth of bifacial panel market have encouraged a closer scrutiny of the integrated power-output and economic viability of bifacial solar farms, where mutual shading will erode some of the anticipated energy gain associated with an isolated, single panel. Towards that goal, in this paper we focus on geography-specific optimizations of ground mounted vertical bifacial solar farms for the entire world. For local irradiance, we combine the measured meteorological data with the clear-sky model. In addition, we consider the detailed effects of direct, diffuse, and albedo light. We assume the panel is configured into sub-strings with bypass-diodes. Based on calculated light collection and panel output, we analyze the optimum farm design for maximum yearly output at any given location in the world. Our results predict that, regardless of the geographical location, a vertical bifacial farm will yield 10-20% more energy than a traditional monofacial farm for a practical row-spacing of 2m . With the prospect of additional 5-20% energy gain from reduced soiling and tilt optimization, bifacial solar farm do offer a viable technology option for large-scale solar energy generation.",0.0769230769],["the algorithm is based on the interpretation of such a realization as a point in the","Counting realizations of Laman graphs on the sphere","summarize: We present an algorithm that computes the number of realizations of a Laman graph on a sphere for a general choice of the angles between the vertices. The algorithm is based on the interpretation of such a realization as a point in the moduli space of stable curves of genus zero with marked points, and on the explicit description, due to Keel, of the Chow ring of this space.",0.4390243902],["task-based programming models demonstrate efficiency in the development of scientific applications on modern high-performance platforms","Increasing the Degree of Parallelism Using Speculative Execution in Task-based Runtime Systems","summarize: Task-based programming models have demonstrated their efficiency in the development of scientific applications on modern high-performance platforms. They allow delegation of the management of parallelization to the runtime system , which is in charge of the data coherency, the scheduling, and the assignment of the work to the computational units. However, some applications have a limited degree of parallelism such that no matter how efficient the RS implementation, they may not scale on modern multicore CPUs. In this paper, we propose using speculation to unleash the parallelism when it is uncertain if some tasks will modify data, and we formalize a new methodology to enable speculative execution in a graph of tasks. This description is partially implemented in our new C++ RS called SPETABARU, which is capable of executing tasks in advance if some others are not certain to modify the data. We study the behavior of our approach to compute Monte Carlo and replica exchange Monte Carlo simulations.",0.2777777778],["study reveals a relation between performance and success in soccer competitions. results of","Quantifying the relation between performance and success in soccer","summarize: The availability of massive data about sports activities offers nowadays the opportunity to quantify the relation between performance and success. In this study, we analyze more than 6,000 games and 10 million events in six European leagues and investigate this relation in soccer competitions. We discover that a team's position in a competition's final ranking is significantly related to its typical performance, as described by a set of technical features extracted from the soccer data. Moreover we find that, while victory and defeats can be explained by the team's performance during a game, it is difficult to detect draws by using a machine learning approach. We then simulate the outcomes of an entire season of each league only relying on technical data, i.e. excluding the goals scored, exploiting a machine learning model trained on data from past seasons. The simulation produces a team ranking which is close to the actual ranking, suggesting that a complex systems' view on soccer has the potential of revealing hidden patterns regarding the relation between performance and success.",0.6470588235],["the considered kinetic models feature an opinion density depending on an additional microscopic variable, identified","Hydrodynamic models of preference formation in multi-agent societies","summarize: In this paper, we discuss the passage to hydrodynamic equations for kinetic models of opinion formation. The considered kinetic models feature an opinion density depending on an additional microscopic variable, identified with the personal preference. This variable describes an opinion-driven polarisation process, leading finally to a choice among some possible options, as it happens e.g. in referendums or elections. Like in the kinetic theory of rarefied gases, the derivation of hydrodynamic equations is essentially based on the computation of the local equilibrium distribution of the opinions from the underlying kinetic model. Several numerical examples validate the resulting model, shedding light on the crucial role played by the distinction between opinion and preference formation on the choice processes in multi-agent societies.",0.1],["Prediction of a sex","Cross Section Prediction for Inclusive Production of Z Boson in ","summarize: Prediction of ",0.0868869717],["the scheme is based on higher order interference between quantum down-converted light and classical coherent","HIgh-Noon States with High Flux of Photons Using coherent Beam Stimulated Non-Collinear Parametric Down Conversion","summarize: We show how to reach high fidelity NOON states with a high count rate inside optical interferometers. Recently it has been shown that by mixing squeezed and coherent light at a beamsplitter it is possible to generate NOON states of arbitrary N with a fidelity as high as 94%. ). The scheme is based on higher order interference between quantum down-converted light and classical coherent light. However, this requires optimizing the amplitude ratio of classical to quantum light thereby limiting the overall count rate for the interferometric super-resolution signal. We propose using coherent-beam-stimulated non-collinear down converted light as input to the interferometer. Our scheme is based on stimulation of non-collinear parametric down conversion by two-mode coherent light. We have somehow a better flexibility of choosing the amplitude ratio in generating NOON states. This enables super-resolution intensity exceeding the previous scheme by many orders of magnitude. Therefore we hope to improve the magnitude of N-fold super-resolution in quantum interferometry for arbitrary N by using bright light sources. We give some results for N=4 and 5.",0.1875],["optical nonlinearity in optical waveguides has found applications in optical switching, optical wavelength","Enhanced optical nonlinearities in air-cladding silicon pedestal waveguides","summarize: The third-order optical nonlinearity in optical waveguides has found applications in optical switching, optical wavelength conversion, optical frequency comb generation, and ultrafast optical signal processing. The development of an integrated waveguide platform with a high nonlinearity is therefore important for nonlinear integrated photonics. Here, we report the observation of an enhancement in the nonlinearity of an air-cladding silicon pedestal waveguide. We observe enhanced nonlinear spectral broadening compared to a conventional silicon-on-insulator waveguide. At the center wavelength of 1555 nm, the nonlinear-index coefficient of air-cladding silicon pedestal waveguide is measured to be about 5% larger than that of a conventional silicon-on-insulator waveguide. We observe enhanced spectral broadening from self-phase modulation of an optical pulse in the pedestal waveguide. The interaction of light with the confined acoustic phonons in the pedestal structure gives rise to a larger nonlinear-index coefficient. The experimental results agree well with the theoretical models.",0.3548387097],["deep hashing approaches fail to fully explore semantic correlations. dubbed deep coll","Deep Collaborative Discrete Hashing with Semantic-Invariant Structure","summarize: Existing deep hashing approaches fail to fully explore semantic correlations and neglect the effect of linguistic context on visual attention learning, leading to inferior performance. This paper proposes a dual-stream learning framework, dubbed Deep Collaborative Discrete Hashing , which constructs a discriminative common discrete space by collaboratively incorporating the shared and individual semantics deduced from visual features and semantic labels. Specifically, the context-aware representations are generated by employing the outer product of visual embeddings and semantic encodings. Moreover, we reconstruct the labels and introduce the focal loss to take advantage of frequent and rare concepts. The common binary code space is built on the joint learning of the visual representations attended by language, the semantic-invariant structure construction and the label distribution correction. Extensive experiments demonstrate the superiority of our method.",0.0],["the determination of the gravitofluid-static field required as initial field in forthcoming fluid-","Fluid statics of a self-gravitating perfect-gas isothermal sphere","summarize: We open the paper with introductory considerations describing the motivations of our long-term research plan targeting gravitomagnetism, illustrating the fluid-dynamics numerical test case selected for that purpose, that is, a perfect-gas sphere contained in a solid shell located in empty space sufficiently away from other masses, and defining the main objective of this study: the determination of the gravitofluid-static field required as initial field in forthcoming fluid-dynamics calculations. The determination of the gravitofluid-static field requires the solution of the isothermal-sphere Lane-Emden equation. We do not follow the habitual approach of the literature based on the prescription of the central density as boundary condition; we impose the gravitational field at the solid-shell internal wall. As the discourse develops, we point out differences and similarities between the literature's and our approach. We show that the nondimensional formulation of the problem hinges on a unique physical characteristic number that we call gravitational number because it gauges the self-gravity effects on the gas' fluid statics. We illustrate and discuss numerical results; some peculiarities, such as gravitational-number upper bound and multiple solutions, lead us to investigate the thermodynamics of the physical system, particularly entropy and energy, and preliminarily explore whether or not thermodynamic-stability reasons could provide justification for either selection or exclusion of multiple solutions. We close the paper with a summary of the present study in which we draw conclusions and describe future work.",0.2272727273],["the task of understanding a text and answering questions is not easy. the research is","Enhancing lexical-based approach with external knowledge for Vietnamese multiple-choice machine reading comprehension","summarize: Although Vietnamese is the 17th most popular native-speaker language in the world, there are not many research studies on Vietnamese machine reading comprehension , the task of understanding a text and answering questions about it. One of the reasons is because of the lack of high-quality benchmark datasets for this task. In this work, we construct a dataset which consists of 2,783 pairs of multiple-choice questions and answers based on 417 Vietnamese texts which are commonly used for teaching reading comprehension for elementary school pupils. In addition, we propose a lexical-based MRC method that utilizes semantic similarity measures and external knowledge sources to analyze questions and extract answers from the given text. We compare the performance of the proposed model with several baseline lexical-based and neural network-based models. Our proposed method achieves 61.81% by accuracy, which is 5.51% higher than the best baseline model. We also measure human performance on our dataset and find that there is a big gap between machine-model and human performances. This indicates that significant progress can be made on this task. The dataset is freely available on our website for research purposes.",0.2692307692],["model is used to depict flow of services in the Information Technology department of a government ministry","Process Description, Behavior, and Control","summarize: Modeling processes are the activities of capturing and representing processes and control of their dynamic behavior. Desired features of the model include capture of relevant aspects of a real phenomenon, understandability, and completeness of static and dynamic specifications. This paper proposes a diagrammatic language for engineering process modeling that provides an integration tool for capturing the static description of processes, framing their behaviors in terms of events, and utilizing the resultant model for controlling processes. Without loss of generality, the focus of the paper is on process modeling in the area of computer engineering, and specifically, on modeling of computer services. To demonstrate the viability of the method, the proposed model is applied to depicting flow of services in the Information Technology department of a government ministry.",0.0869565217],["weed classification system uses visual data to enable autonomous precision weeding without making prior assumptions about","A Rapidly Deployable Classification System using Visual Data for the Application of Precision Weed Management","summarize: In this work we demonstrate a rapidly deployable weed classification system that uses visual data to enable autonomous precision weeding without making prior assumptions about which weed species are present in a given field. Previous work in this area relies on having prior knowledge of the weed species present in the field. This assumption cannot always hold true for every field, and thus limits the use of weed classification systems based on this assumption. In this work, we obviate this assumption and introduce a rapidly deployable approach able to operate on any field without any weed species assumptions prior to deployment. We present a three stage pipeline for the implementation of our weed classification system consisting of initial field surveillance, offline processing and selective labelling, and automated precision weeding. The key characteristic of our approach is the combination of plant clustering and selective labelling which is what enables our system to operate without prior weed species knowledge. Testing using field data we are able to label 12.3 times fewer images than traditional full labelling whilst reducing classification accuracy by only 14%.",0.0],["the adiabatic limit procedure associates with every solution of Abelian Higgs","Seiberg-Witten theory as a complex version of Abelian Higgs model","summarize: The adiabatic limit procedure associates with every solution of Abelian Higgs model in dimensions a geodesic in the moduli space of static solutions. We show that the same procedure for Seiberg--Witten equations on 4-dimensional symplectic manifolds introduced by Taubes may be considered as a complex -dimensional version of the -dimensional picture. More precisely, the adiabatic limit procedure in the 4-dimensional case associates with a solution of Seiberg--Witten equations a pseudoholomorphic divisor which may be treated as a complex version of a geodesic in -dimensional case.",0.3636363636],["the human reasoning process is rarely a one-way process from an input leading to an output","Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs","summarize: The human reasoning process is seldom a one-way process from an input leading to an output. Instead, it often involves a systematic deduction by ruling out other possible outcomes as a self-checking mechanism. In this paper, we describe the design of a hybrid neural network for logical learning that is similar to the human reasoning through the introduction of an auxiliary input, namely the indicators, that act as the hints to suggest logical outcomes. We generate these indicators by digging into the hidden information buried underneath the original training data for direct or indirect suggestions. We used the MNIST data to demonstrate the design and use of these indicators in a convolutional neural network. We trained a series of such hybrid neural networks with variations of the indicators. Our results show that these hybrid neural networks are very robust in generating logical outcomes with inherently higher prediction accuracy than the direct use of the original input and output in apparent models. Such improved predictability with reassured logical confidence is obtained through the exhaustion of all possible indicators to rule out all illogical outcomes, which is not available in the apparent models. Our logical learning process can effectively cope with the unknown unknowns using a full exploitation of all existing knowledge available for learning. The design and implementation of the hints, namely the indicators, become an essential part of artificial intelligence for logical learning. We also introduce an ongoing application setup for this hybrid neural network in an autonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized grasping pose through logical learning.",0.1724137931],["numerical simulations of Nambu-Goto cosmic strings show that the loop distribution relax","Cosmic string loop production functions","summarize: Numerical simulations of Nambu-Goto cosmic strings in an expanding universe show that the loop distribution relaxes to an universal configuration, the so-called scaling regime, which is of power law shape on large scales. Precise estimations of the power law exponent are, however, still matter of debate while numerical simulations do not incorporate all the radiation and backreaction effects expected to affect the network dynamics at small scales. By using a Boltzmann approach, we show that the steepness of the loop production function with respect to loops size is associated with drastic changes in the cosmological loop distribution. For a scale factor varying as a~t^nu, we find that sub-critical loop production functions, having a Polchinski-Rocha exponent chi = \/2, are shown to be IR-physics dependent and this generically prevents the loop distribution to relax towards scaling. In the latter situation, we discuss the additional regularisations needed for convergence and show that, although a scaling regime can still be reached, the shape of the cosmological loop distribution is modified compared to the naive expectation. Finally, we discuss the implications of our findings.",0.0833333333],["a statistical pattern-recognition method is developed to deal with the uncertainty of driving","Statistical Pattern Recognition for Driving Styles Based on Bayesian Probability and Kernel Density Estimation","summarize: Driving styles have a great influence on vehicle fuel economy, active safety, and drivability. To recognize driving styles of path-tracking behaviors for different divers, a statistical pattern-recognition method is developed to deal with the uncertainty of driving styles or characteristics based on probability density estimation. First, to describe driver path-tracking styles, vehicle speed and throttle opening are selected as the discriminative parameters, and a conditional kernel density function of vehicle speed and throttle opening is built, respectively, to describe the uncertainty and probability of two representative driving styles, e.g., aggressive and normal. Meanwhile, a posterior probability of each element in feature vector is obtained using full Bayesian theory. Second, a Euclidean distance method is involved to decide to which class the driver should be subject instead of calculating the complex covariance between every two elements of feature vectors. By comparing the Euclidean distance between every elements in feature vector, driving styles are classified into seven levels ranging from low normal to high aggressive. Subsequently, to show benefits of the proposed pattern-recognition method, a cross-validated method is used, compared with a fuzzy logic-based pattern-recognition method. The experiment results show that the proposed statistical pattern-recognition method for driving styles based on kernel density estimation is more efficient and stable than the fuzzy logic-based method.",0.3411435553],["the largest dataset of optimized molecular geometries and electronic properties calculated by the PM","PubChemQC PM6: A dataset of 221 million molecules with optimized molecular geometries and electronic properties","summarize: We report on the largest dataset of optimized molecular geometries and electronic properties calculated by the PM6 method for 92.9% of the 91.2 million molecules cataloged in PubChem Compounds retrieved on Aug. 29, 2016. In addition to neutral states, we also calculated those for cationic, anionic, and spin flipped electronic states of 56.2%, 49.7%, and 41.3% of the molecules, respectively. Thus, the grand total calculated is 221 million molecules. The dataset is available at http:\/\/pubchemqc.riken.jp\/pm6_dataset.html under the Creative Commons Attribution 4.0 International license.",0.5237228136],["An","Completely positive semidefinite rank","summarize: An ",0.0],["in 1968, Golomb and Welch conjectured that there does not exist perfect","On the nonexistence of linear perfect Lee codes","summarize: In 1968, Golomb and Welch conjectured that there does not exist perfect Lee code in ",0.25],["the Foley Artist is responsible for creating an overlay soundtrack that helps the movie come alive for the","AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent Videos with Deep Learning","summarize: In movie productions, the Foley Artist is responsible for creating an overlay soundtrack that helps the movie come alive for the audience. This requires the artist to first identify the sounds that will enhance the experience for the listener thereby reinforcing the Directors's intention for a given scene. In this paper, we present AutoFoley, a fully-automated deep learning tool that can be used to synthesize a representative audio track for videos. AutoFoley can be used in the applications where there is either no corresponding audio file associated with the video or in cases where there is a need to identify critical scenarios and provide a synthesized, reinforced soundtrack. An important performance criterion of the synthesized soundtrack is to be time-synchronized with the input video, which provides for a realistic and believable portrayal of the synthesized sound. Unlike existing sound prediction and generation architectures, our algorithm is capable of precise recognition of actions as well as inter-frame relations in fast moving video clips by incorporating an interpolation technique and Temporal Relationship Networks . We employ a robust multi-scale Recurrent Neural Network associated with a Convolutional Neural Network for a better understanding of the intricate input-to-output associations over time. To evaluate AutoFoley, we create and introduce a large scale audio-video dataset containing a variety of sounds frequently used as Foley effects in movies. Our experiments show that the synthesized sounds are realistically portrayed with accurate temporal synchronization of the associated visual inputs. Human qualitative testing of AutoFoley show over 73% of the test subjects considered the generated soundtrack as original, which is a noteworthy improvement in cross-modal research in sound synthesis.",0.2592592593],["this article provides an accessible illustration of the measurement approach. we apply it to a quantum","Classicalization by phase space measurements","summarize: This article provides an accessible illustration of the measurement approach to the study of the quantum-classical transition suitable for beginning graduate students. As an example, we apply it to a quantum system with a general quadratic Hamiltonian and obtain the exact solution of the dynamics for an arbitrary measurement strength.",0.2692307692],["a large number of resources are available mostly in English. a corpus called Personal","TxPI-u: A Resource for Personality Identification of Undergraduates","summarize: Resources such as labeled corpora are necessary to train automatic models within the natural language processing field. Historically, a large number of resources regarding a broad number of problems are available mostly in English. One of such problems is known as Personality Identification where based on a psychological model , the goal is to find the traits of a subject's personality given, for instance, a text written by the same subject. In this paper we introduce a new corpus in Spanish called Texts for Personality Identification . This corpus will help to develop models to automatically assign a personality trait to an author of a text document. Our corpus, TxPI-u, contains information of 416 Mexican undergraduate students with some demographics information such as, age, gender, and the academic program they are enrolled. Finally, as an additional contribution, we present a set of baselines to provide a comparison scheme for further research.",0.3333333333],["optical radiation from a neutron star merger originates from radioactive decay of unstable nucli","Radioactive ","summarize: Gravitational waves and electromagnetic radiations from a neutron star merger were discovered on 17 August 2017. Multiband observations of the optical transient have identified brightness and spectrum features broadly consistent with theoretical predictions. According to the theoretical model, the optical radiation from a neutron star merger originates from the radioactive decay of unstable nuclides freshly synthesized in the merger ejecta. In about a day the ejecta transits from an optically thick state to an optically thin state due to its subrelativistic expansion. Hence, we expect that about a day after the merger, the gamma-ray photons produced by radioactive decays start to escape from the ejecta and make it bright in the MeV band. In this paper, we study the features of the radioactive gamma-ray emission from a neutron star merger, including the brightness and the spectrum, and discuss the observability of the gamma-ray emission. We find that more than ",0.08],["isogeometric analysis is a high-order discretization method for boundary value problems","Fast multigrid solvers for conforming and non-conforming multi-patch Isogeometric Analysis","summarize: Isogeometric Analysis is a high-order discretization method for boundary value problems that uses a number of degrees of freedom which is as small as for a low-order method. Standard isogeometric discretizations require a global parameterization of the computational domain. In non-trivial cases, the domain is decomposed into patches having separate parameterizations and separate discretization spaces. If the discretization spaces agree on the interfaces between the patches, the coupling can be done in a conforming way. Otherwise, non-conforming discretizations are required. The author and his coworkers have previously introduced multigrid solvers for Isogeometric Analysis for the conforming case. In the present paper, these results are extended to the non-conforming case. Moreover, it is shown that the multigrid solves get even more powerful if the proposed smoother is combined with a Gauss-Seidel smoother.",0.3157894737],["theorem describes the convergence of an infinite array of variants of SGD.","SGD: General Analysis and Improved Rates","summarize: We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form mini-batches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.",0.1],["SPMS interleaves partitioning of sample sort with merging. it sorts sample sort","Resource Oblivious Sorting on Multicores","summarize: We present a deterministic sorting algorithm, SPMS , that interleaves the partitioning of a sample sort with merging. Sequentially, it sorts ",0.0],["a method is proposed for obtaining diffuse field measurements in untreated environments. a","A Novel Method for Obtaining Diffuse Field Measurements for Microphone Calibration","summarize: We propose a straightforward and cost-effective method to perform diffuse soundfield measurements for calibrating the magnitude response of a microphone array. Typically, such calibration is performed in a diffuse soundfield created in reverberation chambers, an expensive and time-consuming process. A method is proposed for obtaining diffuse field measurements in untreated environments. First, a closed-form expression for the spatial correlation of a wideband signal in a diffuse field is derived. Next, we describe a practical procedure for obtaining the diffuse field response of a microphone array in the presence of a non-diffuse soundfield by the introduction of random perturbations in the microphone location. Experimental spatial correlation data obtained is compared with the theoretical model, confirming that it is possible to obtain diffuse field measurements in untreated environments with relatively few loudspeakers. A 30 second test signal played from 4-8 loudspeakers is shown to be sufficient in obtaining a diffuse field measurement using the proposed method. An Eigenmike is then successfully calibrated at two different geographical locations.",0.4782608696],["the transiting exoplanet survey satellite satellites are a list of 1822","TESS Habitable Zone Star Catalog","summarize: We present the Transiting Exoplanet Survey Satellite Habitable Zone Stars Catalog, a list of 1822 nearby stars with a TESS magnitude brighter than T = 12 and reliable distances from Gaia DR2, around which the NASA's TESS mission can detect transiting planets, which receive Earth-like irradiation. For all those stars TESS is sensitive down to 2 Earth radii transiting planets during one transit. For 408 stars TESS can detect such planets down to 1 Earth size during one transit. For 1690 stars, TESS has the sensitivity to detect planets down to 1.6 times Earth-size, a commonly used limit for rocky planets in the literature, receiving Earth-analog irradiation. We select stars from the TESS Candidate Target List, based on TESS Input Catalog Version 7. We update their distances using Gaia Data Release 2, and determine whether the stars will be observed for long enough during the 2 year prime mission to probe their Earth equivalent orbital distance for transiting planets. We discuss the subset of 227 stars for which TESS can probe the full extent of the Habitable Zone, the full region around a star out to about a Mars-equivalent orbit. Observing the TESS Habitable Zone Catalog Stars will also give us deeper insight into the occurrence rate of planets, out to Earth-analog irradiation as well as in the Habitable Zone, especially around cool stars. We present the stars by decreasing angular separation of the 1AU equivalent distance to provide insights into which stars to prioritize for ground-based follow-up observations with upcoming extremely large telescopes.",0.2941176471],["spectral variability studies of the ultra-luminous X-ray source. spec","Exploring the Spectral Variability of the Ultra-luminous X-ray source M81 X-6 with Suzaku and XMM-Newton","summarize: We present X-ray spectral variability studies of the ultra-luminous X-ray source M81 X--6 using and observations performed during 2001--2015. The spectra were first fitted by a standard multi-temperature disk and a thermal Comptonization component which revealed spectral variability where the primary distinction is the change in the optical depth of the Comptonizing component, similar to what has been observed for other ULXs. We also fitted the spectra with a general relativistic accretion disk emission and a power-law component and found that it can reproduce a large part but not all of the spectral variability of the source. The parameters for the black hole mass and spin were found to be degenerate, but the high spin and larger mass solutions provided near Eddington accretion rates consistent with the assumptions of the model. The spectral variation is found to be driven by accretion rate changes leading to three different spectral classes. Thus, our results suggest the possibility of a dominant relativistic disk emission component for some of the spectral states of the source.",0.2053668476],["regularizing OT problems with entropy leads to faster computations and better differentiation using","Regularized Optimal Transport is Ground Cost Adversarial","summarize: Regularizing the optimal transport problem has proven crucial for OT theory to impact the field of machine learning. For instance, it is known that regularizing OT problems with entropy leads to faster computations and better differentiation using the Sinkhorn algorithm, as well as better sample complexity bounds than classic OT. In this work we depart from this practical perspective and propose a new interpretation of regularization as a robust mechanism, and show using Fenchel duality that any convex regularization of OT can be interpreted as ground cost adversarial. This incidentally gives access to a robust dissimilarity measure on the ground space, which can in turn be used in other applications. We propose algorithms to compute this robust cost, and illustrate the interest of this approach empirically.",0.0],["multi-year digital forensic evidence backlogs have become commonplace in law enforcement agencies across","Automated Artefact Relevancy Determination from Artefact Metadata and Associated Timeline Events","summarize: Case-hindering, multi-year digital forensic evidence backlogs have become commonplace in law enforcement agencies throughout the world. This is due to an ever-growing number of cases requiring digital forensic investigation coupled with the growing volume of data to be processed per case. Leveraging previously processed digital forensic cases and their component artefact relevancy classifications can facilitate an opportunity for training automated artificial intelligence based evidence processing systems. These can significantly aid investigators in the discovery and prioritisation of evidence. This paper presents one approach for file artefact relevancy determination building on the growing trend towards a centralised, Digital Forensics as a Service paradigm. This approach enables the use of previously encountered pertinent files to classify newly discovered files in an investigation. Trained models can aid in the detection of these files during the acquisition stage, i.e., during their upload to a DFaaS system. The technique generates a relevancy score for file similarity using each artefact's filesystem metadata and associated timeline events. The approach presented is validated against three experimental usage scenarios.",0.0769230769],["a stochastic latent space will improve the generalization ability of an encoder-","Improving Generalization of Deep Networks for Inverse Reconstruction of Image Sequences","summarize: Deep learning networks have shown state-of-the-art performance in many image reconstruction problems. However, it is not well understood what properties of representation and learning may improve the generalization ability of the network. In this paper, we propose that the generalization ability of an encoder-decoder network for inverse reconstruction can be improved in two means. First, drawing from analytical learning theory, we theoretically show that a stochastic latent space will improve the ability of a network to generalize to test data outside the training distribution. Second, following the information bottleneck principle, we show that a latent representation minimally informative of the input data will help a network generalize to unseen input variations that are irrelevant to the output reconstruction. Therefore, we present a sequence image reconstruction network optimized by a variational approximation of the information bottleneck principle with stochastic latent space. In the application setting of reconstructing the sequence of cardiac transmembrane potential from bodysurface potential, we assess the two types of generalization abilities of the presented network against its deterministic counterpart. The results demonstrate that the generalization ability of an inverse reconstruction network can be improved by stochasticity as well as the information bottleneck.",0.2105263158],["generative models show improved performance over contemporary models for image generation on MNSIT.","Entropy-regularized Optimal Transport Generative Models","summarize: We investigate the use of entropy-regularized optimal transport cost in developing generative models to learn implicit distributions. Two generative models are proposed. One uses EOT cost directly in an one-shot optimization problem and the other uses EOT cost iteratively in an adversarial game. The proposed generative models show improved performance over contemporary models for image generation on MNSIT.",0.0],["the system is a multi-input-multiple-output single user transmission system","Receive Spatial Modulation for Massive MIMO Systems","summarize: In this paper, we consider the downlink of a massive multiple-input-multiple-output single user transmission system operating in the millimeter wave outdoor narrowband channel environment. We propose a novel receive spatial modulation architecture aimed to reduce the power consumption at the user terminal, while attaining a significant throughput. The energy consumption reduction is obtained through the use of analog devices , which reduces the number of radio frequency chains and analog-to-digital-converters . The base station transmits spatial and modulation symbols per channel use. We show that the optimal spatial symbol detector is a threshold detector that can be implemented by using one bit ADC. We derive closed form expressions for the detection threshold at different signal-to-noise-ratio regions showing that a simple threshold can be obtained at high SNR and its performance approaches the exact threshold. We derive expressions for the average bit error probability in the presence and absence of the threshold estimation error showing that a small number of pilot symbols is needed. A performance comparison is done between the proposed system and fully digital MIMO showing that a suitable constellation selection can reduce the performance gap.",0.1538461538],["the hype about sensorimotor learning is now reaching high fever. we present an open-","Multisensory Learning Framework for Robot Drumming","summarize: The hype about sensorimotor learning is currently reaching high fever, thanks to the latest advancement in deep learning. In this paper, we present an open-source framework for collecting large-scale, time-synchronised synthetic data from highly disparate sensory modalities, such as audio, video, and proprioception, for learning robot manipulation tasks. We demonstrate the learning of non-linear sensorimotor mappings for a humanoid drumming robot that generates novel motion sequences from desired audio data using cross-modal correspondences. We evaluate our system through the quality of its cross-modal retrieval, for generating suitable motion sequences to match desired unseen audio or video sequences.",0.0714285714],["an Eulerian finite volume approach has been developed to simulate fully resolved hyperelastic","A conservative and non-dissipative Eulerian formulation for the simulation of soft solids in fluids","summarize: Soft solids in fluids find wide range of applications in science and engineering, especially in the study of biological tissues and membranes. In this study, an Eulerian finite volume approach has been developed to simulate fully resolved incompressible hyperelastic solids immersed in a fluid. We have adopted the recently developed reference map technique by Valkov et. al and assessed multiple improvements for this approach.These modifications maintain the numerical robustness of the solver and allow the simulations without any artificial viscosity in the solid regions . This has also resulted in eliminating the striations of the fluid-solid interface that was seen before and hence obviates the need for any additional routines to achieve a smooth interface. An approximate projection method has been used to project the velocity field onto a divergence free field. Cost and accuracy improvements of the modifications on the method have also been discussed.",0.1984202311],["colon cancer is one of the most common types of cancer. treatment is planned to depend on","Machine learning approach for segmenting glands in colon histology images using local intensity and texture features","summarize: Colon Cancer is one of the most common types of cancer. The treatment is planned to depend on the grade or stage of cancer. One of the preconditions for grading of colon cancer is to segment the glandular structures of tissues. Manual segmentation method is very time-consuming, and it leads to life risk for the patients. The principal objective of this project is to assist the pathologist to accurate detection of colon cancer. In this paper, the authors have proposed an algorithm for an automatic segmentation of glands in colon histology using local intensity and texture features. Here the dataset images are cropped into patches with different window sizes and taken the intensity of those patches, and also calculated texture-based features. Random forest classifier has been used to classify this patch into different labels. A multilevel random forest technique in a hierarchical way is proposed. This solution is fast, accurate and it is very much applicable in a clinical setup.",0.1923076923],["we propose a kernel classifier based on the optimal scoring framework. we provide theoretical","Sparse Feature Selection in Kernel Discriminant Analysis via Optimal Scoring","summarize: We consider the two-group classification problem and propose a kernel classifier based on the optimal scoring framework. Unlike previous approaches, we provide theoretical guarantees on the expected risk consistency of the method. We also allow for feature selection by imposing structured sparsity using weighted kernels. We propose fully-automated methods for selection of all tuning parameters, and in particular adapt kernel shrinkage ideas for ridge parameter selection. Numerical studies demonstrate the superior classification performance of the proposed approach compared to existing nonparametric classifiers.",0.3181818182],["to every half-translation surface, we associate a saddle connection graph. we prove","Affine equivalence and saddle connection graphs of half-translation surfaces","summarize: To every half-translation surface, we associate a saddle connection graph, which is a subgraph of the arc graph. We prove that every isomorphism between two saddle connection graphs is induced by an affine homeomorphism between the underlying half-translation surfaces. We also investigate the automorphism group of the saddle connection graph, and the corresponding quotient graph.",0.5454545455],["Kolmogorov stochasticity parameter enables to quantify randomness in number sequences","Probing the correlations in composite signals","summarize: The technique of degree of randomness is used to model the correlations in sequences containing various subsignals and noise. Kolmogorov stochasticity parameter enables to quantify the randomness in number sequences and hence appears as an efficient tool to distinguish the signals. Numerical experiments for a broad class of composite signals of regular and random properties enable to obtain the qualitative and quantitative criteria for the behavior of the descriptor depending on the input parameters typical to astrophysical signals.",0.0909090909],["a large majority of polar coronal X-ray jets are made by an","Onset of the magnetic explosion in solar polar coronal X-ray jets","summarize: We follow up on the Sterling et al discovery that nearly all polar coronal X-ray jets are made by an explosive eruption of closed magnetic field carrying a miniature filament in its core. In the same X-ray and EUV movies used by Sterling et al , we examine the onset and growth of the driving magnetic explosion in 15 of the 20 jets that they studied. We find evidence that: in a large majority of polar X-ray jets, the runaway internal\/tether-cutting reconnection under the erupting minifilament flux rope starts after both the minifilaments rise and the spire-producing external\/breakout reconnection have started and in a large minority, before the eruption starts there is a current sheet between the explosive closed field and the ambient open field, and the eruption starts with breakout reconnection at that current sheet. The variety of event sequences in the eruptions supports the idea that the magnetic explosions that make polar X-ray jets work the same way as the much larger magnetic explosions that make a flare and coronal mass ejection . That idea, and recent observations indicating that magnetic flux cancelation is the fundamental process that builds the field in and around the pre-jet mininfilament and triggers that fields jet-driving explosion, together suggest that flux cancelation inside the magnetic arcade that explodes in a flare\/CME eruption is usually the fundamental process that builds the explosive field in the core of the arcade and triggers that fields explosion.",0.5],["in ENCI, we transform the complicated relation of a cause-effect pair into a","A Kernel Embedding-based Approach for Nonstationary Causal Model Inference","summarize: Although nonstationary data are more common in the real world, most existing causal discovery methods do not take nonstationarity into consideration. In this letter, we propose a kernel embedding-based approach, ENCI, for nonstationary causal model inference where data are collected from multiple domains with varying distributions. In ENCI, we transform the complicated relation of a cause-effect pair into a linear model of variables of which observations correspond to the kernel embeddings of the cause-and-effect distributions in different domains. In this way, we are able to estimate the causal direction by exploiting the causal asymmetry of the transformed linear model. Furthermore, we extend ENCI to causal graph discovery for multiple variables by transforming the relations among them into a linear nongaussian acyclic model. We show that by exploiting the nonstationarity of distributions, both cause-effect pairs and two kinds of causal graphs are identifiable under mild conditions. Experiments on synthetic and real-world data are conducted to justify the efficacy of ENCI over major existing methods.",0.5],["we propose a novel autoML scheme by leveraging the alternating direction method of multipliers","An ADMM Based Framework for AutoML Pipeline Configuration","summarize: We study the AutoML problem of automatically configuring machine learning pipelines by jointly selecting algorithms and their appropriate hyper-parameters for all steps in supervised learning pipelines. This black-box optimization with mixed integer & continuous variables is a challenging problem. We propose a novel AutoML scheme by leveraging the alternating direction method of multipliers . The proposed framework is able to decompose the optimization problem into easier sub-problems that have a reduced number of variables and circumvent the challenge of mixed variable categories, and incorporate black-box constraints along-side the black-box optimization objective. We empirically evaluate the flexibility , effectiveness ,and unique capability of our proposed scheme on a collection of binary classification data sets from UCI ML& OpenML repositories. We observe that on an average our framework provides significant gains in comparison to other AutoML frameworks , highlighting the practical advantages of this framework.",0.1666666667],["the increasing demand of transmit rates and various requirements of quality of services in vehicular communication scenarios call","Secure Performance Analysis and Optimization for FD-NOMA Vehicular Communications","summarize: Vehicle-to-vehicle communication appeals to increasing research interest as a result of its applications to provide safety information as well as infotainment services. The increasing demand of transmit rates and various requirements of quality of services in vehicular communication scenarios call for the integration of V2V communication systems and potential techniques in the future wireless communications, such as full duplex and non-orthogonal multiple access which enhance spectral efficiency and provide massive connectivity. However, the large amount of data transmission and user connectivity give rise to the concern of security issues and personal privacy. In order to analyze the security performance of V2V communications, we introduce a cooperative NOMA V2V system model with an FD relay. This paper focuses on the security performance of the FD-NOMA based V2V system on the physical layer perspective. We first derive several analytical results of the ergodic secrecy capacity. Then, we propose a secrecy sum rate optimization scheme utilizing the instantaneous channel state information , which is formulated as a non-convex optimization problem. Based on the differential structure of the non-convex constraints, the original problem is approximated and solved by a series of convex optimization problems. Simulation results validate the analytical results and the effectiveness of the secrecy sum rate optimization algorithm.",0.037037037],["a quantum motion on shape space is a theory of absolute space and time. the","Quantum Motion on Shape Space and the Gauge Dependent Emergence of Dynamics and Probability in Absolute Space and Time","summarize: Relational formulations of classical mechanics and gravity have been developed by Julian Barbour and collaborators. Crucial to these formulations is the notion of shape space. We indicate here that the metric structure of shape space allows one to straightforwardly define a quantum motion, a Bohmian mechanics, on shape space. We show how this motion gives rise to the more or less familiar theory in absolute space and time. We find that free motion on shape space, when lifted to configuration space, becomes an interacting theory. Many different lifts are possible corresponding in fact to different choices of gauges. Taking the laws of Bohmian mechanics on shape space as physically fundamental, we show how the theory can be statistically analyzed by using conditional wave functions, for subsystems of the universe, represented in terms of absolute space and time.",0.4874089426],["the biomarker study of how various systemic and cardiovascular diseases affect the retinal vessels","Multi-Task Neural Networks with Spatial Activation for Retinal Vessel Segmentation and Artery\/Vein Classification","summarize: Retinal artery\/vein classification plays a critical role in the clinical biomarker study of how various systemic and cardiovascular diseases affect the retinal vessels. Conventional methods of automated A\/V classification are generally complicated and heavily depend on the accurate vessel segmentation. In this paper, we propose a multi-task deep neural network with spatial activation mechanism that is able to segment full retinal vessel, artery and vein simultaneously, without the pre-requirement of vessel segmentation. The input module of the network integrates the domain knowledge of widely used retinal preprocessing and vessel enhancement techniques. We specially customize the output block of the network with a spatial activation mechanism, which takes advantage of a relatively easier task of vessel segmentation and exploits it to boost the performance of A\/V classification. In addition, deep supervision is introduced to the network to assist the low level layers to extract more semantic information. The proposed network achieves pixel-wise accuracy of 95.70% for vessel segmentation, and A\/V classification accuracy of 94.50%, which is the state-of-the-art performance for both tasks on the AV-DRIVE dataset. Furthermore, we have also tested the model performance on INSPIRE-AVR dataset, which achieves a skeletal A\/V classification accuracy of 91.6%.",0.0625],["the ferromagnetic ground state of the Co2FeAl is energetically","Phase stability and the effect of lattice distortions on electronic properties and half-metallic ferromagnetism of Co2FeAl Heusler alloy: An ab initio study","summarize: Density functional theory calculations within the generalized gradient approximation are employed to study the ground state of Co2FeAl. Various magnetic configurations are considered to find out its most stable phase. The ferromagnetic ground state of the Co2FeAl is energetically observed with an optimized lattice constant of 5.70 . Thereafter, the system was subjected under uniform and non-uniform strains to see their effects on spin polarization and half-metallicity. The effect of spin orbit coupling is considered in the present study. Half-metallicity is only retained under uniform strains started from 0 to +4%, and dropped rapidly from 90% to 16% for the negative strains started from -1% to -6%. We find that the present system is much sensitive under tetragonal distortions as half-metallicity is preserved only for the cubic case. The main reason for the loss of half-metallicity is due to the shift of the bands with respect to the Fermi level. We also discuss the influence of these results on spintronics devices.",0.1072168559],["paper elaborates on gauge prepotentials and linearised super-Cotton","Off-shell massive N=1 supermultiplets in three dimensions","summarize: This paper is mainly concerned with the construction of new off-shell higher spin N=1 supermultiplets in three spacetime dimensions. We elaborate on the gauge prepotentials and linearised super-Cotton tensors for higher spin N=1 superconformal geometry and propose compensating superfields required to formulate off-shell massless higher spin supermultiplets. The corresponding gauge-invariant actions are worked out explicitly using an auxiliary oscillator realisation. We construct, for the first time, off-shell massive higher spin supermultiplets. The gauge-invariant actions for these supermultiplets are obtained by adding Chern-Simons like mass terms to the actions for the massless supermultiplets. For each of the massive gravitino and supergravity multiplets, we propose two dually equivalent formulations.",0.1111111111],["we introduce a set of new conditions under which we prove the average cost optimality inequality via","Average Cost Optimality Inequality for Markov Decision Processes with Borel Spaces and Universally Measurable Policies","summarize: We consider average-cost Markov decision processes with Borel state and action spaces and universally measurable policies. For the nonnegative cost model and an unbounded cost model with a Lyapunov-type stability character, we introduce a set of new conditions under which we prove the average cost optimality inequality via the vanishing discount factor approach. Unlike most existing results on the ACOI, our result does not require any compactness and continuity conditions on the MDPs. Instead, the main idea is to use the almost-uniform-convergence property of a pointwise convergent sequence of measurable functions as asserted in Egoroff's theorem. Our conditions are formulated in order to exploit this property. Among others, we require that for each state, on selected subsets of actions at that state, the state transition stochastic kernel is majorized by finite measures. We combine this majorization property of the transition kernel with Egoroff's theorem to prove the ACOI.",0.25],["two-neutrino double electron capture is a second-order Weak process","First observation of two-neutrino double electron capture in ","summarize: Two-neutrino double electron capture is a second-order Weak process with predicted half-lives that surpass the age of the Universe by many orders of magnitude. Until now, indications for ",0.5454545455],["ourmodel is an ensemble of frame-level models withtest-time augmentation. we","BERT for Large-scale Video Segment Classification with Test-time Augmentation","summarize: This paper presents our approach to the third YouTube-8M video understanding competition that challenges par-ticipants to localize video-level labels at scale to the pre-cise time in the video where the label actually occurs. Ourmodel is an ensemble of frame-level models such as GatedNetVLAD and NeXtVLAD and various BERT models withtest-time augmentation. We explore multiple ways to ag-gregate BERT outputs as video representation and variousways to combine visual and audio information. We proposetest-time augmentation as shifting video frames to one leftor right unit, which adds variety to the predictions and em-pirically shows improvement in evaluation metrics. We firstpre-train the model on the 4M training video-level data, andthen fine-tune the model on 237K annotated video segment-level data. We achieve MAP@100K 0.7871 on private test-ing video segment data, which is ranked 9th over 283 teams.",0.0],["a reasonable approach to determine casual structures is to base merely on the low-order","Recovering Causal Structures from Low-Order Conditional Independencies","summarize: One of the common obstacles for learning causal models from data is that high-order conditional independence relationships between random variables are difficult to estimate. Since CI tests with conditioning sets of low order can be performed accurately even for a small number of observations, a reasonable approach to determine casual structures is to base merely on the low-order CIs. Recent research has confirmed that, e.g. in the case of sparse true causal models, structures learned even from zero- and first-order conditional independencies yield good approximations of the models. However, a challenging task here is to provide methods that faithfully explain a given set of low-order CIs. In this paper, we propose an algorithm which, for a given set of conditional independencies of order less or equal to ",0.2083333333],["the graph theory concept is designed to create a relaxation of the vertex degeneracy","High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality","summarize: We develop the first parallel graph coloring heuristics with strong theoretical guarantees on work and depth and coloring quality. The key idea is to design a relaxation of the vertex degeneracy order, a well-known graph theory concept, and to color vertices in the order dictated by this relaxation. This introduces a tunable amount of parallelism into the degeneracy ordering that is otherwise hard to parallelize. This simple idea enables significant benefits in several key aspects of graph coloring. For example, one of our algorithms ensures polylogarithmic depth and a bound on the number of used colors that is superior to all other parallelizable schemes, while maintaining work-efficiency. In addition to provable guarantees, the developed algorithms have competitive run-times for several real-world graphs, while almost always providing superior coloring quality. Our degeneracy ordering relaxation is of separate interest for algorithms outside the context of coloring.",0.2608695652],["differential privacy and local differential privacy are frameworks to protect sensitive information in data collections","Utility-Preserving Privacy Mechanisms for Counting Queries","summarize: Differential privacy and local differential privacy are frameworks to protect sensitive information in data collections. They are both based on obfuscation. In DP the noise is added to the result of queries on the dataset, whereas in LPD the noise is added directly on the individual records, before being collected. The main advantage of LPD with respect to DP is that it does not need to assume a trusted third party. The main disadvantage is that the trade-off between privacy and utility is usually worse than in DP, and typically to retrieve reasonably good statistics from the locally sanitized data it is necessary to have a huge collection of them. In this paper, we focus on the problem of estimating counting queries from collections of noisy answers, and we propose a variant of LDP based on the addition of geometric noise. Our main result is that the geometric noise has a better statistical utility than other LPD mechanisms from the literature.",0.1],["correctly classified examples tend to have greater maximum softmax probabilities than erroneously","A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks","summarize: We consider the two related problems of detecting if an example is misclassified or out-of-distribution. We present a simple baseline that utilizes probabilities from softmax distributions. Correctly classified examples tend to have greater maximum softmax probabilities than erroneously classified and out-of-distribution examples, allowing for their detection. We assess performance by defining several tasks in computer vision, natural language processing, and automatic speech recognition, showing the effectiveness of this baseline across all. We then show the baseline can sometimes be surpassed, demonstrating the room for future research on these underexplored detection tasks.",0.0833333333],["existing spatial representation languages are not sufficient for describing spatial configurations used in complex tasks.","From Spatial Relations to Spatial Configurations","summarize: Spatial Reasoning from language is essential for natural language understanding. Supporting it requires a representation scheme that can capture spatial phenomena encountered in language as well as in images and videos. Existing spatial representations are not sufficient for describing spatial configurations used in complex tasks. This paper extends the capabilities of existing spatial representation languages and increases coverage of the semantic aspects that are needed to ground the spatial meaning of natural language text in the world. Our spatial relation language is able to represent a large, comprehensive set of spatial concepts crucial for reasoning and is designed to support the composition of static and dynamic spatial configurations. We integrate this language with the Abstract Meaning Representation annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with the fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole.",0.0],["software development enterprises are under constant pressure to improve management techniques and development processes. these disciplines must","Exploring the relations between net benefits of IT projects and CIOs' perception of quality of software development disciplines","summarize: Software development enterprises are under consistent pressure to improve their management techniques and development processes. These are comprised of several disciplines like requirements acquisition, design, coding, testing, etc. that must be continuously improved and individually tailored to suit specific software development project. This paper presents an evaluation approach that enables the enterprises to increase development process net benefits by improving disciplines' quality and increasing developers' satisfaction. Our approach builds on Kano's model of quality. Based on an empirical study of top 1000 enterprises from Slovenia we find that application of software development methodologies in individual development disciplines significantly relates to net benefits of IT projects. The results show that different types of Kano quality are present in individual disciplines. Enterprises should be cautious when altering must-be quality disciplines like testing or deployment as they can significantly disrupt the established routines, cause great dissatisfaction between developers and significantly reduce benefits. On the other hand, changing the attractive quality disciplines like requirements acquisition can notably increase developers' satisfaction and benefits but is less likely to disrupt the established routines.",0.2828619432],["matrix factorization is at the heart of many machine learning algorithms. it allows us to de","Predicting Pairwise Relations with Neural Similarity Encoders","summarize: Matrix factorization is at the heart of many machine learning algorithms, for example, dimensionality reduction or recommender systems relying on collaborative filtering. Understanding a singular value decomposition of a matrix as a neural network optimization problem enables us to decompose large matrices efficiently while dealing naturally with missing values in the given matrix. But most importantly, it allows us to learn the connection between data points' feature vectors and the matrix containing information about their pairwise relations. In this paper we introduce a novel neural network architecture termed Similarity Encoder , which is designed to simultaneously factorize a given target matrix while also learning the mapping to project the data points' feature vectors into a similarity preserving embedding space. This makes it possible to, for example, easily compute out-of-sample solutions for new data points. Additionally, we demonstrate that SimEc can preserve non-metric similarities and even predict multiple pairwise relations between data points at once.",0.25],["boundary conditions are a common or special boundary universality class. boundary conditions are e","Why boundary conditions do not generally determine the universality class for boundary critical behavior","summarize: Interacting field theories for systems with a free surface frequently exhibit distinct universality classes of boundary critical behaviors depending on gross surface properties. The boundary condition satisfied by the continuum field theory on some scale may or may not be decisive for the universality class that applies. In many recent papers on boundary field theories it is taken for granted that Dirichlet or Neumann boundary conditions decide whether the ordinary or special boundary universality class is observed. While true in a certain sense for the Dirichlet boundary condition, this is not the case for the Neumann boundary condition. Building on results that have been worked out in the 1980s, but have not always been appropriately appreciated in the literature, the subtle role of boundary conditions and their scale dependence is elucidated and the question of whether or not they determine the observed boundary universality class is discussed.",0.6388888889],["two-way relay is potentially an effective approach to spectrum sharing and aggregation.","On the DoF of Two-way ","summarize: Two-way relay is potentially an effective approach to spectrum sharing and aggregation by allowing simultaneous bidirectional transmissions between source-destinations pairs. This paper studies the two-way ",0.0],["a single model, TartanVO, can be generalized to real-world dataset","TartanVO: A Generalizable Learning-based VO","summarize: We present the first learning-based visual odometry model, which generalizes to multiple datasets and real-world scenarios and outperforms geometry-based methods in challenging scenes. We achieve this by leveraging the SLAM dataset TartanAir, which provides a large amount of diverse synthetic data in challenging environments. Furthermore, to make our VO model generalize across datasets, we propose an up-to-scale loss function and incorporate the camera intrinsic parameters into the model. Experiments show that a single model, TartanVO, trained only on synthetic data, without any finetuning, can be generalized to real-world datasets such as KITTI and EuRoC, demonstrating significant advantages over the geometry-based methods on challenging trajectories. Our code is available at https:\/\/github.com\/castacks\/tartanvo.",0.3529411765],["proposed approach consists in adding a saliency branch to an existing CNN architecture.","Saliency for Fine-grained Object Recognition in Domains with Scarce Training Data","summarize: This paper investigates the role of saliency to improve the classification accuracy of a Convolutional Neural Network for the case when scarce training data is available. Our approach consists in adding a saliency branch to an existing CNN architecture which is used to modulate the standard bottom-up visual features from the original image input, acting as an attentional mechanism that guides the feature extraction process. The main aim of the proposed approach is to enable the effective training of a fine-grained recognition model with limited training samples and to improve the performance on the task, thereby alleviating the need to annotate large dataset. % The vast majority of saliency methods are evaluated on their ability to generate saliency maps, and not on their functionality in a complete vision pipeline. Our proposed pipeline allows to evaluate saliency methods for the high-level task of object recognition. We perform extensive experiments on various fine-grained datasets under different conditions and show that saliency can considerably improve the network's performance, especially for the case of scarce training data. Furthermore, our experiments show that saliency methods that obtain improved saliency maps also translate to saliency methods that yield improved performance gains when applied in an object recognition pipeline.",0.4347826087],["sputtered grown Ti films on Si3N4\/Si substrate have been reported","Interface study of thermally driven chemical kinetics involved in Ti\/Si3N4 based metal-substrate assembly by X-ray photoelectron spectroscopy","summarize: Diffusion mediated interaction in metal-substrate assembly during high temperature annealing leads to possible formation of new composite materials. Here, sputtered grown Ti films on Si3N4\/Si substrate has been reported to produce titanium nitride and silicide based binary composites while undergoing high vacuum annealing process at temperatures 650C and above. Diffusion of thermally decomposed Si and N atoms from Si3N4 and their subsequent chemical reaction with Ti have been probed by X-ray photo electron spectroscopy. For annealing at 800C and above, most of the Si atoms show preferences to stay in elemental form rather than developing silicide phase with Ti. Whereas at lower annealing temperature, silicide becomes the dominant phase for decomposed Si atoms. However, N atoms react promptly with Ti and form TiN which appears as the majority phase for each of the studied annealing temperature. Further, the nitride and silicide phases across the films have been compared quantitatively for various annealing temperature and the maximum silicide formation is observed for the sample annealed at 780C. Finally, the thermally anchored metal-substrate interaction mechanism can be exploited to fabricate disordered superconducting TiN films where TiSi2 and Si can be used to tune the level of disorder by altering the annealing temperature.",0.1489755911],["the graph theory concept is designed to create a relaxation of the vertex degeneracy","High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality","summarize: We develop the first parallel graph coloring heuristics with strong theoretical guarantees on work and depth and coloring quality. The key idea is to design a relaxation of the vertex degeneracy order, a well-known graph theory concept, and to color vertices in the order dictated by this relaxation. This introduces a tunable amount of parallelism into the degeneracy ordering that is otherwise hard to parallelize. This simple idea enables significant benefits in several key aspects of graph coloring. For example, one of our algorithms ensures polylogarithmic depth and a bound on the number of used colors that is superior to all other parallelizable schemes, while maintaining work-efficiency. In addition to provable guarantees, the developed algorithms have competitive run-times for several real-world graphs, while almost always providing superior coloring quality. Our degeneracy ordering relaxation is of separate interest for algorithms outside the context of coloring.",0.2608695652],["ethnicity is a key demographic attribute of human beings. it plays a vital","Learned Features are better for Ethnicity Classification","summarize: Ethnicity is a key demographic attribute of human beings and it plays a vital role in automatic facial recognition and have extensive real world applications such as Human Computer Interaction ; demographic based classification; biometric based recognition; security and defense to name a few. In this paper we present a novel approach for extracting ethnicity from the facial images. The proposed method makes use of a pre trained Convolutional Neural Network to extract the features and then Support Vector Machine with linear kernel is used as a classifier. This technique uses translational invariant hierarchical features learned by the network, in contrast to previous works, which use hand crafted features such as Local Binary Pattern ; Gabor etc. Thorough experiments are presented on ten different facial databases which strongly suggest that our approach is robust to different expressions and illuminations conditions. Here we consider ethnicity classification as a three class problem including Asian, African-American and Caucasian. Average classification accuracy over all databases is 98.28%, 99.66% and 99.05% for Asian, African-American and Caucasian respectively.",0.4074074074],["imageCLEFmed Caption task is to develop a system that labels radiology images with","A Competitive Deep Neural Network Approach for the ImageCLEFmed Caption 2020 Task","summarize: The aim of ImageCLEFmed Caption task is to develop a system that automatically labels radiology images with relevant medical concepts. We describe our Deep Neural Network based approach for tackling this problem. On the challenge test set of 3,534 radiology images, our system achieves an F1 score of 0.375 and ranks high, 12th among all systems that were successfully submitted to the challenge, whereby we only rely on the provided data sources and do not use any external medical knowledge or ontologies, or pretrained models from other medical image repositories or application domains.",0.3],["work is the first attempt to evaluate and compare felderated learning and split neural networks in","End-to-End Evaluation of Federated Learning and Split Learning for Internet of Things","summarize: This work is the first attempt to evaluate and compare felderated learning and split neural networks in real-world IoT settings in terms of learning performance and device implementation overhead. We consider a variety of datasets, different model architectures, multiple clients, and various performance metrics. For learning performance, which is specified by the model accuracy and convergence speed metrics, we empirically evaluate both FL and SplitNN under different types of data distributions such as imbalanced and non-independent and identically distributed data. We show that the learning performance of SplitNN is better than FL under an imbalanced data distribution, but worse than FL under an extreme non-IID data distribution. For implementation overhead, we end-to-end mount both FL and SplitNN on Raspberry Pis, and comprehensively evaluate overheads including training time, communication overhead under the real LAN setting, power consumption and memory usage. Our key observations are that under IoT scenario where the communication traffic is the main concern, the FL appears to perform better over SplitNN because FL has the significantly lower communication overhead compared with SplitNN, which empirically corroborate previous statistical analysis. In addition, we reveal several unrecognized limitations about SplitNN, forming the basis for future research.",0.25],["we consider random random.","Local semicircle law for random regular graphs","summarize: We consider random ",0.1574555176],["we investigate the non-linear response and energy absorption in bulk silicon irradi","Ultrafast energy absorption and photoexcitation of bulk plasmon in crystalline silicon subjected to intense near-infrared ultrashort laser pulses","summarize: We investigate the non-linear response and energy absorption in bulk silicon irradiated by intense 12-fs near-infrared laser pulses. Depending on the laser intensity, we distinguish two regimes of non-linear absorption of the laser energy: for low intensities, energy deposition and photoionization involve perturbative three-photon transition through the direct bandgap of silicon. For laser intensities near and above 10",0.3465889484],["full-reference image quality assessment is to predict the quality of an image. the goal","A combined full-reference image quality assessment approach based on convolutional activation maps","summarize: The goal of full-reference image quality assessment is to predict the quality of an image as perceived by human observers with using its pristine, reference counterpart. In this study, we explore a novel, combined approach which predicts the perceptual quality of a distorted image by compiling a feature vector from convolutional activation maps. More specifically, a reference-distorted image pair is run through a pretrained convolutional neural network and the activation maps are compared with a traditional image similarity metric. Subsequently, the resulted feature vector is mapped onto perceptual quality scores with the help of a trained support vector regressor. A detailed parameter study is also presented in which the design choices of the proposed method is reasoned. Furthermore, we study the relationship between the amount of training images and the prediction performance. Specifically, it is demonstrated that the proposed method can be trained with few amount of data to reach high prediction performance. Our best proposal - ActMapFeat - is compared to the state-of-the-art on six publicly available benchmark IQA databases, such as KADID-10k, TID2013, TID2008, MDID, CSIQ, and VCL-FER. Specifically, our method is able to significantly outperform the state-of-the-art on these benchmark databases.",0.3],["we develop an adaptive protocol that conceals the secret key. the network is a state","Covert Secret Key Generation with an Active Warden","summarize: We investigate the problem of covert and secret key generation over a state-dependent discrete memoryless channel with one-way public discussion in which an adversary, the warden, may arbitrarily choose the channel state. We develop an adaptive protocol that, under conditions that we explicitly specify, not only allows the transmitter and the legitimate receiver to exchange a secret key but also conceals from the active warden whether the protocol is being run. When specialized to passive adversaries that do not control the channel state, we partially characterize the covert secret key capacity. In particular, the covert secret key capacity is sometimes equal to the covert capacity of the channel, so that secrecy comes for free.",0.1739130435],["many biological and neural systems can be seen as networks of interacting periodic processes. synchro","Understanding the dynamics of biological and neural oscillator networks through exact mean-field reductions: a review","summarize: Many biological and neural systems can be seen as networks of interacting periodic processes. Importantly, their functionality depends on the emerging collective dynamics of the network. Synchrony of oscillations is one of the most prominent examples of such collective behavior and has been associated both with function and dysfunction. Understanding how network structure and interactions, as well as the microscopic properties of individual units, shape the emerging collective dynamics is critical to find factors that lead to malfunction. However, many biological systems such as the brain consist of a large number of dynamical units. Hence, their analysis has either relied on simplified heuristic models on a coarse scale, or the analysis comes at a huge computational cost. Here we review recently introduced approaches, known as the Ott-Antonsen and Watanabe-Strogatz reductions, allowing one to simplify the analysis by bridging small and large scales. Thus, reduced model equations are obtained that exactly describe the collective dynamics for each subpopulation in the oscillator network via few collective variables only. The resulting equations are next-generation models: Rather than being heuristic, they exactly link microscopic and macroscopic descriptions and therefore accurately capture microscopic properties of the underlying system. At the same time, they are sufficiently simple to analyze without great computational effort. In the last decade, these reduction methods have become instrumental in understanding how network structure and interactions shape the collective dynamics and the emergence of synchrony. We review this progress based on concrete examples and outline possible limitations. Finally, we discuss how linking the reduced models with experimental data can guide the way towards the development of new treatment approaches, for example, for neurological disease.",0.3333333333],["the lattice operations of join and meet were defined for set partitions in the nineteenth","A Graph-theoretic Method to Define any Boolean Operation on Partitions","summarize: The lattice operations of join and meet were defined for set partitions in the nineteenth century, but no new logical operations on partitions were defined and studied during the twentieth century. Yet there is a simple and natural graph-theoretic method presented here to define any n-ary Boolean operation on partitions. An equivalent closure-theoretic method is also defined. In closing, the question is addressed of why it took so long for all Boolean operations to be defined for partitions.",0.15],["generative models are deep generative latent variable models. the learned generative model capture","Characterizing and Avoiding Problematic Global Optima of Variational Autoencoders","summarize: Variational Auto-encoders are deep generative latent variable models consisting of two components: a generative model that captures a data distribution p by transforming a distribution p over latent space, and an inference model that infers likely latent codes for each data point . Recent work shows that traditional training methods tend to yield solutions that violate modeling desiderata: the learned generative model captures the observed data distribution but does so while ignoring the latent codes, resulting in codes that do not represent the data ; Kim et al. ); the aggregate of the learned latent codes does not match the prior p. This mismatch means that the learned generative model will be unable to generate realistic data with samples from p; Tomczak and Welling ). In this paper, we demonstrate that both issues stem from the fact that the global optima of the VAE training objective often correspond to undesirable solutions. Our analysis builds on two observations: the generative model is unidentifiable - there exist many generative models that explain the data equally well, each with different properties and bias in the VAE objective - the VAE objective may prefer generative models that explain the data poorly but have posteriors that are easy to approximate. We present a novel inference method, LiBI, mitigating the problems identified in our analysis. On synthetic datasets, we show that LiBI can learn generative models that capture the data distribution and inference models that better satisfy modeling assumptions when traditional methods struggle to do so.",0.0],["fusion algorithm integrates improved version of both, A* algorithm and artificial potential field method","New Fusion Algorithm provides an alternative approach to Robotic Path planning","summarize: For rapid growth in technology and automation, human tasks are being taken over by robots as robots have proven to be better with both speed and precision. One of the major and widespread usages of these robots is in the industrial businesses, where they are employed to carry massive loads in and around work areas. As these working environments might not be completely localized and could be dynamically changing, new approaches must be evaluated to guarantee a crash-free way of performing duties. This paper presents a new and efficient fusion algorithm for solving the path planning problem in a custom 2D environment. This fusion algorithm integrates an improved and optimized version of both, A* algorithm and the Artificial potential field method. Firstly, an initial or preliminary path is planned in the environmental model by adopting the A* algorithm. The heuristic function of this A* algorithm is optimized and improved according to the environmental model. This is followed by selecting and saving the key nodes in the initial path. Lastly, on the basis of these saved key nodes, path smoothing is done by artificial potential field method. Our simulation results carried out using Python viz. libraries indicate that the new fusion algorithm is feasible and superior in smoothness performance and can satisfy as a time-efficient and cheaper alternative to conventional A* strategies of path planning.",0.6583333333],["Quasi-set theory is based on a theory of a q","Individuality, quasi-sets and the double-slit experiment","summarize: Quasi-set theory ",0.3888888889],["line simplification involves removing small things while retaining large ones. the line simplification","Line Simplification","summarize: As an important practice of map generalization, the aim of line simplification is to reduce the number of points without destroying the essential shape or the salient character of a cartographic curve. This subject has been well-studied in the literature. This entry attempts to introduce how line simplification can be guided by fractal geometry, or the recurring scaling pattern of far more small things than large ones. The line simplification process involves nothing more than removing small things while retaining large ones based on head\/tail breaks.",0.0],["authors have recently defined the R'enyi information dimension rate rate.","On the Information Dimension of Multivariate Gaussian Processes","summarize: The authors have recently defined the R\\'enyi information dimension rate ",0.0833333333],["network pruning can reduce test-time resource requirements. but large networks are resource hungry at both","Picking Winning Tickets Before Training by Preserving Gradient Flow","summarize: Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation . We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.",0.0],["multi-goal RL objective is based on weighted entropy","Maximum Entropy-Regularized Multi-Goal Reinforcement Learning","summarize: In Multi-Goal Reinforcement Learning, an agent learns to achieve multiple goals with a goal-conditioned policy. During learning, the agent first collects the trajectories into a replay buffer, and later these trajectories are selected randomly for replay. However, the achieved goals in the replay buffer are often biased towards the behavior policies. From a Bayesian perspective, when there is no prior knowledge about the target goal distribution, the agent should learn uniformly from diverse achieved goals. Therefore, we first propose a novel multi-goal RL objective based on weighted entropy. This objective encourages the agent to maximize the expected return, as well as to achieve more diverse goals. Secondly, we developed a maximum entropy-based prioritization framework to optimize the proposed objective. For evaluation of this framework, we combine it with Deep Deterministic Policy Gradient, both with or without Hindsight Experience Replay. On a set of multi-goal robotic tasks of OpenAI Gym, we compare our method with other baselines and show promising improvements in both performance and sample-efficiency.",0.0],["asteroseismology is a complementary tool to study DM. the stellar","Asteroseismic constraints on Asymmetric Dark Matter: Light particles with an effective spin-dependent coupling","summarize: So far, direct detection searches have come empty handed in their quest for Dark Matter . Meanwhile, asteroseismology arises as a complementary tool to study DM, as its accumulation in a star can enhance energy transport, by providing a conduction mechanism, producing significant changes in the stellar structure during the course of the star's evolution. The stellar core, particularly affected by the presence of DM, can be investigated through precise asteroseismic diagnostics. We modelled three stars including DM energy transport: the Sun, a slightly less massive and much older star, KIC 7871531 , and a more massive and younger one, KIC 8379927 . We considered both the case of Weakly Interactive Massive Particles, albeit with a low annihilation, and the case of Asymmetric DM for which the number of trapped particles in the star can be much greater. By analysing these models with asteroseismic separation ratios weighted towards the core, we found indications limiting the effective spin-dependent DM-proton coupling for masses of a few GeV. This independent result is very close to the most recent and most stringent direct detection DM constraints.",0.2469394069],["optimisation and control framework enables storage system to combine services with peak shaving. we adopt","Optimal Combination of Frequency Control and Peak Shaving with Battery Storage Systems","summarize: Combining revenue streams by providing multiple services with battery storage systems increases profitability and enhances the investment case. In this work, we present a novel optimisation and control framework that enables a storage system to optimally combine the provision of primary frequency control services with peak shaving of a consumption profile. We adopt a dynamic programming framework to connect the daily bidding in frequency control markets with the longer term peak shaving objective: reducing the maximum consumption peak over an entire billing period. The framework also allows to aggregate frequency control capacity of multiple batteries installed at different sites, creating synergies when the consumption profile peaks occur on different times. Using a case study of two batteries at two industrial sites, we show that the presented approach increases net profit of the batteries significantly compared to using the batteries for only peak shaving or frequency control.",0.1875],["a synthesis technique of heat treatment is developed to grow 2D graphene oxide sheet","Facile synthesis of 2D graphene oxide sheet enveloping ultrafine 1D LiMn2O4 as interconnected framework to enhance cathodic property for Li-ion battery","summarize: Cubic spinel lithium manganese oxide has been able to attract a great deal of attention over the years as a promising cathode material for large scale lithium ion batteries. Here a facile hydrothermal route followed by solid state reaction is developed using as grown ultrafine alpha-MnO2 nanorods to prepare one dimensional LiMn2O4 with 10-50nm diameters. To enhance the cathodic property of these nanorods, a unique synthesis technique of heat treatment is developed to grow 2D graphene oxide sheet enveloping 1D LiMn2O4 as interconnected framework. This nanocomposite 3D porous cathode exhibits a high specific charge capacity of 130mAh\/g at 0.05C rate and Coulombic efficiency of about 98% after 100 cycles in the potential window of 3.5 to 4.3V versus Li\/Li+ with promising initial charge capacity retention of about 87%, and outstanding structural stability even after 100 cycles. Enhancement in the lithiation and delithiation processes leading to improved performance is likely to have its origin in the 2D conducting graphene oxide sheets. It allows for decreasing the Mn dissolution, improve the electron conductivity and reduce the Li-ion path diffusion inside the favourable morphology and crystallinity of the ultrafine 1D LiMn2O4 nanorods, giving rise to a promising cathode nanocomposite.",0.4043537731],["tin-selenide and tin-sulfide classes undergo","Electronic, vibrational, and electron-phonon coupling properties in SnSe","summarize: The tin-selenide and tin-sulfide classes of materials undergo multiple structural transitions under high pressure leading to periodic lattice distortions, superconductivity, and topologically non-trivial phases, yet a number of controversies exist regarding the structural transformations in these systems. We perform first-principles calculations within the framework of density functional theory and a careful comparison of our results with available experiments on SnSe",0.1097623272],["triboelectrification in conducting materials can be explained by electron transfer between different","Atomistic Field Theory for contact electrification of dielectrics","summarize: The triboelectrification of conducting materials can be explained by electron transfer between different Fermi levels. However, triboelectrification in dielectrics is poorly understood. The surface dipole formations are shown to be caused by the contact-induced surface lattice deformations. An Atomistic Field Theory based formulation is utilized to calculate the distribution of the polarization, electric and potential fields. The induced fields are considered as the driving force for charge transfer. The simulation results show that a MgO\/BaTiO3 tribopair can generate up to 104 V\/cm^2, which is comprable with the data in the published literature.",0.0],["the BGNRJ-H exhibits high ZT values of 9.65 and 5.55","Bilayer graphene nanoribbons junction with aligned holes exhibiting high ZT values","summarize: We investigate the thermoelectric performance of armchair graphene nanoribbon , bilayer GNRs junction and BGNRJ with holes by the first principles calculation with non-equilibrium Green function. It is found that the BGNRJ-H exhibits high ZT values of 9.65 and 5.55 at 300K. The reason of these significantly larger ZT values than previously observed has been calculated due to reduced thermal conductivity and enhanced electrical conductivity. The low thermal conductance comes from the van der waals interaction between two graphene layers. The increased electrical conductivity can be attributed to the coupling effect of aligned holes in both layers. It is found from analysis results that the electron transmission of the BGNRJ-H is much stronger than a normal BGNRJ, which gives rise to the higher electrical conductance and outstanding ZT values.",0.2714512254],["graphene is a promising material for applications in spintronics requiring long distance","Optospintronics in graphene via proximity coupling","summarize: The observation of micron size spin relaxation makes graphene a promising material for applications in spintronics requiring long distance spin communication. However, spin dependent scatterings at the contact\/graphene interfaces affect the spin injection efficiencies and hence prevent the material from achieving its full potential. While this major issue could be eliminated by nondestructive direct optical spin injection schemes, graphenes intrinsically low spin orbit coupling strength and optical absorption place an obstacle in their realization. We overcome this challenge by creating sharp artificial interfaces between graphene and WSe2 monolayers. Application of a circularly polarized light activates the spin polarized charge carriers in the WSe2 layer due to its spin coupled valley selective absorption. These carriers diffuse into the superjacent graphene layer, transport over a 3.5 um distance, and are finally detected electrically using BN\/Co contacts in a non local geometry. Polarization dependent measurements confirm the spin origin of the non local signal.",0.3043478261],["'neural speed reading' can model natural language by'reading' input token","Neural Speed Reading with Structural-Jump-LSTM","summarize: Recurrent neural networks can model natural language by sequentially 'reading' input tokens and outputting a distributed representation of each token. Due to the sequential nature of RNNs, inference time is linearly dependent on the input length, and all inputs are read regardless of their importance. Efforts to speed up this inference, known as 'neural speed reading', either ignore or skim over part of the input. We present Structural-Jump-LSTM: the first neural speed reading model to both skip and jump text during inference. The model consists of a standard LSTM and two agents: one capable of skipping single words when reading, and one capable of exploiting punctuation structure , sentence end symbols , or end of text markers) to jump ahead after reading a word. A comprehensive experimental evaluation of our model against all five state-of-the-art neural reading models shows that Structural-Jump-LSTM achieves the best overall floating point operations reduction , while keeping the same accuracy or even improving it compared to a vanilla LSTM that reads the whole text.",0.0],["previous joint sGGM estimators fail to use existing knowledge. sGGM","A Fast and Scalable Joint Estimator for Integrating Additional Knowledge in Learning Multiple Related Sparse Gaussian Graphical Models","summarize: We consider the problem of including additional knowledge in estimating sparse Gaussian graphical models from aggregated samples, arising often in bioinformatics and neuroimaging applications. Previous joint sGGM estimators either fail to use existing knowledge or cannot scale-up to many tasks under a high-dimensional situation. In this paper, we propose a novel \\underlineoint \\underlinelementary \\underlinestimator incorporating additional \\underlinenowledge to infer multiple related sparse Gaussian Graphical models from large-scale heterogeneous data. Using domain knowledge as weights, we design a novel hybrid norm as the minimization objective to enforce the superposition of two weighted sparsity constraints, one on the shared interactions and the other on the task-specific structural patterns. This enables JEEK to elegantly consider various forms of existing knowledge based on the domain at hand and avoid the need to design knowledge-specific optimization. JEEK is solved through a fast and entry-wise parallelizable solution that largely improves the computational efficiency of the state-of-the-art ",0.0345637665],["research has opened up new directions of research for identification and containment of fake news. research","Combating Fake News: A Survey on Identification and Mitigation Techniques","summarize: The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.",0.04],["the Newton limit of gravity is studied in the presence of Lorentz-violating gravitation","Testing local Lorentz invariance with short-range gravity","summarize: The Newton limit of gravity is studied in the presence of Lorentz-violating gravitational operators of arbitrary mass dimension. The linearized modified Einstein equations are obtained and the perturbative solutions are constructed and characterized. We develop a formalism for data analysis in laboratory experiments testing gravity at short range and demonstrate that these tests provide unique sensitivity to deviations from local Lorentz invariance.",0.1666666667],["quadratic M-convex functions are a generalization of valuated mat","The quadratic M-convexity testing problem","summarize: M-convex functions, which are a generalization of valuated matroids, play a central role in discrete convex analysis. Quadratic M-convex functions constitute a basic and important subclass of M-convex functions, which has a close relationship with phylogenetics as well as valued constraint satisfaction problems. In this paper, we consider the quadratic M-convexity testing problem , which is the problem of deciding whether a given quadratic function on ",0.2352941176],["this second part of the series treats spin.","Uniform energy bound and Morawetz estimate for extreme components of spin fields in the exterior of a slowly rotating Kerr black hole II: linearized gravity","summarize: This second part of the series treats spin ",0.0447873631],["we study the conventional MCC where tasks are offloaded to the cloud through a wireless access","Multi-user Multi-task Offloading and Resource Allocation in Mobile Cloud Systems","summarize: We consider a general multi-user Mobile Cloud Computing system where each mobile user has multiple independent tasks. These mobile users share the computation and communication resources while offloading tasks to the cloud. We study both the conventional MCC where tasks are offloaded to the cloud through a wireless access point, and MCC with a computing access point , where the CAP serves both as the network access gateway and a computation service provider to the mobile users. We aim to jointly optimize the offloading decisions of all users as well as the allocation of computation and communication resources, to minimize the overall cost of energy, computation, and delay for all users. The optimization problem is formulated as a non-convex quadratically constrained quadratic program, which is NP-hard in general. For the case without a CAP, an efficient approximate solution named MUMTO is proposed by using separable semidefinite relaxation , followed by recovery of the binary offloading decision and optimal allocation of the communication resource. To solve the more complicated problem with a CAP, we further propose an efficient three-step algorithm named MUMTO-C comprising of generalized MUMTO SDR with CAP, alternating optimization, and sequential tuning, which always computes a locally optimal solution. For performance benchmarking, we further present numerical lower bounds of the minimum system cost with and without the CAP. By comparison with this lower bound, our simulation results show that the proposed solutions for both scenarios give nearly optimal performance under various parameter settings, and the resultant efficient utilization of a CAP can bring substantial cost benefit.",0.1739130435],["haptic device can provide kinesthetic feedback to the user's index finger","Effects of Different Hand-Grounding Locations on Haptic Performance With a Wearable Kinesthetic Haptic Device","summarize: Grounding of kinesthetic feedback against a user's hand can increase the portability and wearability of a haptic device. However, the effects of different hand-grounding locations on haptic perception of a user are unknown. In this letter, we investigate the effects of three different hand-grounding locations-back of the hand, proximal phalanx of the index finger, and middle phalanx of the index finger-on haptic perception using a newly designed wearable haptic device. The novel device can provide kinesthetic feedback to the user's index finger in two directions: along the finger-axis and in the finger's flexion-extension movement direction. We measure users' haptic perception for each grounding location through a psychophysical experiment for each of the two feedback directions. Results show that among the studied locations, grounding at proximal phalanx has a smaller average just noticeable difference for both feedback directions, indicating a more sensitive haptic perception. The realism of the haptic feedback, based on user ratings, was the highest with grounding at the middle phalanx for feedback along the finger axis, and at the proximal phalanx for feedback in the flexion-extension direction. Users identified the haptic feedback as most comfortable with grounding at the back of the hand for feedback along the finger axis and at the proximal phalanx for feedback in the flexion-extension direction. These findings show that the choice of grounding location has a significant impact on the user's haptic perception and qualitative experience. The results provide insights for designing next-generation wearable hand-grounded kinesthetic devices to achieve better haptic performance and user experience in virtual reality and teleoperated robotic applications.",0.0634416989],["proposed method is compared with other iterative regularization methods. it is used to","The Averaged Kaczmarz Iteration for Solving Inverse Problems","summarize: We introduce a new iterative regularization method for solving inverse problems that can be written as systems of linear or non-linear equations in Hilbert spaces. The proposed averaged Kaczmarz method can be seen as a hybrid method between the Landweber and the Kaczmarz method. As the Kaczmarz method, the proposed method only requires evaluation of one direct and one adjoint sub-problem per iterative update. On the other, similar to the Landweber iteration, it uses an average over previous auxiliary iterates which increases stability. We present a convergence analysis of the AVEK iteration. Further, detailed numerical studies are presented for a tomographic image reconstruction problem, namely the limited data problem in photoacoustic tomography. Thereby, the AVEK is compared with other iterative regularization methods including standard Landweber and Kaczmarz iterations, as well as recently proposed accelerated versions based on error minimizing relaxation strategies.",0.0],["hash tables are ubiquitous in computer science for efficient access to large datasets. but there","A Faster Algorithm for Cuckoo Insertion and Bipartite Matching in Large Graphs","summarize: Hash tables are ubiquitous in computer science for efficient access to large datasets. However, there is always a need for approaches that offer compact memory utilisation without substantial degradation of lookup performance. Cuckoo hashing is an efficient technique of creating hash tables with high space utilisation and offer a guaranteed constant access time. We are given ",0.125],["a posteriori error estimator is randomized to a random random right-hand side","Randomized residual-based error estimators for parametrized equations","summarize: We propose a randomized a posteriori error estimator for reduced order approximations of parametrized differential equations. The error estimator has several important properties: the effectivity is close to unity with prescribed lower and upper bounds at specified high probability; the estimator does not require the calculation of stability constants; the online cost to evaluate the a posteriori error estimator is commensurate with the cost to find the reduced order approximation; the probabilistic bounds extend to many queries with only modest increase in cost. To build this estimator, we first estimate the norm of the error with a Monte-Carlo estimator using Gaussian random vectors whose covariance is chosen according to the desired error measure, e.g. user-defined norms or quantity of interest. Then, we introduce a dual problem with random right-hand side the solution of which allows us to rewrite the error estimator in terms of the residual of the original equation. In order to have a fast-to-evaluate estimator, model order reduction methods can be used to approximate the random dual solutions. Here, we propose a greedy algorithm that is guided by a scalar quantity of interest depending on the error estimator. Numerical experiments on a multi-parametric Helmholtz problem demonstrate that this strategy yields rather low-dimensional reduced dual spaces.",0.5862068966],["SR technique is implemented in a sensitive magnetometer. it is used to detect nuclear","High Resolution Magnetic Resonance Spectroscopy Using Solid-State Spins","summarize: We demonstrate a synchronized readout technique for spectrally selective detection of oscillating magnetic fields with sub-millihertz resolution, using coherent manipulation of solid state spins. The SR technique is implemented in a sensitive magnetometer ) based on nitrogen vacancy centers in diamond, and used to detect nuclear magnetic resonance signals from liquid-state samples. We obtain NMR spectral resolution ~3 Hz, which is nearly two orders of magnitude narrower than previously demonstrated with NV based techniques, using a sample volume of ~1 picoliter. This is the first application of NV-detected NMR to sense Boltzmann-polarized nuclear spin magnetization, and the first to observe chemical shifts and J-couplings.",0.2],["study deals with the problem of pricing compound options. an analytic formula for compound options","Pricing compound and extendible options under mixed fractional Brownian motion with jumps","summarize: This study deals with the problem of pricing compound options when the underlying asset follows a mixed fractional Brownian motion with jumps. An analytic formula for compound options is derived under the risk neutral measure. Then, these results are applied to value extendible options. Moreover, some special cases of the formula are discussed and numerical results are provided.",0.35],["a deep learning algorithm with self-supervision is proposed in this paper: SAR2","SAR2SAR: a semi-supervised despeckling algorithm for SAR images","summarize: Speckle reduction is a key step in many remote sensing applications. By strongly affecting synthetic aperture radar images, it makes them difficult to analyse. Due to the difficulty to model the spatial correlation of speckle, a deep learning algorithm with self-supervision is proposed in this paper: SAR2SAR. Multi-temporal time series are leveraged and the neural network learns to restore SAR images by only looking at noisy acquisitions. To this purpose, the recently proposed noise2noise framework has been employed. The strategy to adapt it to SAR despeckling is presented, based on a compensation of temporal changes and a loss function adapted to the statistics of speckle. A study with synthetic speckle noise is presented to compare the performances of the proposed method with other state-of-the-art filters. Then, results on real images are discussed, to show the potential of the proposed algorithm. The code is made available to allow testing and reproducible research in this field.",0.3888888889],["monotonic formulas are linear combinations of the von Neumann entropies. they","Monotonicity Under Local Operations: Linear Entropic Formulas","summarize: All correlation measures, classical and quantum, must be monotonic under local operations. In this paper, we characterize monotonic formulas that are linear combinations of the von Neumann entropies associated with the quantum state of a physical system that has n parts. We show that these formulas form a polyhedral convex cone, which we call the monotonicity cone, and enumerate its facets. We illustrate its structure and prove that it is equivalent to the cone of monotonic formulas implied by strong subadditivity. We explicitly compute its extremal rays for n up to 5. We also consider the symmetric monotonicity cone, in which the formulas are required to be invariant under subsystem permutations. We describe this cone fully for all n. In addition, we show that these results hold even when states and operations are constrained to be classical.",0.0],["mass discrepancy between lensing and baryonic mass is larger when acceleration of the","Mass Discrepancy-Acceleration Relation in Einstein Rings","summarize: We study the Mass Discrepancy-Acceleration Relation of 57 elliptical galaxies by their Einstein rings from the Sloan Lens ACS Survey . The mass discrepancy between the lensing mass and the baryonic mass derived from population synthesis is larger when the acceleration of the elliptical galaxy lenses is smaller. The MDAR is also related to surface mass density discrepancy. At the Einstein ring, these lenses belong to high-surface-mass density galaxies. Similarly, we find that the discrepancy between the lensing and stellar surface mass density is small. It is consistent with the recent discovery of dynamical surface mass density discrepancy in disk galaxies where the discrepancy is smaller when surface density is larger. We also find relativistic modified Newtonian dynamics can naturally explain the MDAR and surface mass density discrepancy in 57 Einstein rings. Moreover, the lensing mass, the dynamical mass and the stellar mass of these galaxies are consistent with each other in relativistic MOND.",0.0625],["the detection of the first electromagnetic counterpart to the binary neutron star merger remnant GW1708","Probing the magnetic field in the GW170817 outflow using H.E.S.S. observations","summarize: The detection of the first electromagnetic counterpart to the binary neutron star merger remnant GW170817 established the connection between short ",0.3333333333],["apelin exerts a positive inotropic effect in humans. it is an important","The hypotensive effect of activated apelin receptor is correlated with \\b-arrestin recruitment","summarize: The apelinergic system is an important player in the regulation of both vascular tone and cardiovascular function, making this physiological system an attractive target for drug development for hypertension, heart failure and ischemic heart disease. Indeed, apelin exerts a positive inotropic effect in humans whilst reducing peripheral vascular resistance. In this study, we investigated the signaling pathways through which apelin exerts its hypotensive action. We synthesized a series of apelin-13 analogs whereby the C-terminal Phe13 residue was replaced by natural or unnatural amino acids. In HEK293 cells expressing APJ, we evaluated the relative efficacy of these compounds to activate Gi1 and GoA G-proteins, recruit \\b-arrestins 1 and 2 , and inhibit cAMP production. Calculating the transduction ratio for each pathway allowed us to identify several analogs with distinct signaling profiles. Furthermore, we found that these analogs delivered i.v. to Sprague-Dawley rats exerted a wide range of hypotensive responses. Indeed, two compounds lost their ability to lower blood pressure, while other analogs significantly reduced blood pressure as apelin-13. Interestingly, analogs that did not lower blood pressure were less effective at recruiting \\barrs. Finally, using Spearman correlations, we established that the hypotensive response was significantly correlated with \\barr recruitment but not with G protein- dependent signaling. In conclusion, our results demonstrated that the \\barr recruitment potency is involved in the hypotensive efficacy of activated APJ.",0.5714285714],["Graph jobs represent a wide variety of computation-intensive tasks. graph jobs are represented","Allocation of Computation-Intensive Graph Jobs over Vehicular Clouds in IoV","summarize: Graph jobs represent a wide variety of computation-intensive tasks in which computations are represented by graphs consisting of components and edges . Recent years have witnessed dramatic growth in smart vehicles and computation-intensive graph jobs, which pose new challenges to the provision of efficient services related to the Internet of Vehicles. Fortunately, vehicular clouds formed by a collection of vehicles, which allows jobs to be offloaded among vehicles, can substantially alleviate heavy on-board workloads and enable on-demand provisioning of computational resources. In this paper, we present a novel framework for vehicular clouds that maps components of graph jobs to service providers via opportunistic vehicle-to-vehicle communication. Then, graph job allocation over vehicular clouds is formulated as a non-linear integer programming with respect to vehicles' contact duration and available resources, aiming to minimize job completion time and data exchange cost. The problem is addressed for two scenarios: low-traffic and rush-hours. For the former, we determine the optimal solutions for the problem. In the latter case, given the intractable computations for deriving feasible allocations, we propose a novel low complexity randomized graph job allocation mechanism by considering hierarchical tree based subgraph isomorphism. We evaluate the performance of our proposed algorithms through extensive simulations.",0.2727272727],["two-neutrino double electron capture is a second-order Weak process","First observation of two-neutrino double electron capture in ","summarize: Two-neutrino double electron capture is a second-order Weak process with predicted half-lives that surpass the age of the Universe by many orders of magnitude. Until now, indications for ",0.5454545455],["n = 4 gives back the duality relation while n = 5 gives back the 5","Functional equations for Rogers dilogarithm","summarize: This paper proves a new family of functional equations for Rogers dilogarithm. These equations rely on the combinatorics of dihedral coordinates on moduli spaces of curves of genus 0, M 0,n. For n = 4 we find back the duality relation while n = 5 gives back the 5 terms relation. It is then proved that the whole family reduces to the 5 terms relation. In the author's knownledge, it is the first time that an infinite family of functional equations for the dilogarithm with an increasing number of variables ) is reduced to the 5 terms relation.",0.2],["the optimal open-loop solution passes by the optimal steady-state for consecutive time instants","Stability and performance in transient average constrained economic MPC without terminal constraints","summarize: In this paper, we investigate system theoretic properties of transient average constrained economic model predictive control without terminal constraints. We show that the optimal open-loop solution passes by the optimal steady-state for consecutive time instants. Using this turnpike property and suitable controllability conditions, we provide closed-loop performance bounds. Furthermore, stability is proved by combining the rotated value function with an input-to-state Lyapunov function of an extended state related to the transient average constraints. The results are illustrated with a numerical example.",0.0588235294],["lookahead BO faces the risk of error propagation through its increased dependence on a possibly","Why Non-myopic Bayesian Optimization is Promising and How Far Should We Look-ahead? A Study via Rollout","summarize: Lookahead, also known as non-myopic, Bayesian optimization aims to find optimal sampling policies through solving a dynamic programming formulation that maximizes a long-term reward over a rolling horizon. Though promising, lookahead BO faces the risk of error propagation through its increased dependence on a possibly mis-specified model. In this work we focus on the rollout approximation for solving the intractable DP. We first prove the improving nature of rollout in tackling lookahead BO and provide a sufficient condition for the used heuristic to be rollout improving. We then provide both a theoretical and practical guideline to decide on the rolling horizon stagewise. This guideline is built on quantifying the negative effect of a mis-specified model. To illustrate our idea, we provide case studies on both single and multi-information source BO. Empirical results show the advantageous properties of our method over several myopic and non-myopic BO algorithms.",0.382707403],["cross-lingual NER is a model for high-resource languages. it is","What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis","summarize: Building named entity recognition models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages to low-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our model achieves competitive performance with the state-of-the-art. We further analyze how transfer learning works for cross-lingual NER on two transferable factors: sequential order and multilingual embeddings, and investigate how model performance varies across entity lengths. Finally, we conduct a case-study on a non-Latin language, Bengali, which suggests that leveraging knowledge from Wikipedia will be a promising direction to further improve the model performances. Our results can shed light on future research for improving cross-lingual NER.",0.4825799563],["geospatial datasets are split into chunks that are processed individually. geoRock","GeoRocket: A scalable and cloud-based data store for big geospatial files","summarize: We present GeoRocket, a software for the management of very large geospatial datasets in the cloud. GeoRocket employs a novel way to handle arbitrarily large datasets by splitting them into chunks that are processed individually. The software has a modern reactive architecture and makes use of existing services including Elasticsearch and storage back ends such as MongoDB or Amazon S3. GeoRocket is schema-agnostic and supports a wide range of heterogeneous geospatial file formats. It is also format-preserving and does not alter imported data in any way. The main benefits of GeoRocket are its performance, scalability, and usability, which make it suitable for a number of scientific and commercial use cases dealing with very high data volumes, complex datasets, and high velocity . GeoRocket also provides many opportunities for further research in the area of geospatial data management.",0.0769230769],["target prior information is based on a pre-learned target dictionary. the target","Sparse and Low-Rank Matrix Decomposition for Automatic Target Detection in Hyperspectral Imagery","summarize: Given a target prior information, our goal is to propose a method for automatically separating targets of interests from the background in hyperspectral imagery. More precisely, we regard the given hyperspectral image as being made up of the sum of low-rank background HSI and a sparse target HSI that contains the targets based on a pre-learned target dictionary constructed from some online spectral libraries. Based on the proposed method, two strategies are briefly outlined and evaluated to realize the target detection on both synthetic and real experiments.",0.3703703704],["computational design powered by crowds can solve this maximization problem by leveraging human computation.","Computational Design with Crowds","summarize: Computational design is aimed at supporting or automating design processes using computational techniques. However, some classes of design tasks involve criteria that are difficult to handle only with computers. For example, visual design tasks seeking to fulfill aesthetic goals are difficult to handle purely with computers. One promising approach is to leverage human computation; that is, to incorporate human input into the computation process. Crowdsourcing platforms provide a convenient way to integrate such human computation into a working system. In this chapter, we discuss such computational design with crowds in the domain of parameter tweaking tasks in visual design. Parameter tweaking is often performed to maximize the aesthetic quality of designed objects. Computational design powered by crowds can solve this maximization problem by leveraging human computation. We discuss the opportunities and challenges of computational design with crowds with two illustrative examples: estimating the objective function to facilitate interactive design exploration by a designer and directly searching for the optimal parameter setting that maximizes the objective function .",0.0],["the Variant-rule derives from the Precursor-rule by interchanging","The Variant-Rule, Another Logically Universal Rule","summarize: The Variant-rule derives from the Precursor-rule by interchanging two classes of its 28 isotropic mappings. Although this small mutation conserves most glider types and stable blocks, glider-gun engines are changed, as are most large scale pattern behaviors, illustrating both the robustness and fragility of evolution. We demonstrate these newly discovered structures and dynamics, and utilising two different glider types, build the logical gates required for universality in the logical sence.",0.2],["this paper investigates the use of game theoretic representations to represent and learn how to","Teach Me What You Want to Play: Learning Variants of Connect Four through Human-Robot Interaction","summarize: This paper investigates the use of game theoretic representations to represent and learn how to play interactive games such as Connect Four. We combine aspects of learning by demonstration, active learning, and game theory allowing a robot to leverage its developing representation of the game to conduct question\/answer sessions with a person, thus filling in gaps in its knowledge. The paper demonstrates a method for teaching a robot the win conditions of the game Connect Four and its variants using a single demonstration and a few trial examples with a question and answer session led by the robot. Our results show that the robot can learn arbitrary win conditions for the game with little prior knowledge of the win conditions and then play the game with a human utilizing the learned win conditions. Our experiments also show that some questions are more important for learning the game's win conditions. We believe that this method could be broadly applied to a variety of interactive learning scenarios.",0.1578947368],["quadratic M-convex functions are a generalization of valuated mat","The quadratic M-convexity testing problem","summarize: M-convex functions, which are a generalization of valuated matroids, play a central role in discrete convex analysis. Quadratic M-convex functions constitute a basic and important subclass of M-convex functions, which has a close relationship with phylogenetics as well as valued constraint satisfaction problems. In this paper, we consider the quadratic M-convexity testing problem , which is the problem of deciding whether a given quadratic function on ",0.2352941176],["the first CNN-based system for baseline extraction. it uses deep convolutional nets","Baseline Detection in Historical Documents using Convolutional U-Nets","summarize: Baseline detection is still a challenging task for heterogeneous collections of historical documents. We present a novel approach to baseline extraction in such settings, turning out the winning entry to the ICDAR 2017 Competition on Baseline detection . It utilizes deep convolutional nets for both, the actual extraction of baselines, as well as for a simple form of layout analysis in a pre-processing step. To the best of our knowledge it is the first CNN-based system for baseline extraction applying a U-net architecture and sliding window detection, profiting from a high local accuracy of the candidate lines extracted. Final baseline post-processing complements our approach, compensating for inaccuracies mainly due to missing context information during sliding window detection. We experimentally evaluate the components of our system individually on the cBAD dataset. Moreover, we investigate how it generalizes to different data by means of the dataset used for the baseline extraction task of the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts . A comparison with the results reported for HisDoc shows that it also outperforms the contestants of the latter.",0.0],["HEVC is a multi-frame in-loop filter that leverages multiple adjacent","A DenseNet Based Approach for Multi-Frame In-Loop Filter in HEVC","summarize: High efficiency video coding has brought outperforming efficiency for video compression. To reduce the compression artifacts of HEVC, we propose a DenseNet based approach as the in-loop filter of HEVC, which leverages multiple adjacent frames to enhance the quality of each encoded frame. Specifically, the higher-quality frames are found by a reference frame selector . Then, a deep neural network for multi-frame in-loop filter is developed to enhance the quality of each encoded frame by utilizing the spatial information of this frame and the temporal information of its neighboring higher-quality frames. The MIF-Net is built on the recently developed DenseNet, benefiting from the improved generalization capacity and computational efficiency. Finally, experimental results verify the effectiveness of our multi-frame in-loop filter, outperforming the HM baseline and other state-of-the-art approaches.",0.2666666667],["first-principles simulations were conducted to explore the thermal stability, mechanical properties and electronic","Theoretical realization of two-dimensional M32 metal-organic frameworks","summarize: Most recently, Cu-hexahydroxybenzene MOF was for the time experimentally realized, through a kinetically controlled approach. Cu-HHB belongs to the family of conductive MOFs with a chemical formula of M32. Motivated by the recent experimental advance in the fabrication of Cu-HHB, we conducted extensive first-principles simulations to explore the thermal stability, mechanical properties and electronic characteristics of M32 monolayers. First-principles results confirm that all considered 2D porous lattices are thermally stable at high temperatures over 1500 K. It was moreover found that these novel 2D structures can exhibit linear elasticity with considerable tensile strengths, revealing their suitability for practical applications in nanodevices.Depending on the metal and chalcogen atoms in M32 monolayers, they can yield various electronic and magnetic properties, such as; magnetic semiconducting, perfect half metallic, magnetic and nonmagnetic metallic behaviours. This work highlights the outstanding physics of M32 2D porous lattices and will hopefully help to expand this conductive MOF family, as promising candidates to design advanced energy storage\/conversion, electronics and spintronics systems.",0.0],["event logs capture execution of business processes in terms of executed activities. existing techniques for such","PRIPEL: Privacy-Preserving Event Log Publishing Including Contextual Information","summarize: Event logs capture the execution of business processes in terms of executed activities and their execution context. Since logs contain potentially sensitive information about the individuals involved in the process, they should be pre-processed before being published to preserve the individuals' privacy. However, existing techniques for such pre-processing are limited to a process' control-flow and neglect contextual information, such as attribute values and durations. This thus precludes any form of process analysis that involves contextual factors. To bridge this gap, we introduce PRIPEL, a framework for privacy-aware event log publishing. Compared to existing work, PRIPEL takes a fundamentally different angle and ensures privacy on the level of individual cases instead of the complete log. This way, contextual information as well as the long tail process behaviour are preserved, which enables the application of a rich set of process analysis techniques. We demonstrate the feasibility of our framework in a case study with a real-world event log.",0.2],["We present results for the next year.","Study of the effect of newly calculated phase space factor on beta-decay half-lives","summarize: We present results for ",0.0606246922],["many cool stars possess complex magnetic fields that are considered to undertake a central role in the structuri","A basal contribution from p-modes to the Alfv\\'enic wave flux in the Sun's corona","summarize: Many cool stars possess complex magnetic fields that are considered to undertake a central role in the structuring and energising of their atmospheres . Alfv\\'enic waves are thought to make a critical contribution to energy transfer along these magnetic fields, with the potential to heat plasma and accelerate stellar winds . Despite Alfv\\'enic waves having been identified in the Sun's atmosphere, the nature of the basal wave energy flux is poorly understood. It is generally assumed that the associated Poynting flux is generated solely in the photosphere and propagates into the corona, typically through the continuous buffeting of magnetic fields by turbulent convective cells . Here we provide evidence that the Sun's internal acoustic modes also contribute to the basal flux of Alfv\\'enic waves, delivering a spatially ubiquitous input to the coronal energy balance that is sustained over the solar cycle. Alfv\\'enic waves are thus a fundamental feature of the Sun's corona. Acknowledging that internal acoustic modes have a key role in injecting additional Poynting flux into the upper atmospheres of Sun-like stars has potentially significant consequences for the modelling of stellar coronae and winds.",0.28],["a behavior-based approach to detecting malicious logins is presented. a graph","Detecting malicious logins as graph anomalies","summarize: Authenticated lateral movement via compromised accounts is a common adversarial maneuver that is challenging to discover with signature- or rules-based intrusion detection systems. In this work a behavior-based approach to detecting malicious logins to novel systems indicative of lateral movement is presented, in which a user's historical login activity is used to build a model of putative normal behavior. This historical login activity is represented as a collection of daily login graphs, which encode authentications among accessed systems. Each system, or graph vertex, is described by a set of graph centrality measures that characterize it and the local topology of its login graph. The unsupervised technique of non-negative matrix factorization is then applied to this set of features to assign each vertex to a role that summarizes how the system participates in logins. The reconstruction error quantifying how well each vertex fits into its role is then computed, and the statistics of this error can be used to identify outlier vertices that correspond to systems involved in unusual logins. We test this technique with a small cohort of privileged accounts using real login data from an operational enterprise network. The ability of the method to identify malicious logins among normal activity is tested with simulated graphs of login activity representative of adversarial lateral movement. We find that the method is generally successful at detecting a broad range of lateral movement for each user, with false positive rates significantly lower than those resulting from alerts based solely on login novelty.",0.52],["large deviation principle on phase space is proved for a class of Markov processes known as random","Large deviations in a population dynamics with catastrophes","summarize: The large deviation principle on phase space is proved for a class of Markov processes known as random population dynamics with catastrophes. In the paper we study the process which corresponds to the random population dynamics with linear growth and uniform catastrophes, where an eliminating portion of the population is chosen uniformly. The large deviation result provides an optimal trajectory of large fluctuation: it shows how the large fluctuations occur for this class of processes.",0.3928571429],["canonical trace and Wodzicki residue on closed manifold characterised by","Spectral ","summarize: The canonical trace and the Wodzicki residue on classical pseudodifferential operators on a closed manifold are characterised by their locality and shown to be preserved under lifting to the universal covering as a result of their local feature. As a consequence, we lift a class of spectral ",0.0],["A. Capelli gave a necessary and sufficient condition for the reducibility of the.","Irreducibility of ","summarize: A. Capelli gave a necessary and sufficient condition for the reducibility of ",0.1052631579],["we study the conventional MCC where tasks are offloaded to the cloud through a wireless access","Multi-user Multi-task Offloading and Resource Allocation in Mobile Cloud Systems","summarize: We consider a general multi-user Mobile Cloud Computing system where each mobile user has multiple independent tasks. These mobile users share the computation and communication resources while offloading tasks to the cloud. We study both the conventional MCC where tasks are offloaded to the cloud through a wireless access point, and MCC with a computing access point , where the CAP serves both as the network access gateway and a computation service provider to the mobile users. We aim to jointly optimize the offloading decisions of all users as well as the allocation of computation and communication resources, to minimize the overall cost of energy, computation, and delay for all users. The optimization problem is formulated as a non-convex quadratically constrained quadratic program, which is NP-hard in general. For the case without a CAP, an efficient approximate solution named MUMTO is proposed by using separable semidefinite relaxation , followed by recovery of the binary offloading decision and optimal allocation of the communication resource. To solve the more complicated problem with a CAP, we further propose an efficient three-step algorithm named MUMTO-C comprising of generalized MUMTO SDR with CAP, alternating optimization, and sequential tuning, which always computes a locally optimal solution. For performance benchmarking, we further present numerical lower bounds of the minimum system cost with and without the CAP. By comparison with this lower bound, our simulation results show that the proposed solutions for both scenarios give nearly optimal performance under various parameter settings, and the resultant efficient utilization of a CAP can bring substantial cost benefit.",0.1739130435],["proposed solution uses weightless neural network known as Wisard. it is used to decide","Weightless Neural Network with Transfer Learning to Detect Distress in Asphalt","summarize: The present paper shows a solution to the problem of automatic distress detection, more precisely the detection of holes in paved roads. To do so, the proposed solution uses a weightless neural network known as Wisard to decide whether an image of a road has any kind of cracks. In addition, the proposed architecture also shows how the use of transfer learning was able to improve the overall accuracy of the decision system. As a verification step of the research, an experiment was carried out using images from the streets at the Federal University of Tocantins, Brazil. The architecture of the developed solution presents a result of 85.71% accuracy in the dataset, proving to be superior to approaches of the state-of-the-art.",0.2],["the nucleus is an activated process. the nucleus is a critical","Seeding Approach to nucleation in the NVT ensemble: the case of bubble cavitation in overstretched Lennard Jones fluids","summarize: Simulations are widely used to study nucleation in first order phase transitions due to the fact that they have access to the relevant length and time scales. However, simulations face the problem that nucleation is an activated process. Therefore, rare event simulation techniques are needed to promote the formation of the critical nucleus. The Seeding method, where the simulations are started with the nucleus already formed, has proven quite useful in efficiently providing estimates of the nucleation rate for a wide range of orders of magnitude. So far, Seeding has been employed in the NPT ensemble, where the nucleus either grows or redissolves. Thus, several trajectories have to be run in order to find the thermodynamic conditions that make the seeded nucleus critical. Moreover, the nucleus lifetime is short and the statistics for obtaining its properties is consequently poor. To deal with these shortcomings we extend the Seeding method to the NVT ensemble. We focus on the problem of bubble nucleation in a mestastable Lennard Jones fluid. We show that, in the NVT ensemble, it is possible to equilibrate and stabilise critical bubbles for a long time. The nucleation rate inferred from NVT-Seeding is fully consistent with that coming from NPT-Seeding. The former is quite suitable to obtain the nucleation rate along isotherms, whereas the latter is preferable if the dependence of the rate with temperature at constant pressure is required. Care should be taken with finite size effects when using NVT-Seeding. Further work is required to extend NVT seeding to other sorts of phase transitions.",0.2268057178],["study aims to understand current state of the art of existing platforms. analyzed 16 platforms","Low-code Engineering for Internet of things: A state of research","summarize: Developing Internet of Things systems has to cope with several challenges mainly because of the heterogeneity of the involved sub-systems and components. With the aim of conceiving languages and tools supporting the development of IoT systems, this paper presents the results of the study, which has been conducted to understand the current state of the art of existing platforms, and in particular low-code ones, for developing IoT systems. By analyzing sixteen platforms, a corresponding set of features has been identified to represent the functionalities and the services that each analyzed platform can support. We also identify the limitations of already existing approaches and discuss possible ways to improve and address them in the future.",0.2777777778],["paper presents early work aiming at the development of a new framework. we consider the","Online Optimisation for Online Learning and Control -- From No-Regret to Generalised Error Convergence","summarize: This paper presents early work aiming at the development of a new framework for the design and analysis of algorithms for online learning based prediction and control. Firstly, we consider the task of predicting values of a function or time series based on incrementally arriving sequences of inputs by utilising online programming. Introducing a generalisation of standard notions of convergence, we derive theoretical guarantees on the asymptotic behaviour of the prediction accuracies when prediction models are updated by a no-external-regret algorithm. We prove generalised learning guarantees for online regression and provide an example of how this can be applied to online learning-based control. We devise a model-reference adaptive controller with novel online performance guarantees on tracking success in the presence of a priori dynamic uncertainty. Our theoretical results are accompanied by illustrations on simple regression and control problems.",0.2173913043],["spatial population dynamics typically focus on interplay between dispersal events and birth\/death","Spatial memory and taxis-driven pattern formation in model ecosystems","summarize: Mathematical models of spatial population dynamics typically focus on the interplay between dispersal events and birth\/death processes. However, for many animal communities, significant arrangement in space can occur on shorter timescales, where births and deaths are negligible. This phenomenon is particularly prevalent in populations of larger, vertebrate animals who often reproduce only once per year or less. To understand spatial arrangements of animal communities on such timescales, we use a class of diffusion-taxis equations for modelling inter-population movement responses between ",0.1538461538],["classical sketch has similar effect on optimization properties of MRR. classical sketch does not have this","Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging","summarize: We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee, instead, the approximation error is governed by a subtle interplay between the mass in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR, these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching.",0.0952380952],["ferromagnetic rare earth-gold intermetallic compounds are produced on a class","On-Surface Synthesis of Graphene Nanoribbons on Two-Dimensional Rare Earth-Gold Intermetallic Compounds","summarize: Here, we demonstrate two reliable routes for the fabrication of armchair-edge graphene nanoribbons on TbAu2\/Au, belonging to a class of two-dimensional ferromagnetic rare earth-gold intermetallic compounds. On-surface synthesis directly on TbAu2 leads to the formation of GNRs, which are short and interconnected with each other. In contrast, the intercalation approach - on-surface synthesis of GNRs directly on Au followed by rare earth intercalation - yields GNRs on TbAu2\/Au, where both the ribbons and TbAu2 are of high quality comparable with those directly grown on clean Au. Besides, the as-grown ribbons retain the same band gap while changing from p-doping to weak n-doping mainly due to a change in the work function of the substrate after the rare earth intercalation. The intercalation approach might also be employed to fabricate other types of GNRs on various rare earth intermetallic compounds, providing platforms to tailor the electronic and magnetic properties of GNRs on magnetic substrates.",0.4790315743],["convolutional networks have demonstrated higher accuracy in predicting future tumor volumes. but current 2","Spatio-Temporal Convolutional LSTMs for Tumor Growth Prediction by Learning 4D Longitudinal Patient Data","summarize: Prognostic tumor growth modeling via volumetric medical imaging observations can potentially lead to better outcomes of tumor treatment and surgical planning. Recent advances of convolutional networks have demonstrated higher accuracy than traditional mathematical models in predicting future tumor volumes. This indicates that deep learning-based techniques may have great potentials on addressing such problem. However, current 2D patch-based modeling approaches cannot make full use of the spatio-temporal imaging context of the tumor's longitudinal 4D data. Moreover, they are incapable to predict clinically-relevant tumor properties, other than volumes. In this paper, we exploit to formulate the tumor growth process through convolutional Long Short-Term Memory that extract tumor's static imaging appearances and capture its temporal dynamic changes within a single network. We extend ConvLSTM into the spatio-temporal domain by jointly learning the inter-slice 3D contexts and the longitudinal or temporal dynamics from multiple patient studies. Our approach can incorporate other non-imaging patient information in an end-to-end trainable manner. Experiments are conducted on the largest 4D longitudinal tumor dataset of 33 patients to date. Results validate that the ST-ConvLSTM produces a Dice score of 83.2%+-5.1% and a RVD of 11.2%+-10.8%, both significantly outperforming other compared methods of linear model, ConvLSTM, and generative adversarial network under the metric of predicting future tumor volumes. Additionally, our new method enables the prediction of both cell density and CT intensity numbers. Last, we demonstrate the generalizability of ST-ConvLSTM by employing it in 4D medical image segmentation task, which achieves an averaged Dice score of 86.3+-1.2% for left-ventricle segmentation in 4D ultrasound with 3 seconds per patient.",0.1333333333],["low-latency beam phase feed-forward system was built to stabilize the arrival time of","Stabilization of the arrival time of a relativistic electron beam to the 50 fs level","summarize: We report the results of a low-latency beam phase feed-forward system built to stabilize the arrival time of a relativistic electron beam. The system was operated at the Compact Linear Collider Test Facility at CERN where the beam arrival time was stabilized to approximately 50 fs. The system latency was 350 ns and the correction bandwidth >23 MHz. The system meets the requirements for CLIC.",0.3957248858],["a graph of the graph shows the sex of the sex of the","Lower bounding the Folkman numbers ","summarize: For a graph ",0.1351351351],["the algorithms under consideration are evaluated with unique metrics for internal and external connectivity of communities. the","An Empirical Study of Community Detection Algorithms on Social and Road Networks","summarize: Community detection in social networks is a problem with considerable interest, since, discovering communities reveals hidden information about networks. There exist many algorithms to detect inherent community structures and recently few of them are investigated on social networks. However, it is non-trivial to decide the best approach in the presence of diverse nature of graphs, in terms of density and sparsity, and inadequate analysis of the results. Therefore, in this study, we analyze and compare various algorithms to detect communities in two networks, namely social and road networks, with varying structural properties. The algorithms under consideration are evaluated with unique metrics for internal and external connectivity of communities that includes internal density, average degree, cut ratio, conductance, normalized cut, and average Jaccard Index. The evaluation results revealed key insights about selected algorithms and underlying community structures.",0.1052631579],["Fe","Molecular beam epitaxy preparation and in situ characterization of FeTe thin films","summarize: We have synthesized Fe",0.0000167017],["the method can be used to classify benchmark datasets. it can also work in batch","A New Oscillating-Error Technique for Classifiers","summarize: This paper describes a new method for reducing the error in a classifier. It uses an error correction update that includes the very simple rule of either adding or subtracting the error adjustment, based on whether the variable value is currently larger or smaller than the desired value. While a traditional neuron would sum the inputs together and then apply a function to the total, this new method can change the function decision for each input value. This gives added flexibility to the convergence procedure, where through a series of transpositions, variables that are far away can continue towards the desired value, whereas variables that are originally much closer can oscillate from one side to the other. Tests show that the method can successfully classify some benchmark datasets. It can also work in a batch mode, with reduced training times and can be used as part of a neural network architecture. Some comparisons with an earlier wave shape paper are also made.",0.0555555556],["a local Lipschitz stability estimate for the Schrodinger equation is found","Calder\\'on's Inverse Problem with a Finite Number of Measurements II: Independent Data","summarize: We prove a local Lipschitz stability estimate for Gel'fand-Calder\\'on's inverse problem for the Schr\\odinger equation. The main novelty is that only a finite number of boundary input data is available, and those are independent of the unknown potential, provided it belongs to a known finite-dimensional subspace of ",0.3043669054],["hazard function of empirical duration data is dominated by a bathtub curve.","Modelling the Dropout Patterns of MOOC Learners","summarize: We adopted survival analysis for the viewing durations of massive open online courses. The hazard function of empirical duration data is dominated by a bathtub curve and has the Lindy effect in its tail. To understand the evolutionary mechanisms underlying these features, we categorized learners into two classes due to their different distributions of viewing durations, namely lognormal distribution and power law with exponential cutoff. Two random differential equations are provided to describe the growth patterns of viewing durations for the two classes respectively. The expected duration change rate of the learners featured by lognormal distribution is supposed to be dependent on their past duration, and that of the rest learners is supposed to be inversely proportional to time. Solutions to the equations predict the features of viewing duration distributions, and those of the hazard function. The equations also reveal the feature of memory and that of memorylessness for the viewing behaviors of the two classes respectively.",0.15],["Graph analytics power a range of applications in finance, networking and business logistics.","A Closer Look at Lightweight Graph Reordering","summarize: Graph analytics power a range of applications in areas as diverse as finance, networking and business logistics. A common property of graphs used in the domain of graph analytics is a power-law distribution of vertex connectivity, wherein a small number of vertices are responsible for a high fraction of all connections in the graph. These richly-connected vertices inherently exhibit high reuse. However, their sparse distribution in memory leads to a severe underutilization of on-chip cache capacity. Prior works have proposed lightweight skew-aware vertex reordering that places hot vertices adjacent to each other in memory, reducing the cache footprint of hot vertices. However, in doing so, they may inadvertently destroy the inherent community structure within the graph, which may negate the performance gains achieved from the reduced footprint of hot vertices. In this work, we study existing reordering techniques and demonstrate the inherent tension between reducing the cache footprint of hot vertices and preserving original graph structure. We quantify the potential performance loss due to disruption in graph structure for different graph datasets. We further show that reordering techniques that employ fine-grain reordering significantly increase misses in the higher level caches, even when they reduce misses in the last-level cache. To overcome the limitations of existing reordering techniques, we propose Degree-Based Grouping , a novel lightweight reordering technique that employs a coarse-grain reordering to largely preserve graph structure while reducing the cache footprint of hot vertices. Our evaluation on 40 combinations of various graph applications and datasets shows that, compared to a baseline with no reordering, DBG yields an average application speed-up of 16.8% vs 11.6% for the best-performing existing lightweight technique.",0.1666666667],["the detailed observation of the distribution of redshifts and chirp masses of binary black hole","Effect of gravitational lensing on the distribution of gravitational waves from distant binary black hole mergers","summarize: The detailed observation of the distribution of redshifts and chirp masses of binary black hole mergers is expected to provide a clue to their origin. In this paper, we develop a hybrid model of the probability distribution function of gravitational lensing magnification taking account of both strong and weak gravitational lensing, and use it to study the effect of gravitational lensing magnification on the distribution of gravitational waves from distant binary black hole mergers detected in ongoing and future gravitational wave observations. We find that the effect of gravitational lensing magnification is significant at high ends of observed chirp mass and redshift distributions. While a high mass tail in the observed chirp mass distribution is produced by highly magnified gravitational lensing events, we find that highly demagnified images of strong lensing events produce a high redshift tail in the observed redshift distribution, which can easily be observed in the third-generation gravitational wave observatories. Such a demagnified, apparently high redshift event is expected to be accompanied by a magnified image that is observed typically ",0.4880906009],["generating functions for the colored HOMFLY-PT polynomials of rational links","Rational links and DT invariants of quivers","summarize: We prove that the generating functions for the colored HOMFLY-PT polynomials of rational links are specializations of the generating functions of the motivic Donaldson-Thomas invariants of appropriate quivers that we naturally associate with these links. This shows that the conjectural links-quivers correspondence of Kucharski-Reineke-Sto\\vi\\'c-Sukowski as well as the LMOV conjecture hold for rational links. Along the way, we extend the links-quivers correspondence to tangles and, thus, explore elements of a skein theory for motivic Donaldson-Thomas invariants.",0.2],["a semi-Automated Framework for soFtware Requirements pri","SAFFRON: A Semi-Automated Framework for Software Requirements Prioritization","summarize: Due to dynamic nature of current software development methods, changes in requirements are embraced and given proper consideration. However, this triggers the rank reversal problem which involves re-prioritizing requirements based on stakeholders' feedback. It incurs significant cost because of time elapsed in large number of human interactions. To solve this issue, a Semi-Automated Framework for soFtware Requirements priOritizatioN is presented in this paper. For a particular requirement, SAFFRON predicts appropriate stakeholders' ratings to reduce human interactions. Initially, item-item collaborative filtering is utilized to estimate similarity between new and previously elicited requirements. Using this similarity, stakeholders who are most likely to rate requirements are determined. Afterwards, collaborative filtering based on latent factor model is used to predict ratings of those stakeholders. The proposed approach is implemented and tested on RALIC dataset. The results illustrate consistent correlation, similar to state of the art approaches, with the ground truth. In addition, SAFFRON requires 13.5-27% less human interaction for re-prioritizing requirements.",0.6068145298],["the sex of the u.s.","Optimal ","summarize: The ",0.0],["chiral DWs are stabilized in these systems due to the spin-orbit","Collective Coordinate Models of Domain Wall Motion in Perpendicularly Magnetized Systems under the Spin Hall Effect and Longitudinal Fields","summarize: Recent studies on heterostructures of ultrathin ferromagnets sandwiched between a heavy metal layer and an oxide have highlighted the importance of spin-orbit coupling and broken inversion symmetry in domain wall motion. Specifically, chiral DWs are stabilized in these systems due to the Dzyaloshinskii-Moriya interaction . SOC can also lead to enhanced current induced DW motion, with the spin Hall effect suggested as the dominant mechanism for this observation. The efficiency of SHE driven DW motion depends on the internal magnetic structure of the DW, which could be controlled using externally applied longitudinal in-plane fields. In this work, micromagnetic simulations and collective coordinate models are used to study current-driven DW motion under longitudinal in-plane fields in perpendicularly magnetized samples with strong DMI. Several extended collective coordinate models are developed to reproduce the micromagnetic results. While these extended models show improvements over traditional models of this kind, there are still discrepancies between them and micromagnetic simulations which require further work.",0.1115134803],["constant feedback conjugates are the only commutative feedback pairs. constant feedback conjugates","Commutativity of Systems with their Feedback Conjugates","summarize: After introducing commutativity concept and summarizing the relevant literature, this work is focused on the commutativity of feedback conjugates. It is already known that a linear time-varying differential system describing a single input-single output dynamical system is always commutative with its constant gain feedback pairs. In this article, it is proven that among the time-varying feedback conjugates of a linear time-varying system, constant feedback conjugates are the only commutative feedback pairs and any of the time-varying feedback conjugates cannot constitutes a commutative pair of a linear time-varying system.",0.0454545455],["the moon exhibits striking geological asymmetries in elevation, crustal thickness,","Are the Moon's nearside-farside asymmetries the result of a giant impact?","summarize: The Moon exhibits striking geological asymmetries in elevation, crustal thickness, and composition between its nearside and farside. Although several scenarios have been proposed to explain these asymmetries, their origin remains debated. Recent remote sensing observations suggest that the crust on the farside highlands consists of two layers: a primary anorthositic layer with thickness of ~30-50 km and on top a more mafic-rich layer ~10 km thick; and the nearside exhibits a large area of low-Ca pyroxene that has been interpreted to have an impact origin. These observations support the idea that the lunar nearside-farside asymmetries may be the result of a giant impact. Here, using quantitative numerical modeling, we test the hypothesis that a giant impact on the early Moon can explain the striking differences in elevation, crustal thickness, and composition between the nearside and farside of the Moon. We find that a large impactor, impacting the current nearside with a low velocity, can form a mega-basin and reproduce the characteristics of the crustal asymmetry and structures comparable to those observed on the current Moon, including the nearside lowlands and the farside's mafic-rich layer on top of a primordial anorthositic crust. Our model shows that the excavated deep-seated KREEP material, deposited close to the basin rim, slumps back into the basin and covers the entire basin floor; subsequent large impacts can transport the shallow KREEP material to the surface, resulting in its observed distribution. In addition, our model suggests that prior to the asymmetry-forming impact, the Moon may have had an 182W anomaly compared to the immediate post-giant impact Earth's mantle, as predicted if the Moon was created through a giant collision with the proto-Earth.",0.1645158942],["this paper is the first internet-wide active measurement study. it aims to en","Don't Forget to Lock the Front Door! Inferring the Deployment of Source Address Validation of Inbound Traffic","summarize: This paper concerns the problem of the absence of ingress filtering at the network edge, one of the main causes of important network security issues. Numerous network operators do not deploy the best current practice - Source Address Validation that aims at mitigating these issues. We perform the first Internet-wide active measurement study to enumerate networks not filtering incoming packets by their source address. The measurement method consists of identifying closed and open DNS resolvers handling requests coming from the outside of the network with the source address from the range assigned inside the network under the test. The proposed method provides the most complete picture of the inbound SAV deployment state at network providers. We reveal that 32 673 Autonomous Systems and 197 641 Border Gateway Protocol prefixes are vulnerable to spoofing of inbound traffic. Finally, using the data from the Spoofer project and performing an open resolver scan, we compare the filtering policies in both directions.",0.1470282961],["bounded stochastic processes can be induced by overdamped approximation","Boundedness vs Unboundedness of A Noise Linked to Tsallis q-Statistics: The Role of The Overdamped Approximation","summarize: An apparently ideal way to generate continuous bounded stochastic processes is to consider the stochastically perturbed motion of a point of small mass in an infinite potential well, under overdamped approximation. Here, however, we show that the aforementioned procedure can be fallacious and lead to incorrect results. We indeed provide a counter-example concerning one of the most employed bounded noises, hereafter called Tsallis-Stariolo-Borland noise, which admits the well known Tsallis q-statistics as stationary density. In fact, we show that for negative values of the Tsallis parameter q , the motion resulting from the overdamped approximation is unbounded. We then investigate the cause of the failure of Kramers first type approximation, and we formally show that the solutions of the full Newtonian non-approximated model are bounded, following the physical intuition. Finally, we provide a new family of bounded noises extending the TSB noise, the boundedness of whose solutions we formally show.",0.0510473138],["non-negative self-adjoint operator L has a spectral gap","A note on local Hardy spaces","summarize: We consider a non-negative self-adjoint operator L satisfying generalized Gaussian estimates on a doubling metric measure space, and show that if L has a spectral gap then the local and global Hardy spaces defined by means of appropriate square functions coincide.",0.2142857143],["ambitwistor strings remain solvable worldsheet theories when coupled to curved background","Amplitudes on plane waves from ambitwistor strings","summarize: In marked contrast to conventional string theory, ambitwistor strings remain solvable worldsheet theories when coupled to curved background fields. We use this fact to consider the quantization of ambitwistor strings on plane wave metric and plane wave gauge field backgrounds. In each case, the worldsheet model is anomaly free as a consequence of the background satisfying the field equations. We derive vertex operators for gravitons and gluons on these backgrounds from the worldsheet CFT, and study the 3-point functions of these vertex operators on the Riemann sphere. These worldsheet correlation functions reproduce the known results for 3-point scattering amplitudes of gravitons and gluons in gravitational and gauge theoretic plane wave backgrounds, respectively.",0.25],["customer lifetime value prediction system deployed at ASOS.com. system provides daily estimates of","Customer Lifetime Value Prediction Using Embeddings","summarize: We describe the Customer LifeTime Value prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We detail the system deployed at ASOS and show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features.",0.0],["Observability is the property that enables to distinguish two different locations in two different locations in","Observability and Synchronization of Neuron Models","summarize: Observability is the property that enables to distinguish two different locations in ",0.0357142857],["numerical simulations of Nambu-Goto cosmic strings show that the loop distribution relax","Cosmic string loop production functions","summarize: Numerical simulations of Nambu-Goto cosmic strings in an expanding universe show that the loop distribution relaxes to an universal configuration, the so-called scaling regime, which is of power law shape on large scales. Precise estimations of the power law exponent are, however, still matter of debate while numerical simulations do not incorporate all the radiation and backreaction effects expected to affect the network dynamics at small scales. By using a Boltzmann approach, we show that the steepness of the loop production function with respect to loops size is associated with drastic changes in the cosmological loop distribution. For a scale factor varying as a~t^nu, we find that sub-critical loop production functions, having a Polchinski-Rocha exponent chi = \/2, are shown to be IR-physics dependent and this generically prevents the loop distribution to relax towards scaling. In the latter situation, we discuss the additional regularisations needed for convergence and show that, although a scaling regime can still be reached, the shape of the cosmological loop distribution is modified compared to the naive expectation. Finally, we discuss the implications of our findings.",0.0833333333],["customer lifetime value prediction system deployed at ASOS.com. system provides daily estimates of","Customer Lifetime Value Prediction Using Embeddings","summarize: We describe the Customer LifeTime Value prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We detail the system deployed at ASOS and show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features.",0.0],["boron-graphdiyne nanosheet is a two-dimensional nano","Enhancement in hydrogen storage capacities of light metal functionalized Boron Graphdiyne nanosheets","summarize: The recent experimental synthesis of the two-dimensional boron-graphdiyne nanosheet has motivated us to investigate its structural, electronic,and energy storage properties. BGDY is a particularly attractive candidate for this purpose due to uniformly distributed pores which can bind the light-metal atoms. Our DFTcalculations reveal that BGDY can accommodate multiple light-metal dopants with significantly high binding energies. The stabilities of metal functionalized BGDY monolayers have been confirmed through ab initio molecular dynamics simulations. Furthermore, significant charge-transfer between the dopantsand BGDY sheet renders the metal with a significant positive charge, which is a prerequisite for adsorbing hydrogen molecules with appropriate binding energies.This results in exceptionally high H2 storage capacities of 14.29, 11.11, 9.10 and 8.99 wt% for the Li, Na, K and Ca dopants, respectively. These H2storage capacities are much higher than many 2D materials such as graphene, graphane, graphdiyne, graphyne, C2N, silicene, and phosphorene. Average H2 adsorption energies for all the studied systems fall within an ideal window of 0.17-0.40 eV\/H2. We have also performed thermodynamic analysis to study the adsorption\/desorption behavior of H2, which confirmsthat desorption of the H2molecules occurs at practical conditions of pressure and temperature.",0.2341050989],["a RL algorithm with a lower discount factor can act as a regularizer","Discount Factor as a Regularizer in Reinforcement Learning","summarize: Specifying a Reinforcement Learning task involves choosing a suitable planning horizon, which is typically modeled by a discount factor. It is known that applying RL algorithms with a lower discount factor can act as a regularizer, improving performance in the limited data regime. Yet the exact nature of this regularizer has not been investigated. In this work, we fill in this gap. For several Temporal-Difference learning methods, we show an explicit equivalence between using a reduced discount factor and adding an explicit regularization term to the algorithm's loss. Motivated by the equivalence, we empirically study this technique compared to standard ",0.4473684211],["naive solutions for polynomially solvable problems have been made in recent","On problems equivalent to -convolution","summarize: In recent years, significant progress has been made in explaining the apparent hardness of improving upon the naive solutions for many fundamental polynomially solvable problems. This progress has come in the form of conditional lower bounds -- reductions from a problem assumed to be hard. The hard problems include 3SUM, All-Pairs Shortest Path, SAT, Orthogonal Vectors, and others. In the ",0.0909090909],["multi-core fibers were characterized using correlation Optical Time Domain Reflecto","Group Delay Measurements of Multicore Fibers with Correlation Optical Time Domain Reflectometry","summarize: Several multi-core fibers were characterized using Correlation Optical Time Domain Reflectometry in terms of propagation delay and polarization mode dispersion . The results show that the propagation delay in the cores depends on the position of the core in the fiber and that the differential delay between the cores varies with temperature.",0.3274923012],["classical sketch has similar effect on optimization properties of MRR. classical sketch does not have this","Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging","summarize: We address the statistical and optimization impacts of the classical sketch and Hessian sketch used to approximately solve the Matrix Ridge Regression problem. Prior research has quantified the effects of classical sketch on the strictly simpler least squares regression problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR: namely, it recovers nearly optimal solutions. By contrast, Hessian sketch does not have this guarantee, instead, the approximation error is governed by a subtle interplay between the mass in the responses and the optimal objective value. For both types of approximation, the regularization in the sketched MRR problem results in significantly different statistical properties from those of the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the bias and variance of sketched MRR, these bounds show that classical sketch significantly increases the variance, while Hessian sketch significantly increases the bias. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases the gap between the risks of the true and sketched solutions to the MRR problem. Thus, in parallel or distributed settings, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the increased statistical risk incurred by sketching.",0.0952380952],["we propose a modular decision making algorithm to navigate intersections. we first present a","Safe Reinforcement Learning with Scene Decomposition for Navigating Complex Urban Environments","summarize: Navigating urban environments represents a complex task for automated vehicles. They must reach their goal safely and efficiently while considering a multitude of traffic participants. We propose a modular decision making algorithm to autonomously navigate intersections, addressing challenges of existing rule-based and reinforcement learning approaches. We first present a safe RL algorithm relying on a model-checker to ensure safety guarantees. To make the decision strategy robust to perception errors and occlusions, we introduce a belief update technique using a learning based approach. Finally, we use a scene decomposition approach to scale our algorithm to environments with multiple traffic participants. We empirically demonstrate that our algorithm outperforms rule-based methods and reinforcement learning techniques on a complex intersection scenario.",0.3571428571],["a completely automated public Turing test to tell computers and humans apart, CA","Efficient and Secure Flash-based Gaming CAPTCH","summarize: With the growth of connectivity to smart grids, new applications, and the changing interaction between customer and energy clouds, clouds are more vulnerable to denial-of-service attacks. Efficient detection methods are required to authenticate, detect and control attackers. Completely Automated Public Turing test to tell Computers and Humans Apart, CAPTCHA, is one efficient tool to thwart denial of service attacks. The server presents the user with a client puzzle to solve in order to gain access to the service or website. The puzzle should be hard enough for computers, but easy for humans to solve. Several methods have been suggested including the popular image-based, as well as video-based, and text-based CAPTCHAs. In this paper, we present a new Flash-based gaming CAPTCHA to differentiate bots from humans. We propose a drag and drop client puzzle where the user will play a simple game to answer a visual question. Our method turns out to be convenient, easy for users and challenging for bots. Additionally, it has gaming aspect, which makes it interesting to users of all age groups.",0.3],["linear complexity iterative and log-linear complexity direct solvers are developed for the","","summarize: Linear complexity iterative and log-linear complexity direct solvers are developed for the volume integral equation based general large-scale electrodynamic analysis. The dense VIE system matrix is first represented by a new cluster-based multilevel low-rank representation. In this representation, all the admissible blocks associated with a single cluster are grouped together and represented by a single low-rank block, whose rank is minimized based on prescribed accuracy. From such an initial representation, an efficient algorithm is developed to generate a minimal-rank ",0.0],["we provide methods for in-database support of decision making under uncertainty. many important decision","Stochastic Package Queries in Probabilistic Databases","summarize: We provide methods for in-database support of decision making under uncertainty. Many important decision problems correspond to selecting a package that jointly satisfy a set of constraints while minimizing some overall cost function; in most real-world problems, the data is uncertain. We provide methods for specifying -- via a SQL extension -- and processing stochastic package queries , in order to solve optimization problems over uncertain data, right where the data resides. Prior work in stochastic programming uses Monte Carlo methods where the original stochastic optimization problem is approximated by a large deterministic optimization problem that incorporates many scenarios, i.e., sample realizations of the uncertain data values. For large database tables, however, a huge number of scenarios is required, leading to poor performance and, often, failure of the solver software. We therefore provide a novel SummarySearch algorithm that, instead of trying to solve a large deterministic problem, seamlessly approximates it via a sequence of smaller problems defined over carefully crafted summaries of the scenarios that accelerate convergence to a feasible and near-optimal solution. Experimental results on our prototype system show that SummarySearch can be orders of magnitude faster than prior methods at finding feasible and high-quality packages.",0.0],["a new type of dependent thinning for point processes in continuous space is proposed.","Determinantal thinning of point processes with network learning applications","summarize: A new type of dependent thinning for point processes in continuous space is proposed, which leverages the advantages of determinantal point processes defined on finite spaces and, as such, is particularly amenable to statistical, numerical, and simulation techniques. It gives a new point process that can serve as a network model exhibiting repulsion. The properties and functions of the new point process, such as moment measures, the Laplace functional, the void probabilities, as well as conditional characteristics can be estimated accurately by simulating the underlying point process, which can be taken, for example, to be Poisson. This is in contrast finite Gibbs point processes, which, instead of thinning, require weighting the Poisson realizations, involving usually intractable normalizing constants. Models based on determinantal point processes are also well suited for statistical learning techniques, allowing the models to be fitted to observed network patterns with some particular geometric properties. We illustrate this approach by imitating with determinantal thinning the well-known Matrn~II hard-core thinning, as well as a soft-core thinning depending on nearest-neighbour triangles. These two examples demonstrate how the proposed approach can lead to new, statistically optimized, probabilistic transmission scheduling schemes.",0.5789473684],["in TBS-C-NOMA, the intra-cell user forwards the symbols of","Threshold-based Selective Cooperative-NOMA","summarize: In this letter, we propose threshold-based selective cooperative-NOMA to increase the data reliability of conventional cooperative-NOMA networks. In TBS-C-NOMA, the intra-cell user forwards the symbols of cell-edge user after successive interference canceler only if the signal-to-interference plus noise ratio is greater than the pre-determined threshold value. Hence, the data reliability of the cell-edge user is increased by eliminating the effect of the error propagation. We derive closed-form end-to-end exact bit error probability of proposed system for various modulation constellations. Then, the optimum threshold value is analyzed in order to minimize BEP. The obtained expressions are validated via simulations and it is revealed that TBS-C-NOMA outperforms C-NOMA and full diversity order is achieved.",0.0],["software testing is becoming a critical part of the development cycle of embedded devices. the goal","Side-Channel Aware Fuzzing","summarize: Software testing is becoming a critical part of the development cycle of embedded devices, enabling vulnerability detection. A well-studied approach of software testing is fuzz-testing , during which mutated input is sent to an input-processing software while its behavior is monitored. The goal is to identify faulty states in the program, triggered by malformed inputs. Even though this technique is widely performed, fuzzing cannot be applied to embedded devices to its full extent. Due to the lack of adequately powerful I\/O capabilities or an operating system the feedback needed for fuzzing cannot be acquired. In this paper we present and evaluate a new approach to extract feedback for fuzzing on embedded devices using information the power consumption leaks. Side-channel aware fuzzing is a threefold process that is initiated by sending an input to a target device and measuring its power consumption. First, we extract features from the power traces of the target device using machine learning algorithms. Subsequently, we use the features to reconstruct the code structure of the analyzed firmware. In the final step we calculate a score for the input, which is proportional to the code coverage. We carry out our proof of concept by fuzzing synthetic software and a light-weight AES implementation running on an ARM Cortex-M4 microcontroller. Our results show that the power side-channel carries information relevant for fuzzing.",0.0769230769],["theorem is a new type of the darbo fixed point theore","A New Type of Darbo's Fixed Point Theorem Defined by The Sequences of Functions","summarize: In this paper, we introduce a new type of Darbo's fixed point theorem by using concept of function sequences with shifting distance property. Afterward, we investigate existence of fixed point under this the theorem. Also we are going to give interesting example held the conditions of sequences of functions",0.1015067182],["a counterexample originates from the study of complex, ternary, cubic","A Note on Normalizations of Orbit Closures","summarize: We give a negative answer to a question by J.M. Landsberg on the nature of normalizations of orbit closures. A counterexample originates from the study of complex, ternary, cubic forms.",0.2307692308],["this article presents advances in resource allocation for downlink non-orthogonal multiple access systems","Resource Allocation for Downlink NOMA Systems: Key Techniques and Open Issues","summarize: This article presents advances in resource allocation for downlink non-orthogonal multiple access systems, focusing on user pairing and power allocation algorithms. The former pairs the users to obtain the high capacity gain by exploiting the channel gain difference between the users, while the later allocates power to users in each cluster to balance system throughput and user fairness. Additionally, the article introduces the concept of cluster fairness and proposes the divideand- next largest difference-based UP algorithm to distribute the capacity gain among the NOMA clusters in a controlled manner. Furthermore, performance comparison between multiple-input multiple-output NOMA and MIMO-OMA is conducted when users have pre-defined quality of service. Simulation results are presented, which validate the advantages of NOMA over OMA. Finally, the article provides avenues for further research on RA for downlink NOMA.",0.1428571429],["proposed solution uses weightless neural network known as Wisard. it is used to decide","Weightless Neural Network with Transfer Learning to Detect Distress in Asphalt","summarize: The present paper shows a solution to the problem of automatic distress detection, more precisely the detection of holes in paved roads. To do so, the proposed solution uses a weightless neural network known as Wisard to decide whether an image of a road has any kind of cracks. In addition, the proposed architecture also shows how the use of transfer learning was able to improve the overall accuracy of the decision system. As a verification step of the research, an experiment was carried out using images from the streets at the Federal University of Tocantins, Brazil. The architecture of the developed solution presents a result of 85.71% accuracy in the dataset, proving to be superior to approaches of the state-of-the-art.",0.2],["the 4-vector measures the distance between reference points on neighboring world lines in spacetime man","Operational significance of the deviation equation in relativistic geodesy","summarize: Deviation equation: Second order differential equation for the 4-vector which measures the distance between reference points on neighboring world lines in spacetime manifolds. Relativistic geodesy: Science representing the Earth , including the measurement of its gravitational field, in a four-dimensional curved spacetime using differential-geometric methods in the framework of Einstein's theory of gravitation .",0.2],["ferromagnets show 100% spin-polarization at the Fermi level","Fe3Se4: A Possible Ferrimagnetic Half-Metal?","summarize: Half-metallic ferromagnets show 100% spin-polarization at the Fermi level and are ideal candidates for spintronic applications. Despite the extensive research in the field, very few materials have been discovered so far. Here we present results of electronic band structure calculations based on density functional theory and extensive physical-property measurements for Fe3Se4 revealing signatures of half-metallicity. The spin-polarized electronic band structure calculations predict half-metallic ferrimagnetism for Fe3Se4. The electrical resistivity follows exponentially suppressed electron-magnon scattering mechanism in the low-temperature regime and show a magnetoresistance effect that changes the sign from negative to positive with decreasing temperature around 100 K. Other intriguing observations include the anomalous behavior of Hall resistance below 100 K and an anomalous Hall coefficient that roughly follows the \\r2 behavior.",0.0],["Continual learning studies agents that learn from streams of tasks without forgetting previous tasks while adapt","Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning","summarize: Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.",0.0526315789],["Molecular motors walk along filaments until they detach stochastically.","Force-dependent unbinding rate of molecular motors from stationary optical trap data","summarize: Molecular motors walk along filaments until they detach stochastically with a force-dependent unbinding rate. Here, we show that this unbinding rate can be obtained from the analysis of experimental data of molecular motors moving in stationary optical traps. Two complementary methods are presented, based on the analysis of the distribution for the unbinding forces and of the motor's force traces. In the first method, analytically derived force distributions for slip bonds, slip-ideal bonds, and catch bonds are used to fit the cumulative distributions of the unbinding forces. The second method is based on the statistical analysis of the observed force traces. We validate both methods with stochastic simulations and apply them to experimental data for kinesin-1.",0.0889708225],["a quintessence model is associated with non-Abelian gauge fields","Gaugessence: a dark energy model with early time radiation-like equation of state","summarize: In this work, we study a new quintessence model associated with non-Abelian gauge fields, minimally coupled to Einstein gravity. This gauge theory has been recently introduced and studied as an inflationary model, called gauge-flation. Here, however, we are interested in the late time cosmology of the model in the presence of matter and radiation to explain the present time accelerating Universe. During the radiation and matter eras, the gauge field tracks radiation and basically acts like a dark radiation sector. As we approach lower redshifts, the dark component takes the form of a dark energy source which eventually becomes the dominate part of the energy budget of the Universe. Due to the tracking feature of our model, solutions with different initial values are attracted to a common trajectory. The existence of early dark radiation is a robust prediction of our model which contributes to the effective number of relativistic species, ",0.3858245518],["SM-games codify a common design pattern in machine learning. they include","Smooth markets: A basic mechanism for organizing gradient-based learners","summarize: With the success of modern machine learning, it is becoming increasingly important to understand and control how learning algorithms interact. Unfortunately, negative results from game theory show there is little hope of understanding or controlling general n-player games. We therefore introduce smooth markets , a class of n-player games with pairwise zero sum interactions. SM-games codify a common design pattern in machine learning that includes GANs, adversarial training, and other recent algorithms. We show that SM-games are amenable to analysis and optimization using first-order methods.",0.3333333333],["a simple optical fiber connector with optical and mechanical guides is designed and fabricated. two","Fabrication of optical components with nm- to mm-scale critical features using three-dimensional direct laser writing","summarize: A powerful fabrication strategy based on three-dimensional direct laser writing for the rapid prototyping of opto-mechanical components with critical features ranging from several hundred nm to a few mm is demonstrated here. As an example, a simple optical fiber connector with optical and mechanical guides as well as integrated micro-optical elements with nano-structured surfaces is designed and fabricated. In contrast to established three-dimensional direct laser writing, two different polymers are combined in the fabrication process in order to achieve a drastic reduction in fabrication time by substantially reducing the optical tool path. A good agreement between the as-fabricated connector and nominal dimensions has been obtained. The developed approach allows the rapid prototyping of optomechanical components with multi-scale critical features. It is, therefore, envisioned to substantially accelerate the development cycle by integrating functional mechanical and optical elements in a single component.",0.4074074074],["the present and future of evolutionary algorithms depends on the proper use of modern parallel and distributed computing infrastructure","It is Time for New Perspectives on How to Fight Bloat in GP","summarize: The present and future of evolutionary algorithms depends on the proper use of modern parallel and distributed computing infrastructures. Although still sequential approaches dominate the landscape, available multi-core, many-core and distributed systems will make users and researchers to more frequently deploy parallel version of the algorithms. In such a scenario, new possibilities arise regarding the time saved when parallel evaluation of individuals are performed. And this time saving is particularly relevant in Genetic Programming. This paper studies how evaluation time influences not only time to solution in parallel\/distributed systems, but may also affect size evolution of individuals in the population, and eventually will reduce the bloat phenomenon GP features. This paper considers time and space as two sides of a single coin when devising a more natural method for fighting bloat. This new perspective allows us to understand that new methods for bloat control can be derived, and the first of such a method is described and tested. Experimental data confirms the strength of the approach: using computing time as a measure of individuals' complexity allows to control the growth in size of genetic programming individuals.",0.0384615385],["neural-eIT models are expanded on to be used for the first time. a","A model of electrical impedance tomography on peripheral nerves for a neural-prosthetic control interface","summarize: Objective: A model is presented to evaluate the viability of using electrical impedance tomography with a nerve cuff to record neural activity in peripheral nerves. Approach: Established modelling approaches in neural-EIT are expanded on to be used, for the first time, on myelinated fibres which are abundant in mammalian peripheral nerves and transmit motor commands. Main results: Fibre impedance models indicate activity in unmyelinated fibres can be screened out using operating frequencies above 100 Hz. At 1 kHz and 10 mm electrode spacing, impedance magnitude of inactive intra-fascicle tissue and the fraction changes during neural activity are estimated to be 1,142 .cm and -8.8x10-4, respectively, with a transverse current, and 328 .cm & -0.30, respectively with a longitudinal current. We show that a novel EIT drive and measurement electrode pattern which utilises longitudinal current and longitudinal differential boundary voltage measurements could distinguish activity in different fascicles of a three-fascicle mammalian nerve using pseudo-experimental data synthesised to replicate real operating conditions. Significance: The results of this study provide an estimate of the transient change in impedance of intra-fascicle tissue during neural activity in mammalian nerve, and present a viable EIT electrode pattern, both of which are critical steps towards implementing EIT in a nerve cuff for neural prosthetics interfaces.",0.4629805393],["coding is a manual effort while the automation of such task is not straight forward.","Ensemble model for pre-discharge icd10 coding prediction","summarize: The translation of medical diagnosis to clinical coding has wide range of applications in billing, aetiology analysis, and auditing. Currently, coding is a manual effort while the automation of such task is not straight forward. Among the challenges are the messy and noisy clinical records, case complexities, along with the huge ICD10 code space. Previous work mainly relied on discharge notes for prediction and was applied to a very limited data scale. We propose an ensemble model incorporating multiple clinical data sources for accurate code predictions. We further propose an assessment mechanism to provide confidence rates in predicted outcomes. Extensive experiments were performed on two new real-world clinical datasets with unaltered case-mix distributions from Maharaj Nakorn Chiang Mai Hospital. We obtain multi-label classification accuracies of 0.73 and 0.58 for average precision, 0.56 and 0.35 for F1-scores and 0.71 and 0.4 accuracy in predicting principal diagnosis for inpatient and outpatient datasets respectively.",0.1666666667],["a RL algorithm with a lower discount factor can act as a regularizer","Discount Factor as a Regularizer in Reinforcement Learning","summarize: Specifying a Reinforcement Learning task involves choosing a suitable planning horizon, which is typically modeled by a discount factor. It is known that applying RL algorithms with a lower discount factor can act as a regularizer, improving performance in the limited data regime. Yet the exact nature of this regularizer has not been investigated. In this work, we fill in this gap. For several Temporal-Difference learning methods, we show an explicit equivalence between using a reduced discount factor and adding an explicit regularization term to the algorithm's loss. Motivated by the equivalence, we empirically study this technique compared to standard ",0.4473684211],["we propose an efficient numerical algorithm for the solution of diffeomorphic image registration problems","A Semi-Lagrangian two-level preconditioned Newton-Krylov solver for constrained diffeomorphic image registration","summarize: We propose an efficient numerical algorithm for the solution of diffeomorphic image registration problems. We use a variational formulation constrained by a partial differential equation , where the constraints are a scalar transport equation. We use a pseudospectral discretization in space and second-order accurate semi-Lagrangian time stepping scheme for the transport equations. We solve for a stationary velocity field using a preconditioned, globalized, matrix-free Newton-Krylov scheme. We propose and test a two-level Hessian preconditioner. We consider two strategies for inverting the preconditioner on the coarse grid: a nested preconditioned conjugate gradient method and a nested Chebyshev iterative method with a fixed number of iterations. We test the performance of our solver in different synthetic and real-world two-dimensional application scenarios. We study grid convergence and computational efficiency of our new scheme. We compare the performance of our solver against our initial implementation that uses the same spatial discretization but a standard, explicit, second-order Runge-Kutta scheme for the numerical time integration of the transport equations and a single-level preconditioner. Our improved scheme delivers significant speedups over our original implementation. As a highlight, we observe a 20",0.3571428571],["the IRS consists of a large number of low-cost and energy-efficient passive antenna","Intelligent Reflecting and Transmitting Surface Aided Millimeter Wave Massive MIMO","summarize: In this paper, we study two novel massive MIMO architectures for millimeter wave communications which comprise few active antennas, each equipped with a dedicated radio frequency chain, that illuminate a nearby large intelligent reflecting\/transmitting surface . The IRS consists of a large number of low-cost and energy-efficient passive antenna elements which are able to reflect a phase-shifted version of the incident electromagnetic field. Similar to lens array antennas, IRS\/ITS-aided antenna architectures are energy efficient due to the almost lossless over-the-air connection between the active antennas and the intelligent surface. However, unlike for LA antennas, for which the number of active antennas has to linearly grow with the number of passive elements due to the non-reconfigurablility of the lens, for IRS\/ITS-aided antennas, the reconfigurablility of the IRS\/ITS facilitates scaling up the number of radiating passive elements without increasing the number of costly and bulky active antennas. We show that the constraints that the precoders for IRS\/ITS-aided antennas have to meet differ from those of conventional MIMO architectures. Taking these constraints into account and exploiting the sparsity of mmWave channels, we design two efficient precoders and develop a power consumption model for IRS\/ITS-aided antennas that takes into account the impacts of the IRS\/ITS imperfections, namely the spillover loss, taper loss, aperture loss, and phase shifter loss. Our simulation results reveal that unlike conventional MIMO architectures, IRS\/ITS-aided antennas are both highly energy efficient and fully scalable in terms of the number of transmitting antennas. Therefore, IRS\/ITS-aided antennas are promising candidates for realizing the potential of mmWave massive MIMO communications in practice.",0.3],["video forensic techniques look for traces that are mostly ineffective. this is because they","Efficient video integrity analysis through container characterization","summarize: Most video forensic techniques look for traces within the data stream that are, however, mostly ineffective when dealing with strongly compressed or low resolution videos. Recent research highlighted that useful forensic traces are also left in the video container structure, thus offering the opportunity to understand the life-cycle of a video file without looking at the media stream itself. In this paper we introduce a container-based method to identify the software used to perform a video manipulation and, in most cases, the operating system of the source device. As opposed to the state of the art, the proposed method is both efficient and effective and can also provide a simple explanation for its decisions. This is achieved by using a decision-tree-based classifier applied to a vectorial representation of the video container structure. We conducted an extensive validation on a dataset of 7000 video files including both software manipulated contents , and videos exchanged through social media platforms . This dataset has been made available to the research community. The proposed method achieves an accuracy of 97.6% in distinguishing pristine from tampered videos and classifying the editing software, even when the video is cut without re-encoding or when it is downscaled to the size of a thumbnail. Furthermore, it is capable of correctly identifying the operating system of the source device for most of the tampered videos.",0.125],["the government has been one of the most responsive to COVID-19. the government has","The determinants of COVID-19 case fatality rate in the Italian regions and provinces: an analysis of environmental, demographic, and healthcare factors","summarize: The Italian government has been one of the most responsive to COVID-19 emergency, through the adoption of quick and increasingly stringent measures to contain the outbreak. Despite this, Italy has suffered a huge human and social cost, especially in Lombardy. The aim of this paper is dual: i) first, to investigate the reasons of the case fatality rate differences across Italian 20 regions and 107 provinces, using a multivariate OLS regression approach; and ii) second, to build a taxonomy of provinces with similar mortality risk of COVID-19, by using the Ward hierarchical agglomerative clustering method. I considered health system metrics, environmental pollution, climatic conditions, demographic variables, and three ad hoc indexes that represent the health system saturation. The results showed that overall health care efficiency, physician density, and average temperature helped to reduce the CFR. By the contrary, population aged 70 and above, car and firm density, level of air pollutants , relative average humidity, COVID-19 prevalence, and all three indexes of health system saturation were positively associated with the CFR. Population density, social vertical integration, and altitude were not statistically significant. In particular, the risk of dying increases with age, as 90 years old and above had a three-fold greater risk than the 80 to 89 years old and four-fold greater risk than 70 to 79 years old. Moreover, the cluster analysis showed that the highest mortality risk was concentrated in the north of the country, while the lowest risk was associated with southern provinces. Finally, since prevalence and health system saturation indexes played the most important role in explaining the CFR variability, a significant part of the latter may have been caused by the massive stress of the Italian health system.",0.1516326649],["3D convolutional neural networks are difficult to train because they are parameter-expensive","Temporal Factorization of 3D Convolutional Kernels","summarize: 3D convolutional neural networks are difficult to train because they are parameter-expensive and data-hungry. To solve these problems we propose a simple technique for learning 3D convolutional kernels efficiently requiring less training data. We achieve this by factorizing the 3D kernel along the temporal dimension, reducing the number of parameters and making training from data more efficient. Additionally we introduce a novel dataset called Video-MNIST to demonstrate the performance of our method. Our method significantly outperforms the conventional 3D convolution in the low data regime . Finally, our model achieves competitive results in the high data regime using up to 45% fewer parameters.",0.1428571429],["proposed technique uses deep neural network to map processed audio into quality score. the technique does not","Referenceless Performance Evaluation of Audio Source Separation using Deep Neural Networks","summarize: Current performance evaluation for audio source separation depends on comparing the processed or separated signals with reference signals. Therefore, common performance evaluation toolkits are not applicable to real-world situations where the ground truth audio is unavailable. In this paper, we propose a performance evaluation technique that does not require reference signals in order to assess separation quality. The proposed technique uses a deep neural network to map the processed audio into its quality score. Our experiment results show that the DNN is capable of predicting the sources-to-artifacts ratio from the blind source separation evaluation toolkit without the need for reference signals.",0.0],["constrained counting and sampling are two fundamental problems in Computer Science. constrained counting and sampling","Constrained Counting and Sampling: Bridging the Gap between Theory and Practice","summarize: Constrained counting and sampling are two fundamental problems in Computer Science with numerous applications, including network reliability, privacy, probabilistic reasoning, and constrained-random verification. In constrained counting, the task is to compute the total weight, subject to a given weighting function, of the set of solutions of the given constraints. In constrained sampling, the task is to sample randomly, subject to a given weighting function, from the set of solutions to a set of given constraints. Consequently, constrained counting and sampling have been subject to intense theoretical and empirical investigations over the years. Prior work, however, offered either heuristic techniques with poor guarantees of accuracy or approaches with proven guarantees but poor performance in practice. In this thesis, we introduce a novel hashing-based algorithmic framework for constrained sampling and counting that combines the classical algorithmic technique of universal hashing with the dramatic progress made in combinatorial reasoning tools, in particular, SAT and SMT, over the past two decades. The resulting frameworks for counting and sampling can handle formulas with up to million variables representing a significant boost up from the prior state of the art tools' capability to handle few hundreds of variables. If the initial set of constraints is expressed as Disjunctive Normal Form , ApproxMC is the only known Fully Polynomial Randomized Approximation Scheme that does not involve Monte Carlo steps. By exploiting the connection between definability of formulas and variance of the distribution of solutions in a cell defined by 3-universal hash functions, we introduced an algorithmic technique, MIS, that reduced the size of XOR constraints employed in the underlying universal hash functions by as much as two orders of magnitude.",0.275862069],["emerging technologies that make use of new materials are at the forefront in the race for the physical implementation","Experimental realisation of tunable ferroelectric\/superconductor N\/STO 1D photonic crystals in the whole visible spectrum","summarize: Emergent technologies that make use of novel materials and quantum properties of light states are at the forefront in the race for the physical implementation, encoding and transmission of information. Photonic crystals enter this paradigm with optical materials that allow the control of light propagation and can be used for optical communication, and photonics and electronics integration making use of materials ranging from semiconductors, to metals, metamaterials, and topological insulators, to mention but a few. In particular, here we show how designer superconductor materials integrated into PCs fabrication allow for an extraordinary reduction of electromagnetic waves damping and possibilitate their optimal propagation and tuning through the structure, below critical superconductor temperature. We experimentally demonstrate, for the first time, a successful integration of ferroelectric and superconductor materials into a one-dimensional PC composed of N\/STO bilayers that work in the whole visible spectrum, and below critical superconductor temperature . Theoretical calculations support, for different number of bilayers N, the effectiveness of the produced 1D PCs and pave the way for novel optoelectronics integration and information processing in the visible spectrum at low temperature, while preserving their electric and optical properties.",0.2],["linear-algebraic type system.","The Vectorial ","summarize: We describe a type system for the linear-algebraic ",0.0],["Graph Optimal Transport aims to create a graph matching problem. the learned","Graph Optimal Transport for Cross-Domain Alignment","summarize: Cross-domain alignment between two sets of entities is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, with no training signals to explicitly encourage alignment. The learned attention matrices are also dense and lacks interpretability. We propose Graph Optimal Transport , a principled framework that germinates from recent advances in Optimal Transport . In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities into a dynamically-constructed graph. Two types of OT distances are considered: Wasserstein distance for node matching; and Gromov-Wasserstein distance for edge matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer. The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization.",0.35],["study used time series data for trend analysis and data forecasting. data on the ten","Modeling National Trends on Health in the Philippines Using ARIMA","summarize: Health is a very important prerequisite in peoples well-being and happiness. Several studies were more focused on presenting the occurrence on specific disease like forecasting the number of dengue and malaria cases. This paper utilized the time series data for trend analysis and data forecasting using ARIMA model to visualize the trends of health data on the ten leading causes of deaths, leading cause of morbidity and leading cause of infants deaths particularly in the Philippines presented in a tabular data. Figures for each disease trend are presented individually with the use of the GRETL software. Forecasting results of the leading causes of death showed that Diseases of the heart, vascular system, accidents, Chronic lower respiratory diseases and Chronic Tuberculosis showed a slight changed of the forecasted data, Malignant neoplasms showed unstable behavior of the forecasted data, and Pneumonia, diabetes mellitus, Nephritis, nephrotic syndrome and nephrosis and certain conditions originating in perinatal showed a decreasing patterns based on the forecasted data.",0.0909090909],["the comparison points to the possibility of quantization of the magnetic and the electric field fluxes","Would be the photon a composed particle? quantization of field fluxes in electromagnetic radiation","summarize:  Here it is made a comparative analysis between the classical and the quantum expressions for the energy of electromagnetic radiation . The comparison points to the possibility of the quantization of the magnetic and the electric field fluxes in the ER.",0.3666666667],["we study the effects of gravitationally-driven decoherence on tunneling processes","Cosmological Decoherence from Thermal Gravitons","summarize: We study the effects of gravitationally-driven decoherence on tunneling processes associated with false vacuum decays, such as the Coleman--De~Luccia instanton. We compute the thermal graviton-induced decoherence rate for a wave function describing a perfect fluid of nonzero energy density in a finite region. When the effective cosmological constant is positive, the thermal graviton background sourced by a de Sitter horizon provides an unavoidable decoherence effect, which may have important consequences for tunneling processes in cosmological history. We discuss generalizations and consequences of this effect and comment on its observability and applications to black hole physics.",0.0909090909],["topological constraint theory captures the important atomic topology that controls macroscopic properties","Topological Constraint Theory and Rigidity of Glasses","summarize: Topological constraint theory has become an increasingly popular tool to predict the compositional dependence of glass properties or pinpoint promising compositions with tailored functionalities. This approach reduces complex disordered networks into simpler mechanical trusses. Thereby, topological constraint theory captures the important atomic topology that controls macroscopic properties while filtering out less relevant second-order structural details. As such, topological constraint theory can be used to decode the genome of glass, that is, to identify and decipher how the basic structural building blocks of glasses control their engineering properties---in the same way as the human genome offers information that serves as a blueprint for an individual's growth and development. Thanks to its elegance and simplicity, topological constraint theory has enabled the development of various physics-based models that can analytically predict various properties of glass. In this Chapter, I introduce some general background in glass science, concepts of atomic rigidity, and topological constraint theory. The topological constraints enumeration scheme is presented for various archetypical glasses and is used to understand the origin of their glass-forming ability. Finally, various topological models enabling the prediction of glass properties are reviewed, with a focus on hardness, fracture toughness, viscosity, fragility, glass transition temperature, and dissolution kinetics.",0.0],["NP-completed.","Turing Kernelization for Finding Long Paths and Cycles in Restricted Graph Classes","summarize: The NP-complete ",0.0],["magnetic field is believed to play an important role in at least some core-collapse supernova","The impact of non-dipolar magnetic fields in core-collapse supernovae","summarize: The magnetic field is believed to play an important role in at least some core-collapse supernovae if its magnitude reaches ",0.3125],["semi-automatic system able to detect large mammals in semi-arid Savanna","Detecting animals in African Savanna with UAVs and the crowds","summarize: Unmanned aerial vehicles offer new opportunities for wildlife monitoring, with several advantages over traditional field-based methods. They have readily been used to count birds, marine mammals and large herbivores in different environments, tasks which are routinely performed through manual counting in large collections of images. In this paper, we propose a semi-automatic system able to detect large mammals in semi-arid Savanna. It relies on an animal-detection system based on machine learning, trained with crowd-sourced annotations provided by volunteers who manually interpreted sub-decimeter resolution color images. The system achieves a high recall rate and a human operator can then eliminate false detections with limited effort. Our system provides good perspectives for the development of data-driven management practices in wildlife conservation. It shows that the detection of large mammals in semi-arid Savanna can be approached by processing data provided by standard RGB cameras mounted on affordable fixed wings UAVs.",0.1818181818],["a sequence enables a protein to acquire a specific stable conformation. this","Deep Robust Framework for Protein Function Prediction using Variable-Length Protein Sequences","summarize: Amino acid sequence portrays most intrinsic form of a protein and expresses primary structure of protein. The order of amino acids in a sequence enables a protein to acquire a particular stable conformation that is responsible for the functions of the protein. This relationship between a sequence and its function motivates the need to analyse the sequences for predicting protein functions. Early generation computational methods using BLAST, FASTA, etc. perform function transfer based on sequence similarity with existing databases and are computationally slow. Although machine learning based approaches are fast, they fail to perform well for long protein sequences . In this paper, we introduce a novel method for construction of two separate feature sets for protein sequences based on analysis of 1) single fixed-sized segments and 2) multi-sized segments, using bi-directional long short-term memory network. Further, model based on proposed feature set is combined with the state of the art Multi-lable Linear Discriminant Analysis features based model to improve the accuracy. Extensive evaluations using separate datasets for biological processes and molecular functions demonstrate promising results for both single-sized and multi-sized segments based feature sets. While former showed an improvement of +3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00% respectively for two datasets over the state of the art MLDA based classifier. After combining two models, there is a significant improvement of +7.41% and +9.21% respectively for two datasets compared to MLDA based classifier. Specifically, the proposed approach performed well for the long protein sequences and superior overall performance.",0.3],["a theory for interacting spreading dynamics on complex networks is lacking. we develop a","Phase diagrams of interacting spreading dynamics in complex networks","summarize: Epidemic spreading processes in the real world can interact with each other in a cooperative, competitive, or asymmetric way, requiring a description based on coevolution dynamics. Rich phenomena such as discontinuous outbreak transitions and hystereses can arise, but a full picture of these behaviors in the parameter space is lacking. We develop a theory for interacting spreading dynamics on complex networks through spectral dimension reduction. In particular, we derive from the microscopic quenched mean-field equations a two-dimensional system in terms of the macroscopic variables, which enables a full phase diagram to be determined analytically. The diagram predicts critical phenomena that were known previously but only numerically, such as the interplay between discontinuous transition and hysteresis as well as the emergence and role of tricritical points.",0.7083333333],["quantum circuits with charge conservation initialize the system in random product states. we study the","Dynamics of Renyi entanglement entropy in local quantum circuits with charge conservation","summarize: In local quantum circuits with charge conservation, we initialize the system in random product states and study the dynamics of the Renyi entanglement entropy ",0.3333333333],["sensitivity metric helps in identifying the parameters that cause maximum fluctuations in the output.","Sensitivity and Covariance in Stochastic Complementarity Problems with an Application to Natural Gas Markets","summarize: We provide an efficient method to approximate the covariance between decision variables and uncertain parameters in solutions to a general class of stochastic nonlinear complementarity problems. We also develop a sensitivity metric to quantify the uncertainty propagation in the problem by determining the change in the variance of the output variables due to a change in the variance of an input parameter. The covariance matrix of the solution variables quantifies the uncertainty in the output and pairs correlated variables and parameters. The sensitivity metric helps in identifying the parameters that cause maximum fluctuations in the output. The method developed in this paper optimizes the use of gradients and matrix multiplications which makes it particularly useful for large-scale problems. Having developed this method, we extend the deterministic version of the North American Natural Gas Model to incorporate effects due to uncertainty in the parameters of the demand function, supply function, tariffs, and investment costs. We then use the sensitivity metrics to identify the parameters that impact the equilibrium the most.",0.1],["the faa is considering remote ID systems for unmanned aerial vehicles. the systems","Experiments with a LoRaWAN-Based Remote ID System for Locating Unmanned Aerial Vehicles ","summarize: Federal Aviation Administration of the United States is considering Remote ID systems for unmanned aerial vehicles . These systems act as license plates used on automobiles, but they transmit information using radio waves. To be useful, the transmissions in such systems need to reach long distances to minimize the number of ground stations to capture these transmissions. LoRaWAN is designed as a cheap long-range technology to be used for long-range communication for the Internet of Things. Several manufacturers make LoRaWAN modules, which are readily available on the market and are, therefore, ideal for the UAVs Remote IDs at a low cost. In this paper, we present our experiences in using LoRaWAN technology as a communication technology. Our experiments to identify and locate the UAV systems uncovered several issues of using LoRaWAN in such systems that are documented in this paper. Using several ground stations, we can determine the location of a UAV equipped with a LoRaWAN module that transmits the UAV Remote ID. Hence, it can help identify UAVs that unintentionally, or intentionally, fly into restricted zones.",0.1176470588],["ciphers generate a primitive group, according to hypotheses. cip","On the primitivity of PRESENT and other lightweight ciphers","summarize: We provide two sufficient conditions to guarantee that the round functions of a translation based cipher generate a primitive group. Furthermore, under the same hypotheses, and assuming that a round of the cipher is strongly proper and consists of m-bit S-Boxes, with m = 3; 4 or 5, we prove that such a group is the alternating group. As an immediate consequence, we deduce that the round functions of some lightweight translation based ciphers, such as the PRESENT cipher, generate the alternating group.",0.25],["deep learning is experiencing a trend towards producing reproducible research. deepdIVA is","Improving Reproducible Deep Learning Workflows with DeepDIVA","summarize: The field of deep learning is experiencing a trend towards producing reproducible research. Nevertheless, it is still often a frustrating experience to reproduce scientific results. This is especially true in the machine learning community, where it is considered acceptable to have black boxes in your experiments. We present DeepDIVA, a framework designed to facilitate easy experimentation and their reproduction. This framework allows researchers to share their experiments with others, while providing functionality that allows for easy experimentation, such as: boilerplate code, experiment management, hyper-parameter optimization, verification of data integrity and visualization of data and results. Additionally, the code of DeepDIVA is well-documented and supported by several tutorials that allow a new user to quickly familiarize themselves with the framework.",0.0555555556],["set cover problem is a combination of elements set cover problem and set multi-cover problem","Approximation Algorithm for the Partial Set Multi-Cover Problem","summarize: Partial set cover problem and set multi-cover problem are two generalizations of set cover problem. In this paper, we consider the partial set multi-cover problem which is a combination of them: given an element set ",0.0909090909],["Sullivan's non-wandering domain theorem is a partial general","Non-wandering Fatou components for strongly attracting polynomial skew products","summarize: We show a partial generalization of Sullivan's non-wandering domain theorem in complex dimension two. More precisely, we show the non-existence of wandering Fatou components for polynomial skew products of ",0.3151774652],["consider two elements in the tangent bundle of the Euclidean plane. consider","A geometric approach to shortest bounded curvature paths","summarize: Consider two elements in the tangent bundle of the Euclidean plane ",0.0],["we developed a workflow to run and share computational studies on the public cloud Microsoft Azure.","Reproducible Workflow on a Public Cloud for Computational Fluid Dynamics","summarize: In a new effort to make our research transparent and reproducible by others, we developed a workflow to run and share computational studies on the public cloud Microsoft Azure. It uses Docker containers to create an image of the application software stack. We also adopt several tools that facilitate creating and managing virtual machines on compute nodes and submitting jobs to these nodes. The configuration files for these tools are part of an expanded reproducibility package that includes workflow definitions for cloud computing, in addition to input files and instructions. This facilitates re-creating the cloud environment to re-run the computations under the same conditions. Although cloud providers have improved their offerings, many researchers using high-performance computing are still skeptical about cloud computing. Thus, we ran benchmarks for tightly coupled applications to confirm that the latest HPC nodes of Microsoft Azure are indeed a viable alternative to traditional on-site HPC clusters. We also show that cloud offerings are now adequate to complete computational fluid dynamics studies with in-house research software that uses parallel computing with GPUs. Finally, we share with the community what we have learned from nearly two years of using Azure cloud to enhance transparency and reproducibility in our computational simulations.",0.2857142857],["this paper develops a more general theory of sequences of dependent categorical random variables","Vertical Dependency in Sequences of Categorical Random Variables","summarize: This paper develops a more general theory of sequences of dependent categorical random variables, extending the works of Korzeniowski and Traylor that studied first-kind dependency in sequences of Bernoulli and categorical random variables, respectively. A more natural form of dependency, sequential dependency, is defined and shown to retain the property of identically distributed but dependent elements in the sequence. The cross-covariance of sequentially dependent categorical random variables is proven to decrease exponentially in the dependency coefficient ",0.347826087],["existing spatial representation languages are not sufficient for describing spatial configurations used in complex tasks.","From Spatial Relations to Spatial Configurations","summarize: Spatial Reasoning from language is essential for natural language understanding. Supporting it requires a representation scheme that can capture spatial phenomena encountered in language as well as in images and videos. Existing spatial representations are not sufficient for describing spatial configurations used in complex tasks. This paper extends the capabilities of existing spatial representation languages and increases coverage of the semantic aspects that are needed to ground the spatial meaning of natural language text in the world. Our spatial relation language is able to represent a large, comprehensive set of spatial concepts crucial for reasoning and is designed to support the composition of static and dynamic spatial configurations. We integrate this language with the Abstract Meaning Representation annotation schema and present a corpus annotated by this extended AMR. To exhibit the applicability of our representation scheme, we annotate text taken from diverse datasets and show how we extend the capabilities of existing spatial representation languages with the fine-grained decomposition of semantics and blend it seamlessly with AMRs of sentences and discourse representations as a whole.",0.0],["large balls on unbounded model hypersurfaces in carnot-carath'","On Uniform Large-Scale Volume Growth for the Carnot-Carath\\'eodory Metric on Unbounded Model Hypersurfaces in ","summarize: We consider the rate of volume growth of large Carnot-Carath\\'eodory metric balls on a class of unbounded model hypersurfaces in ",0.1042155049],["the overdamped equation of motion is a particle subject to spatially varying Lorent","Anomalous fluxes in overdamped Brownian dynamics with Lorentz force","summarize: We study the stochastic motion of a particle subject to spatially varying Lorentz force in the small-mass limit. The limiting procedure yields an additional drift term in the overdamped equation that cannot be obtained by simply setting mass to zero in the velocity Langevin equation. We show that whereas the overdamped equation of motion accurately captures the position statistics of the particle, it leads to unphysical fluxes in the system that persist in the long time limit; an anomalous result inconsistent with thermal equilibrium. These fluxes are calculated analytically from the overdamped equation of motion and found to be in quantitative agreement with Brownian dynamics simulations. Our study suggests that the overdamped approximation, though perfectly suited for position statistics, can yield unphysical values for velocity-dependent variables such as flux and entropy production.",0.3157894737],["we choose Maxwell-power-Yang-Mills theory defined in the AdS","Complexity growth and shock wave geometry in AdS-Maxwell-power-Yang-Mills theory","summarize: We study effects of non-abelian gauge fields on the holographic characteristics for instance the evolution of computational complexity. To do so we choose Maxwell-power-Yang-Mills theory defined in the AdS space-time. Then we seek the impact of charge of the YM field on the complexity growth rate by using ",0.5147898598],["KG embeddings are an emerging technique for a variety of downstream tasks.","SE-KGE: A Location-Aware Knowledge Graph Embedding Model for Geographic Question Answering and Spatial Semantic Lifting","summarize: Learning knowledge graph embeddings is an emerging technique for a variety of downstream tasks such as summarization, link prediction, information retrieval, and question answering. However, most existing KG embedding models neglect space and, therefore, do not perform well when applied to spatial data and tasks. For those models that consider space, most of them primarily rely on some notions of distance. These models suffer from higher computational complexity during training while still losing information beyond the relative distance between entities. In this work, we propose a location-aware KG embedding model called SE-KGE. It directly encodes spatial information such as point coordinates or bounding boxes of geographic entities into the KG embedding space. The resulting model is capable of handling different types of spatial reasoning. We also construct a geographic knowledge graph as well as a set of geographic query-answer pairs called DBGeo to evaluate the performance of SE-KGE in comparison to multiple baselines. Evaluation results show that SE-KGE outperforms these baselines on the DBGeo dataset for geographic logic query answering task. This demonstrates the effectiveness of our spatially-explicit model and the importance of considering the scale of different geographic entities. Finally, we introduce a novel downstream task called spatial semantic lifting which links an arbitrary location in the study area to entities in the KG via some relations. Evaluation on DBGeo shows that our model outperforms the baseline by a substantial margin.",0.4581181077],["we consider an extension of the standard model based on the group.","A ","summarize: We consider an extension of the standard model based on the group ",0.0],["Suppose you are a sexy man.","On the gaps between non-zero Fourier coefficients of eigenforms with CM","summarize: Suppose ",0.0543247761],["a new classification approach for EEG time series based on Recurrent Neural Networks","Deep Classification of Epileptic Signals","summarize: Electrophysiological observation plays a major role in epilepsy evaluation. However, human interpretation of brain signals is subjective and prone to misdiagnosis. Automating this process, especially seizure detection relying on scalp-based Electroencephalography and intracranial EEG, has been the focus of research over recent decades. Nevertheless, its numerous challenges have inhibited a definitive solution. Inspired by recent advances in deep learning, we propose a new classification approach for EEG time series based on Recurrent Neural Networks via the use of Long-Short Term Memory networks. The proposed deep network effectively learns and models discriminative temporal patterns from EEG sequential data. Especially, the features are automatically discovered from the raw EEG data without any pre-processing step, eliminating humans from laborious feature design task. We also show that, in the epilepsy scenario, simple architectures can achieve competitive performance. Using simple architectures significantly benefits in the practical scenario considering their low computation complexity and reduced requirement for large training datasets. Using a public dataset, a multi-fold cross-validation scheme exhibited an average validation accuracy of 95.54\\% and an average AUC of 0.9582 of the ROC curve among all sets defined in the experiment. This work reinforces the benefits of deep learning to be further attended in clinical applications and neuroscientific research.",0.2],["apnea episodes result in a rise in beat-to-beat blood","Mathematical Modeling of Arterial Blood Pressure Using Photo-Plethysmography Signal in Breath-hold Maneuver","summarize: Recent research has shown that each apnea episode results in a significant rise in the beat-to-beat blood pressure and by a drop to the pre-episode levels when patient resumes normal breathing. While the physiological implications of these repetitive and significant oscillations are still unknown, it is of interest to quantify them. Since current array of instruments deployed for polysomnography studies does not include beat-to-beat measurement of blood pressure, but includes oximetry, it is both of clinical interest to estimate the magnitude of BP oscillations from the photoplethysmography signal that is readily available from sleep lab oximeters. We have investigated a new method for continuous estimation of systolic , diastolic , and mean blood pressure waveforms from PPG. Peaks and troughs of PPG waveform are used as input to a 5th order autoregressive moving average model to construct estimates of SBP, DBP, and MBP waveforms. Since breath hold maneuvers are shown to simulate apnea episodes faithfully, we evaluated the performance of the proposed method in 7 subjects in supine position doing 5 breath maneuvers with 90s of normal breathing between them. The modeling error ranges were -0.88+-4.87 to -2.19+-5.73 ; 0.29+-2.39 to -0.97+-3.83 ; and -0.42+-2.64 to -1.17+-3.82 . The cross validation error ranges were 0.28+-6.45 to -1.74+-6.55 ; 0.09+-3.37 to -0.97+-3.67 ; and 0.33+-4.34 to -0.87+-4.42 . The level of estimation error in, as measured by the root mean squared of the model residuals, was less than 7 mmHg",0.4299187863],["an exoskeleton is a wearable electromechanical structure that can be used by disabled","Voice Controlled Upper Body Exoskeleton: A Development For Industrial Application","summarize: An exoskeleton is a wearable electromechanical structure that is intended to resemble and allow movements in a manner similar to the human skeletal system. They can be used by both disabled and able people alike to increase physical strength in carrying out tasks that would be otherwise difficult, or as a rehabilitation device to aid in physiotherapeutic activities of a weakened body part. This paper intends to introduce a voicecontrolled upper body exoskeleton for industrial applications which can aid workers wearing it by reducing stresses on their arms and shoulders over longer periods and add up to 20kg more strength in lifting applications. The 3D design, calculations and considerations, and load analysis are presented along with brief results of a basic prototype model of the exoskeleton.",0.0833333333],["a set of labeled training pairs is based on a convolutional neural","Modeling Surface Appearance from a Single Photograph using Self-augmented Convolutional Neural Networks","summarize: We present a convolutional neural network based solution for modeling physically plausible spatially varying surface reflectance functions from a single photograph of a planar material sample under unknown natural illumination. Gathering a sufficiently large set of labeled training pairs consisting of photographs of SVBRDF samples and corresponding reflectance parameters, is a difficult and arduous process. To reduce the amount of required labeled training data, we propose to leverage the appearance information embedded in unlabeled images of spatially varying materials to self-augment the training process. Starting from an initial approximative network obtained from a small set of labeled training pairs, we estimate provisional model parameters for each unlabeled training exemplar. Given this provisional reflectance estimate, we then synthesize a novel temporary labeled training pair by rendering the exact corresponding image under a new lighting condition. After refining the network using these additional training samples, we re-estimate the provisional model parameters for the unlabeled data and repeat the self-augmentation process until convergence. We demonstrate the efficacy of the proposed network structure on spatially varying wood, metals, and plastics, as well as thoroughly validate the effectiveness of the self-augmentation training process.",0.6428571429],["pinnerSage is an end-to-end recommender system that represents each user via","PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest","summarize: Latent user representations are widely adopted in the tech industry for powering personalized recommender systems. Most prior work infers a single high dimensional embedding to represent a user, which is a good starting point but falls short in delivering a full understanding of the user's interests. In this work, we introduce PinnerSage, an end-to-end recommender system that represents each user via multi-modal embeddings and leverages this rich representation of users to provides high quality personalized recommendations. PinnerSage achieves this by clustering users' actions into conceptually coherent clusters with the help of a hierarchical clustering method and summarizes the clusters via representative pins for efficiency and interpretability. PinnerSage is deployed in production at Pinterest and we outline the several design decisions that makes it run seamlessly at a very large scale. We conduct several offline and online A\/B experiments to show that our method significantly outperforms single embedding methods.",0.0],["coronagraph observational phenomena known as coronal inflows and in\/out pairs","A Model for Coronal Inflows and In\/Out Pairs","summarize: This report presents a three-dimensional numerical magnetohydrodynamics model of the white-light coronagraph observational phenomena known as coronal inflows and in\/out pairs. Coronal inflows in the LASCO\/C2 field of view were thought to arise from the dynamic and intermittent release of solar wind plasma associated with the helmet streamer belt as the counterpart to outward-propagating streamer blobs, formed by magnetic reconnection. This interpretation was essentially confirmed with the subsequent identification of in\/out pairs and the multispacecraft observations of their 3D structure. The MHD simulation results show relatively narrow lanes of density depletion form high in the corona and propagate inwards with sinuous motion which has been characterized as `tadpole-like' in coronagraph imagery. The height--time evolution and velocity profiles of the simulation inflows and in\/out pairs are compared to their corresponding observations and a detailed analysis of the underlying magnetic field structure associated with the synthetic white-light and mass density evolution is presented. Understanding the physical origin of this structured component of the slow solar wind's intrinsic variability could make a significant contribution to solar wind modeling and the interpretation of remote and in-situ observations from Parker Solar Probe and Solar Orbiter.",0.1],["class of monotone operators is deemed suitable for symbolic representation and manipulation within a computer algebra","Symbolic computation with monotone operators","summarize: We consider a class of monotone operators which are appropriate for symbolic representation and manipulation within a computer algebra system. Various structural properties of the class are investigated as well as the role played by maximal monotonicity within the class. In particular, we show that there is a natural correspondence between our class of monotone operators and the subdifferentials of convex functions belonging to a class of convex functions deemed suitable for symbolic computation of Fenchel conjugates which were previously studied by Bauschke & von Mohrenschildt and by Borwein & Hamilton. A number of illustrative examples utilizing the introduced class of operators are provided including computation of proximity operators, recovery of a convex penalty function associated with the hard thresholding operator, and computation of superexpectations, superdistributions and superquantiles with specialization to risk measures.",0.16],["cellular force response is a powerful tool to adjust stiffness through nonlinear material properties","Active prestress leads to an apparent stiffening of cells through geometrical effects","summarize: Tuning of active prestress e.g. through activity of molecular motors constitutes a powerful cellular tool to adjust cellular stiffness through nonlinear material properties. Understanding this tool is an important prerequisite for our comprehension of cellular force response, cell shape dynamics and tissue organisation. Experimental data obtained from cell-mechanical measurements often show a simple linear dependence between mechanical prestress and measured differential elastic moduli. While these experimental findings could point to stress-induced structural changes in the material, we propose here a surprisingly simple alternative explanation in a theoretical study. We show how geometrical effects can give rise to increased cellular force response of cells in the presence of active prestress. The associated effective stress-stiffening is disconnected from actual stress-induced changes of the elastic modulus and should therefore be regarded as an apparent stiffening of the material. We argue that new approaches in experimental design are necessary to separate this apparent stress-stiffening due to geometrical effects from actual nonlinearities of the elastic modulus in prestressed cellular material.",0.35],["the world is becoming more interconnected every day. scenarios based on the internet of Things","Blockchain in the Internet of Things: Architectures and Implementation","summarize: The world is becoming more interconnected every day. With the high technological evolution and the increasing deployment of it in our society, scenarios based on the Internet of Things can be considered a reality nowadays. However, and before some predictions become true , many efforts must be carried out in terms of scalability and security. In this study we propose and evaluate a new approach based on the incorporation of Blockchain into current IoT scenarios. The main contributions of this study are as follows: i) an in-depth analysis of the different possibilities for the integration of Blockchain into IoT scenarios, focusing on the limited processing capabilities and storage space of most IoT devices, and the economic cost and performance of current Blockchain technologies; ii) a new method based on a novel module named BIoT Gateway that allows both unidirectional and bidirectional communications with IoT devices on real scenarios, allowing to exchange any kind of data; and iii) the proposed method has been fully implemented and validated on two different real-life IoT scenarios, extracting very interesting findings in terms of economic cost and execution time. The source code of our implementation is publicly available in the Ethereum testnet.",0.2777777778],["photonic crystal-based biosensors are highly effective and low-cost. they are","Hybrid 1D Plasmonic\/Photonic Crystals are Responsive to Escherichia Coli","summarize: Photonic crystal-based biosensors hold great promise as valid and low-cost devices for real-time monitoring of a variety of biotargets. Given the high processability and easiness of read-out even for unskilled operators, these systems can be highly appealing for the detection of bacterial contaminants in food and water. Here, we propose a novel hybrid plasmonic\/photonic device that is responsive to Escherichia coli, which is one of the most hazardous pathogenic bacterium. Our system consists of a thin layer of silver, a metal that exhibits both a plasmonic behavior and a well-known biocidal activity, on top of a solution processed 1D photonic crystal. We attribute the bio-responsivity to the modification of the dielectric properties of the silver film upon bacterial contamination, an effect that likely stems from the formation of polarization charges at the Ag\/bacterium interface within a sort of bio-doping mechanism. Interestingly, this triggers a blue-shift in the photonic response. This work demonstrates that our hybrid plasmonic\/photonic device can be a low-cost and portable platform for the detection of common contaminants in food and water.",0.1666666667],["neural networks trained with noisy labels have the capacity to overfit noisy labels frequently found in real-","Coresets for Robust Training of Neural Networks against Noisy Labels","summarize: Modern neural networks have the capacity to overfit noisy labels frequently found in real-world datasets. Although great progress has been made, existing techniques are limited in providing theoretical guarantees for the performance of the neural networks trained with noisy labels. Here we propose a novel approach with strong theoretical guarantees for robust training of deep networks trained with noisy labels. The key idea behind our method is to select weighted subsets of clean data points that provide an approximately low-rank Jacobian matrix. We then prove that gradient descent applied to the subsets do not overfit the noisy labels. Our extensive experiments corroborate our theory and demonstrate that deep networks trained on our subsets achieve a significantly superior performance compared to state-of-the art, e.g., 6% increase in accuracy on CIFAR-10 with 80% noisy labels, and 7% increase in accuracy on mini Webvision.",0.0909090909],["a new type of dependent thinning for point processes in continuous space is proposed.","Determinantal thinning of point processes with network learning applications","summarize: A new type of dependent thinning for point processes in continuous space is proposed, which leverages the advantages of determinantal point processes defined on finite spaces and, as such, is particularly amenable to statistical, numerical, and simulation techniques. It gives a new point process that can serve as a network model exhibiting repulsion. The properties and functions of the new point process, such as moment measures, the Laplace functional, the void probabilities, as well as conditional characteristics can be estimated accurately by simulating the underlying point process, which can be taken, for example, to be Poisson. This is in contrast finite Gibbs point processes, which, instead of thinning, require weighting the Poisson realizations, involving usually intractable normalizing constants. Models based on determinantal point processes are also well suited for statistical learning techniques, allowing the models to be fitted to observed network patterns with some particular geometric properties. We illustrate this approach by imitating with determinantal thinning the well-known Matrn~II hard-core thinning, as well as a soft-core thinning depending on nearest-neighbour triangles. These two examples demonstrate how the proposed approach can lead to new, statistically optimized, probabilistic transmission scheduling schemes.",0.5789473684],["a cluster of sensors be connected by the communication links. each sensor observes a","Distributed Data Compression in Sensor Clusters: A Maximum Independent Flow Approach","summarize: Let a cluster of sensors be connected by the communication links, each link having a capacity upper bound. Each sensor observes a discrete random variable in private and one sensor serves as a cluster header or sink. Here, we formulate the problem of how to let the sensors encode their observations such that the direction of compressed data is a feasible flow towards the sink. We demonstrate that this problem can be solved by an existing maximum independent flow algorithm in polynomial time. Further, we reveal that this algorithm in fact determines an optimal solution by recursively pushing the remaining randomness in the sources via unsaturated communication links towards the sink. We then show that the MIF algorithm can be implemented in a distributed manner. For those networks with integral communication capacities, we propose an integral MIF algorithm which completes much faster than MIF. Finally, we point out that the nature of the data compression problem in a sensor cluster is to seek the maximum independent information flow in the intersection of two submodular polyhedra, which can be further utilized to improve the MIF algorithm in the future.",0.380952381],["Dirac spinor fields in five and four dimensions share many features. in five dimensions we","Dirac spinors and their application to Bianchi-I space-times in 5 dimensions","summarize: We consider a five-dimensional Einstein--Cartan spacetime upon which Dirac spinor fields can be defined. Dirac spinor fields in five and four dimensions share many features, like the fact that both are described by four-component spinor fields, but they are also characterized by strong differences, like the fact that in five dimensions we do not have the possibility to project on left-handed and right-handed chiral parts unlike what happens in the four-dimensional instance: we conduct a polar decomposition of the spinorial fields, so to highlight all similarities and discrepancies. As an application of spinor fields in five dimensions, we study Bianchi-I spacetimes, verifying whether the Dirac fields in five dimensions can give rise to inflation or dark-energy dominated cosmological eras or not.",0.3913043478],["we show the convergence results for iterates of Bregman projections onto closed linear sub","On the rate of convergence of iterated Bregman projections and of the alternating algorithm","summarize: We study the alternating algorithm for the computation of the metric projection onto the closed sum of two closed subspaces in uniformly convex and uniformly smooth Banach spaces. For Banach spaces which are convex and smooth of power type, we exhibit a condition which implies linear convergence of this method. We show these convergence results for iterates of Bregman projections onto closed linear subspaces. Using an intimate connection between the metric projection onto a closed linear subspace and the Bregman projection onto its annihilator, we deduce the convergence rate results for the alternating algorithm from the corresponding results for the iterated Bregman projection method.",0.3571428571],["we analyze required launch conditions, the effect of multiplexing and noise on the nonlinear","Silicon Photonics DWDM NLFT Soliton Transmitter","summarize: We investigate the transmission of densely multiplexed solitons using a photonic integrated chip and the nonlinear Fourier-transform and analyze required launch conditions, the effect of multiplexing and noise on the nonlinear spectrum, and equalization techniques that can be used to enhance the transmission performance.",0.1578947368],["study of chemical reactions at very low energies is of importance for understanding of chemical processes.","Cold ion chemistry","summarize: Studying chemical reactions at very low temperatures is of importance for the understanding of fundamental physical and chemical processes. At very low energies, collisions are dominated by only a few partial waves. Thus, studies in this regime allow the characterisation of quantum effects which depend on the collisional angular momentum, e.g., reactive scattering resonances and tunneling through centrifugal barriers. Additionally, the dynamics of ultralow-energy collisions is dominated by long-range interactions, i.e., universal chemical forces. Thus, experiments in this domain probe the details of intermolecular interactions which is of general relevance for the understanding of chemical processes. Moreover, the increasingly more precise experiments provide valuable data for benchmarking theoretical models and quantum-chemical calculations. Studies of cold ion-neutral reactions in the laboratory rely on techniques for the generation of cold ions and cold neutrals. Many of these techniques do not only allow the translational cooling of the molecules, but also a precise reparation of their internal quantum state. Thus, by an accurate preparation of both the energy and state of the reaction partners as well as by the application of external electric, magnetic or optical fields, prospects open up to study and control chemical processes at an unprecedented level of accuracy. In this chapter, we will review salient theoretical concepts and recent experimental developments for studies of ion-molecule reactions at low temperatures. We first discuss the theoretical background for the description of low-energy ion-molecule reactions. Next we present key experimental methods for laboratory studies of cold ion-molecule reactions. The chapter concludes with a review of illustrative results and an outlook on future directions.",0.0416666667],["smart thermostats are becoming pervasive and ubiquitous in residential and commercial buildings. smart thermostat","Beyond Control: Enabling Smart Thermostats For Leakage Detection","summarize: Smart thermostats, with multiple sensory abilities, are becoming pervasive and ubiquitous, in both residential and commercial buildings. By analyzing occupants' behavior, adjusting set temperature automatically, and adapting to temporal and spatial changes in the atmosphere, smart thermostats can maximize both - energy savings and user comfort. In this paper, we study smart thermostats for refrigerant leakage detection. Retail outlets, such as milk-booths and quick service restaurants set up cold-rooms to store perishable items. In each room, a refrigeration unit is used to maintain a suitable temperature for the stored products. Often, refrigerant leaks through the coils of the refrigeration unit which slowly diminishes the cooling capacity of the refrigeration unit while allowing it to be functional. Such leaks waste significant energy, risk occupants' health, and impact the quality of stored perishable products. While store managers usually fail to sense the early symptoms of such leaks, current techniques to report refrigerant leakage are often not scalable. We propose Greina - to continuously monitor the readily available ambient information from the thermostat and timely report such leaks. We evaluate our approach on 74 outlets of a retail enterprise and results indicate that Greina can report the leakage a week in advance when compared to manual reporting.",0.0476190476],["the computational bottleneck is because of the similarity between a read and candidate locations in that","GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment in DNA Short Read Mapping","summarize: Motivation: High throughput DNA sequencing technologies generate an excessive number of small DNA segments -- called short reads -- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and candidate locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: it is implemented using quadratic-time dynamic programming algorithms, and the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment operations. Results: We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays , which can perform pre-alignment much faster than software. GateKeeper can be integrated with any mapper that performs sequence alignment for verification. When implemented on a single FPGA chip, GateKeeper maintains high accuracy while providing up to 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance , respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability: https:\/\/github.com\/BilkentCompGen\/GateKeeper",0.2413793103],["deep Convolutional Neural Network has shown promising progress in many applications fields. we applied","Deep Neural Network with l2-norm Unit for Brain Lesions Detection","summarize: Automated brain lesions detection is an important and very challenging clinical diagnostic task because the lesions have different sizes, shapes, contrasts, and locations. Deep Learning recently has shown promising progress in many application fields, which motivates us to apply this technology for such important problem. In this paper, we propose a novel and end-to-end trainable approach for brain lesions classification and detection by using deep Convolutional Neural Network . In order to investigate the applicability, we applied our approach on several brain diseases including high and low-grade glioma tumor, ischemic stroke, Alzheimer diseases, by which the brain Magnetic Resonance Images have been applied as an input for the analysis. We proposed a new operating unit which receives features from several projections of a subset units of the bottom layer and computes a normalized l2-norm for next layer. We evaluated the proposed approach on two different CNN architectures and number of popular benchmark datasets. The experimental results demonstrate the superior ability of the proposed approach.",0.2],["the re-ranking method uses representations of demographic groups computed using a label","Using Image Fairness Representations in Diversity-Based Re-ranking for Recommendations","summarize: The trade-off between relevance and fairness in personalized recommendations has been explored in recent works, with the goal of minimizing learned discrimination towards certain demographics while still producing relevant results. We present a fairness-aware variation of the Maximal Marginal Relevance re-ranking method which uses representations of demographic groups computed using a labeled dataset. This method is intended to incorporate fairness with respect to these demographic groups. We perform an experiment on a stock photo dataset and examine the trade-off between relevance and fairness against a well known baseline, MMR, by using human judgment to examine the results of the re-ranking when using different fractions of a labeled dataset, and by performing a quantitative analysis on the ranked results of a set of query images. We show that our proposed method can incorporate fairness in the ranked results while obtaining higher precision than the baseline, while our case study shows that even a limited amount of labeled data can be used to compute the representations to obtain fairness. This method can be used as a post-processing step for recommender systems and search.",0.3125],["this volume contains the proceedings of the Eighth Workshop on Mathematically Structured Functional Programm","Proceedings Eighth Workshop on Mathematically Structured Functional Programming","summarize: This volume contains the proceedings of the Eighth Workshop on Mathematically Structured Functional Programming . The meeting was originally scheduled to take place in Dublin, Ireland on the 25th of April as a satellite event of the European Joint Conferences on Theory & Practice of Software . Due to the COVID-19 pandemic, ETAPS 2020, and consequently MSFP 2020, has been postponed to a date yet to be determined. The MSFP workshop highlights applications of mathematical structures to programming applications. We promote the use of category theory, type theory, and formal language semantics to the development of simple and reasonable programs. This year's papers cover a variety of topics ranging from array programming to dependent types to effects.",0.5],["low thermal conductivity is favorable for preserving the temperature gradient between the two ends of a","Ultralow thermal conductivity from transverse acoustic phonon suppression in distorted crystalline -MgAgSb","summarize: Low thermal conductivity is favorable for preserving the temperature gradient between the two ends of a thermoelectric material in order to ensure continuous electron current generation. In high-performance thermoelectric materials, there are two main low thermal conductivity mechanisms: the phonon anharmonic in PbTe and SnSe and phonon scattering resulting from the dynamic disorder in AgCrSe2 and CuCrSe2, which have been successfully revealed by inelastic neutron scattering. Using neutron scattering and ab initio calculations, we report here a mechanism of static local structure distortion combined with phonon-anharmonic-induced ultralow lattice thermal conductivity in -MgAgSb. Since the transverse acoustic phonons are almost fully scattered by the compound's intrinsic distorted rocksalt sublattice, the heat is mainly transported by the longitudinal acoustic phonons. The ultralow thermal conductivity in -MgAgSb is attributed to its atomic dynamics being altered by the structure distortion, which presents a possible microscopic route to enhance the performance of similar thermoelectric materials.",0.44],["we study the dynamics of the fluctuations of the variance.","Dynamics of fluctuations in the Gaussian model with dissipative Langevin Dynamics","summarize: We study the dynamics of the fluctuations of the variance ",0.3016124727],["sensitivity and sensing range are arguably the greatest challenges in microwave sensor design. recent","Strongly Enhanced Sensitivity in Planar Microwave Sensors Based on Metamaterial coupling","summarize: Limited sensitivity and sensing range are arguably the greatest challenges in microwave sensor design. Recent attempts to improve these properties have relied on metamaterial- inspired open-loop resonators coupled to transmission lines . Although the strongly resonant properties of the OLR sensitively reflect small changes in the environment through a shift in its resonance frequency, the resulting sensitivities remain ultimately limited by the level of coupling between the OLR and the TL. This work introduces a novel solution to this problem that employs negative-refractiveindex TL MTMs to substantially improve this coupling so as to fully exploit its resonant properties. A MTM-infused planar microwave sensor is designed for operation at 2.5 GHz, and is shown to exhibit a significant improvement in sensitivity and linearity. A rigorous signal-flow analysis of the sensor is proposed and shown to provide a fully analytical description of all salient features of both the conventional and MTM-infused sensors. Full-wave simulations confirm the analytical predictions, and all data demonstrate excellent agreement with measurements of a fabricated prototype. The proposed device is shown to be especially useful in the characterization of commonly-available high-permittivity liquids as well as in sensitively distinguishing concentrations of ethanol\/methanol in water.",0.1333333333],["a new approach is data-driven SP. it involves integrating the equations of","Data-driven super-parameterization using deep learning: Experimentation with multi-scale Lorenz 96 systems and transfer-learning","summarize: To make weather\/climate modeling computationally affordable, small-scale processes are usually represented in terms of the large-scale, explicitly-resolved processes using physics-based or semi-empirical parameterization schemes. Another approach, computationally more demanding but often more accurate, is super-parameterization , which involves integrating the equations of small-scale processes on high-resolution grids embedded within the low-resolution grids of large-scale processes. Recently, studies have used machine learning to develop data-driven parameterization schemes. Here, we propose a new approach, data-driven SP , in which the equations of the small-scale processes are integrated data-drivenly using ML methods such as recurrent neural networks. Employing multi-scale Lorenz 96 systems as testbed, we compare the cost and accuracy of parameterized low-resolution , SP, DD-P, and DD-SP models. We show that with the same computational cost, DD-SP substantially outperforms LR, and is better than DD-P, particularly when scale separation is lacking. DD-SP is much cheaper than SP, yet its accuracy is the same in reproducing long-term statistics and often comparable in short-term forecasting. We also investigate generalization, finding that when models trained on data from one system are applied to a system with different forcing , the models often do not generalize, particularly when the short-term prediction accuracy is examined. But we show that transfer-learning, which involves re-training the data-driven model with a small amount of data from the new system, significantly improves generalization. Potential applications of DD-SP and transfer-learning in climate\/weather modeling and the expected challenges are discussed.",0.4089086287],["the fundamental power allocation requirements for NOMA systems are investigated. for any minimum QoS","On the Power Allocation Limits for Downlink Multi-user NOMA with QoS","summarize: The fundamental power allocation requirements for NOMA systems with minimum quality of service requirements are investigated. For any minimum QoS rate ",0.3125],["bow shocks are created by a magnetized shock model. the molecules formed in the","H","summarize: When a fast moving star or a protostellar jet hits an interstellar cloud, the surrounding gas gets heated and illuminated: a bow shock is born which delineates the wake of the impact. In such a process, the new molecules that are formed and excited in the gas phase become accessible to observations. In this article, we revisit models of H2 emission in these bow shocks. We approximate the bow shock by a statistical distribution of planar shocks computed with a magnetized shock model. We improve on previous works by considering arbitrary bow shapes, a finite irradiation field, and by including the age effect of non-stationary C-type shocks on the excitation diagram and line profiles of H2. We also examine the dependence of the line profiles on the shock velocity and on the viewing angle: we suggest that spectrally resolved observations may greatly help to probe the dynamics inside the bow shock. For reasonable bow shapes, our analysis shows that low velocity shocks largely contribute to H2 excitation diagram. This can result in an observational bias towards low velocities when planar shocks are used to interpret H2 emission from an unresolved bow. We also report a large magnetization bias when the velocity of the planar model is set independently. Our 3D models reproduce excitation diagrams in BHR71 and Orion bow shocks better than previous 1D models. Our 3D model is also able to reproduce the shape and width of the broad H2 1-0S line profile in an Orion bow shock.",0.0],["Blau and Michaeli introduce a concept for inverse problems of signal processing. we","Rate-Distortion-Perception Tradeoff of Variable-Length Source Coding for General Information Sources","summarize: Blau and Michaeli recently introduced a novel concept for inverse problems of signal processing, that is, the perception-distortion tradeoff. We introduce their tradeoff into the rate distortion theory of variable-length lossy source coding in information theory, and clarify the tradeoff among information rate, distortion and perception for general information sources. We also discuss the fixed-length coding with average distortion criterion that was missing in the previous letter.",0.4117647059],["we present optimal binary pebbling algorithms for in-place reversal of one-","Binary Pebbling Algorithms for In-Place Reversal of One-Way Hash Chains","summarize: We present optimal binary pebbling algorithms for in-place reversal of one-way hash chains. For a hash chain of length ",0.1818181818],["network operators rely on transmission planning tools designed for generalized environments. a challenge to","Improved propagation models for lte path loss prediction in urban & suburban Ghana","summarize: To maximize the benefits of LTE cellular networks, careful and proper planning is needed. This requires the use of accurate propagation models to quantify the path loss required for base station deployment. Deployed LTE networks in Ghana can barely meet the desired 100Mbps throughput leading to customer dissatisfaction. Network operators rely on transmission planning tools designed for generalized environments that come with already embedded propagation models suited to other environments. A challenge therefore to Ghanaian transmission Network planners will be choosing an accurate and precise propagation model that best suits the Ghanaian environment. Given this, extensive LTE path loss measurements at 800MHz and 2600MHz were taken in selected urban and suburban environments in Ghana and compared with 6 commonly used propagation models. Improved versions of the Ericson, SUI, and ECC-33 developed in this study predict more precisely the path loss in Ghanaian environments compared with commonly used propagation models.",0.3913043478],["the BRCC is useful for monitoring fraction, rate and\/or proportion data sets when they","Beta regression control chart for monitoring fractions and proportions","summarize: Regression control charts are usually used to monitor variables of interest that are related to control variables. However, for fraction and\/or proportion data, the use of standard regression control charts may not be adequate, since the linear regression model assumes the normality of the interest variable. To work around this problem, we propose the beta regression control chart . The BRCC is useful for monitoring fraction, rate and\/or proportion data sets when they are related to control variables. The proposed control chart assumes that the mean and dispersion parameters of beta distributed variables are related to the exogenous variables, being modeled using regression structures. The BRCC is numerically assessed through an extensive Monte Carlo simulation study, showing good performance in terms of average run length . Two applications to real data are presented, evidencing the practical applicability of the proposed method.",0.2],["linear regularization theory is based on the universal property of the tensor product","Non-convex regularization of bilinear and quadratic inverse problems by tensorial lifting","summarize: Considering the question: how non-linear may a non-linear operator be in order to extend the linear regularization theory, we introduce the class of dilinear mappings, which covers linear, bilinear, and quadratic operators between Banach spaces. The corresponding dilinear inverse problems cover blind deconvolution, deautoconvolution, parallel imaging in MRI, and the phase retrieval problem. Based on the universal property of the tensor product, the central idea is here to lift the non-linear mappings to linear representatives on a suitable topological tensor space. At the same time, we extend the class of usually convex regularization functionals to the class of diconvex functionals, which are likewise defined by a tensorial lifting. Generalizing the concepts of subgradients and Bregman distances from convex analysis to the new framework, we analyse the novel class of dilinear inverse problems and establish convergence rates under similar conditions than in the linear setting. Considering the deautoconvolution problem as specific application, we derive satisfiable source conditions and validate the theoretical convergence rates numerically.",0.3333333333],["the high level trigger is a compute cluster. it reconstructs collisions as recorded","Online Calibration of the TPC Drift Time in the ALICE High Level Trigger","summarize: ALICE is one of four major experiments at the Large Hadron Collider at CERN. The High Level Trigger is a compute cluster, which reconstructs collisions as recorded by the ALICE detector in real-time. It employs a custom online data-transport framework to distribute data and workload among the compute nodes. ALICE employs subdetectors sensitive to environmental conditions such as pressure and temperature, e.g. the Time Projection Chamber . A precise reconstruction of particle trajectories requires the calibration of these detectors. Performing the calibration in real time in the HLT improves the online reconstructions and renders certain offline calibration steps obsolete speeding up offline physics analysis. For LHC Run 3, starting in 2020 when data reduction will rely on reconstructed data, online calibration becomes a necessity. Reconstructed particle trajectories build the basis for the calibration making a fast online-tracking mandatory. The main detectors used for this purpose are the TPC and ITS . Reconstructing the trajectories in the TPC is the most compute-intense step. We present several improvements to the ALICE High Level Trigger developed to facilitate online calibration. The main new development for online calibration is a wrapper that can run ALICE offline analysis and calibration tasks inside the HLT. On top of that, we have added asynchronous processing capabilities to support long-running calibration tasks in the HLT framework, which runs event-synchronously otherwise. In order to improve the resiliency, an isolated process performs the asynchronous operations such that even a fatal error does not disturb data taking. We have complemented the original loop-free HLT chain with ZeroMQ data-transfer components. ",0.2],["general model contains many existing plane-based clustering methods. the general model is a","A general model for plane-based clustering with loss function","summarize: In this paper, we propose a general model for plane-based clustering. The general model contains many existing plane-based clustering methods, e.g., k-plane clustering , proximal plane clustering , twin support vector clustering and its extensions. Under this general model, one may obtain an appropriate clustering method for specific purpose. The general model is a procedure corresponding to an optimization problem, where the optimization problem minimizes the total loss of the samples. Thereinto, the loss of a sample derives from both within-cluster and between-cluster. In theory, the termination conditions are discussed, and we prove that the general model terminates in a finite number of steps at a local or weak local optimal point. Furthermore, based on this general model, we propose a plane-based clustering method by introducing a new loss function to capture the data distribution precisely. Experimental results on artificial and public available datasets verify the effectiveness of the proposed method.",0.375],["deep learning is experiencing a trend towards producing reproducible research. deepdIVA is","Improving Reproducible Deep Learning Workflows with DeepDIVA","summarize: The field of deep learning is experiencing a trend towards producing reproducible research. Nevertheless, it is still often a frustrating experience to reproduce scientific results. This is especially true in the machine learning community, where it is considered acceptable to have black boxes in your experiments. We present DeepDIVA, a framework designed to facilitate easy experimentation and their reproduction. This framework allows researchers to share their experiments with others, while providing functionality that allows for easy experimentation, such as: boilerplate code, experiment management, hyper-parameter optimization, verification of data integrity and visualization of data and results. Additionally, the code of DeepDIVA is well-documented and supported by several tutorials that allow a new user to quickly familiarize themselves with the framework.",0.0555555556],["the 'exponential Monte Carlo with Counter' is a new method for adaptive","An Evaluation of Monte Carlo-Based Hyper-Heuristic for Interaction Testing of Industrial Embedded Software Applications","summarize: Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the Exponential Monte Carlo with Counter . Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results of this study show that Q-EMCQ is an efficient technique for test case generation. Additionally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach.",0.4761459703],["decentralized visual simultaneous localization and mapping is a powerful tool for multi-robot applications","Data-Efficient Decentralized Visual SLAM","summarize: Decentralized visual simultaneous localization and mapping is a powerful tool for multi-robot applications in environments where absolute positioning systems are not available. Being visual, it relies on cameras, cheap, lightweight and versatile sensors, and being decentralized, it does not rely on communication to a central ground station. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and co-optimization, existing decentralized visual SLAM systems regularly exchange the full map data between all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: in the first stage a compact full-image descriptor is deterministically sent to only one robot. In the second stage, which is only executed if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available data and we provide open access to the code.",0.2173913043],["we derive an analytical model describing the effect of filtering on amplified spontaneous emission","Effect of Electrical Filtering on Level Dependent ASE Noise","summarize: We derive an analytical model describing the effect of filtering on amplified spontaneous emission noise during or after opto-electronic conversion. In particular, we show that electrical filtering results in a further reduction of the signal quality factor associated with an effective increase of the noise levels and can lead to counter-intuitive dependencies of the measured signal quality on the characteristics of the test setup. Closed form equations are compared with numerical models and experiments, showing excellent agreement.",0.1111111111],["the development of transparent Reinforcement Learning models has received much attention. but there are few","Contrastive Explanations for Reinforcement Learning in terms of Expected Consequences","summarize: Machine Learning models become increasingly proficient in complex tasks. However, even for experts in the field, it can be difficult to understand what the model learned. This hampers trust and acceptance, and it obstructs the possibility to correct the model. There is therefore a need for transparency of machine learning models. The development of transparent classification models has received much attention, but there are few developments for achieving transparent Reinforcement Learning models. In this study we propose a method that enables a RL agent to explain its behavior in terms of the expected consequences of state transitions and outcomes. First, we define a translation of states and actions to a description that is easier to understand for human users. Second, we developed a procedure that enables the agent to obtain the consequences of a single action, as well as its entire policy. The method calculates contrasts between the consequences of a policy derived from a user query, and of the learned policy of the agent. Third, a format for generating explanations was constructed. A pilot survey study was conducted to explore preferences of users for different explanation properties. Results indicate that human users tend to favor explanations about policy rather than about single actions.",0.1764705882],["a limited number of available investigations have focused on non-reacting flows near engine surfaces","Flame\/flow dynamics at the piston surface of an IC engine measured by high-speed PLIF and PTV","summarize: Resolving fluid transport at engine surfaces is required to predict transient heat loss, which is becoming increasingly important for the development of high-efficiency internal combustion engines . The limited number of available investigations have focused on non-reacting flows near engine surfaces, while this work focuses on the near-wall flow field dynamics in response to a propagating flame front. Flow-field and flame distributions were measured simultaneously at kHz repetition rates using particle tracking velocimetry and planar laser induced fluorescence of sulfur dioxide . Measurements were performed near the piston surface of an optically accessible engine operating at 800 rpm with homogeneous, stoichiometric isooctane-air mixtures. High-speed measurements reveal a strong interdependency between near-wall flow and flame development which also influences subsequent combustion. A conditional analysis is performed to analyze flame\/flow dynamics at the piston surface for cycles with weak and strong flow velocities parallel to the surface. Faster flame propagation associated with higher velocities before ignition demonstrates a stronger flow acceleration ahead of the flame. Flow acceleration associated with an advancing flame front is a transient feature that strongly influences boundary layer development. The distance from the wall to 75% maximum velocity is analyzed to compare boundary layer development between fired and motored datasets. Decreases in 75 are strongly related to flow acceleration produced by an approaching flame front. Measurements reveal strong deviations of the boundary layer flow between fired and motored datasets, emphasizing the need to consider transient flow behavior when modelling boundary layer physics for reacting flows.",0.3611991249],["natural language processing is a branch of computer science that combines artificial intelligence with linguistics","Challenges Encountered in Turkish Natural Language Processing Studies","summarize: Natural language processing is a branch of computer science that combines artificial intelligence with linguistics. It aims to analyze a language element such as writing or speaking with software and convert it into information. Considering that each language has its own grammatical rules and vocabulary diversity, the complexity of the studies in this field is somewhat understandable. For instance, Turkish is a very interesting language in many ways. Examples of this are agglutinative word structure, consonant\/vowel harmony, a large number of productive derivational morphemes , derivation and syntactic relations, a complex emphasis on vocabulary and phonological rules. In this study, the interesting features of Turkish in terms of natural language processing are mentioned. In addition, summary info about natural language processing techniques, systems and various sources developed for Turkish are given.",0.25],["fluoroscopic imaging can further acquire X-ray images at video frame rates. this","Modelling Errors in X-ray Fluoroscopic Imaging Systems Using Photogrammetric Bundle Adjustment With a Data-Driven Self-Calibration Approach","summarize: X-ray imaging is a fundamental tool of routine clinical diagnosis. Fluoroscopic imaging can further acquire X-ray images at video frame rates, thus enabling non-invasive in-vivo motion studies of joints, gastrointestinal tract, etc. For both the qualitative and quantitative analysis of static and dynamic X-ray images, the data should be free of systematic biases. Besides precise fabrication of hardware, software-based calibration solutions are commonly used for modelling the distortions. In this primary research study, a robust photogrammetric bundle adjustment was used to model the projective geometry of two fluoroscopic X-ray imaging systems. However, instead of relying on an expert photogrammetrist's knowledge and judgement to decide on a parametric model for describing the systematic errors, a self-tuning data-driven approach is used to model the complex non-linear distortion profile of the sensors. Quality control from the experiment showed that 0.06 mm to 0.09 mm 3D reconstruction accuracy was achievable post-calibration using merely 15 X-ray images. As part of the bundle adjustment, the location of the virtual fluoroscopic system relative to the target field can also be spatially resected with an RMSE between 3.10 mm and 3.31 mm.",0.1653533794],["the faa is considering remote ID systems for unmanned aerial vehicles. the systems","Experiments with a LoRaWAN-Based Remote ID System for Locating Unmanned Aerial Vehicles ","summarize: Federal Aviation Administration of the United States is considering Remote ID systems for unmanned aerial vehicles . These systems act as license plates used on automobiles, but they transmit information using radio waves. To be useful, the transmissions in such systems need to reach long distances to minimize the number of ground stations to capture these transmissions. LoRaWAN is designed as a cheap long-range technology to be used for long-range communication for the Internet of Things. Several manufacturers make LoRaWAN modules, which are readily available on the market and are, therefore, ideal for the UAVs Remote IDs at a low cost. In this paper, we present our experiences in using LoRaWAN technology as a communication technology. Our experiments to identify and locate the UAV systems uncovered several issues of using LoRaWAN in such systems that are documented in this paper. Using several ground stations, we can determine the location of a UAV equipped with a LoRaWAN module that transmits the UAV Remote ID. Hence, it can help identify UAVs that unintentionally, or intentionally, fly into restricted zones.",0.1176470588],["a central potential is dominated by a central potential. the system is driven into","Isotropic-Nematic Phase Transitions in Gravitational Systems","summarize: We examine dense self-gravitating stellar systems dominated by a central potential, such as nuclear star clusters hosting a central supermassive black hole. Different dynamical properties of these systems evolve on vastly different timescales. In particular, the orbital-plane orientations are typically driven into internal thermodynamic equilibrium by vector resonant relaxation before the orbital eccentricities or semimajor axes relax. We show that the statistical mechanics of such systems exhibit a striking resemblance to liquid crystals, with analogous ordered-nematic and disordered-isotropic phases. The ordered phase consists of bodies orbiting in a disk in both directions, with the disk thickness depending on temperature, while the disordered phase corresponds to a nearly isotropic distribution of the orbit normals. We show that below a critical value of the total angular momentum, the system undergoes a first-order phase transition between the ordered and disordered phases. At the critical point the phase transition becomes second-order while for higher angular momenta there is a smooth crossover. We also find metastable equilibria containing two identical disks with mutual inclinations between ",0.375],["stellar populations were measured using a stellar population synthesis code STARLIGHT. we","Eight luminous early-type galaxies in nearby pairs and sparse groups I. Stellar populations spatially analysed","summarize: We present a detailed spatial analysis of stellar populations based on long-slit optical spectra in a sample of eight luminous early-type galaxies selected from nearby sparse groups and pairs, three of them may be interacting with a galaxy of similar mass. We have measured luminosity-weighted averages of age, , , and to add empirical data relative to the influence of galaxy mass, environment, interaction, and AGN feedback in their formation and evolution. The stellar population of the individual galaxies were determined through the stellar population synthesis code STARLIGHT using semi-empirical simple stellar population models. Radial variations of luminosity-weighted means of age, , , and were measured up to half of the effective radius of each galaxy. We found trends between these values and the nuclear stellar velocity dispersion. There are also relations between the metallicity\/age gradients and the velocity dispersion. Contributions of 1-4 Gyr old stellar populations were found in IC5328 and NGC6758 as well as 4-8 Gyr old ones in NGC5812. Extended gas is present in IC5328, NGC1052, NGC1209, and NGC6758, and the presence of a LINER is identified in all these galaxies. The regions up to one effective radius of all galaxies are dominated by ",0.2966860126],["income inequality among rideshare drivers due to discriminative cancellations from riders. we proposed an","Trading the System Efficiency for the Income Equality of Drivers in Rideshare","summarize: Several scientific studies have reported the existence of the income gap among rideshare drivers based on demographic factors such as gender, age, race, etc. In this paper, we study the income inequality among rideshare drivers due to discriminative cancellations from riders, and the tradeoff between the income inequality with the system efficiency . We proposed an online bipartite-matching model where riders are assumed to arrive sequentially following a distribution known in advance. The highlight of our model is the concept of acceptance rate between any pair of driver-rider types, where types are defined based on demographic factors. Specially, we assume each rider can accept or cancel the driver assigned to her, each occurs with a certain probability which reflects the acceptance degree from the rider type towards the driver type. We construct a bi-objective linear program as a valid benchmark and propose two LP-based parameterized online algorithms. Rigorous online competitive ratio analysis is offered to demonstrate the flexibility and efficiency of our online algorithms in balancing the two conflicting goals, promotions of fairness and profit. Experimental results on a real-world dataset are provided as well, which confirm our theoretical predictions.",0.0],["modified detailed balance model captures light-absorption dependent short-circuit current, contact","Quantifying Efficiency Loss of Perovskite Solar Cells by a Modified Detailed Balance Model","summarize: A modified detailed balance model is built to understand and quantify efficiency loss of perovskite solar cells. The modified model captures the light-absorption dependent short-circuit current, contact and transport-layer modified carrier transport, as well as recombination and photon-recycling influenced open-circuit voltage. Our theoretical and experimental results show that for experimentally optimized perovskite solar cells with the power conversion efficiency of 19%, optical loss of 25%, non-radiative recombination loss of 35%, and ohmic loss of 35% are the three dominant loss factors for approaching the 31% efficiency limit of perovskite solar cells. We also find that the optical loss will climb up to 40% for a thin-active-layer design. Moreover, a misconfigured transport layer will introduce above 15% of energy loss. Finally, the perovskite-interface induced surface recombination, ohmic loss, and current leakage should be further reduced to upgrade device efficiency and eliminate hysteresis effect. The work contributes to fundamental understanding of device physics of perovskite solar cells. The developed model offers a systematic design and analysis tool to photovoltaic science and technology.",0.0],["we proposed a transfer learning-based English language learning chatbot. the output generated by","The design and implementation of Language Learning Chatbot with XAI using Ontology and Transfer Learning","summarize: In this paper, we proposed a transfer learning-based English language learning chatbot, whose output generated by GPT-2 can be explained by corresponding ontology graph rooted by fine-tuning dataset. We design three levels for systematically English learning, including phonetics level for speech recognition and pronunciation correction, semantic level for specific domain conversation, and the simulation of free-style conversation in English - the highest level of language chatbot communication as free-style conversation agent. For academic contribution, we implement the ontology graph to explain the performance of free-style conversation, following the concept of XAI to visualize the connections of neural network in bionics, and explain the output sentence from language model. From implementation perspective, our Language Learning agent integrated the mini-program in WeChat as front-end, and fine-tuned GPT-2 model of transfer learning as back-end to interpret the responses by ontology graph.",0.3507561487],["graph drawings are based on the notions of thickness and antithickness. the","Thickness and Antithickness of Graphs","summarize: This paper studies questions about duality between crossings and non-crossings in graph drawings via the notions of thickness and antithickness. The thickness of a graph ",0.1875],["numerical simulations of Nambu-Goto cosmic strings show that the loop distribution relax","Cosmic string loop production functions","summarize: Numerical simulations of Nambu-Goto cosmic strings in an expanding universe show that the loop distribution relaxes to an universal configuration, the so-called scaling regime, which is of power law shape on large scales. Precise estimations of the power law exponent are, however, still matter of debate while numerical simulations do not incorporate all the radiation and backreaction effects expected to affect the network dynamics at small scales. By using a Boltzmann approach, we show that the steepness of the loop production function with respect to loops size is associated with drastic changes in the cosmological loop distribution. For a scale factor varying as a~t^nu, we find that sub-critical loop production functions, having a Polchinski-Rocha exponent chi = \/2, are shown to be IR-physics dependent and this generically prevents the loop distribution to relax towards scaling. In the latter situation, we discuss the additional regularisations needed for convergence and show that, although a scaling regime can still be reached, the shape of the cosmological loop distribution is modified compared to the naive expectation. Finally, we discuss the implications of our findings.",0.0833333333],["we examine the distinguishing features of network-aware programming, service-oriented computing, auto","A Formal Approach to the Engineering of Domain-Specific Distributed Systems","summarize: We review some results regarding specification, programming and verification of different classes of distributed systems which stemmed from the research of the Concurrency and Mobility Group at University of Firenze. More specifically, we examine the distinguishing features of network-aware programming, service-oriented computing, autonomic computing, and collective adaptive systems programming. We then present an overview of four different languages, namely Klaim, Cows, Scel and AbC. For each language, we discuss design choices, present syntax and semantics, show how the different formalisms can be used to model and program a travel booking scenario, and describe programming environments and verification techniques.",0.1818181818],["proposed safe policy maximizes probability of disturbances remaining in desired set. a system remains","A dynamic game approach to distributionally robust safety specifications for stochastic systems","summarize: This paper presents a new safety specification method that is robust against errors in the probability distribution of disturbances. Our proposed distributionally robust safe policy maximizes the probability of a system remaining in a desired set for all times, subject to the worst possible disturbance distribution in an ambiguity set. We propose a dynamic game formulation of constructing such policies and identify conditions under which a non-randomized Markov policy is optimal. Based on this existence result, we develop a practical design approach to safety-oriented stochastic controllers with limited information about disturbance distributions. This control method can be used to minimize another cost function while ensuring safety in a probabilistic way. However, an associated Bellman equation involves infinite-dimensional minimax optimization problems since the disturbance distribution may have a continuous density. To resolve computational issues, we propose a duality-based reformulation method that converts the infinite-dimensional minimax problem into a semi-infinite program that can be solved using existing convergent algorithms. We prove that there is no duality gap, and that this approach thus preserves optimality. The results of numerical tests confirm that the proposed method is robust against distributional errors in disturbances, while a standard stochastic safety specification tool is not.",0.3913043478],["a model of anisotropic molecules with three-dimensional orientations interacting via","Nematic first order phase transition for liquid crystals in the van der Waals-Kac limit","summarize: In this paper we revisit and extend some mathematical aspects of Onsager's theory of liquid crystals that have been investigated in recent years by different communities . We introduce a model of anisotropic molecules with three-dimensional orientations interacting via a Kac-type interaction. We prove that, in the limit in which the range of the interaction is sent to infinity after the thermodynamic limit, the free energy tends to the infimum of an effective energy functional \\`a la Onsager. We then prove that, if the spherical harmonic transform of the angular interaction has a negative minimum, this effective free energy functional displays a first order phase transition as the total density of the system increases.",0.2681280184],["this 'fundamental diagram' evolves in a possibly universal way with three distinct","The fundamental diagram of urbanization","summarize: The process of urbanization is one of the most important phenomenon of our societies and it is only recently that the availability of massive amounts of geolocalized historical data allows us to address quantitatively some of its features. Here, we discuss how the number of buildings evolves with population and we show on different datasets that this `fundamental diagram' evolves in a possibly universal way with three distinct phases. After an initial pre-urbanization phase, the first phase is a rapid growth of the number of buildings versus population. In a second regime, where residences are converted into another use , the population decreases while the number of buildings stays approximatively constant. In another subsequent phase, the number of buildings and the population grow again and correspond to a re-densification of cities. We propose a stochastic model based on these simple mechanisms to reproduce the first two regimes and show that it is in excellent agreement with empirical observations. These results bring evidences for the possibility of constructing a minimal model that could serve as a tool for understanding quantitatively urbanization and the future evolution of cities.",0.3157894737],["a MCTS variant may only encounter states with an explicit, extrinsic","Preference-Based Monte Carlo Tree Search","summarize: Monte Carlo tree search is a popular choice for solving sequential anytime problems. However, it depends on a numeric feedback signal, which can be difficult to define. Real-time MCTS is a variant which may only rarely encounter states with an explicit, extrinsic reward. To deal with such cases, the experimenter has to supply an additional numeric feedback signal in the form of a heuristic, which intrinsically guides the agent. Recent work has shown evidence that in different areas the underlying structure is ordinal and not numerical. Hence erroneous and biased heuristics are inevitable, especially in such domains. In this paper, we propose a MCTS variant which only depends on qualitative feedback, and therefore opens up new applications for MCTS. We also find indications that translating absolute into ordinal feedback may be beneficial. Using a puzzle domain, we show that our preference-based MCTS variant, wich only receives qualitative feedback, is able to reach a performance level comparable to a regular MCTS baseline, which obtains quantitative feedback.",0.1764705882],["a close curve that touches the boundary of each disk can be connected by a conveyor belt","Existence and hardness of conveyor belts","summarize: An open problem of Manuel Abellanas asks whether every set of disjoint closed unit disks in the plane can be connected by a conveyor belt, which means a tight simple closed curve that touches the boundary of each disk, possibly multiple times. We prove three main results. First, for unit disks whose centers are both ",0.2857142857],["students used audience response systems and guided inquiry worksheets differently. students used each of the tools","The role of pedagogical tools in active learning: a case for sense-making","summarize: Evidence from the research literature indicates that both audience response systems and guided inquiry worksheets can lead to greater student engagement, learning, and equity in the STEM classroom. We compare the use of these two tools in large enrollment STEM courses delivered in different contexts, one in biology and one in engineering. The instructors studied utilized each of the active learning tools differently. In the biology course, ARS questions were used mainly to check in with students and assess if they were correctly interpreting and understanding worksheet questions. The engineering course presented ARS questions that afforded students the opportunity to apply learned concepts to new scenarios towards improving students conceptual understanding. In the biology course, the GIWs were primarily used in stand-alone activities, and most of the information necessary for students to answer the questions was contained within the worksheet in a context that aligned with a disciplinary model. In the engineering course, the instructor intended for students to reference their lecture notes and rely on their conceptual knowledge of fundamental principles from the previous ARS class session in order to successfully answer the GIW questions. However, while their specific implementation structures and practices differed, both instructors used these tools to build towards the same basic disciplinary thinking and sense-making processes of conceptual reasoning, quantitative reasoning, and metacognitive thinking.",0.1],["triangulations in the u.s. for the u.s. tri","Unified Analytical Volume Distribution of Poisson-Delaunay Simplex and its Application to Coordinated Multi-Point Transmission","summarize: For Poisson-Delaunay triangulations in ",0.0726717773],["Arkan's polar encoder and successive cancellation decoder have complexities of","Relaxed Polar Codes","summarize: Polar codes are the latest breakthrough in coding theory, as they are the first family of codes with explicit construction that provably achieve the symmetric capacity of discrete memoryless channels. Arkan's polar encoder and successive cancellation decoder have complexities of ",0.0],["the effective source size is estimated from the extent of the experimental biprism interference pattern created on","Spatial coherence of electron beams from field emitters and its effect on the resolution of imaged objects","summarize: Sub-nanometer and nanometer-sized tips provide high coherence electron sources. Conventionally, the effective source size is estimated from the extent of the experimental biprism interference pattern created on the detector by applying the van Cittert Zernike theorem. Previously reported experimental intensity distributions on the detector exhibit Gaussian distribution and our simulations show that this is an indication that such electron sources must be at least partially coherent. This, in turn means that strictly speaking the Van Cittert Zernike theorem cannot be applied, since it assumes an incoherent source. The approach of applying the van Cittert Zernike theorem is examined in more detail by performing simulations of interference patterns for the electron sources of different size and different coherence length, evaluating the effective source size from the extent of the simulated interference pattern and comparing the obtained result with the pre-defined value. The intensity distribution of the source is assumed to be Gaussian distributed, as it is observed in experiments. The visibility or the contrast in the simulated holograms is found to be always less than 1 which agrees well with previously reported experimental results and thus can be explained solely by the Gaussian intensity distribution of the source. The effective source size estimated from the extent of the interference pattern turns out to be of about 2-3 times larger than the pre-defined size, but it is approximately equal to the intrinsic resolution of the imaging system. A simple formula for estimating the intrinsic resolution, which could be useful when employing nano-tips in in-line Gabor holography or point-projection microscopy, is provided.",0.25],["miquel dynamics is a discrete-time dynamical system on the space of square-","A first integrability result for Miquel dynamics","summarize: Miquel dynamics is a discrete-time dynamical system on the space of square-grid circle patterns. For biperiodic circle patterns with both periods equal to two, we show that the dynamics corresponds to translation on an elliptic curve, thus providing the first integrability result for this dynamics. The main tool is a geometric interpretation of the addition law on the normalization of binodal quartic curves.",0.1666666667],["new non-learned feature uses graph wavelets to decompose the Dirichlet","MGCN: Descriptor Learning using Multiscale GCNs","summarize: We propose a novel framework for computing descriptors for characterizing points on three-dimensional surfaces. First, we present a new non-learned feature that uses graph wavelets to decompose the Dirichlet energy on a surface. We call this new feature wavelet energy decomposition signature . Second, we propose a new multiscale graph convolutional network to transform a non-learned feature to a more discriminative descriptor. Our results show that the new descriptor WEDS is more discriminative than the current state-of-the-art non-learned descriptors and that the combination of WEDS and MGCN is better than the state-of-the-art learned descriptors. An important design criterion for our descriptor is the robustness to different surface discretizations including triangulations with varying numbers of vertices. Our results demonstrate that previous graph convolutional networks significantly overfit to a particular resolution or even a particular triangulation, but MGCN generalizes well to different surface discretizations. In addition, MGCN is compatible with previous descriptors and it can also be used to improve the performance of other descriptors, such as the heat kernel signature, the wave kernel signature, or the local point signature.",0.1],["the number of methods available for classification of multi-label data has increased rapidly over recent years","Multi-label Methods for Prediction with Sequential Data","summarize: The number of methods available for classification of multi-label data has increased rapidly over recent years, yet relatively few links have been made with the related task of classification of sequential data. If labels indices are considered as time indices, the problems can often be seen as equivalent. In this paper we detect and elaborate on connections between multi-label methods and Markovian models, and study the suitability of multi-label methods for prediction in sequential data. From this study we draw upon the most suitable techniques from the area and develop two novel competitive approaches which can be applied to either kind of data. We carry out an empirical evaluation investigating performance on real-world sequential-prediction tasks: electricity demand, and route prediction. As well as showing that several popular multi-label algorithms are in fact easily applicable to sequencing tasks, our novel approaches, which benefit from a unified view of these areas, prove very competitive against established methods.",0.0555555556],["CF uses user-item behavior interactions only. auxiliary information such as product reviews","Transfer Meets Hybrid: A Synthetic Approach for Cross-Domain Collaborative Filtering with Text","summarize: Collaborative filtering is the key technique for recommender systems . CF exploits user-item behavior interactions only and hence suffers from the data sparsity issue. One research thread is to integrate auxiliary information such as product reviews and news titles, leading to hybrid filtering methods. Another thread is to transfer knowledge from other source domains such as improving the movie recommendation with the knowledge from the book domain, leading to transfer learning methods. In real-world life, no single service can satisfy a user's all information needs. Thus it motivates us to exploit both auxiliary and source information for RSs in this paper. We propose a novel neural model to smoothly enable Transfer Meeting Hybrid methods for cross-domain recommendation with unstructured text in an end-to-end manner. TMH attentively extracts useful content from unstructured text via a memory module and selectively transfers knowledge from a source domain via a transfer network. On two real-world datasets, TMH shows better performance in terms of three ranking metrics by comparing with various baselines. We conduct thorough analyses to understand how the text content and transferred knowledge help the proposed model.",0.0],["NILM is unidentifiable and therefore a challenge problem because the infer","Transfer Learning for Non-Intrusive Load Monitoring","summarize: Non-intrusive load monitoring is a technique to recover source appliances from only the recorded mains in a household. NILM is unidentifiable and thus a challenge problem because the inferred power value of an appliance given only the mains could not be unique. To mitigate the unidentifiable problem, various methods incorporating domain knowledge into NILM have been proposed and shown effective experimentally. Recently, among these methods, deep neural networks are shown performing best. Arguably, the recently proposed sequence-to-point learning is promising for NILM. However, the results were only carried out on the same data domain. It is not clear if the method could be generalised or transferred to different domains, e.g., the test data were drawn from a different country comparing to the training data. We address this issue in the paper, and two transfer learning schemes are proposed, i.e., appliance transfer learning and cross-domain transfer learning . For ATL, our results show that the latent features learnt by a `complex' appliance, e.g., washing machine, can be transferred to a `simple' appliance, e.g., kettle. For CTL, our conclusion is that the seq2point learning is transferable. Precisely, when the training and test data are in a similar domain, seq2point learning can be directly applied to the test data without fine tuning; when the training and test data are in different domains, seq2point learning needs fine tuning before applying to the test data. Interestingly, we show that only the fully connected layers need fine tuning for transfer learning. Source code can be found at https:\/\/github.com\/MingjunZhong\/transferNILM.",0.1875],["machine learning and computer vision methods show good performance in medical imagery analysis. but only a","Domain Shift in Computer Vision models for MRI data analysis: An Overview","summarize: Machine learning and computer vision methods are showing good performance in medical imagery analysis. Yetonly a few applications are now in clinical use and one of the reasons for that is poor transferability of themodels to data from different sources or acquisition domains. Development of new methods and algorithms forthe transfer of training and adaptation of the domain in multi-modal medical imaging data is crucial for thedevelopment of accurate models and their use in clinics. In present work, we overview methods used to tackle thedomain shift problem in machine learning and computer vision. The algorithms discussed in this survey includeadvanced data processing, model architecture enhancing and featured training, as well as predicting in domaininvariant latent space. The application of the autoencoding neural networks and their domain-invariant variationsare heavily discussed in a survey. We observe the latest methods applied to the magnetic resonance imaging data analysis and conclude on their performance as well as propose directions for further research.",0.3076923077],["we consider a minimal physical model where the fluid self-organizes in a con","Order Out of Chaos: Slowly Reversing Mean Flows Emerge from Turbulently Generated Internal Waves","summarize: We demonstrate via direct numerical simulations that a periodic, oscillating mean flow spontaneously develops from turbulently generated internal waves. We consider a minimal physical model where the fluid self-organizes in a convective layer adjacent to a stably stratified one. Internal waves are excited by turbulent convective motions, then nonlinearly interact to produce a mean flow reversing on timescales much longer than the waves' period. Our results demonstrate for the first time that the three-scale dynamics due to convection, waves, and mean flow is generic and hence can occur in many astrophysical and geophysical fluids. We discuss efforts to reproduce the mean flow in reduced models, where the turbulence is bypassed. We demonstrate that wave intermittency, resulting from the chaotic nature of convection, plays a key role in the mean-flow dynamics, which thus cannot be captured using only second-order statistics of the turbulent motions.",0.4428509507],["the 4 SCAO systems of LBT are being upgraded. the system will push the current","Bringing SOUL on sky","summarize: The SOUL project is upgrading the 4 SCAO systems of LBT, pushing the current guide star limits of about 2 magnitudes fainter thanks to Electron Multiplied CCD detector. This improvement will open the NGS SCAO correction to a wider number of scientific cases from high contrast imaging in the visible to extra-galactic source in the NIR. The SOUL systems are today the unique case where pyramid WFS, adaptive secondary and EMCCD are used together. This makes SOUL a pathfinder for most of the ELT SCAO systems like the one of GMT, MICADO and HARMONI of E-ELT, where the same key technologies will be employed. Today we have 3 SOUL systems installed on the telescope in commissioning phase. The 4th system will be installed in a few months. We will present here the results achieved during daytime testing and commissioning nights up to the present date.",0.0],["a new population of close dual active galactic nuclei is being investigated. the","Hubble Space Telescope Wide Field Camera 3 Identifies an ","summarize: Kiloparsec-scale dual active galactic nuclei are active supermassive black hole pairs co-rotating in galaxies with separations of less than a few kpc. Expected to be a generic outcome of hierarchical galaxy formation, their frequency and demographics remain uncertain. We have carried out an imaging survey with the Hubble Space Telescope Wide Field Camera 3 of AGNs with double-peaked narrow emission lines. HST\/WFC3 offers high image quality in the near-infrared to resolve the two stellar nuclei, and in the optical to resolve from ionized gas in the narrow-line regions. This combination has proven to be key in sorting out alternative scenarios. With HST\/WFC3 we are able to explore a new population of close dual AGNs at more advanced merger stages than can be probed from the ground. Here we show that the AGN SDSS J0924+0510, which had previously shown two stellar bulges, contains two spatially distinct regions consistent with a dual AGN. While we cannot completely exclude cross-ionization from a single central engine, the nearly equal ratios of strongly suggest a dual AGN with a projected angular separation of 0.4, corresponding to a projected physical separation of ",0.2105263158],["perovskites have been praised for their exceptional photovoltaic and optoe","Efficient indoor p-i-n hybrid perovskite solar cells using low temperature solution processed NiO as hole extraction layers","summarize: Hybrid perovskites have received tremendous attention due to their exceptional photovoltaic and optoelectronic properties. Among the two widely used perovskite solar cell device architectures of n-ip and p-i-n, the latter is interesting in terms of its simplicity of fabrication and lower energy input. However this structure mostly uses PEDOT:PSS as a hole transporting layer which can accelerate the perovskite solar cell degradation. Hence the development of stable, inorganic hole extraction layers , without compromising the simplicity of device fabrication is crucial in this fast-growing photovoltaic field. Here we demonstrate a low temperature solution - processed and ultrathin NiO nanoparticle thin films as an efficient HEL for CH3NH3PbI3 based perovskite solar cells. We measure a power conversion efficiency of 13.3 % on rigid glass substrates and 8.5 % on flexible substrates. A comparison with PEDOT:PSS based MAPbI3 solar cells shows that NiO based solar cells have higher short circuit current density and improved open circuit voltage . Apart from the photovoltaic performance under 1 Sun, the efficient hole extraction property of NiO is demonstrated for indoor lighting as well with a PCE of 23.0 % for NiO based CH3NH3PbI2.9Cl0.1 p-i-n solar cells under compact fluorescent lighting. Compared to the perovskite solar cells fabricated on PEDOT:PSS HEL, better shelf-life stability is observed for perovskite solar cells fabricated on NiO HEL. Detailed microstructural and photophysical investigations imply uniform morphology, lower recombination losses, and improved charge transfer properties for CH3NH3PbI3 grown on NiO HEL.",0.0],["a graph-based model of programmable environments is proposed. the results are","Modeling, Simulating and Configuring Programmable Wireless Environments for Multi-User Multi-Objective Networking","summarize: Programmable wireless environments enable the software-defined propagation of waves within them, yielding exceptional performance potential. Several building-block technologies have been implemented and evaluated at the physical layer. The present work contributes a network-layer scheme to configure such environments for multiple users and objectives, and for any physical-layer technology. Supported objectives include any combination of Quality of Service and power transfer optimization, eavesdropping and Doppler effect mitigation, in multi-cast or uni-cast settings. Additionally, a graph-based model of programmable environments is proposed, which incorporates core physical observations and efficiently separates physical and networking concerns. Evaluation takes place in a specially developed, free simulation tool, and in a variety of environments. Performance gains over regular propagation are highlighted, reaching important insights on the user capacity of programmable environments.",0.25],["stellar populations were measured using a stellar population synthesis code STARLIGHT. we","Eight luminous early-type galaxies in nearby pairs and sparse groups I. Stellar populations spatially analysed","summarize: We present a detailed spatial analysis of stellar populations based on long-slit optical spectra in a sample of eight luminous early-type galaxies selected from nearby sparse groups and pairs, three of them may be interacting with a galaxy of similar mass. We have measured luminosity-weighted averages of age, , , and to add empirical data relative to the influence of galaxy mass, environment, interaction, and AGN feedback in their formation and evolution. The stellar population of the individual galaxies were determined through the stellar population synthesis code STARLIGHT using semi-empirical simple stellar population models. Radial variations of luminosity-weighted means of age, , , and were measured up to half of the effective radius of each galaxy. We found trends between these values and the nuclear stellar velocity dispersion. There are also relations between the metallicity\/age gradients and the velocity dispersion. Contributions of 1-4 Gyr old stellar populations were found in IC5328 and NGC6758 as well as 4-8 Gyr old ones in NGC5812. Extended gas is present in IC5328, NGC1052, NGC1209, and NGC6758, and the presence of a LINER is identified in all these galaxies. The regions up to one effective radius of all galaxies are dominated by ",0.2966860126],["quantum computers promise significant speedups in solving problems intractable for conventional computers. but","Just Like the Real Thing: Fast Weak Simulation of Quantum Computation","summarize: Quantum computers promise significant speedups in solving problems intractable for conventional computers but, despite recent progress, remain limited in scaling and availability. Therefore, quantum software and hardware development heavily rely on simulation that runs on conventional computers. Most such approaches perform strong simulation in that they explicitly compute amplitudes of quantum states. However, such information is not directly observable from a physical quantum computer because quantum measurements produce random samples from probability distributions defined by those amplitudes. In this work, we focus on weak simulation that aims to produce outputs which are statistically indistinguishable from those of error-free quantum computers. We develop algorithms for weak simulation based on quantum state representation in terms of decision diagrams. We compare them to using state-vector arrays and binary search on prefix sums to perform sampling. Empirical validation shows, for the first time, that this enables mimicking of physical quantum computers of significant scale.",0.0588235294],["proposed scheme is optimal with 1 or 2 sets of data. it is suboptimal with","Energy-Minimizing Bit Allocation For Powerline OFDM With Multiple Delay Constraints","summarize: We propose a bit-allocation scheme for powerline orthogonal frequency-division multiplexing that minimizes total transmit energy subject to total-bit and delay constraints. Multiple delay requirements stem from different sets of data that a transmitter must time-multiplex and transmit to a receiver. The proposed bit allocation takes into account the channel power-to-noise density ratio of subchannels as well as statistic of narrowband interference and impulsive noise that is pervasive in powerline communication channels. The proposed scheme is optimal with 1 or 2 sets of data, and is suboptimal with more than 2 sets of data. However, numerical examples show that the proposed scheme performs close to the optimum. Also, it is less computationally complex than the optimal scheme especially when minimizing total energy over large number of data sets. We also compare the proposed scheme with some existing schemes and find that our scheme requires less total transmit energy when the number of delay constraints is large.",0.1363636364],["data collected from time series, such as financial data of companies quoted in stock exchange, health care","Signal Processing on Graphs: Causal Modeling of Unstructured Data","summarize: Many applications collect a large number of time series, for example, the financial data of companies quoted in a stock exchange, the health care data of all patients that visit the emergency room of a hospital, or the temperature sequences continuously measured by weather stations across the US. These data are often referred to as unstructured. A first task in its analytics is to derive a low dimensional representation, a graph or discrete manifold, that describes well the interrelations among the time series and their intrarelations across time. This paper presents a computationally tractable algorithm for estimating this graph that structures the data. The resulting graph is directed and weighted, possibly capturing causal relations, not just reciprocal correlations as in many existing approaches in the literature. A convergence analysis is carried out. The algorithm is demonstrated on random graph datasets and real network time series datasets, and its performance is compared to that of related methods. The adjacency matrices estimated with the new method are close to the true graph in the simulated data and consistent with prior physical knowledge in the real dataset tested.",0.15],["Graph Optimal Transport aims to create a graph matching problem. the learned","Graph Optimal Transport for Cross-Domain Alignment","summarize: Cross-domain alignment between two sets of entities is fundamental to both computer vision and natural language processing. Existing methods mainly focus on designing advanced attention mechanisms to simulate soft alignment, with no training signals to explicitly encourage alignment. The learned attention matrices are also dense and lacks interpretability. We propose Graph Optimal Transport , a principled framework that germinates from recent advances in Optimal Transport . In GOT, cross-domain alignment is formulated as a graph matching problem, by representing entities into a dynamically-constructed graph. Two types of OT distances are considered: Wasserstein distance for node matching; and Gromov-Wasserstein distance for edge matching. Both WD and GWD can be incorporated into existing neural network models, effectively acting as a drop-in regularizer. The inferred transport plan also yields sparse and self-normalized alignment, enhancing the interpretability of the learned model. Experiments show consistent outperformance of GOT over baselines across a wide range of tasks, including image-text retrieval, visual question answering, image captioning, machine translation, and text summarization.",0.35],["the effective source size is estimated from the extent of the experimental biprism interference pattern created on","Spatial coherence of electron beams from field emitters and its effect on the resolution of imaged objects","summarize: Sub-nanometer and nanometer-sized tips provide high coherence electron sources. Conventionally, the effective source size is estimated from the extent of the experimental biprism interference pattern created on the detector by applying the van Cittert Zernike theorem. Previously reported experimental intensity distributions on the detector exhibit Gaussian distribution and our simulations show that this is an indication that such electron sources must be at least partially coherent. This, in turn means that strictly speaking the Van Cittert Zernike theorem cannot be applied, since it assumes an incoherent source. The approach of applying the van Cittert Zernike theorem is examined in more detail by performing simulations of interference patterns for the electron sources of different size and different coherence length, evaluating the effective source size from the extent of the simulated interference pattern and comparing the obtained result with the pre-defined value. The intensity distribution of the source is assumed to be Gaussian distributed, as it is observed in experiments. The visibility or the contrast in the simulated holograms is found to be always less than 1 which agrees well with previously reported experimental results and thus can be explained solely by the Gaussian intensity distribution of the source. The effective source size estimated from the extent of the interference pattern turns out to be of about 2-3 times larger than the pre-defined size, but it is approximately equal to the intrinsic resolution of the imaging system. A simple formula for estimating the intrinsic resolution, which could be useful when employing nano-tips in in-line Gabor holography or point-projection microscopy, is provided.",0.25],["the graph of a Hecke operator encodes all information about the action of this operator on","On graphs of Hecke operators","summarize: The graph of a Hecke operator encodes all information about the action of this operator on automorphic forms. Let ",0.2580645161],["film production is preferred due to faster processing times and lower processing costs. films can be produced","Crack initiation of printed lines predicted with digital image correlation","summarize: Printing of metallic films has been preferred for roll-to-roll processes over vacuum technologies due to faster processing times and lower processing costs. Films can be produced by depositing inks containing suspended metallic particles within a solvent and then heating the films to both remove the solvent and sinter the particles. The resulting printed structure, electrical and mechanical behavior of the printed films has been studied to better understand their electro-mechanical response to loading and eventual brittle fracture. This study evaluated the electro-mechanical behavior of 1.25 m printed Ag films using in-situ resistance and in-situ imaging methods. Digital image correlation was utilized with confocal laser scanning microscope images to better visualize crack initiation during tensile straining. This technique showed that cracks initiated earlier in the thicker areas of the film than in lower areas because of a higher density of printing defects and the increased thickness.",0.0],["geochemical sampling, geologic mapping, and geophysical surveys have been performed. a","Perceiving the crust in 3D: a model integrating geological, geochemical, and geophysical data","summarize: Regional characterization of the continental crust has classically been performed through either geologic mapping, geochemical sampling, or geophysical surveys. Rarely are these techniques fully integrated, due to limits of data coverage, quality, and\/or incompatible datasets. We combine geologic observations, geochemical sampling, and geophysical surveys to create a coherent 3-D geologic model of a 50 x 50 km upper crustal region surrounding the SNOLAB underground physics laboratory in Canada, which includes the Southern Province, the Superior Province, the Sudbury Structure and the Grenville Front Tectonic Zone. Nine representative aggregate units of exposed lithologies are geologically characterized, geophysically constrained, and probed with 109 rock samples supported by compiled geochemical databases. A detailed study of the lognormal distributions of U and Th abundances and of their correlation permits a bivariate analysis for a robust treatment of the uncertainties. A downloadable 3D numerical model of U and Th distribution defines an average heat production of 1.5",0.5394871823],["coloured noise excitation is a non-markovian response. the coloured","A systematic path to non-Markovian dynamics: New response pdf evolution equations under Gaussian coloured noise excitation","summarize: Determining evolution equations governing the probability density function of non-Markovian responses to random differential equations excited by coloured noise, is an important issue arising in various problems of stochastic dynamics, advanced statistical physics and uncertainty quantification of macroscopic systems. In the present work, such equations are derived for a scalar, nonlinear RDE under additive coloured Gaussian noise excitation, through the stochastic Liouville equation. The latter is an exact, yet non-closed equation, involving aver-ages over the time history of the non-Markovian response. This nonlocality is treated by applying an extension of the Novikov-Furutsu theorem and a novel approximation, employing a stochastic Volterra-Taylor functional expansion around instantaneous response moments, leading to efficient, closed, approximate equations for the response pdf. These equations retain a tractable amount of nonlocality and nonlinearity, and they are valid in both the transient and long-time regimes for any correlation function of the excitation. Also, they include as special cases various existing relevant models, and generalize Hanggi's ansatz in a rational way. Numerical results for a bistable nonlinear RDE confirm the accuracy and the efficiency of the new equations. Extension to the multidimensional case is feasible, yet laborious.",0.3062838827],["we analyse a group of participants with more than 10,000 participants. we find that engagement pe","Is this pofma? Analysing public opinion and misinformation in a COVID-19 Telegram group chat","summarize: We analyse a Singapore-based COVID-19 Telegram group with more than 10,000 participants. First, we study the group's opinion over time, focusing on four dimensions: participation, sentiment, topics, and psychological features. We find that engagement peaked when the Ministry of Health raised the disease alert level, but this engagement was not sustained. Second, we search for government-identified misinformation in the group. We find that government-identified misinformation is rare, and that messages discussing these pieces of misinformation express skepticism.",0.3103448276],["the Newton limit of gravity is studied in the presence of Lorentz-violating gravitation","Testing local Lorentz invariance with short-range gravity","summarize: The Newton limit of gravity is studied in the presence of Lorentz-violating gravitational operators of arbitrary mass dimension. The linearized modified Einstein equations are obtained and the perturbative solutions are constructed and characterized. We develop a formalism for data analysis in laboratory experiments testing gravity at short range and demonstrate that these tests provide unique sensitivity to deviations from local Lorentz invariance.",0.1666666667],["a generalized similarity measure was introduced to compare texts. the similarity measure was","On the role of words in the network structure of texts: application to authorship attribution","summarize: Well-established automatic analyses of texts mainly consider frequencies of linguistic units, e.g. letters, words and bigrams, while methods based on co-occurrence networks consider the structure of texts regardless of the nodes label . In this paper, we reconcile these distinct viewpoints by introducing a generalized similarity measure to compare texts which accounts for both the network structure of texts and the role of individual words in the networks. We use the similarity measure for authorship attribution of three collections of books, each composed of 8 authors and 10 books per author. High accuracy rates were obtained with typical values from 90% to 98.75%, much higher than with the traditional the TF-IDF approach for the same collections. These accuracies are also higher than taking only the topology of networks into account. We conclude that the different properties of specific words on the macroscopic scale structure of a whole text are as relevant as their frequency of appearance; conversely, considering the identity of nodes brings further knowledge about a piece of text represented as a network.",0.222289905],["the Foley Artist is responsible for creating an overlay soundtrack that helps the movie come alive for the","AutoFoley: Artificial Synthesis of Synchronized Sound Tracks for Silent Videos with Deep Learning","summarize: In movie productions, the Foley Artist is responsible for creating an overlay soundtrack that helps the movie come alive for the audience. This requires the artist to first identify the sounds that will enhance the experience for the listener thereby reinforcing the Directors's intention for a given scene. In this paper, we present AutoFoley, a fully-automated deep learning tool that can be used to synthesize a representative audio track for videos. AutoFoley can be used in the applications where there is either no corresponding audio file associated with the video or in cases where there is a need to identify critical scenarios and provide a synthesized, reinforced soundtrack. An important performance criterion of the synthesized soundtrack is to be time-synchronized with the input video, which provides for a realistic and believable portrayal of the synthesized sound. Unlike existing sound prediction and generation architectures, our algorithm is capable of precise recognition of actions as well as inter-frame relations in fast moving video clips by incorporating an interpolation technique and Temporal Relationship Networks . We employ a robust multi-scale Recurrent Neural Network associated with a Convolutional Neural Network for a better understanding of the intricate input-to-output associations over time. To evaluate AutoFoley, we create and introduce a large scale audio-video dataset containing a variety of sounds frequently used as Foley effects in movies. Our experiments show that the synthesized sounds are realistically portrayed with accurate temporal synchronization of the associated visual inputs. Human qualitative testing of AutoFoley show over 73% of the test subjects considered the generated soundtrack as original, which is a noteworthy improvement in cross-modal research in sound synthesis.",0.2592592593],["v=2\/3 edge mode is based on a carefully designed double-quantum","Synthesizing a Fractional v=2\/3 State from Particle and Hole States","summarize: Topological edge-reconstruction occurs in hole-conjugate states of the fractional quantum Hall effect. The frequently studied polarized state of filling factor v=2\/3 was originally proposed to harbor two counter-propagating edge modes: a downstream v=1 and an upstream v=1\/3. However, charge equilibration between these two modes always led to an observed downstream v=2\/3 charge mode accompanied by an upstream neutral mode . Here, we present a new approach to synthetize the v=2\/3 edge mode from its basic counter-propagating charged constituents, allowing a controlled equilibration between the two counter-propagating charge modes. This novel platform is based on a carefully designed double-quantum-well, which hosts two populated electronic sub-bands , with corresponding filling factors, vl & vu. By separating the 2D plane to two gated intersecting halves, each with different fillings, counter-propagating chiral modes can be formed along the intersection line. Equilibration between these modes can be controlled with the top gates' voltage and the magnetic field. Our measurements of the two-terminal conductance G2T and the presence of a neutral mode allowed following the transition from the non-equilibrated charged modes, manifested by G2T=e2\/h, to the fully equilibrated modes, with a downstream charge mode with G2T=e2\/h accompanied by an upstream neutral mode.",0.4615384615],["the ongoing pandemic threatens the health of humans and causes great economic losses. we","A Data-Driven Network Model for the Emerging COVID-19 Epidemics in Wuhan, Toronto and Italy","summarize: The ongoing Coronavirus Disease 2019 pandemic threatens the health of humans and causes great economic losses. Predictive modelling and forecasting the epidemic trends are essential for developing countermeasures to mitigate this pandemic. We develop a network model, where each node represents an individual and the edges represent contacts between individuals where the infection can spread. The individuals are classified based on the number of contacts they have each day and their infection status. The transmission network model was respectively fitted to the reported data for the COVID-19 epidemic in Wuhan , Toronto , and the Italian Republic using a Markov Chain Monte Carlo optimization algorithm. Our model fits all three regions well with narrow confidence intervals and could be adapted to simulate other megacities or regions. The model projections on the role of containment strategies can help inform public health authorities to plan control measures.",0.1764705882],["eigensolvers dealing with low rank perturbations of unitary and unitary matric","When is a matrix unitary or Hermitian plus low rank?","summarize: Hermitian and unitary matrices are two representatives of the class of normal matrices whose full eigenvalue decomposition can be stably computed in quadratic computing com plexity. Recently, fast and reliable eigensolvers dealing with low rank perturbations of unitary and Hermitian matrices were proposed. These structured eigenvalue problems appear naturally when computing roots, via confederate linearizations, of polynomials expressed in, e.g., the monomial or Chebyshev basis. Often, however, it is not known beforehand whether or not a matrix can be written as the sum of an Hermitian or unitary matrix plus a low rank perturbation. We propose necessary and sufficient conditions characterizing the class of Hermitian or unitary plus low rank matrices. The number of singular values deviating from 1 determines the rank of a perturbation to bring a matrix to unitary form. A similar condition holds for Hermitian matrices; the eigenvalues of the skew-Hermitian part differing from 0 dictate the rank of the perturbation. We prove that these relations are linked via the Cayley transform. Based on these conditions we are able to identify the closest Hermitian and unitary plus low rank matrix in Frobenius and spectral norm and a practical Lanczos iteration to detect the low rank perturbation is presented. Numerical tests prove that this straightforward algorithm is robust with respect to noise.",0.3076923077],["we introduce the notion of connection thickness in a Cayley graph. it was well","Connectedness of spheres in Cayley graphs","summarize: We introduce the notion of connection thickness of spheres in a Cayley graph, related to dead-ends and their retreat depth. It was well-known that connection thickness is bounded for finitely presented one-ended groups. We compute that for natural generating sets of lamplighter groups on a line or on a tree, connection thickness is linear or logarithmic respectively. We show that it depends strongly on the generating set. We give an example where the metric induced at the thickness of connection gives diameter of order ",0.3157894737],["emph for a graph for a graph.","Sequences of radius ","summarize: A \\emph for a graph ",0.1176470588],["algebra associated with the cyclic quiver of length is associated with the cyclic qui","Categorical representations and KLR algebras","summarize: We prove that the KLR algebra associated with the cyclic quiver of length ",0.0434782609],["chemical reaction network theory investigates whether binomial ideals are generated by binomia","A Linear Algebra Approach for Detecting Binomiality of Steady State Ideals of Reversible Chemical Reaction Networks","summarize: Motivated by problems from Chemical Reaction Network Theory, we investigate whether steady state ideals of reversible reaction networks are generated by binomials. We take an algebraic approach considering, besides concentrations of species, also rate constants as indeterminates. This leads us to the concept of unconditional binomiality, meaning binomiality for all values of the rate constants. This concept is different from conditional binomiality that applies when rate constant values or relations among rate constants are given. We start by representing the generators of a steady state ideal as sums of binomials, which yields a corresponding coefficient matrix. On these grounds we propose an efficient algorithm for detecting unconditional binomiality. That algorithm uses exclusively elementary column and row operations on the coefficient matrix. We prove asymptotic worst case upper bounds on the time complexity of our algorithm. Furthermore, we experimentally compare its performance with other existing methods.",0.0],["the sex of the u.s.","On the p-regularized trust region subproblem","summarize: The ",0.2339230723],["secondary multiuser network of cognitive radio systems is a multiuser diversity scheme. the secondary","Asymptotic Performance Analysis of Generalized User Selection for Interference-Limited Multiuser Secondary Networks","summarize: We analyze the asymptotic performance of a generalized multiuser diversity scheme for an interference-limited secondary multiuser network of underlay cognitive radio systems. Assuming a large number of secondary users and that the noise at each secondary user's receiver is negligible compared to the interference from the primary transmitter, the secondary transmitter transmits information to the ",0.2857142857],["a total of 22 major items were added, including four epijournals and four supplementary","Astrophysicists and physicists as creators of ArXiv-based commenting resources for their research communities. An initial survey","summarize: This paper conveys the outcomes of what results to be the first, though initial, overview of commenting platforms and related 2.0 resources born within and for the astrophysical community . Experiences were added, mainly in the physics domain, for a total of 22 major items, including four epijournals, and four supplementary resources, thus casting some light onto an unexpected richness and consonance of endeavours. These experiences rest almost entirely on the contents of the database ArXiv, which adds to its merits that of potentially setting the grounds for web 2.0 resources, and research behaviours, to be explored. Most of the experiences retrieved are UK and US based, but the resulting picture is international, as various European countries, China and Australia have been actively involved. Final remarks about creation patterns and outcome of these resources are outlined. The results integrate the previous studies according to which the web 2.0 is presently of limited use for communication in astrophysics and vouch for a role of researchers in the shaping of their own professional communication tools that is greater than expected. Collaterally, some aspects of ArXiv s recent pathway towards partial inclusion of web 2.0 features are touched upon. Further investigation is hoped for.",0.3152283272],["two-dimensional systems may admit hexatic phase and hexatic-liquid transition","Attraction tames two-dimensional melting: from continuous to discontinuous transitions","summarize: Two-dimensional systems may admit a hexatic phase and hexatic-liquid transitions of different natures. The determination of their phase diagrams proved challenging, and indeed those of hard-disks, hard regular polygons, and inverse power-law potentials, have been only recently clarified. In this context, the role of attractive forces is currently speculative, despite their prevalence at both the molecular and colloidal scale. Here we demonstrate, via numerical simulations, that attraction promotes a discontinuous melting scenario with no hexatic phase. At high-temperature, Lennard-Jones particles and attractive polygons follow the shape-dominated melting scenario observed in hard-disks and hard polygons, respectively. Conversely, all systems melt via a first-order transition with no hexatic phase at low temperature, where attractive forces dominate. The intermediate temperature melting scenario is shape-dependent. Our results suggest that, in colloidal experiments, the tunability of the strength of the attractive forces allows for the observation of different melting scenario in the same system.",0.2],["construction is used by a construction used by a construction by a construction by a","An Automaton Group with PSPACE-Complete Word Problem","summarize: We construct an automaton group with a PSPACE-complete word problem, proving a conjecture due to Steinberg. Additionally, the constructed group has a provably more difficult, namely EXPSPACE-complete, compressed word problem and acts over a binary alphabet. Thus, it is optimal in terms of the alphabet size. Our construction directly simulates the computation of a Turing machine in an automaton group and, therefore, seems to be quite versatile. It combines two ideas: the first one is a construction used by D'Angeli, Rodaro and the first author to obtain an inverse automaton semigroup with a PSPACE-complete word problem and the second one is to utilize a construction used by Barrington to simulate circuits of bounded degree and logarithmic depth in the group of even permutations over five elements.",0.0754716981],["paper presents multi-scale approach to predict paper hygro-mechanical behaviour.","Hygro-mechanical properties of paper fibrous networks through asymptotic homogenization and comparison with idealized models","summarize: This paper presents a multi-scale approach to predict the effective hygro-mechanical behaviour of paper sheets based on the properties of the underlying fibrous network. Despite the vast amount of literature on paper hygro-expansion, the functional dependence of the effective material properties on the micro-structural features remains yet unclear. In this work, a micro-structural model of the paper fibrous network is first developed by random deposition of the fibres within a planar region according to an orientation probability density function. Asymptotic homogenization is used to determine its effective properties numerically. Alternatively, two much more idealized micro-structural models are considered, one based on a periodic lattice structure with a regular network of perpendicular fibres and one based on the Voigt average. Despite their simplicity, they reproduce representative micro-structural features, such as the orientation anisotropy and network level hygro-elastic properties. These alternative models can be solved analytically, providing closed-form expressions that explicitly reveal the influence of the individual micro-scale parameters on the effective hygro-mechanical response. The trend predicted by the random network model is captured reasonably well by the two idealized models. The resulting hygro-mechanical properties are finally compared with experimental data reported in the literature, revealing an adequate quantitative agreement.",0.1564782057],["symmetrized versions of Hammersley type point sets in the unit square","Optimal discrepancy rate of point sets in Besov spaces with negative smoothness","summarize: We consider the local discrepancy of a symmetrized version of Hammersley type point sets in the unit square. As a measure for the irregularity of distribution we study the norm of the local discrepancy in Besov spaces with dominating mixed smoothness. It is known that for Hammersley type points this norm has the best possible rate provided that the smoothness parameter of the Besov space is nonnegative. While these point sets fail to achieve the same for negative smoothness, we will prove in this note that the symmetrized versions overcome this defect. We conclude with some consequences on discrepancy in further function spaces with dominating mixed smoothness and on numerical integration based on quasi-Monte Carlo rules.",0.3804586318],["scalar, sectional and Ricci curvatures are constructed on simplicial piece","Piecewise Flat Curvature and Ricci Flow in Three Dimensions","summarize: Discrete forms of the scalar, sectional and Ricci curvatures are constructed on simplicial piecewise flat triangulations of smooth manifolds, depending directly on the simplicial structure and a choice of dual tessellation. This is done by integrating over volumes which include appropriate samplings of hinges for each type of curvature, with the integrals based on the parallel transport of vectors around hinges. Computations for triangulations of a diverse set of manifolds show these piecewise flat curvatures to converge to their smooth values. The Ricci curvature also gives a piecewise flat Ricci flow as a fractional rate of change of edge-lengths, again converging to the smooth Ricci flow for the manifolds tested.",0.25],["the transport coefficients are calculated from first principles. we compare our results to recent observations","Perpendicular Diffusion of Solar Energetic Particles: Model Results and Implications for Electrons","summarize: The processes responsible for the effective longitudinal transport of solar energetic particles are still not completely understood. We address this issue by simulating SEP electron propagation using a spatially 2D transport model that includes perpendicular diffusion. By implementing, as far as possible, the most reasonable estimates of the transport coefficients, we compare our results, in a qualitative manner, to recent observations , focusing on the longitudinal distribution of the peak intensity, the maximum anisotropy and the onset time. By using transport coefficients which are derived from first principles, we limit the number of free parameters in the model to: the probability of SEPs following diffusing magnetic field lines, quantified by ",0.0],["a loxodrome on the surface of the globe is related to the straight line","On some information geometric structures concerning Mercator projections","summarize: Some information geometric structures concerning the Mercator projections are studied. It is known that a loxodrome on the surface of the globe is related to the straight line on a Mercator map by the Mercator projection. It is not well known that an affine connection with torsion plays a fundamental role to describe an auto-parallel path on the surface. Based on these information geometric structures, Gauss distribution is reconsidered from the view point of the affine connection with a torsion. Some relations with deformed functions are also pointed out.",0.1739130435],["topological insulators are materials that have a gapped bulk energy spectrum. they","Quantum interference of topological states of light","summarize: Topological insulators are materials that have a gapped bulk energy spectrum, but contain protected in-gap states appearing at their surface. These states exhibit remarkable properties such as unidirectional propagation and robustness to noise that offer an opportunity to improve the performance and scalability of quantum technologies. For quantum applications, it is essential that the topological states are indistinguishable. Here we report high-visibility quantum interference of single photon topological states in an integrated photonic circuit. Two topological boundary-states, initially at opposite edges of a coupled waveguide array, are brought into proximity, where they interfere and undergo a beamsplitter operation. We observe ",0.2],["Douglas--Rachford algorithm is popular for solving convex and nonconvex","The Douglas--Rachford algorithm for a hyperplane and a doubleton","summarize: The Douglas--Rachford algorithm is a popular algorithm for solving both convex and nonconvex feasibility problems. While its behaviour is settled in the convex inconsistent case, the general nonconvex inconsistent case is far from being fully understood. In this paper, we focus on the most simple nonconvex inconsistent case: when one set is a hyperplane and the other a doubleton . We present a characterization of cycling in this case which --- somewhat surprisingly --- depends on whether the ratio of the distance of the points to the hyperplane is rational or not. Furthermore, we provide closed-form expressions as well as several concrete examples which illustrate the dynamical richness of this algorithm.",0.4545454545],["the so-called is a powerful tool in qualitative studies of one dimensional parabolic","The Zero Number Diminishing Property under General Boundary Conditions","summarize: The so-called is a powerful tool in qualitative studies of one dimensional parabolic equations, which says that, under the zero- or non-zero-Dirichlet boundary conditions, the number of zeroes of the solution ",0.2105263158],["the geoBIM benchmark project is aimed at finding such evidence by involving external volunteers","Reference study of CityGML software support: the GeoBIM benchmark 2019 -- Part II","summarize: OGC CityGML is an open standard for 3D city models intended to foster interoperability and support various applications. However, through our practical experience and discussions with practitioners, we have noticed several problems related to the implementation of the standard and the use of standardized data. Nevertheless, a systematic investigation of these issues has never been performed, and there is thus insufficient evidence that can be used for tackling the problems. The GeoBIM benchmark project is aimed at finding such evidence by involving external volunteers, reporting on tools behaviour about relevant aspects , analysed and described in this paper. This study explicitly pointed out the critical points embedded in the format as an evidence base for future development. This paper is in tandem with Part I, describing the results of the benchmark related to IFC, counterpart of CityGML within building information modelling.",0.1428571429],["ACTIN is a Python program to calculate stellar activity indices. it reads","ACTIN: A tool to calculate stellar activity indices","summarize: Magnetic activity in the atmospheres of stars produces a number of spectroscopic signatures that are visible in the shape and strength of spectral lines. These signatures can be used to access, among other things, the variability of the magnetic activity, or its infuence on other parameters such as the measured radial velocity . This latter is of utmost importance for the detection and characterization of planets orbiting other stars. ACTIN is a Python program to calculate stellar activity indices. The program reads input data either from .fits files returned by the pipelines of spectrographs, or from .rdb tables. It extracts automatically the spectral data required to calculate spectral activity indices. The output is an .rdb table, with the calculated stellar activity indices for each date , as well as the RV and Cross-Correlation Function profile parameters, if available. It also outputs timeseries plots of the activity indices and plots the spectral lines used to compute the indices.",0.5263157895],["in-vehicle human object identification plays an important role in vision-based automated vehicle","In-Vehicle Object Detection in the Wild for Driverless Vehicles","summarize: In-vehicle human object identification plays an important role in vision-based automated vehicle driving systems while objects such as pedestrians and vehicles on roads or streets are the primary targets to protect from driverless vehicles. A challenge is the difficulty to detect objects in moving under the wild conditions, while illumination and image quality could drastically vary. In this work, to address this challenge, we exploit Deep Convolutional Generative Adversarial Networks with Single Shot Detector to handle with the wild conditions. In our work, a GAN was trained with low-quality images to handle with the challenges arising from the wild conditions in smart cities, while a cascaded SSD is employed as the object detector to perform with the GAN. We used tested our approach under wild conditions using taxi driver videos on London street in both daylight and night times, and the tests from in-vehicle videos demonstrate that this strategy can drastically achieve a better detection rate under the wild conditions.",0.0625],["the Feynman propagator of a causal set contains the complete information about the causal","Towards Spectral Geometry for Causal Sets","summarize: We show that the Feynman propagator of a causal set contains the complete information about the causal set. Intuitively, this is because the Feynman propagator, being a correlator that decays with distance, provides a measure for the invariant distance between pairs of events. Further, we show that even the spectra alone of the propagator and d'Alembertian already carry large amounts of geometric information about their causal set. This geometric information is basis independent and also gauge invariant in the sense that it is relabeling invariant . We provide numerical evidence that the associated spectral distance between causal sets can serve as a measure for the geometric similarity between causal sets.",0.125],["the occurring variations are motivated by measurements of an available set of resonators. the","Uncertainty Quantification for Maxwell's Eigenproblem based on Isogeometric Analysis and Mode Tracking","summarize: The electromagnetic field distribution as well as the resonating frequency of various modes in superconducting cavities used in particle accelerators for example are sensitive to small geometry deformations. The occurring variations are motivated by measurements of an available set of resonators from which we propose to extract a small number of relevant and independent deformations by using a truncated Karhunen-Lo\\`eve expansion. The random deformations are used in an expressive uncertainty quantification workflow to determine the sensitivity of the eigenmodes. For the propagation of uncertainty, a stochastic collocation method based on sparse grids is employed. It requires the repeated solution of Maxwell's eigenvalue problem at predefined collocation points, i.e., for cavities with perturbed geometry. The main contribution of the paper is ensuring the consistency of the solution, i.e., matching the eigenpairs, among the various eigenvalue problems at the stochastic collocation points. To this end, a classical eigenvalue tracking technique is proposed that is based on homotopies between collocation points and a Newton-based eigenvalue solver. The approach can be efficiently parallelized while tracking the eigenpairs. In this paper, we propose the application of isogeometric analysis since it allows for the exact description of the geometrical domains with respect to common computer-aided design kernels, for a straightforward and convenient way of handling geometrical variations and smooth solutions.",0.0555555556],["the convergence rate of such refined isogeometric analysis is equivalent to that of the","Spectral approximation properties of isogeometric analysis with variable continuity","summarize: We study the spectral approximation properties of isogeometric analysis with local continuity reduction of the basis. Such continuity reduction results in a reduction in the interconnection between the degrees of freedom of the mesh, which allows for large savings in computational requirements during the solution of the resulting linear system. The continuity reduction results in extra degrees of freedom that modify the approximation properties of the method. The convergence rate of such refined isogeometric analysis is equivalent to that of the maximum continuity basis. We show how the breaks in continuity and inhomogeneity of the basis lead to artefacts in the frequency spectra, such as stopping bands and outliers, and present a unified description of these effects in finite element method, isogeometric analysis, and refined isogeometric analysis. Accuracy of the refined isogeometric analysis approximations can be improved by using non-standard quadrature rules. In particular, optimal quadrature rules lead to large reductions in the eigenvalue errors and yield two extra orders of convergence similar to classical isogeometric analysis.",0.3],["a single spin\/valley flavor takes all the carriers from its partially filled peers.","Cascade of Phase Transitions and Dirac Revivals in Magic Angle Graphene","summarize: Twisted bilayer graphene near the magic angle exhibits remarkably rich electron correlation physics, displaying insulating, magnetic, and superconducting phases. Here, using measurements of the local electronic compressibility, we reveal that these phases originate from a high-energy state with an unusual sequence of band populations. As carriers are added to the system, rather than filling all the four spin and valley flavors equally, we find that the population occurs through a sequence of sharp phase transitions, which appear as strong asymmetric jumps of the electronic compressibility near integer fillings of the moire lattice. At each transition, a single spin\/valley flavor takes all the carriers from its partially filled peers, resetting them back to the vicinity of the charge neutrality point. As a result, the Dirac-like character observed near the charge neutrality reappears after each integer filling. Measurement of the in-plane magnetic field dependence of the chemical potential near filling factor one reveals a large spontaneous magnetization, further substantiating this picture of a cascade of symmetry breakings. The sequence of phase transitions and Dirac revivals is observed at temperatures well above the onset of the superconducting and correlated insulating states. This indicates that the state we reveal here, with its strongly broken electronic flavor symmetry and revived Dirac-like electronic character, is a key player in the physics of magic angle graphene, forming the parent state out of which the more fragile superconducting and correlated insulating ground states emerge.",0.3636363636],["fusion estimation is to best utilize multiple sensor data containing noises for the purpose of","A New Approach to Linear\/Nonlinear Distributed Fusion Estimation Problem","summarize: Disturbance noises are always bounded in a practical system, while fusion estimation is to best utilize multiple sensor data containing noises for the purpose of estimating a quantity--a parameter or process. However, few results are focused on the information fusion estimation problem under bounded noises. In this paper, we study the distributed fusion estimation problem for linear time-varying systems and nonlinear systems with bounded noises, where the addressed noises do not provide any statistical information, and are unknown but bounded. When considering linear time-varying fusion systems with bounded noises, a new local Kalman-like estimator is designed such that the square error of the estimator is bounded as time goes to ",0.125],["proposed method relies on the construction of time-dependent reduced spaces. the approxim","Dynamical model reduction method for solving parameter-dependent dynamical systems","summarize: We propose a projection-based model order reduction method for the solution of parameter-dependent dynamical systems. The proposed method relies on the construction of time-dependent reduced spaces generated from evaluations of the solution of the full-order model at some selected parameters values. The approximation obtained by Galerkin projection is the solution of a reduced dynamical system with a modified flux which takes into account the time dependency of the reduced spaces. An a posteriori error estimate is derived and a greedy algorithm using this error estimate is proposed for the adaptive selection of parameters values. The resulting method can be interpreted as a dynamical low-rank approximation method with a subspace point of view and a uniform control of the error over the parameter set.",0.125],["spectroscopy was carried out with the inter University centre for Astronomy and Astrophysics","Spectroscopic Survey of H Emission Line Stars Associated with Bright Rimmed Clouds","summarize: The results of a spectroscopic survey of H alpha emission line stars associated with fourteen bright rimmed clouds are presented. Slit-less optical spectroscopy was carried out with the Inter University Centre for Astronomy and Astrophysics 2m telescope and IUCAA Faint Object Spectrograph and Camera . H alpha emission line was detected from 173 objects. Among them 85 objects have a strong H alpha emission line with its equivalent width larger than 10 A. Those are classical T Tauri stars. 52 objects have a weak H alpha emission line with its equivalent width less than 10 A and do not show intrinsic near-infrared excess. Those are weak-line T Tauri stars. On the other hand, 36 objects have a weak H alpha emission line , although they show intrinsic near-infrared excess. Such objects are not common in low-mass star forming regions. Those are misfits of the general concept on formation process of a low-mass star, in which it evolves from a classical T Tauri star to a weak-line T Tauri star. Those might be weak-line T Tauri stars with a flared disk in which gas is heated by ultraviolet radiation from a nearby early-type star. Alternatively, we propose pre-transitional disk objects as their evolutional stage.",0.0769230769],["v=2\/3 edge mode is based on a carefully designed double-quantum","Synthesizing a Fractional v=2\/3 State from Particle and Hole States","summarize: Topological edge-reconstruction occurs in hole-conjugate states of the fractional quantum Hall effect. The frequently studied polarized state of filling factor v=2\/3 was originally proposed to harbor two counter-propagating edge modes: a downstream v=1 and an upstream v=1\/3. However, charge equilibration between these two modes always led to an observed downstream v=2\/3 charge mode accompanied by an upstream neutral mode . Here, we present a new approach to synthetize the v=2\/3 edge mode from its basic counter-propagating charged constituents, allowing a controlled equilibration between the two counter-propagating charge modes. This novel platform is based on a carefully designed double-quantum-well, which hosts two populated electronic sub-bands , with corresponding filling factors, vl & vu. By separating the 2D plane to two gated intersecting halves, each with different fillings, counter-propagating chiral modes can be formed along the intersection line. Equilibration between these modes can be controlled with the top gates' voltage and the magnetic field. Our measurements of the two-terminal conductance G2T and the presence of a neutral mode allowed following the transition from the non-equilibrated charged modes, manifested by G2T=e2\/h, to the fully equilibrated modes, with a downstream charge mode with G2T=e2\/h accompanied by an upstream neutral mode.",0.4615384615],["the paper addresses simultaneous, high-precision measurement and analysis of generic reference signals. the","High-Precision Measurement of Sine and Pulse Reference Signals using Software-Defined Radio","summarize: This paper addresses simultaneous, high-precision measurement and analysis of generic reference signals by using inexpensive commercial off-the-shelf Software Defined Radio hardware. Sine reference signals are digitally down-converted to baseband for the analysis of phase deviations. Hereby, we compare the precision of the fixed-point hardware Digital Signal Processing chain with a custom Single Instruction Multiple Data x86 floating-point implementation. Pulse reference signals are analyzed by a software trigger that precisely locates the time where the slope passes a certain threshold. The measurement system is implemented and verified using the Universal Software Radio Peripheral N210 by Ettus Research LLC. Applying standard 10 MHz and 1 PPS reference signals for testing, a measurement precision of 0.36 ps and 16.6 ps is obtained, respectively. In connection with standard PC hardware, the system allows long-term acquisition and storage of measurement data over several weeks. A comparison is given to the Dual Mixer Time Difference and Time Interval Counter , which are state-of-the-art measurement methods for sine and pulse signal analysis, respectively. Furthermore, we show that our proposed USRP-based approach outperforms measurements with a high-grade Digital Sampling Oscilloscope.",0.1333333333],["SSDs have gained tremendous attention in computing and storage systems. the cost per capacity of SSD","ReCA: an Efficient Reconfigurable Cache Architecture for Storage Systems with Online Workload Characterization","summarize: In recent years, SSDs have gained tremendous attention in computing and storage systems due to significant performance improvement over HDDs. The cost per capacity of SSDs, however, prevents them from entirely replacing HDDs in such systems. One approach to effectively take advantage of SSDs is to use them as a caching layer to store performance critical data blocks to reduce the number of accesses to disk subsystem. Due to characteristics of Flash-based SSDs such as limited write endurance and long latency on write operations, employing caching algorithms at the Operating System level necessitates to take such characteristics into consideration. Previous caching techniques are optimized towards only one type of application, which affects both generality and applicability. In addition, they are not adaptive when the workload pattern changes over time. This paper presents an efficient Reconfigurable Cache Architecture for storage systems using a comprehensive workload characterization to find an optimal cache configuration for I\/O intensive applications. For this purpose, we first investigate various types of I\/O workloads and classify them into five major classes. Based on this characterization, an optimal cache configuration is presented for each class of workloads. Then, using the main features of each class, we continuously monitor the characteristics of an application during system runtime and the cache organization is reconfigured if the application changes from one class to another class of workloads. The cache reconfiguration is done online and workload classes can be extended to emerging I\/O workloads in order to maintain its efficiency with the characteristics of I\/O requests. Experimental results obtained by implementing ReCA in a server running Linux show that the proposed architecture improves performance and lifetime up to 24\\% and 33\\%, respectively.",0.0526315789],["medical research suggests the anterior-posterior -diameter of the inferior","Estimation and Tracking of AP-diameter of the Inferior Vena Cava in Ultrasound Images Using a Novel Active Circle Algorithm","summarize: Medical research suggests that the anterior-posterior -diameter of the inferior vena cava and its associated temporal variation as imaged by bedside ultrasound is useful in guiding fluid resuscitation of the critically-ill patient. Unfortunately, indistinct edges and gaps in vessel walls are frequently present which impede accurate estimation of the IVC AP-diameter for both human operators and segmentation algorithms. The majority of research involving use of the IVC to guide fluid resuscitation involves manual measurement of the maximum and minimum AP-diameter as it varies over time. This effort proposes using a time-varying circle fitted inside the typically ellipsoid IVC as an efficient, consistent and novel approach to tracking and approximating the AP-diameter even in the context of poor image quality. In this active-circle algorithm, a novel evolution functional is proposed and shown to be a useful tool for ultrasound image processing. The proposed algorithm is compared with an expert manual measurement, and state-of-the-art relevant algorithms. It is shown that the algorithm outperforms other techniques and performs very close to manual measurement.",0.119706541],["the pumped charge is quantized only when the pumping time is a multiple of","Topologically quantized current in quasiperiodic Thouless pumps","summarize: Thouless pumps are topologically nontrivial states of matter with quantized charge transport, which can be realized in atomic gases loaded into an optical lattice. This topological state is analogous to the quantum Hall state. However, contrarily to the exact, extremely precise, and robust quantization of the Hall conductance, the pumped charge is strictly quantized only when the pumping time is a multiple of a characteristic timescale, i.e., the pumping cycle duration. Here, we show instead that the pumped current becomes exactly quantized, independently from the pumping time, if the system is led into a quasiperiodic, incommensurate regime. In this quasiperiodic and topologically nontrivial state, the Bloch bands and the Berry curvature become flat, the pumped charge becomes linear in time, while the current becomes steady, topologically quantized, and proportional to the Chern number. The quantization of the current is exact up to exponentially small corrections. This has to be contrasted with the case of the commensurate regime, where the current is not constant, and the pumped charge is quantized only at integer multiples of the pumping cycle.",0.2],["dynamical array consists of a family of functions.","The Lindeberg theorem for Gibbs-Markov dynamics","summarize: A dynamical array consists of a family of functions ",0.1333333333],["we establish the rate region of an extended Gray-Wyner system for 2-DMS","Extended Gray-Wyner System with Complementary Causal Side Information","summarize: We establish the rate region of an extended Gray-Wyner system for 2-DMS ",0.1666666667],["the model has a family of steady state solutions called twisted states. we prove","Stability of twisted states in the continuum Kuramoto model","summarize: We study a nonlocal diffusion equation approximating the dynamics of coupled phase oscillators on large graphs. Under appropriate assumptions, the model has a family of steady state solutions called twisted states. We prove a sufficient condition for stability of twisted states with respect to perturbations in the Sobolev and BV spaces. As an application, we study stability of twisted states in the Kuramoto model on small-world graphs.",0.4285714286],["a new formula is proposed to estimate MBH in WLQs. the","Black hole masses of weak emission line quasars based on continuum fit method","summarize: We studied optical-ultraviolet spectral energy distribution of 10 weak emission-line quasars which lie at redshifts z = 0.19 and 1.43 < z < 3.48. The theoretical models of their accretion disk continua are created based on the Novikov-Thorne equations. It allows us to estimate masses of their supermassive black holes and accretion rates. We determined the virial factor for WLQs and note its anti-correlation with the full width at half maximum of Hbeta emission-line , alpha = ). By comparison with the previously estimated BH masses, the underestimation of MBH is noticed with a mean factor 4-5 which depends on the measured full width. We proposed the new formula to estimate MBH in WLQs based on their observed FWHM and luminosities at 5100 A. In our opinion, WLQs are also normal quasars visible in a reactivation stage.",0.3206741993],["optical unit consists of a flat, circular acrylic plate coated with tetraphen","A Model for the Global Quantum Efficiency for a TPB-based Wavelength-Shifting System used with Photomultiplier Tubes in Liquid Argon in MicroBooNE","summarize: We present a model for the Global Quantum Efficiency of the MicroBooNE optical units. An optical unit consists of a flat, circular acrylic plate, coated with tetraphenyl butadiene , positioned near the photocathode of a 20.2-cm diameter photomultiplier tube. The plate converts the ultra-violet scintillation photons from liquid argon into visible-spectrum photons to which the cryogenic phototubes are sensitive. The GQE is the convolution of the efficiency of the plates that convert the 128 nm scintillation light from liquid argon to visible light, the efficiency of the shifted light to reach the photocathode, and the efficiency of the cryogenic photomultiplier tube. We develop a GEANT4-based model of the optical unit, based on first principles, and obtain the range of probable values for the expected number of detected photoelectrons given the known systematic errors on the simulation parameters. We compare results from four measurements of the ",0.1491683851],["a framework to analyse such schemes for general nonlinear systems has been recently proposed.","On Generation of Virtual Outputs via Signal Injection: Application to Observer Design for Electromechanical Systems","summarize: Probing signal injection is a well-established technique to extract additional information from a weakly observable dynamical system. Using averaging theory, a framework to analyse such schemes for general nonlinear systems has been recently proposed in , where it is shown that the signal injection may be used to generate a new high frequency component of the systems output that can be used for state observation or controller design. A key step for the success of this technique is the implementation of a filter to reconstruct this virtual output from the measurement of the overall systems output. The main contribution of this paper is to propose a new filter with guaranteed convergence properties that outperforms the classical designs. The method is applied to a general class of electromechanical systems, and its performance is assessed via simulations and experiments on the benchmark example of a 1-dof magnetic levitation system.",0.4189782509],["the CE is built from the Fourier spectrum of fluctuations around the mean-field. it","Information Dynamics at a Phase Transition","summarize: We propose a new way of investigating phase transitions in the context of information theory. We use an information-entropic measure of spatial complexity known as configurational entropy to quantify both the storage and exchange of information in a lattice simulation of a Ginzburg-Landau model with a scalar order parameter coupled to a heat bath. The CE is built from the Fourier spectrum of fluctuations around the mean-field and reaches a minimum at criticality. In particular, we investigate the behavior of CE near and at criticality, exploring the relation between information and the emergence of ordered domains. We show that as the temperature is increased from below, the CE displays three essential scaling regimes at different spatial scales: scale free, turbulent, and critical. Together, they offer an information-entropic characterization of critical behavior where the storage and processing of information is maximized at criticality.",0.05],["normalization layers are known to improve convergence and generalization. they are part of many state","A Domain Agnostic Normalization Layer for Unsupervised Adversarial Domain Adaptation","summarize: We propose a normalization layer for unsupervised domain adaption in semantic scene segmentation. Normalization layers are known to improve convergence and generalization and are part of many state-of-the-art fully-convolutional neural networks. We show that conventional normalization layers worsen the performance of current Unsupervised Adversarial Domain Adaption , which is a method to improve network performance on unlabeled datasets and the focus of our research. Therefore, we propose a novel Domain Agnostic Normalization layer and thereby unlock the benefits of normalization layers for unsupervised adversarial domain adaptation. In our evaluation, we adapt from the synthetic GTA5 data set to the real Cityscapes data set, a common benchmark experiment, and surpass the state-of-the-art. As our normalization layer is domain agnostic at test time, we furthermore demonstrate that UADA using Domain Agnostic Normalization improves performance on unseen domains, specifically on Apolloscape and Mapillary.",0.0],["model to describe generalized wave-particle instability in quasi-neutral plasma. we","A Quasi-Linear Diffusion Model for Resonant Wave-Particle Instability in Homogeneous Plasma","summarize: In this paper, we develop a model to describe the generalized wave-particle instability in a quasi-neutral plasma. We analyze the quasi-linear diffusion equation for particles by expressing an arbitrary unstable and resonant wave mode as a Gaussian wave packet, allowing for an arbitrary direction of propagation with respect to the background magnetic field. We show that the localized energy density of the Gaussian wave packet determines the velocity-space range in which the dominant wave-particle instability and counter-acting damping contributions are effective. Moreover, we derive a relation describing the diffusive trajectories of resonant particles in velocity space under the action of such an interplay between the wave-particle instability and damping. For the numerical computation of our theoretical model, we develop a mathematical approach based on the Crank-Nicolson scheme to solve the full quasi-linear diffusion equation. Our numerical analysis solves the time evolution of the velocity distribution function under the action of a dominant wave-particle instability and counteracting damping and shows a good agreement with our theoretical description. As an application, we use our model to study the oblique fast-magnetosonic\/whistler instability, which is proposed as a scattering mechanism for strahl electrons in the solar wind. In addition, we numerically solve the full Fokker-Planck equation to compute the time evolution of the electron-strahl distribution function under the action of Coulomb collisions with core electrons and protons after the collisionless action of the oblique fast-magnetosonic\/whistler instability.",0.1645158942],["present paper examines the vacuum bosonic currents in the geometry of a compactified","Vacuum bosonic currents induced by a compactified cosmic string in dS background","summarize: In the present paper, we study the vacuum bosonic currents in the geometry of a compactified cosmic string in the background of the de Sitter spacetime. The currents are induced by magnetic fluxes, one running along the cosmic string and another one enclosed by the compact dimension. To develop the analysis, we obtain the complete set of normalized bosonic wave-functions obeying a quasiperiodicity condition. In this context, we calculate the azimuthal and axial current densities and we show that these quantities are explicitly decomposed into two contributions: one originating from the geometry of a straight uncompactified cosmic string and the other induced by the compactification. We also compare the results with the literature in the case of a massive fermionic field in the same geometry.",0.45],["the trifocal essential matrix is a generalization of the ordinary essential matrix.","On Some Properties of Calibrated Trifocal Tensors","summarize: In two-view geometry, the essential matrix describes the relative position and orientation of two calibrated images. In three views, a similar role is assigned to the calibrated trifocal tensor. It is a particular case of the trifocal tensor and thus it inherits all its properties but, due to the smaller degrees of freedom, satisfies a number of additional algebraic constraints. Some of them are described in this paper. More specifically, we define a new notion --- the trifocal essential matrix. On the one hand, it is a generalization of the ordinary essential matrix, and, on the other hand, it is closely related to the calibrated trifocal tensor. We prove the two necessary and sufficient conditions that characterize the set of trifocal essential matrices. Based on these characterizations, we propose three necessary conditions on a calibrated trifocal tensor. They have a form of 15 quartic and 99 quintic polynomial equations. We show that in the practically significant real case the 15 quartic constraints are also sufficient.",0.1538461538],["matrix factorization may not be well suited for the matrix completion problem. we show that","Scalable Probabilistic Matrix Factorization with Graph-Based Priors","summarize: In matrix factorization, available graph side-information may not be well suited for the matrix completion problem, having edges that disagree with the latent-feature relations learnt from the incomplete data matrix. We show that removing these ",0.0],["we prove existence and regularity results for weak solutions of non linear elliptic systems with","Existence and regularity results for weak solutions to ","summarize: We prove existence and regularity results for weak solutions of non linear elliptic systems with non variational structure satisfying ",0.4375],["fraud acts as a major deterrent to a companys growth if uncontrolled","Implementation of Correlation and Regression Models for Health Insurance Fraud in Covid-19 Environment using Actuarial and Data Science Techniques","summarize: Fraud acts as a major deterrent to a companys growth if uncontrolled. It challenges the fundamental value of Trust in the Insurance business. COVID-19 brought additional challenges of increased potential fraud to health insurance business. This work describes implementation of existing and enhanced fraud detection methods in the pre-COVID-19 and COVID-19 environments. For this purpose, we have developed an innovative enhanced fraud detection framework using actuarial and data science techniques. Triggers specific to COVID-19 are identified in addition to the existing triggers. We have also explored the relationship between insurance fraud and COVID-19. To determine this we calculated Pearson correlation coefficient and fitted logarithmic regression model between fraud in health insurance and COVID-19 cases. This work uses two datasets: health insurance dataset and Kaggle dataset on COVID-19 cases for the same select geographical location in India. Our experimental results shows Pearson correlation coefficient of 0.86, which implies that the month on month rate of fraudulent cases is highly correlated with month on month rate of COVID-19 cases. The logarithmic regression performed on the data gave the r-squared value of 0.91 which indicates that the model is a good fit. This work aims to provide much needed tools and techniques for health insurance business to counter the fraud.",0.3255205017],["quantum fingerprinting reduces communication complexity of determination whether two fingerprints are identical.","Quantum Fingerprinting over AWGN Channels with Power-Limited Optical Signals","summarize: Quantum fingerprinting reduces communication complexity of determination whether two ",0.0],["the standard retraining technique, fine-tuning, trains the unprune","Comparing Rewinding and Fine-tuning in Neural Network Pruning","summarize: Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding ), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.",0.0],["spin orbit torque driven domain wall devices as synapses and transistor based analog circuits","On-chip learning for domain wall synapse based Fully Connected Neural Network","summarize: Spintronic devices are considered as promising candidates in implementing neuromorphic systems or hardware neural networks, which are expected to perform better than other existing computing systems for certain data classification and regression tasks. In this paper, we have designed a feedforward Fully Connected Neural Network with no hidden layer using spin orbit torque driven domain wall devices as synapses and transistor based analog circuits as neurons. A feedback circuit is also designed using transistors, which at every iteration computes the change in weights of the synapses needed to train the network using Stochastic Gradient Descent method. Subsequently it sends write current pulses to the domain wall based synaptic devices which move the domain walls and updates the weights of the synapses. Through a combination of micromagnetic simulations, analog circuit simulations and numerically solving FCNN training equations, we demonstrate on-chip training of the designed FCNN on the MNIST database of handwritten digits in this paper. We report the training and test accuracies, energy consumed in the synaptic devices for the training and possible issues with hardware implementation of FCNN that can limit its test accuracy.",0.2666666667],["body composition analysis is known to be associated with many diseases including diabetes, cancers and cardiovascular diseases","Automatic segmentation of CT images for ventral body composition analysis","summarize: Purpose: Body composition is known to be associated with many diseases including diabetes, cancers and cardiovascular diseases. In this paper, we developed a fully automatic body tissue decomposition procedure to segment three major compartments that are related to body composition analysis - subcutaneous adipose tissue , visceral adipose tissue and muscle. Three additional compartments - the ventral cavity, lung and bones were also segmented during the segmentation process to assist segmentation of the major compartments. Methods: A convolutional neural network model with densely connected layers was developed to perform ventral cavity segmentation. An image processing workflow was developed to segment the ventral cavity in any patient's CT using the CNN model, then further segment the body tissue into multiple compartments using hysteresis thresholding followed by morphological operations. It is important to segment ventral cavity firstly to allow accurate separation of compartments with similar Hounsfield unit inside and outside the ventral cavity. Results: The ventral cavity segmentation CNN model was trained and tested with manually labelled ventral cavities in 60 CTs. Dice scores for ventral cavity segmentation were 0.966+\/-0.012. Tested on CT datasets with intravenous and oral contrast, the Dice scores were 0.96+\/-0.02, 0.94+\/-0.06, 0.96+\/-0.04, 0.95+\/-0.04 and 0.99+\/-0.01 for bone, VAT, SAT, muscle and lung, respectively. The respective Dice scores were 0.97+\/-0.02, 0.94+\/-0.07, 0.93+\/-0.06, 0.91+\/-0.04 and 0.99+\/-0.01 for non-contrast CT datasets. Conclusion: A body tissue decomposition procedure was developed to automatically segment multiple compartments of the ventral body. The proposed method enables fully automated quantification of 3D ventral body composition metrics from CT images.",0.2173913043],["social contacts play a crucial role in promoting large scale spreading. ordinary contacts play","Close and ordinary social contacts: how important are they in promoting large-scale contagion?","summarize: An outstanding problem of interdisciplinary interest is to understand quantitatively the role of social contacts in contagion dynamics. In general, there are two types of contacts: close ones among friends, colleagues and family members, etc., and ordinary contacts from encounters with strangers. Typically, social reinforcement occurs for close contacts. Taking into account both types of contacts, we develop a contact-based model for social contagion. We find that, associated with the spreading dynamics, for random networks there is coexistence of continuous and discontinuous phase transitions, but for heterogeneous networks the transition is continuous. We also find that ordinary contacts play a crucial role in promoting large scale spreading, and the number of close contacts determines not only the nature of the phase transitions but also the value of the outbreak threshold in random networks. For heterogeneous networks from the real world, the abundance of close contacts affects the epidemic threshold, while its role in facilitating the spreading depends on the adoption threshold assigned to it. We uncover two striking phenomena. First, a strong interplay between ordinary and close contacts is necessary for generating prevalent spreading. In fact, only when there are propagation paths of reasonable length which involve both close and ordinary contacts are large scale outbreaks of social contagions possible. Second, abundant close contacts in heterogeneous networks promote both outbreak and spreading of the contagion through the transmission channels among the hubs, when both values of the threshold and transmission rate among ordinary contacts are small. We develop a theoretical framework to obtain an analytic understanding of the main findings on random networks, with support from extensive numerical computations.",0.6129032258],["symmetry breaking operators for real reductive groups have been classed. we illustrate","Conformal symmetry breaking on differential forms and some applications","summarize: Rapid progress has been made recently on symmetry breaking operators for real reductive groups. Based on Program A-C for branching problems , we illustrate a scheme of the classification of symmetry breaking operators by an example of conformal representations on differential forms on the model space ",0.25],["filtering for hidden Markov models is linked to the notion of duality. the filter","Optimal filtering and the dual process","summarize: We link optimal filtering for hidden Markov models to the notion of duality for Markov processes. We show that when the signal is dual to a process that has two components, one deterministic and one a pure death process, and with respect to functions that define changes of measure conjugate to the emission density, the filtering distributions evolve in the family of finite mixtures of such measures and the filter can be computed at a cost that is polynomial in the number of observations. Special cases of our framework include the Kalman filter, and computable filters for the Cox-Ingersoll-Ross process and the one-dimensional Wright-Fisher process, which have been investigated before. The dual we obtain for the Cox-Ingersoll-Ross process appears to be new in the literature.",0.2352941176],["the algorithm will propagate labels from the labeled to the unlabeled pixels","Simple Interactive Image Segmentation using Label Propagation through kNN graphs","summarize: Many interactive image segmentation techniques are based on semi-supervised learning. The user may label some pixels from each object and the SSL algorithm will propagate the labels from the labeled to the unlabeled pixels, finding object boundaries. This paper proposes a new SSL graph-based interactive image segmentation approach, using undirected and unweighted kNN graphs, from which the unlabeled nodes receive contributions from other nodes . It is simpler than many other techniques, but it still achieves significant classification accuracy in the image segmentation task. Computer simulations are performed using some real-world images, extracted from the Microsoft GrabCut dataset. The segmentation results show the effectiveness of the proposed approach.",0.0],["the 2+1 Dirac-Moshinsky oscillator coupled to an external is","Mapping of the 2+1 Dirac-Moshinsky Oscillator Coupled to an External Isospin Field onto Jaynes-Cummings Model","summarize: In this paper, the 2+1 Dirac-Moshinsky oscillator coupled to an external isospin field is mapped onto the Jaynes-Cummings model , which describes the interaction between two two-level systems and a quantum single-mode field. The time-dependent wave function and the density matrix are obtained in the two cases. In the first case, the quantum number state is considered, while in the second case, the coherent state is considered as an initial state. The effect of both the detuning parameter and the coherence angle are studied on the entanglement and on the population inversion. It has been shown that the coherent state gives good description than the number state for the entanglement and the population inversion.",0.2852317328],["chess players engaged in problems of increasing difficulty while recording their behavior. a multi","Multimodal Observation and Interpretation of Subjects Engaged in Problem Solving","summarize: In this paper we present the first results of a pilot experiment in the capture and interpretation of multimodal signals of human experts engaged in solving challenging chess problems. Our goal is to investigate the extent to which observations of eye-gaze, posture, emotion and other physiological signals can be used to model the cognitive state of subjects, and to explore the integration of multiple sensor modalities to improve the reliability of detection of human displays of awareness and emotion. We observed chess players engaged in problems of increasing difficulty while recording their behavior. Such recordings can be used to estimate a participant's awareness of the current situation and to predict ability to respond effectively to challenging situations. Results show that a multimodal approach is more accurate than a unimodal one. By combining body posture, visual attention and emotion, the multimodal approach can reach up to 93% of accuracy when determining player's chess expertise while unimodal approach reaches 86%. Finally this experiment validates the use of our equipment as a general and reproducible tool for the study of participants engaged in screen-based interaction and\/or problem solving.",0.380952381],["human cooperation does not require sheer computational power, but rather relies on intuition, cultural norm","Cooperating with Machines","summarize: Since Alan Turing envisioned Artificial Intelligence , a major driving force behind technical progress has been competition with human cognition. Historical milestones have been frequently associated with computers matching or outperforming humans in difficult cognitive tasks , or defeating humans in strategic zero-sum encounters . In contrast, less attention has been given to developing autonomous machines that establish mutually cooperative relationships with people who may not share the machine's preferences. A main challenge has been that human cooperation does not require sheer computational power, but rather relies on intuition , cultural norms , emotions and signals , and pre-evolved dispositions toward cooperation , common-sense mechanisms that are difficult to encode in machines for arbitrary contexts. Here, we combine a state-of-the-art machine-learning algorithm with novel mechanisms for generating and acting on signals to produce a new learning algorithm that cooperates with people and other machines at levels that rival human cooperation in a variety of two-player repeated stochastic games. This is the first general-purpose algorithm that is capable, given a description of a previously unseen game environment, of learning to cooperate with people within short timescales in scenarios previously unanticipated by algorithm designers. This is achieved without complex opponent modeling or higher-order theories of mind, thus showing that flexible, fast, and general human-machine cooperation is computationally achievable using a non-trivial, but ultimately simple, set of algorithmic mechanisms.",0.0],["GLUE benchmark is a suite of language understanding tasks which has seen dramatic progress in the","Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark","summarize: The GLUE benchmark is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing . Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.",0.36],["a comprehensive study on the evolution of Stoner factor with doping concentration for various doped","Stoner factors of doped 122 Fe-based superconductors: First principles results","summarize: A comprehensive study on the evolution of Stoner factor with doping concentration for various doped 122 systems of Fe-based superconductors is presented. Our first principles electronic structure calculations reveal that for Co\/Ru doping at Fe sites or P doping at As sites result in a reduction of Stoner factor with increasing doping concentration. On the contrary, in case of Na\/K doping at the Ba sites, Stoner factor is enhanced for higher doping concentrations. This may be considered as an indicator of elevation of magnetic fluctuation in these systems. We find that the Stoner factor uniquely follows the variation of the pnictide height z",0.3636363636],["four classical undergraduate physics experiments were done with everyday objects and low-cost sensors. mechanical","Low-Cost Experiments with Everyday Objects for Homework Assignments","summarize: We describe four classical undergraduate physics experiments that were done with everyday objects and low-cost sensors: mechanical oscillations, transmittance of light through a slab of matter, beam deformation under load, and thermal relaxation due to heat loss. We used these experiments to train students for experimental homework projects but they could be used and expanded in a variety of contexts: lecture demonstrations, low cost students' labs, science projects, distance learning courses...",0.0714285714],["a novel method for gradient-based optimization of black-box simulators. we propose","Black-Box Optimization with Local Generative Surrogates","summarize: We propose a novel method for gradient-based optimization of black-box simulators using differentiable local surrogate models. In fields such as physics and engineering, many processes are modeled with non-differentiable simulators with intractable likelihoods. Optimization of these forward models is particularly challenging, especially when the simulator is stochastic. To address such cases, we introduce the use of deep generative models to iteratively approximate the simulator in local neighborhoods of the parameter space. We demonstrate that these local surrogates can be used to approximate the gradient of the simulator, and thus enable gradient-based optimization of simulator parameters. In cases where the dependence of the simulator on the parameter space is constrained to a low dimensional submanifold, we observe that our method attains minima faster than baseline methods, including Bayesian optimization, numerical optimization, and approaches using score function gradient estimators.",0.3125],["Let us know what you think about it!","Sufficient criteria and sharp geometric conditions for observability in Banach spaces","summarize: Let ",0.0],["we consider full feedback from the loads, bandit feedback, and two intermediate types of feedback","Setpoint Tracking with Partially Observed Loads","summarize: We use online convex optimization for setpoint tracking with uncertain, flexible loads. We consider full feedback from the loads, bandit feedback, and two intermediate types of feedback: partial bandit where a subset of the loads are individually observed and the rest are observed in aggregate, and Bernoulli feedback where in each round the aggregator receives either full or bandit feedback according to a known probability. We give sublinear regret bounds in all cases. We numerically evaluate our algorithms on examples with thermostatically controlled loads and electric vehicles.",0.0],["the transmitter harvests energy from a dedicated energy RF source in the sleep state.","Energy-efficient Resource Allocation for Wirelessly Powered Backscatter Communications","summarize: In this letter, we consider a wireless-powered backscatter communication network, where the transmitter first harvests energy from a dedicated energy RF source in the sleep state, and then backscatters information and harvests energy simultaneously through a reflection coefficient. Our goal is to maximize the achievable energy efficiency of the WP-BackCom network via jointly optimizing time allocation, reflection coefficient and transmit power of the dedicated energy RF source. The optimization problem is non-convex and challenging to solve. We develop an efficient Dinkelbach-based iterative algorithm to obtain the optimal resource allocation scheme. The study shows that for each iteration, the energy-efficient WP-BackCom network is equivalent to either the network in which the transmitter always operates in the active state, or the network in which the dedicated energy RF source adopts the maximum allowed power.",0.2272727273],["the associated cost matrices of these transportation problems are of special structure. we present","Some Aspects on Solving Transportation Problem","summarize: In this paper, we consider a class of transportation problems which arises in sample surveys and other areas of statistics. The associated cost matrices of these transportation problems are of special structure. We observe that the optimality of North West corner solution holds for the general problem where cost component is replaced by a convex function. We revisit assignment problem and present a weighted version of K",0.0],["manual fact-checking initiatives have triggered a number of responses. a large","That is a Known Lie: Detecting Previously Fact-Checked Claims","summarize: The recent proliferation of fake news has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new claim in social media or a new statement by a politician might have already been fact-checked by some trusted fact-checking organization, as viral claims often come back after a while in social media, and politicians like to repeat their favorite statements, true or false, over and over again. As manual fact-checking is very time-consuming , it is important to try to save this effort and to avoid wasting time on claims that have already been fact-checked. Interestingly, despite the importance of the task, it has been largely ignored by the research community so far. Here, we aim to bridge this gap. In particular, we formulate the task and we discuss how it relates to, but also differs from, previous work. We further create a specialized dataset, which we release to the research community. Finally, we present learning-to-rank experiments that demonstrate sizable improvements over state-of-the-art retrieval and textual similarity approaches.",0.32],["Blau and Michaeli introduce a concept for inverse problems of signal processing. we","Rate-Distortion-Perception Tradeoff of Variable-Length Source Coding for General Information Sources","summarize: Blau and Michaeli recently introduced a novel concept for inverse problems of signal processing, that is, the perception-distortion tradeoff. We introduce their tradeoff into the rate distortion theory of variable-length lossy source coding in information theory, and clarify the tradeoff among information rate, distortion and perception for general information sources. We also discuss the fixed-length coding with average distortion criterion that was missing in the previous letter.",0.4117647059],["study presents critical review of disclosed, documented, and malicious cybersecurity incidents in the water sector.","A Review of Cybersecurity Incidents in the Water Sector","summarize: This study presents a critical review of disclosed, documented, and malicious cybersecurity incidents in the water sector to inform safeguarding efforts against cybersecurity threats. The review is presented within a technical context of industrial control system architectures, attack-defense models, and security solutions. Fifteen incidents were selected and analyzed through a search strategy that included a variety of public information sources ranging from federal investigation reports to scientific papers. For each individual incident, the situation, response, remediation, and lessons learned were compiled and described. The findings of this review indicate an increase in the frequency, diversity, and complexity of cyberthreats to the water sector. Although the emergence of new threats, such as ransomware or cryptojacking, was found, a recurrence of similar vulnerabilities and threats, such as insider threats, was also evident, emphasizing the need for an adaptive, cooperative, and comprehensive approach to water cyberdefense.",0.1875],["How many","How many is different? Answer from ideal Bose gas","summarize: How many ",0.0301973834],["expository article revolves around the question to find short presentations of finite simple groups.","Two Generation of Finite Simple Groups","summarize: This expository article revolves around the question to find short presentations of finite simple groups. This subject is one of the most active research areas of group theory in recent times. We bring together several known results on two-generation and ",0.0666666667],["boundary conditions are a common or special boundary universality class. boundary conditions are e","Why boundary conditions do not generally determine the universality class for boundary critical behavior","summarize: Interacting field theories for systems with a free surface frequently exhibit distinct universality classes of boundary critical behaviors depending on gross surface properties. The boundary condition satisfied by the continuum field theory on some scale may or may not be decisive for the universality class that applies. In many recent papers on boundary field theories it is taken for granted that Dirichlet or Neumann boundary conditions decide whether the ordinary or special boundary universality class is observed. While true in a certain sense for the Dirichlet boundary condition, this is not the case for the Neumann boundary condition. Building on results that have been worked out in the 1980s, but have not always been appropriately appreciated in the literature, the subtle role of boundary conditions and their scale dependence is elucidated and the question of whether or not they determine the observed boundary universality class is discussed.",0.6388888889],["positive topological entropy does not imply a strong version of chaos called DC","Distributional chaos in multifractal analysis, recurrence and transitivity","summarize: There are lots of results to study dynamical complexity on irregular sets and level sets of ergodic average from the perspective of density in base space, Hausdorff dimension, Lebesgue positive measure, positive or full topological entropy etc.. However, it is unknown from the viewpoint of chaos. There are lots of results on the relationship of positive topological entropy and various chaos but it is known that positive topological entropy does not imply a strong version of chaos called DC1 so that it is non-trivial to study DC1 on irregular sets and level sets. In this paper we will show that for dynamical system with specification property, there exist uncountable DC1-scrambled subsets in irregular sets and level sets. On the other hand, we also prove that several recurrent levels of points with different recurrent frequency all have uncountable DC1-scrambled subsets. The main technique established to prove above results is that there exists uncountable DC1-scrambled subset in saturated sets.",0.3125],["the classical theory of enzymatic inhibition aims to quantitatively describe the effect of certain","Single-molecule theory of enzymatic inhibition predicts the emergence of inhibitor-activator duality","summarize: The classical theory of enzymatic inhibition aims to quantitatively describe the effect of certain molecules -- called inhibitors -- on the progression of enzymatic reactions, but growing signs indicate that it must be revised to keep pace with the single-molecule revolution that is sweeping through the sciences. Here, we take the single enzyme perspective and rebuild the theory of enzymatic inhibition from the bottom up. We find that accounting for multi-conformational enzyme structure and intrinsic randomness cannot undermine the validity of classical results in the case of competitive inhibition; but that it should strongly change our view on the uncompetitive and mixed modes of inhibition. There, stochastic fluctuations on the single-enzyme level could give rise to inhibitor-activator duality -- a phenomenon in which, under some conditions, the introduction of a molecule whose binding shuts down enzymatic catalysis will counter intuitively work to facilitate product formation. We state -- in terms of experimentally measurable quantities -- a mathematical condition for the emergence of inhibitor-activator duality, and propose that it could explain why certain molecules that act as inhibitors when substrate concentrations are high elicit a non-monotonic dose response when substrate concentrations are low. The fundamental and practical implications of our findings are thoroughly discussed.",0.6],["the TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of","TADPOLE Challenge: Accurate Alzheimer's disease prediction through crowdsourced forecasting of future data","summarize: The TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants train their models and algorithms on historical data from the Alzheimer's Disease Neuroimaging Initiative study. Participants are then required to make forecasts of three key outcomes for ADNI-3 rollover participants: clinical diagnosis, ADAS-Cog 13, and total volume of the ventricles -- which are then compared with future measurements. Strong points of the challenge are that the test data did not exist at the time of forecasting , and that it focuses on the challenging problem of cohort selection for clinical trials by identifying fast progressors. The submission phase of TADPOLE was open until 15 November 2017; since then data has been acquired until April 2019 from 219 subjects with 223 clinical visits and 150 Magnetic Resonance Imaging scans, which was used for the evaluation of the participants' predictions. Thirty-three teams participated with a total of 92 submissions. No single submission was best at predicting all three outcomes. For diagnosis prediction, the best forecast , which was based on gradient boosting, obtained a multiclass area under the receiver-operating curve of 0.931, while for ventricle prediction the best forecast , which was based on disease progression modelling and spline regression, obtained mean absolute error of 0.41% of total intracranial volume . For ADAS-Cog 13, no forecast was considerably better than the benchmark mixed effects model , provided to participants before the submission deadline. Further analysis can help understand which input features and algorithms are most suitable for Alzheimer's disease prediction and for aiding patient stratification in clinical trials.",0.2727272727],["the sex of the u.s.","Optimal ","summarize: The ",0.0],["we used the transverse Ising model to map the observed results of the quantum state onto","Deep Neural Network Detects Quantum Phase Transition","summarize: We detect the quantum phase transition of a quantum many-body system by mapping the observed results of the quantum state onto a neural network. In the present study, we utilized the simplest case of a quantum many-body system, namely a one-dimensional chain of Ising spins with the transverse Ising model. We prepared several spin configurations, which were obtained using repeated observations of the model for a particular strength of the transverse field, as input data for the neural network. Although the proposed method can be employed using experimental observations of quantum many-body systems, we tested our technique with spin configurations generated by a quantum Monte Carlo simulation without initial relaxation. The neural network successfully classified the strength of transverse field only from the spin configurations, leading to consistent estimations of the critical point of our model ",0.0],["we derive bounds on couplings in the standard model effective field theory. the","Consistency of the Standard Model Effective Field Theory","summarize: We derive bounds on couplings in the standard model effective field theory as a consequence of causality and the analytic structure of scattering amplitudes. In the SMEFT, there are 64 independent operators at mass dimension eight that are quartic in bosons and that contain four derivatives and\/or field strengths, including both CP-conserving and CP-violating operators. Using analytic dispersion relation arguments for two-to-two bosonic scattering amplitudes, we derive 27 independent bounds on the sign or magnitude of the couplings. We show that these bounds also follow as a consequence of causality of signal propagation in nonvacuum SM backgrounds. These bounds come in two qualitative forms: i) positivity of couplings of CP-even operators and ii) upper bounds on the magnitude of CP-odd operators in terms of CP-even couplings. We exhibit various classes of example completions, which all satisfy our EFT bounds. These bounds have consequences for current and future particle physics experiments, as part of the observable parameter space is inconsistent with causality and analyticity. To demonstrate the impact of our bounds, we consider applications both to SMEFT constraints derived at colliders and to limits on the neutron electric dipole moment, highlighting the connection between such searches suggested by infrared consistency.",0.1666666667],["we analyze the low energy behavior of differential form-valued waves on black hole spacetimes","Resonance expansions for tensor-valued waves on asymptotically Kerr-de Sitter spaces","summarize: In recent joint work with Vasy, we analyze the low energy behavior of differential form-valued waves on black hole spacetimes. In order to deduce asymptotics and decay from this, one in addition needs high energy estimates for the wave operator acting on sections of the form bundle. The present paper provides these on perturbations of Schwarzschild-de Sitter spaces in all spacetime dimensions ",0.1428571429],["a sequence enables a protein to acquire a specific stable conformation. this","Deep Robust Framework for Protein Function Prediction using Variable-Length Protein Sequences","summarize: Amino acid sequence portrays most intrinsic form of a protein and expresses primary structure of protein. The order of amino acids in a sequence enables a protein to acquire a particular stable conformation that is responsible for the functions of the protein. This relationship between a sequence and its function motivates the need to analyse the sequences for predicting protein functions. Early generation computational methods using BLAST, FASTA, etc. perform function transfer based on sequence similarity with existing databases and are computationally slow. Although machine learning based approaches are fast, they fail to perform well for long protein sequences . In this paper, we introduce a novel method for construction of two separate feature sets for protein sequences based on analysis of 1) single fixed-sized segments and 2) multi-sized segments, using bi-directional long short-term memory network. Further, model based on proposed feature set is combined with the state of the art Multi-lable Linear Discriminant Analysis features based model to improve the accuracy. Extensive evaluations using separate datasets for biological processes and molecular functions demonstrate promising results for both single-sized and multi-sized segments based feature sets. While former showed an improvement of +3.37% and +5.48%, the latter produces an improvement of +5.38% and +8.00% respectively for two datasets over the state of the art MLDA based classifier. After combining two models, there is a significant improvement of +7.41% and +9.21% respectively for two datasets compared to MLDA based classifier. Specifically, the proposed approach performed well for the long protein sequences and superior overall performance.",0.3],["the graph theory concept is designed to create a relaxation of the vertex degeneracy","High-Performance Parallel Graph Coloring with Strong Guarantees on Work, Depth, and Quality","summarize: We develop the first parallel graph coloring heuristics with strong theoretical guarantees on work and depth and coloring quality. The key idea is to design a relaxation of the vertex degeneracy order, a well-known graph theory concept, and to color vertices in the order dictated by this relaxation. This introduces a tunable amount of parallelism into the degeneracy ordering that is otherwise hard to parallelize. This simple idea enables significant benefits in several key aspects of graph coloring. For example, one of our algorithms ensures polylogarithmic depth and a bound on the number of used colors that is superior to all other parallelizable schemes, while maintaining work-efficiency. In addition to provable guarantees, the developed algorithms have competitive run-times for several real-world graphs, while almost always providing superior coloring quality. Our degeneracy ordering relaxation is of separate interest for algorithms outside the context of coloring.",0.2608695652],["we present a system for hyperspectral image segmentation. this system uses multiple class","Deep Learning Hyperspectral Image Classification Using Multiple Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and Morphological Operations","summarize: Herein, we present a system for hyperspectral image segmentation that utilizes multiple class--based denoising autoencoders which are efficiently trained. Moreover, we present a novel hyperspectral data augmentation method for labelled HSI data using linear mixtures of pixels from each class, which helps the system with edge pixels which are almost always mixed pixels. Finally, we utilize a deep neural network and morphological hole-filling to provide robust image classification. Results run on the Salinas dataset verify the high performance of the proposed algorithm.",0.1934582844],["traffic matrix estimation has always caught attention from researchers. the problem formulation uses a dynamic measurement","A Novel Compressed Sensing Technique for Traffic Matrix Estimation of Software Defined Cloud Networks","summarize: Traffic Matrix estimation has always caught attention from researchers for better network management and future planning. With the advent of high traffic loads due to Cloud Computing platforms and Software Defined Networking based tunable routing and traffic management algorithms on the Internet, it is more necessary as ever to be able to predict current and future traffic volumes on the network. For large networks such origin-destination traffic prediction problem takes the form of a large under-constrained and under-determined system of equations with a dynamic measurement matrix. In this work, we present our Compressed Sensing with Dynamic Model Estimation architecture suitable for modern software defined networks. Our main contributions are: we formulate an approach in which measurement matrix in the compressed sensing scheme can be accurately and dynamically estimated through a reformulation of the problem based on traffic demands. We show that the problem formulation using a dynamic measurement matrix based on instantaneous traffic demands may be used instead of a stationary binary routing matrix which is more suitable to modern Software Defined Networks that are constantly evolving in terms of routing by inspection of its Eigen Spectrum using two real world datasets. We also show that linking this compressed measurement matrix dynamically with the measured parameters can lead to acceptable estimation of Origin Destination Traffic flows with marginally poor results with other state-of-art schemes relying on fixed measurement matrices. Furthermore, using this compressed reformulated problem, a new strategy for selection of vantage points for most efficient traffic matrix estimation is also presented through a secondary compression technique based on subset of link measurements.",0.1428571429],["study is designed to help students learn from one or all of the four most commonly used foreign languages","An Interactive Foreign Language Trainer Using Assessment and Feedback Modalities","summarize: English has long been set as the universal language. Basically most, if not all countries in the world know how to speak English or at least try to use it in their everyday communications for the purpose of globalizing. This study is designed to help the students learn from one or all of the four most commonly used foreign languages in the field of Information Technology namely Korean, Mandarin Chinese, Japanese, and Spanish. Composed of a set of words, phrases, and sentences, the program is intended to quickly teach the students in the form of basic, intermediate, and advanced levels. This study has used the Agile model in system development. Functionality, reliability, usability, efficiency, and portability were also considered in determining the level of the acceptability of the system in terms of ISO 25010:2011. This interactive foreign language trainer is built to associate fun with learning, to remedy the lack of perseverance by some in learning a new language, and to make learning the users' favorite playtime activity. The study allows the user to interact with the program which provides support for their learning. Moreover, this study reveals that integrating feedback modalities in the training and assessment modules of the software strengthens and enhances the memory in learning the language.",0.05],["long-term hourly time series representing the PV generation in european countries have been obtained.","Using validated reanalysis data to investigate the impact of the PV system configurations at high penetration levels in European countries","summarize: Long-term hourly time series representing the PV generation in European countries have been obtained and made available under open license. For every country, four different PV configurations, i.e. rooftop, optimum tilt, tracking, and delta have been investigated. These are shown to have a strong influence in the hourly difference between electricity demand and PV generation. To obtain PV time series, irradiance from CFSR reanalysis dataset is converted into electricity generation and aggregated at country level. Prior to conversion, reanalysis irradiance is bias corrected using satellite-based SARAH dataset and a globally-applicable methodology. Moreover, a novel procedure is proposed to infer the orientation and inclination angles representative for PV panels based on the historical PV output throughout the days around summer and winter solstices. A key strength of the methodology is that it doesn't rely on historical PV output data. Consequently, it can be applied in places with no existing knowledge of PV performance.",0.2442896466],["geometric optimal control solves the problem of minimum-time transitions between thermal equilibrium states of the","Minimum-Time Transitions between Thermal Equilibrium States of the Quantum Parametric Oscillator","summarize: In this article, we use geometric optimal control to completely solve the problem of minimum-time transitions between thermal equilibrium states of the quantum parametric oscillator, which finds applications in various physical contexts. We discover a new kind of optimal solutions, absent from all the previous treatments of the problem.",0.2380952381],["cadmium arsenide is a newly discovered three-dimensional dirac semi","Widely Tunable Optical and Thermal Properties of Dirac Semimetal Cd","summarize: In this paper we report a detailed analysis of the temperature-dependent optical properties of epitaxially grown cadmium arsenide , a newly discovered three-dimensional Dirac semimetal. Dynamic Fermi level tuning -- instigated from Pauli-blocking in the linear Dirac cone -- and varying Drude response, generate large variations in the mid and far-infrared optical properties. We demonstrate thermo-optic shifts larger than those of traditional III-V semiconductors, which we attribute to the obtained large thermal expansion coefficient as revealed by first-principles calculations. Electron scattering rate, plasma frequency edge, Fermi level shift, optical conductivity, and electron effective mass analysis of Cd",0.3195854703],["this paper presents a framework for norm-based capacity control for norm-based capacity control","Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units","summarize: This paper presents a general framework for norm-based capacity control for ",0.1379310345],["the DM is connected to the visible sector via a gauged U portal. the","Minimally Extended Left-Right Symmetric Model for Dark Matter with U Portal","summarize: A minimal extension of the left-right symmetric model for neutrino masses that includes a vector-like singlet fermion dark matter is presented with the DM connected to the visible sector via a gauged U portal. We discuss the symmetry breaking in this model and calculate the mass and mixings of the extra heavy neutral gauge boson at the TeV scale. The extra gauge boson can decay to both standard model particles as well to dark matter. We calculate the relic density of the singlet fermion dark matter and its direct detection cross section and use these constraints to obtain the allowed parameter range for the new gauge coupling and the dark matter mass.",0.2],["justification Logics are special kinds of modal logics. they provide a","A note on strong axiomatization of G\\odel-Justification Logic","summarize: Justification Logics are special kinds of modal logics which provide a framework for reasoning about epistemic justification. For this, they extend classical boolean propositional logic by a family of necessity-style modal operators ",0.3333333333],["the results of spectral observations of the NGC 3516 with the 2-m telescope are","Changing looks of the nucleus of Seyfert galaxy NGC 3516 during 2016-2020","summarize: The results of spectral observations of NGC 3516 with the 2-m telescope of the Shamakhy Astrophysical Observatory during 2016-2020 are presented. In the first half of 2016, the intensive broad component Hbeta was found, which indicates a spectral type change compared to 2014, when the broad component was almost invisible. In the second half of 2016, the broad component H",0.4090909091],["a national commission has been established to examine the impact of technology on the economy. in","The Robot Economy: Here It Comes","summarize: Automation is not a new phenomenon, and questions about its effects have long followed its advances. More than a half-century ago, US President Lyndon B. Johnson established a national commission to examine the impact of technology on the economy, declaring that automation can be the ally of our prosperity if we will just look ahead. In this paper, our premise is that we are at a technological inflection point in which robots are developing the capacity to greatly increase their cognitive and physical capabilities, and thus raising questions on labor dynamics. With increasing levels of autonomy and human-robot interaction, intelligent robots could soon accomplish new human-like capabilities such as engaging into social activities. Therefore, an increase in automation and autonomy brings the question of robots directly participating in some economic activities as autonomous agents. In this paper, a technological framework describing a robot economy is outlined and the challenges it might represent in the current socio-economic scenario are pondered.",0.0357142857],["the present paper will focus our attention on the design of the feedback-based feed-forward controller","Feedback stabilization of double pendulum: Application to the crane systems with time-varying rope length","summarize: In the present paper we focus our attention on the design of the feedback-based feed-forward controller asymptotically stabilizing the double-pendulum-type crane system with the time-varying rope length in the desired end position of payload . In principle, we will consider two cases, in the first case, the sway angle of payload is uncontrolled and the second case, when the sway angle of payload is controlled by an external force. Mathematical modelling in the framework of Lagrange formalism and numerical simulation in the Matlab environment indicate the substantial reduction of the transportation time to the desired end position. Another principal novelty of this paper lies in deriving and analysis of a complete mathematical model without approximating the nonlinear terms and without neglecting some structural parameters of systems for the reasons described in the Remark 4.2 and Remark 5.1.",0.2608695652],["the standard retraining technique, fine-tuning, trains the unprune","Comparing Rewinding and Fine-tuning in Neural Network Pruning","summarize: Many neural network pruning algorithms proceed in three steps: train the network to completion, remove unwanted structure to compress the network, and retrain the remaining structure to recover lost accuracy. The standard retraining technique, fine-tuning, trains the unpruned weights from their final trained values using a small fixed learning rate. In this paper, we compare fine-tuning to alternative retraining techniques. Weight rewinding ), rewinds unpruned weights to their values from earlier in training and retrains them from there using the original training schedule. Learning rate rewinding trains the unpruned weights from their final values using the same learning rate schedule as weight rewinding. Both rewinding techniques outperform fine-tuning, forming the basis of a network-agnostic pruning algorithm that matches the accuracy and compression ratios of several more network-specific state-of-the-art techniques.",0.0],["four classical undergraduate physics experiments were done with everyday objects and low-cost sensors. mechanical","Low-Cost Experiments with Everyday Objects for Homework Assignments","summarize: We describe four classical undergraduate physics experiments that were done with everyday objects and low-cost sensors: mechanical oscillations, transmittance of light through a slab of matter, beam deformation under load, and thermal relaxation due to heat loss. We used these experiments to train students for experimental homework projects but they could be used and expanded in a variety of contexts: lecture demonstrations, low cost students' labs, science projects, distance learning courses...",0.0714285714],["new methods have brought a flourish of time series analysis methods. the evaluation requires either collecting","GRATIS: GeneRAting TIme Series with diverse and controllable characteristics","summarize: The explosion of time series data in recent years has brought a flourish of new time series analysis methods, for forecasting, clustering, classification and other tasks. The evaluation of these new methods requires either collecting or simulating a diverse set of time series benchmarking data to enable reliable comparisons against alternative approaches. We propose GeneRAting TIme Series with diverse and controllable characteristics, named GRATIS, with the use of mixture autoregressive models. We simulate sets of time series using MAR models and investigate the diversity and coverage of the generated time series in a time series feature space. By tuning the parameters of the MAR models, GRATIS is also able to efficiently generate new time series with controllable features. In general, as a costless surrogate to the traditional data collection approach, GRATIS can be used as an evaluation tool for tasks such as time series forecasting and classification. We illustrate the usefulness of our time series generation process through a time series forecasting application.",0.1666666667],["commutative algebras in braided tensor categories do not admit faithful","Hopf algebra actions in tensor categories","summarize: We prove that commutative algebras in braided tensor categories do not admit faithful Hopf algebra actions unless they come from group actions. We also show that a group action allows us to see the algebra as the regular algebra in the representation category of the acting group.",0.3],["machine learning and computer vision methods show good performance in medical imagery analysis. but only a","Domain Shift in Computer Vision models for MRI data analysis: An Overview","summarize: Machine learning and computer vision methods are showing good performance in medical imagery analysis. Yetonly a few applications are now in clinical use and one of the reasons for that is poor transferability of themodels to data from different sources or acquisition domains. Development of new methods and algorithms forthe transfer of training and adaptation of the domain in multi-modal medical imaging data is crucial for thedevelopment of accurate models and their use in clinics. In present work, we overview methods used to tackle thedomain shift problem in machine learning and computer vision. The algorithms discussed in this survey includeadvanced data processing, model architecture enhancing and featured training, as well as predicting in domaininvariant latent space. The application of the autoencoding neural networks and their domain-invariant variationsare heavily discussed in a survey. We observe the latest methods applied to the magnetic resonance imaging data analysis and conclude on their performance as well as propose directions for further research.",0.3076923077],["MUltiple SIgnal Classification is a well-known non-it","Appearance of inaccurate results in the MUSIC algorithm with inappropriate wavenumber","summarize: MUltiple SIgnal Classification is a well-known non-iterative location detection algorithm for small, perfectly conducting cracks in inverse scattering problems. However, when the applied wavenumbers are unknown, inaccurate locations of targets are extracted by MUSIC with inappropriate wavenumbers, a fact that has been confirmed by numerical simulations. To date, the reason behind this phenomenon has not been theoretically investigated. Motivated by this fact, we identify the structure of MUSIC-type imaging functionals with inappropriate wavenumbers by establishing a relationship with Bessel functions of order zero of the first kind. This result explains the reasons for inaccurate results. Various results of numerical simulations with noisy data support the identified structure of MUSIC.",0.2258872488],["grid features have recently surpassed vanilla grid-based convolutional features as the de facto","In Defense of Grid Features for Visual Question Answering","summarize: Popularized as 'bottom-up' attention, bounding box based visual features have recently surpassed vanilla grid-based convolutional features as the de facto standard for vision and language tasks like visual question answering . However, it is not clear whether the advantages of regions are the key reasons for the success of bottom-up attention. In this paper, we revisit grid features for VQA, and find they can work surprisingly well - running more than an order of magnitude faster with the same accuracy . Through extensive experiments, we verify that this observation holds true across different VQA models , datasets, and generalizes well to other tasks like image captioning. As grid features make the model design and training process much simpler, this enables us to train them end-to-end and also use a more flexible network design. We learn VQA models end-to-end, from pixels directly to answers, and show that strong performance is achievable without using any region annotations in pre-training. We hope our findings help further improve the scientific understanding and the practical application of VQA. Code and features will be made available.",0.0],["miquel dynamics is a discrete-time dynamical system on the space of square-","A first integrability result for Miquel dynamics","summarize: Miquel dynamics is a discrete-time dynamical system on the space of square-grid circle patterns. For biperiodic circle patterns with both periods equal to two, we show that the dynamics corresponds to translation on an elliptic curve, thus providing the first integrability result for this dynamics. The main tool is a geometric interpretation of the addition law on the normalization of binodal quartic curves.",0.1666666667],["spectroscopy was carried out with the inter University centre for Astronomy and Astrophysics","Spectroscopic Survey of H Emission Line Stars Associated with Bright Rimmed Clouds","summarize: The results of a spectroscopic survey of H alpha emission line stars associated with fourteen bright rimmed clouds are presented. Slit-less optical spectroscopy was carried out with the Inter University Centre for Astronomy and Astrophysics 2m telescope and IUCAA Faint Object Spectrograph and Camera . H alpha emission line was detected from 173 objects. Among them 85 objects have a strong H alpha emission line with its equivalent width larger than 10 A. Those are classical T Tauri stars. 52 objects have a weak H alpha emission line with its equivalent width less than 10 A and do not show intrinsic near-infrared excess. Those are weak-line T Tauri stars. On the other hand, 36 objects have a weak H alpha emission line , although they show intrinsic near-infrared excess. Such objects are not common in low-mass star forming regions. Those are misfits of the general concept on formation process of a low-mass star, in which it evolves from a classical T Tauri star to a weak-line T Tauri star. Those might be weak-line T Tauri stars with a flared disk in which gas is heated by ultraviolet radiation from a nearby early-type star. Alternatively, we propose pre-transitional disk objects as their evolutional stage.",0.0769230769],["the much-used trace distance of coherence was shown to be not a proper measure","The modified trace distance of coherence is constant on most pure states","summarize: Recently, the much-used trace distance of coherence was shown to not be a proper measure of coherence, so a modification of it was proposed. We derive an explicit formula for this modified trace distance of coherence on pure states. Our formula shows that, despite satisfying the axioms of proper coherence measures, it is likely not a good measure to use, since it is maximal on all except for an exponentially-small fraction of pure states.",0.4444444444],["Let us know what you think about it!","Closed range of ","summarize: Let ",0.0],["narrow-sense primitive BCH codes form the most well-studied subclass of B","The Minimum Distance of Some Narrow-Sense Primitive BCH Codes","summarize: Due to wide applications of BCH codes, the determination of their minimum distance is of great interest. However, this is a very challenging problem for which few theoretical results have been reported in the last four decades. Even for the narrow-sense primitive BCH codes, which form the most well-studied subclass of BCH codes, there are very few theoretical results on the minimum distance. In this paper, we present new results on the minimum distance of narrow-sense primitive BCH codes with special Bose distance. We prove that for a prime power ",0.25],["completely random measures represent the key building block of popular stochastic models. the algorithm","A moment-matching Ferguson and Klass algorithm","summarize: Completely random measures represent the key building block of a wide variety of popular stochastic models and play a pivotal role in modern Bayesian Nonparametrics. A popular representation of CRMs as a random series with decreasing jumps is due to Ferguson and Klass . This can immediately be turned into an algorithm for sampling realizations of CRMs or more elaborate models involving transformed CRMs. However, concrete implementation requires to truncate the random series at some threshold resulting in an approximation error. The goal of this paper is to quantify the quality of the approximation by a moment-matching criterion, which consists in evaluating a measure of discrepancy between actual moments and moments based on the simulation output. Seen as a function of the truncation level, the methodology can be used to determine the truncation level needed to reach a certain level of precision. The resulting moment-matching \\FK algorithm is then implemented and illustrated on several popular Bayesian nonparametric models.",0.0625],["the data was combined with information available in DOAJ, CrossRef, OpenDOAR,","Evidence of Open Access of scientific publications in Google Scholar: a large-scale analysis","summarize: This article uses Google Scholar as a source of data to analyse Open Access levels across all countries and fields of research. All articles and reviews with a DOI and published in 2009 or 2014 and covered by the three main citation indexes in the Web of Science were selected for study. The links to freely available versions of these documents displayed in GS were collected. To differentiate between more reliable forms of access and less reliable ones, the data extracted from GS was combined with information available in DOAJ, CrossRef, OpenDOAR, and ROAR. This allowed us to distinguish the percentage of documents in our sample that are made OA by the publisher from those available as Green OA , and those available from other sources . The data shows an overall free availability of 54.6%, with important differences at the country and subject category levels. The data extracted from GS yielded very similar results to those found by other studies that analysed similar samples of documents, but employed different methods to find evidence of OA, thus suggesting a relative consistency among methods.",0.0641348399],["co-Toeplitz quantization scheme is dual to the toeplitz quantization","Co-Toeplitz Operators and their Associated Quantization","summarize: We define co-Toeplitz operators, a new class of Hilbert space operators, in order to define a co-Toeplitz quantization scheme that is dual to the Toeplitz quantization scheme introduced by the author in the setting of symbols that come from a possibly non-commutative algebra with unit. In the present dual setting the symbols come from a possibly non-co-commutative co-algebra with co-unit. However, this co-Toeplitz quantization is a usual quantization scheme in the sense that to each symbol we assign a densely defined linear operator acting in a fixed Hilbert space. Creation and annihilation operators are also introduced as certain types of co-Toeplitz operators, and then their commutation relations provide the way for introducing Planck's constant into this theory. The domain of the co-Toeplitz quantization is then extended as well to a set of co-symbols, which are the linear functionals defined on the co-algebra. A detailed example based on the quantum group ",0.1666666667],["VPA recognize a robust and algorithmically tractable fragment of context-free languages.","Minimization of Visibly Pushdown Automata Using Partial Max-SAT","summarize: We consider the problem of state-space reduction for nondeterministic weakly-hierarchical visibly pushdown automata . VPA recognize a robust and algorithmically tractable fragment of context-free languages that is natural for modeling programs. We define an equivalence relation that is sufficient for language-preserving quotienting of VPA. Our definition allows to merge states that have different behavior, as long as they show the same behavior for reachable equivalent stacks. We encode the existence of such a relation as a Boolean partial maximum satisfiability problem and present an algorithm that quickly finds satisfying assignments. These assignments are sub-optimal solutions to the PMax-SAT problem but can still lead to a significant reduction of states. We integrated our method in the automata-based software verifier Ultimate Automizer and show performance improvements on benchmarks from the software verification competition SV-COMP.",0.3684210526],["a new control method is proprioceptive sonomyographic control. it","Proprioceptive Sonomyographic Control: A novel method of intuitive proportional control of multiple degrees of freedom for upper-extremity amputees","summarize: Technological advances in multi-articulated prosthetic hands have outpaced the methods available to amputees to intuitively control these devices. Amputees often cite difficulty of use as a key contributing factor for abandoning their prosthesis, creating a pressing need for improved control technology. A major challenge of traditional myoelectric control strategies using surface electromyography electrodes has been the difficulty in achieving intuitive and robust proportional control of multiple degrees of freedom. In this paper, we describe a new control method, proprioceptive sonomyographic control that overcomes several limitations of myoelectric control. In sonomyography, muscle mechanical deformation is sensed using ultrasound, as compared to electrical activation, and therefore the resulting control signals can directly control the position of the end effector. Compared to myoelectric control which controls the velocity of the end-effector device, sonomyographic control is more congruent with residual proprioception in the residual limb. We tested our approach with 5 upper-extremity amputees and able-bodied subjects using a virtual target achievement and holding task. Amputees and able-bodied participants demonstrated the ability to achieve positional control for 5 degrees of freedom with an hour of training. Our results demonstrate the potential of proprioceptive sonomyographic control for intuitive dexterous control of multiarticulated prostheses.",0.1839397206],["the first direction focuses on utilizing high-resource languages. the second direction employ","Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation","summarize: Over the last few years two promising research directions in low-resource neural machine translation have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models. Self-supervision improves zero-shot translation quality in multilingual models. Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.",0.1578947368],["the 4 SCAO systems of LBT are being upgraded. the system will push the current","Bringing SOUL on sky","summarize: The SOUL project is upgrading the 4 SCAO systems of LBT, pushing the current guide star limits of about 2 magnitudes fainter thanks to Electron Multiplied CCD detector. This improvement will open the NGS SCAO correction to a wider number of scientific cases from high contrast imaging in the visible to extra-galactic source in the NIR. The SOUL systems are today the unique case where pyramid WFS, adaptive secondary and EMCCD are used together. This makes SOUL a pathfinder for most of the ELT SCAO systems like the one of GMT, MICADO and HARMONI of E-ELT, where the same key technologies will be employed. Today we have 3 SOUL systems installed on the telescope in commissioning phase. The 4th system will be installed in a few months. We will present here the results achieved during daytime testing and commissioning nights up to the present date.",0.0],["a long elastic filament confined to a spherical container is confined","Spontaneous Domain Formation in Spherically-Confined Elastic Filaments","summarize: Although the free energy of a genome packing into a virus is dominated by DNA-DNA interactions, ordering of the DNA inside the capsid is elasticity-driven, suggesting general solutions with DNA organized into spool-like domains. Using analytical calculations and computer simulations of a long elastic filament confined to a spherical container, we show that the ground state is not a single spool as assumed hitherto, but an ordering mosaic of multiple homogeneously-ordered domains. At low densities, we observe concentric spools, while at higher densities, other morphologies emerge, which resemble topological links. We discuss our results in the context of metallic wires, viral DNA, and flexible polymers.",0.5217391304],["we propose a novel way of reducing the number of parameters in the fully connected layers of","Characterizing Sparse Connectivity Patterns in Neural Networks","summarize: We propose a novel way of reducing the number of parameters in the storage-hungry fully connected layers of a neural network by using pre-defined sparsity, where the majority of connections are absent prior to starting training. Our results indicate that convolutional neural networks can operate without any loss of accuracy at less than half percent classification layer connection density, or less than 5 percent overall network connection density. We also investigate the effects of pre-defining the sparsity of networks with only fully connected layers. Based on our sparsifying technique, we introduce the `scatter' metric to characterize the quality of a particular connection pattern. As proof of concept, we show results on CIFAR, MNIST and a new dataset on classifying Morse code symbols, which highlights some interesting trends and limits of sparse connection patterns.",0.2333333333],["Grassmann-Plucker relations show that the various apparent unrelated","Duality, matroids, qubits, twistors and surreal numbers","summarize: We show that via the Grassmann-Pl\\ucker relations, the various apparent unrelated concepts, such as duality, matroids, qubits, twistors and surreal numbers are, in fact, deeply connected. Moreover, we conjecture the possibility that these concepts may be considered as underlying mathematical structures in quantum gravity.",0.0],["the priority of a non-atomic customer is a function of their class and waiting time","Lowest priority waiting time distribution in an accumulating priority L\\'evy queue","summarize: This paper presents a new method for deriving the waiting time distribution of the lowest class in an accumulating priority queue with positive L\\'evy input. The priority of a non-atomic customer is a function of their class and waiting time in the system, and the particles with the highest AP are the next to be processed. The method relies on the construction of a workload overtaking process and solving a first-passage time problem using an appropriate stopping time.",0.4285714286],["a multidisciplinary approach is presented to analyse precipitation process in a model Al-Cu","A multidisciplinary approach to study precipitation kinetics and hardening in an Al-4Cu alloy","summarize: A multidisciplinary approach is presented to analyse the precipitation process in a model Al-Cu alloy. Although this topic has been extensively studied in the past, most of the investigations are focussed either on transmission electron microscopy or on thermal analysis of the processes. The information obtained from these techniques cannot, however, provide a coherent picture of all the complex transformations that take place during decomposition of supersaturated solid solution. Thermal analysis, high resolution dilatometry, transmission electron microscopy and density functional calculations are combined to study precipitation kinetics, interfacial energies, and the effect of second phase precipitates on the mechanical strength of the alloy. Data on both the coherent and semi-coherent orientations of the \/Al interface are reported for the first time. The combination of the different characterization and modelling techniques provides a detailed picture of the precipitation phenomena that take place during aging and of the different contributions to the strength of the alloy. This strategy can be used to analyse and design more complex alloys.",0.7931034483],["facial biometrics has been widely used for identification purposes. but it has recently been researched","Atypical Facial Landmark Localisation with Stacked Hourglass Networks: A Study on 3D Facial Modelling for Medical Diagnosis","summarize: While facial biometrics has been widely used for identification purpose, it has recently been researched as medical biometrics for a range of diseases. In this chapter, we investigate the facial landmark detection for atypical 3D facial modelling in facial palsy cases, while potentially such modelling can assist the medical diagnosis using atypical facial features. In our work, a study of landmarks localisation methods such as stacked hourglass networks is conducted and evaluated to ascertain their accuracy when presented with unseen atypical faces. The evaluation highlights that the state-of-the-art stacked hourglass architecture outperforms other traditional methods.",0.0921235073],["the key technique to solve these problems is pattern recognition. the particle physics community vastly","Quantum Machine Learning and its Supremacy in High Energy Physics","summarize: This article reveals the future prospects of quantum algorithms in high energy physics . Particle identification, knowing their properties and characteristics is a challenging problem in experimental HEP. The key technique to solve these problems is pattern recognition, which is an important application of machine learning and unconditionally used for HEP problems. To execute pattern recognition task for track and vertex reconstruction, the particle physics community vastly use statistical machine learning methods. These methods vary from detector to detector geometry and magnetic field used in the experiment. Here in the present introductory article, we deliver the future possibilities for the lucid application of quantum computation and quantum machine learning in HEP, rather than focusing on deep mathematical structures of techniques arise in this domain.",0.0],["we present new algorithms for computing adjoint ideals of curves. the algorithm yields","Local to global algorithms for the Gorenstein adjoint ideal of a curve","summarize: We present new algorithms for computing adjoint ideals of curves and thus, in the planar case, adjoint curves. With regard to terminology, we follow Gorenstein who states the adjoint condition in terms of conductors. Our main algorithm yields the Gorenstein adjoint ideal G of a given curve as the intersection of what we call local Gorenstein adjoint ideals. Since the respective local computations do not depend on each other, our approach is inherently parallel. Over the rationals, further parallelization is achieved by a modular version of the algorithm which first computes a number of the characteristic p counterparts of G and then lifts these to characteristic zero. As a key ingredient, we establish an efficient criterion to verify the correctness of the lift. Well-known applications are the computation of Riemann-Roch spaces, the construction of points in moduli spaces, and the parametrization of rational curves. We have implemented different variants of our algorithms together with Mnuk's approach in the computer algebra system Singular and give timings to compare the performance of the algorithms.",0.4285714286],["an exoskeleton is a wearable electromechanical structure that can be used by disabled","Voice Controlled Upper Body Exoskeleton: A Development For Industrial Application","summarize: An exoskeleton is a wearable electromechanical structure that is intended to resemble and allow movements in a manner similar to the human skeletal system. They can be used by both disabled and able people alike to increase physical strength in carrying out tasks that would be otherwise difficult, or as a rehabilitation device to aid in physiotherapeutic activities of a weakened body part. This paper intends to introduce a voicecontrolled upper body exoskeleton for industrial applications which can aid workers wearing it by reducing stresses on their arms and shoulders over longer periods and add up to 20kg more strength in lifting applications. The 3D design, calculations and considerations, and load analysis are presented along with brief results of a basic prototype model of the exoskeleton.",0.0833333333],["simple calculations point out that mitochondria are highly proton-deficient microcosms.","Mitochondrial oxidative phosphorylation: Debunking the concepts of electron transport chain, proton pumps, chemiosmosis and rotary ATP synthesis","summarize: Herein , I debunk the long-standing hypotheses that explain mitochondrial oxidative phosphorylation. Simple calculations point out that mitochondria are highly proton-deficient microcosms and therefore, elaborate proton pump machinery are not tenable. Further, other elements like the elaborate electron transport chain, chemiosmosis, rotary ATP synthesis, etc. are also critically evaluated to point out that such complicated systems are non-viable. The communication necessitates a new explanatory paradigm for cellular respiration. In the second part of my work, I have put forward a viable alternative explanatory paradigm for mitochondrial oxidative phosphorylation.",0.0],["in the present contribution, we introduce a wireless optical communication-based system architecture which is shown","Optical Wireless Cochlear Implants","summarize: In the present contribution, we introduce a wireless optical communication-based system architecture which is shown to significantly improve the reliability and the spectral and power efficiency of the transcutaneous link in cochlear implants . We refer to the proposed system as optical wireless cochlear implant .In order to provide a quantified understanding of its design parameters, we establish a theoretical framework that takes into account the channel particularities, the integration area of the internal unit, the transceivers misalignment, and the characteristics of the optical units. To this end, we derive explicit expressions for the corresponding average signal-to-noise-ratio, outage probability, ergodic spectral efficiency and capacity of the transcutaneous optical link . These expressions are subsequently used to assess the dependence of the TOL's communication quality on the transceivers design parameters and the corresponding channels characteristics. The offered analytic results are corroborated with respective results from Monte Carlo simulations. Our findings reveal that OWCI is a particularly promising architecture that drastically increases the reliability and effectiveness of the CI TOL, whilst it requires considerably lower transmit power compared to the corresponding widely-used radio frequency solution.",0.15],["high-level features are often abstract, subjective, and hard to quantify. we present","Music FaderNets: Controllable Music Generation Based On High-Level Features via Low-Level Feature Modelling","summarize: High-level musical qualities are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate sliding faders through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders . Using arousal as an example of a high-level feature, we show that the faders of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes , with only 1% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.",0.0],["in this paper we address the exponential stability of a system of transport equations with intermittent damp","Persistently damped transport on a network of circles","summarize: In this paper we address the exponential stability of a system of transport equations with intermittent damping on a network of ",0.2592592593],["photo response non-uniformity is a camera imaging sensor imperfection.","PRNU-Based Source Device Attribution for YouTube Videos","summarize: Photo Response Non-Uniformity is a camera imaging sensor imperfection which has earned a great interest for source device attribution of digital videos. A majority of recent researches about PRNU-based source device attribution for digital videos do not take into consideration the effects of video compression on the PRNU noise in video frames, but rather consider video frames as isolated images of equal importance. As a result, these methods perform poorly on re-compressed or low bit-rate videos. This paper proposes a novel method for PRNU fingerprint estimation from video frames taking into account the effects of video compression on the PRNU noise in these frames. With this method, we aim to determine whether two videos from unknown sources originate from the same device or not. Experimental results on a large set of videos show that the method we propose is more effective than existing frame-based methods that use either only I frames or all frames, especially on YouTube videos.",0.0833333333],["we explore deviation inequalities for log-semiconvex functions.","Deviation inequalities for convex functions motivated by the Talagrand conjecture","summarize: Motivated by Talagrand's conjecture on regularization properties of the natural semigroup on the Boolean hypercube, and in particular its continuous analogue involving regularization properties of the Ornstein-Uhlenbeck semigroup acting on in-tegrable functions, we explore deviation inequalities for log-semiconvex functions under Gaussian measure.",0.2791881675],["the current rate of the expansion of our Universe, the Hubble constant, can be predicted from","The Expansion of the Universe is Faster than Expected","summarize: The present rate of the expansion of our Universe, the Hubble constant, can be predicted from the cosmological model using measurements of the early Universe, or more directly measured from the late Universe. But as these measurements improved, a surprising disagreement between the two appeared. In 2019, a number of independent measurements of the late Universe using different methods and data provided consistent results making the discrepancy with the early Universe predictions increasingly hard to ignore. We review key advances realized by 2019: -- The local or late Universe measurement of the Hubble constant improved from 10% uncertainty twenty years ago to under 2% by the end of 2019. -- In 2019, multiple independent teams presented measurements with different methods and different calibrations to produce consistent results. -- These late Universe estimations disagree at 4",0.2083333333],["TBIR benefits from query-and-click logs to automatically infer more informative labels","VISIR: Visual and Semantic Image Label Refinement","summarize: The social media explosion has populated the Internet with a wealth of images. There are two existing paradigms for image retrieval: 1) content-based image retrieval , which has traditionally used visual features for similarity search , and 2) tag-based image retrieval , which has relied on user tagging . CBIR now gains semantic expressiveness by advances in deep-learning-based detection of visual labels. TBIR benefits from query-and-click logs to automatically infer more informative labels. However, learning-based tagging still yields noisy labels and is restricted to concrete objects, missing out on generalizations and abstractions. Click-based tagging is limited to terms that appear in the textual context of an image or in queries that lead to a click. This paper addresses the above limitations by semantically refining and expanding the labels suggested by learning-based object detection. We consider the semantic coherence between the labels for different objects, leverage lexical and commonsense knowledge, and cast the label assignment into a constrained optimization problem solved by an integer linear program. Experiments show that our method, called VISIR, improves the quality of the state-of-the-art visual labeling tools like LSDA and YOLO.",0.0],["test a framework that allows for highly automated exploit testing. it supports packing and running applications","TestREx: a Framework for Repeatable Exploits","summarize: Web applications are the target of many well known exploits and also a fertile ground for the discovery of security vulnerabilities. Yet, the success of an exploit depends both on the vulnerability in the application source code and the environment in which the application is deployed and run. As execution environments are complex , we need to have a reliable framework to test whether known exploits can be reproduced in different settings, better understand their effects, and facilitate the discovery of new vulnerabilities. In this paper, we present TestREx - a framework that allows for highly automated, easily repeatable exploit testing in a variety of contexts, so that a security tester may quickly and efficiently perform large-scale experiments with vulnerability exploits. It supports packing and running applications with their environments, injecting exploits, monitoring their success, and generating security reports. We also provide a corpus of example applications, taken from related works or implemented by us.",0.2222222222],["model to reveal mechanism based on interaction between carrier and local potential. potential analogous to","Carrier-potential interaction for high-Tc superconductivity","summarize: The origin of high-temperature superconductivity has been widely debated since its discovery. Here, we propose a model to reveal the mechanism based on the interaction between carrier and local potential. In this model, the potential that is analogous to the lattice point is composed of localized charges and its vibration mediates the coupling of mobile carriers. A Hamiltonian that describes the vibration, coupling, and various interactions among the ordered potentials and carriers is established. By analyzing the Hamiltonian, we find that the vibration of local potential and the interactions, which are determined by the carrier density, control the transition temperature. We show that the transition temperature is high if the local potential is composed of electrons and the mobile carrier is hole because of the strong coupling between them. By replacing the local potential with lattice point, our model is equivalent to the BCS theory. Therefore, our model may provide a general theoretical description on the superconductivity.",0.25],["dragged meniscus occurs for striped prestructures with two orientations.","Dip-coating with prestructured substrates: transfer of simple liquids and Langmuir-Blodgett monolayers","summarize: When a plate is withdrawn from a liquid bath, either a static meniscus forms in the transition region between the bath and the substrate or a liquid film of finite thickness is transferred onto the moving substrate. If the substrate is inhomogeneous, e.g., has a prestructure consisting of stripes of different wettabilities, the meniscus can be deformed or show a complex dynamic behavior. Here we study the free surface shape and dynamics of a dragged meniscus occurring for striped prestructures with two orientations, parallel and perpendicular to the transfer direction. A thin film model is employed that accounts for capillarity through a Laplace pressure and for the spatially varying wettability through a Derjaguin pressure. Numerical continuation is used to obtain steady free surface profiles and corresponding bifurcation diagrams in the case of substrates with different homogeneous wettabilities. Direct numerical simulations are employed in the case of the various striped prestructures. The final part illustrates the importance of our findings for particular applications that involve complex liquids by modeling a Langmuir-Blodgett transfer experiment. There, one transfers a monolayer of an insoluble surfactant that covers the surface of the bath onto the moving substrate. The resulting pattern formation phenomena can be crucially influenced by the hydrodynamics of the liquid meniscus that itself depends on the prestructure on the substrate. In particular, we show how prestructure stripes parallel to the transfer direction lead to the formation of bent stripes in the surfactant coverage after transfer and present similar experimental results.",0.0889708225],["a formula f is proposed to describe the radial properties for all five types of","Radial Distributions of Power and Isotopic Concentrations in Candidate Accident Tolerant Fuel U3Si2 and UO2\/U3Si2 Fuel Pins with FeCrAl Cladding","summarize: Monte Carlo simulations show similarity on radial distributions of power and isotopic concentrations at any effective full power depletion time among five kinds of fuel-cladding combinations with the same cycle length, including the normal UO2-zircaloy combination, the candidate Accident Tolerant Fuel UO2\/U3Si2-FeCrAl combination, and three kinds of candidate ATF U3Si2-FeCrAl combinations. An analytical formula f including the fuel exposure and the relative radial is proposed to describe the radial properties for all five kinds of fuel-cladding combinations. f has the form of the second order polynomial term of s with the exponential type of coefficients depending on x. It is shown that the suggested function f gives a nice description on the simulation data with rather small deviations and can immediately provide radial distribution of power, burnup, and isotopic concentrations of 235U, 238U, 239Pu, and 241Pu at any fuel exposure and relative radius. It is useful to discuss the fuel temperature through the present analytical formula. The realistic radial power distribution gives flatter radial temperature distribution compared with the uniform power distribution. Because of the different thermal conductivities of fuels and claddings and the different thicknesses of claddings, the present discussed five kinds of fuel-cladding combinations have different radial temperature distributions, although their radial power distributions are quite similar. The present work provides an analytical formula to describe the radial properties of the ATF which is expected to be helpful for further neutronic and multi-physics coupling studies.",0.2686992415],["we investigate the structure of sets of LMIs that provide a sufficient condition for stability","A Characterization of Lyapunov Inequalities for Stability of Switched Systems","summarize: We study stability criteria for discrete-time switched systems and provide a meta-theorem that characterizes all Lyapunov theorems of a certain canonical type. For this purpose, we investigate the structure of sets of LMIs that provide a sufficient condition for stability. Various such conditions have been proposed in the literature in the past fifteen years. We prove in this note that a family of languagetheoretic conditions recently provided by the authors encapsulates all the possible LMI conditions, thus putting a conclusion to this research effort. As a corollary, we show that it is PSPACE-complete to recognize whether a particular set of LMIs implies stability of a switched system. Finally, we provide a geometric interpretation of these conditions, in terms of existence of an invariant set.",0.45],["we present an approach to tackle the speaker recognition problem using Triplet Neural Networks.","Latent space representation for multi-target speaker detection and identification with a sparse dataset using Triplet neural networks","summarize: We present an approach to tackle the speaker recognition problem using Triplet Neural Networks. Currently, the ",0.2882563382],["surface-based analyses on brain imaging data adopt some specific brain atlases. the results","Geometric Brain Surface Network For Brain Cortical Parcellation","summarize: A large number of surface-based analyses on brain imaging data adopt some specific brain atlases to better assess structural and functional changes in one or more brain regions. In these analyses, it is necessary to obtain an anatomically correct surface parcellation scheme in an individual brain by referring to the given atlas. Traditional ways to accomplish this goal are through a designed surface-based registration or hand-crafted surface features, although both of them are time-consuming. A recent deep learning approach depends on a regular spherical parameterization of the mesh, which is computationally prohibitive in some cases and may also demand further post-processing to refine the network output. Therefore, an accurate and fully-automatic cortical surface parcellation scheme directly working on the original brain surfaces would be highly advantageous. In this study, we propose an end-to-end deep brain cortical parcellation network, called \\textbf. Through intrinsic and extrinsic graph convolution kernels, DBPN dynamically deciphers neighborhood graph topology around each vertex and encodes the deciphered knowledge into node features. Eventually, a non-linear mapping between the node features and parcellation labels is constructed. Our model is a two-stage deep network which contains a coarse parcellation network with a U-shape structure and a refinement network to fine-tune the coarse results. We evaluate our model in a large public dataset and our work achieves superior performance than state-of-the-art baseline methods in both accuracy and efficiency",0.0666666667],["network data comes with some information about the network edges. network data comes with some information about","OrthoNet: Multilayer Network Data Clustering","summarize: Network data appears in very diverse applications, like biological, social, or sensor networks. Clustering of network nodes into categories or communities has thus become a very common task in machine learning and data mining. Network data comes with some information about the network edges. In some cases, this network information can even be given with multiple views or multiple layers, each one representing a different type of relationship between the network nodes. Increasingly often, network nodes also carry a feature vector. We propose in this paper to extend the node clustering problem, that commonly considers only the network information, to a problem where both the network information and the node features are considered together for learning a clustering-friendly representation of the feature space. Specifically, we design a generic two-step algorithm for multilayer network data clustering. The first step aggregates the different layers of network information into a graph representation given by the geometric mean of the network Laplacian matrices. The second step uses a neural net to learn a feature embedding that is consistent with the structure given by the network layers. We propose a novel algorithm for efficiently training the neural net via stochastic gradient descent, which encourages the neural net outputs to span the leading eigenvectors of the aggregated Laplacian matrix, in order to capture the pairwise interactions on the network, and provide a clustering-friendly representation of the feature space. We demonstrate with an extensive set of experiments on synthetic and real datasets that our method leads to a significant improvement w.r.t. state-of-the-art multilayer graph clustering algorithms, as it judiciously combines nodes features and network information in the node embedding algorithms.",0.0],["VAR-modeling was used for describing signals from connected with working memory brain zones","Application of statistical analysis to working memory problem","summarize: This article is devoted to EEG studying of connectivity cortical areas involved in keeping vision information in working memory. VAR-modeling was used for describing signals got from connected with working memory brain zones. Brain connections were estimated by based in Granger Causality Partial Directed Coherence and then compared by Wilcoxon signed-rank test. In paper connection intensity dependence on executing task was found.",0.1538461538],["note echoes talk given by the second author during the Journ'ee","On the controllability of the Navier-Stokes equation in a rectangle, with a little help of a distributed phantom force","summarize: This note echoes the talk given by the second author during the Journ\\'ees EDP 2018 in Obernai. Its aim is to provide an overview and a sketch of proof of the result obtained by the authors, concerning the controllability of the Navier-Stokes equation. We refer the interested readers to the original paper for the full technical details of the proof, which will be omitted here, to focus on the main underlying ideas.",0.1486846404],["fullerene\/anthracene cluster cations formed in the gas phase","Laboratory formation and photo-chemistry of fullerene\/anthracene cluster cations","summarize: Besides buckminsterfullerene , other fullerenes and their derivatives may also reside in space. In this work, we study the formation and photo-dissociation processes of astronomically relevant fullerene\/anthracene cluster cations in the gas phase. Experiments are carried out using a quadrupole ion trap in combination with time-of-flight mass spectrometry. The results show that fullerene \/anthracene nC60]+ and +), fullerene \/anthracene nC56]+ and +) and fullerene \/anthracene nC66]+ and +) cluster cations, are formed in the gas phase through an ion-molecule reaction pathway. With irradiation, all the fullerene\/anthracene cluster cations dissociate into mono",0.375],["optical networks are a highly relevant subject. a single-ended solution is proposed to","Linearized Bregman Iterations for Automatic Optical Fiber Fault Analysis","summarize: Supervision of the physical layer of optical networks is an extremely relevant subject. To detect fiber faults, single-ended solutions such as the Optical Time Domain Reflectometer allow for precise measurements of fault profiles. Combining the OTDR with a signal processing approach for high-dimensional sparse parameter estimation allows for automated and reliable results in reduced time. In this work, a measurement system composed of a Photon-Counting OTDR data acquisition unit and a processing unit based on a Linearized Bregman Iterations algorithm for automatic fault finding is proposed. An in-depth comparative study of the proposed algorithm's fault-finding prowess in the presence of noise is presented. Characteristics such as sensitivity, specificity, processing time, and complexity, are analysed in simulated environments. Real-life measurements that are conducted using the Photon-Counting OTDR subsystem for data acquisition and the Linearized Bregman-based processing unit for automated data analysis demonstrated accurate results. It is concluded that the proposed measurement system is particularly well suited to the task of fault finding. The natural characteristic of the algorithm fosters embedding the solution in digital hardware, allowing for reduced costs and processing time.",0.5714285714],["proposed scheme combines a typical gradient clustering protocol with evolutionary optimization method. it","Energy Balanced Two-level Clustering for Large-Scale Wireless Sensor Networks based on the Gravitational Search Algorithm","summarize: Organizing sensor nodes in clusters is an effective method for energy preservation in a Wireless Sensor Network . Throughout this research work we present a novel hybrid clustering scheme, that combines a typical gradient clustering protocol with an evolutionary optimization method that is mainly based on the Gravitational Search Algorithm . The proposed scheme aims at improved performance over large in size networks, where classical schemes in most cases lead to non-efficient solutions. It first creates suitably balanced multihop clusters, in which the sensors energy gets larger as coming closer to the cluster head . In the next phase of the proposed scheme a suitable protocol based on the GSA runs to associate sets of cluster heads to specific gateway nodes for the eventual relaying of data to the base station . The fitness function was appropriately chosen considering both the distance from the cluster heads to the gateway nodes and the remaining energy of the gateway nodes, and it was further optimized in order to gain more accurate results for large instances. Extended experimental measurements demonstrate the efficiency and scalability of the presented approach over very large WSNs, as well as its superiority over other known clustering approaches presented in the literature.",0.3334348575],["a new primal-dual mixed finite element method is introduced. the method","A Conforming Primal-Dual Mixed Formulation for the 2D Multiscale Porous Media Flow Problem","summarize: In this paper a new primal-dual mixed finite element method is introduced, aimed to model multiscale problems with several geometric subregions in the domain of interest. In each of these regions porous media fluid flow takes place, but governed by physical parameters at a different scale; additionally, a fluid exchange through contact interfaces occurs between neighboring regions. The well-posedness of the primal-dual mixed finite element formulation on bounded simply connected polygonal domains of the plane is presented. Next, the convergence of the discrete solution to the exact solution of the problem is discussed, together with the convergence rate analysis. Finally, the numerical examples illustrate the method's capabilities to handle multiscale problems and interface discontinuities as well as experimental rates of convergence.",0.2779176394],["beta-Ga2O3 substrates are being developed by epitaxy approaches.","Opportunities and Challenges in MOCVD of \\beta-Ga2O3 for Power Electronic Devices","summarize: Recent breakthroughs in bulk crystal growth of beta-Ga2O3 by the edge-defined film-fed technique has led to the commercialization of large-area beta-Ga2O3 substrates. Standard epitaxy approaches are being utilized to develop various thin-film beta-Ga2O3 based devices including lateral transistors. This article will discuss the challenges for metal organic chemical vapor deposition of beta-Ga2O3 and the design criteria for use of this material system in power electronic device structures.",0.0859111598],["a novel face video based HR monitoring method is proposed. the method is MOMB","MOMBAT: Heart Rate Monitoring from Face Video using Pulse Modeling and Bayesian Tracking","summarize: A non-invasive yet inexpensive method for heart rate monitoring is of great importance in many real-world applications including healthcare, psychology understanding, affective computing and biometrics. Face videos are currently utilized for such HR monitoring, but unfortunately this can lead to errors due to the noise introduced by facial expressions, out-of-plane movements, camera parameters and environmental factors. We alleviate these issues by proposing a novel face video based HR monitoring method MOMBAT, that is, MOnitoring using Modeling and BAyesian Tracking. We utilize out-of-plane face movements to define a novel quality estimation mechanism. Subsequently, we introduce a Fourier basis based modeling to reconstruct the cardiovascular pulse signal at the locations containing the poor quality, that is, the locations affected by out-of-plane face movements. Furthermore, we design a Bayesian decision theory based HR tracking mechanism to rectify the spurious HR estimates. Experimental results reveal that our proposed method, MOMBAT outperforms state-of-the-art HR monitoring methods and performs HR monitoring with an average absolute error of 1.329 beats per minute and the Pearson correlation between estimated and actual heart rate is 0.9746. Moreover, it demonstrates that HR monitoring is significantly",0.2],["Shannon's analysis of the fundamental capacity limits for memoryless communication channels has been refined over time","The Log-Volume of Optimal Codes for Memoryless Channels, Asymptotically Within A Few Nats","summarize: Shannon's analysis of the fundamental capacity limits for memoryless communication channels has been refined over time. In this paper, the maximum volume ",0.125],["we examine the role of dephasing in the transport of an excitation along the","Dephasing assisted transport on a biomimetic ring structure","summarize: We address two-level systems arranged in ring configurations affected by static disorder. In particular we investigate the role of dephasing in the transport of an excitation along the ring. We compare the efficiency of the transfer process on isotropic rings and on biomimetic rings modelled according to the structure of light-harvesting complexes. Our analysis provides a simple but clear and interesting example of how an interplay between the coherent dynamics of the system and the incoherent action of the environment can enhance the transfer capabilities of disordered lattices.",0.16],["blazar GB6 J1040+0617 is the first compelling IceCube","Investigation of two Fermi-LAT gamma-ray blazars coincident with high-energy neutrinos detected by IceCube","summarize: After the identification of the gamma-ray blazar TXS 0506+056 as the first compelling IceCube neutrino source candidate, we perform a systematic analysis of all high-energy neutrino events satisfying the IceCube realtime trigger criteria. We find one additional known gamma-ray source, the blazar GB6 J1040+0617, in spatial coincidence with a neutrino in this sample. The chance probability of this coincidence is 30% after trial correction. For the first time, we present a systematic study of the gamma-ray flux, spectral and optical variability, and multi-wavelength behavior of GB6 J1040+0617 and compare it to TXS 0506+056. We find that TXS 0506+056 shows strong flux variability in the Fermi-LAT gamma-ray band, being in an active state around the arrival of IceCube-170922A, but in a low state during the archival IceCube neutrino flare in 2014\/15. In both cases the spectral shape is statistically compatible with the average spectrum showing no indication of a significant relative increase of a high-energy component. While the association of GB6 J1040+0617 with the neutrino is consistent with background expectations, the source appears to be a plausible neutrino source candidate based on its energetics and multi-wavelength features, namely a bright optical flare and modestly increased gamma-ray activity. Finding one or two neutrinos originating from gamma-ray blazars in the given sample of high-energy neutrinos is consistent with previously derived limits of neutrino emission from gamma-ray blazars, indicating the sources of the majority of cosmic high-energy neutrinos remain unknown.",0.1529318367],["nfold integer programming is a recent variable dimension technique. it is a tool","Scheduling meets n-fold Integer Programming","summarize: Scheduling problems are fundamental in combinatorial optimization. Much work has been done on approximation algorithms for NP-hard cases, but relatively little is known about exact solutions when some part of the input is a fixed parameter. In 2014, Mnich and Wiese initiated a systematic study in this direction. In this paper we continue this study and show that several additional cases of fundamental scheduling problems are fixed parameter tractable for some natural parameters. Our main tool is n-fold integer programming, a recent variable dimension technique which we believe to be highly relevant for the parameterized complexity community. This paper serves to showcase and highlight this technique. Specifically, we show the following four scheduling problems to be fixed-parameter tractable, where p max is the maximum processing time of a job and w max is the maximum weight of a job: - Makespan minimization on uniformly related machines ",0.0869565217],["celluDose is a stochastic simulation-trained feedback control prototype","Dynamic Control of Stochastic Evolution: A Deep Reinforcement Learning Approach to Adaptively Targeting Emergent Drug Resistance","summarize: The challenge in controlling stochastic systems in which low-probability events can set the system on catastrophic trajectories is to develop a robust ability to respond to such events without significantly compromising the optimality of the baseline control policy. This paper presents CelluDose, a stochastic simulation-trained deep reinforcement learning adaptive feedback control prototype for automated precision drug dosing targeting stochastic and heterogeneous cell proliferation. Drug resistance can emerge from random and variable mutations in targeted cell populations; in the absence of an appropriate dosing policy, emergent resistant subpopulations can proliferate and lead to treatment failure. Dynamic feedback dosage control holds promise in combatting this phenomenon, but the application of traditional control approaches to such systems is fraught with challenges due to the complexity of cell dynamics, uncertainty in model parameters, and the need in medical applications for a robust controller that can be trusted to properly handle unexpected outcomes. Here, training on a sample biological scenario identified single-drug and combination therapy policies that exhibit a 100% success rate at suppressing cell proliferation and responding to diverse system perturbations while establishing low-dose no-event baselines. These policies were found to be highly robust to variations in a key model parameter subject to significant uncertainty and unpredictable dynamical changes.",0.1839397206],["triangulations in the u.s. for the u.s. tri","Unified Analytical Volume Distribution of Poisson-Delaunay Simplex and its Application to Coordinated Multi-Point Transmission","summarize: For Poisson-Delaunay triangulations in ",0.0726717773],["gravitational repulsion is an inherent aspect of the Schwarzschild solution. this","Acceleration of particles to high energy via gravitational repulsion in the Schwarzschild field","summarize: Gravitational repulsion is an inherent aspect of the Schwarzschild solution of the Einstein-Hilbert field equations of general relativity. We show that this circumstance means that it is possible to gravitationally accelerate particles to the highest cosmic ray energies.",0.3473970492],["the class we study consists of all atomic monoids of the form. we compare","Factorization invariants of Puiseux monoids generated by geometric sequences","summarize: We study some of the factorization invariants of the class of Puiseux monoids generated by geometric sequences, and we compare and contrast them with the known results for numerical monoids generated by arithmetic sequences. The class we study consists of all atomic monoids of the form ",0.15],["the proportion of false null hypotheses is a very important quantity in statistical modelling and","Uniformly consistently estimating the proportion of false null hypotheses via Lebesgue-Stieltjes integral equations","summarize: The proportion of false null hypotheses is a very important quantity in statistical modelling and inference based on the two-component mixture model and its extensions, and in control and estimation of the false discovery rate and false non-discovery rate. Most existing estimators of this proportion threshold p-values, deconvolve the mixture model under constraints on its components, or depend heavily on the location-shift property of distributions. Hence, they usually are not consistent, applicable to non-location-shift distributions, or applicable to discrete statistics or p-values. To eliminate these shortcomings, we construct uniformly consistent estimators of the proportion as solutions to Lebesgue-Stieltjes integral equations. In particular, we provide such estimators respectively for random variables whose distributions have Riemann-Lebesgue type characteristic functions, form discrete natural exponential families with infinite supports, and form natural exponential families with separable moment sequences. We provide the speed of convergence and uniform consistency class for each such estimator under independence. In addition, we provide example distribution families for which a consistent estimator of the proportion cannot be constructed using our techniques.",0.625],["global design pressures can be difficult to trace in subsystem design. systems physics provides","Statistical Physics of Design","summarize: A key challenge in complex design problems that permeate science and engineering is the need to balance design objectives for specific design elements or subsystems with global system objectives. Global objectives give rise to competing design pressures, whose effects can be difficult to trace in subsystem design. Here, using examples from arrangement problems, we show that the systems-level application of statistical physics principles, which we term systems physics, provides a detailed characterization of subsystem design in terms of the concepts of stress and strain from materials physics. We analyze instances of routing problems in naval architectures, and show that systems physics provides a direct means of classifying architecture types, and quantifying trade-offs between subsystem- and overall performance. Our approach generalizes straightforwardly to design problems in a wide range of other disciplines that require concrete understanding of how the pressure to meet overall design objectives drives the outcomes for component subsystems.",0.0],["the validity of the measurement has been double-checked in the well-mixed","Moran-evolution of cooperation: From well-mixed to heterogeneous complex networks","summarize: Configurational arrangement of network architecture and interaction character of individuals are two most influential factors on the mechanisms underlying the evolutionary outcome of cooperation, which is explained by the well-established framework of evolutionary game theory. In the current study, not only qualitatively but also quantitatively, we measure Moran-evolution of cooperation to support an analytical agreement based on the consequences of the replicator equation in a finite population. The validity of the measurement has been double-checked in the well-mixed network by the Langevin stochastic differential equation and the Gillespie-algorithmic version of Moran-evolution, while in a structured network, the measurement of accuracy is verified by the standard numerical simulation. Considering the Birth-Death and Death-Birth updating rules through diffusion of individuals, the investigation is carried out in the wide range of game environments those relate to the various social dilemmas where we are able to draw a new rigorous mathematical track to tackle the heterogeneity of complex networks. The set of modified criteria reveals the exact fact about the emergence and maintenance of cooperation in the structured population. We find that in general, nature promotes the environment of coexistent traits.",0.1176470588],["perovskites have been praised for their exceptional photovoltaic and optoe","Efficient indoor p-i-n hybrid perovskite solar cells using low temperature solution processed NiO as hole extraction layers","summarize: Hybrid perovskites have received tremendous attention due to their exceptional photovoltaic and optoelectronic properties. Among the two widely used perovskite solar cell device architectures of n-ip and p-i-n, the latter is interesting in terms of its simplicity of fabrication and lower energy input. However this structure mostly uses PEDOT:PSS as a hole transporting layer which can accelerate the perovskite solar cell degradation. Hence the development of stable, inorganic hole extraction layers , without compromising the simplicity of device fabrication is crucial in this fast-growing photovoltaic field. Here we demonstrate a low temperature solution - processed and ultrathin NiO nanoparticle thin films as an efficient HEL for CH3NH3PbI3 based perovskite solar cells. We measure a power conversion efficiency of 13.3 % on rigid glass substrates and 8.5 % on flexible substrates. A comparison with PEDOT:PSS based MAPbI3 solar cells shows that NiO based solar cells have higher short circuit current density and improved open circuit voltage . Apart from the photovoltaic performance under 1 Sun, the efficient hole extraction property of NiO is demonstrated for indoor lighting as well with a PCE of 23.0 % for NiO based CH3NH3PbI2.9Cl0.1 p-i-n solar cells under compact fluorescent lighting. Compared to the perovskite solar cells fabricated on PEDOT:PSS HEL, better shelf-life stability is observed for perovskite solar cells fabricated on NiO HEL. Detailed microstructural and photophysical investigations imply uniform morphology, lower recombination losses, and improved charge transfer properties for CH3NH3PbI3 grown on NiO HEL.",0.0],["stress tensor is adapted to the local flow conditions. it includes extensional","A theoretical framework for steady-state rheometry in generic flow conditions","summarize: We introduce a general decomposition of the stress tensor for incompressible fluids in terms of its components on a tensorial basis adapted to the local flow conditions, which include extensional flows, simple shear flows, and any type of mixed flows. Such a basis is determined solely by the symmetric part of the velocity gradient and allows for a straightforward interpretation of the non-Newtonian response in any local flow conditions. In steady homogeneous flows, the material functions that represent the components of the stress on the adapted basis generalize and complete the classical set of viscometric functions used to characterize the response in simple shear flows. Such a general decomposition of the stress is effective in coherently organizing and interpreting rheological data from laboratory measurements and computational studies in non-viscometric steady flows of great importance for practical applications. The decomposition of the stress in terms with clearly distinct roles is also useful in developing constitutive models.",0.2307692308],["a hybrid analog-digital beamforming scheme is proposed to obtain the PA. the","Energy-Efficient Power Allocation in Uplink mmWave Massive MIMO with NOMA","summarize: In this paper, we study the energy efficiency maximization problem for an uplink millimeter wave massive multiple-input multiple-output system with non-orthogonal multiple access . Multiple two-user clusters are formed according to their channel correlation and gain difference, and NOMA is applied within each cluster. Then, a hybrid analog-digital beamforming scheme is designed to lower the number of radio frequency chains at the base station . On this basis, we formulate a power allocation problem to maximize the EE under users' quality of service requirements. An iterative algorithm is proposed to obtain the PA. Moreover, an enhanced NOMA scheme is also proposed, by exploiting the global information at the BS. Numerical results show that the proposed NOMA schemes achieve superior EE when compared with the conventional orthogonal multiple access scheme.",0.1578947368],["Let us know what you think about it!","Existence and Nonexistence results for anisotropic p-Laplace equation with singular nonlinearities","summarize: Let ",0.0],["quantizers are constrained to use power-of-2 scale-factors and per-","Trained Quantization Thresholds for Accurate and Efficient Fixed-Point Inference of Deep Neural Networks","summarize: We propose a method of training quantization thresholds for uniform symmetric quantizers using standard backpropagation and gradient descent. Contrary to prior work, we show that a careful analysis of the straight-through estimator for threshold gradients allows for a natural range-precision trade-off leading to better optima. Our quantizers are constrained to use power-of-2 scale-factors and per-tensor scaling of weights and activations to make it amenable for hardware implementations. We present analytical support for the general robustness of our methods and empirically validate them on various CNNs for ImageNet classification. We are able to achieve near-floating-point accuracy on traditionally difficult networks such as MobileNets with less than 5 epochs of quantized retraining. Finally, we present Graffitist, a framework that enables automatic quantization of TensorFlow graphs for TQT .",0.0641180388],["an artificial neural network is used to evaluate the effectiveness of six metrics. the algorithm extends","Grain boundary slip transfer classification and metric selection with artificial neural networks","summarize: An artificial neural network is used to evaluate the effectiveness of six metrics and their combinations to assess whether slip transfers across grain boundaries in coarse-grained oligocrystalline Al foils \\citep. This approach extends the one- or two-dimensional projections formerly applied to analyze slip transfer. The accuracy of this binary classification reaches around \\pcnt for the best single metric and around \\pcnt when considering two or more metrics simultaneously. The results suggest slip transfer mostly depends on the geometric relationship between grains. Training a double-layer network having \\num nodes per hidden layer with \\num measurements is sufficient to render the maximum accuracy.",0.2222222222],["the proposed method is referred to as a weighted prediction error method. the","A unified convolutional beamformer for simultaneous denoising and dereverberation","summarize: This paper proposes a method for estimating a convolutional beamformer that can perform denoising and dereverberation simultaneously in an optimal way. The application of dereverberation based on a weighted prediction error method followed by denoising based on a minimum variance distortionless response beamformer has conventionally been considered a promising approach, however, the optimality of this approach cannot be guaranteed. To realize the optimal integration of denoising and dereverberation, we present a method that unifies the WPE dereverberation method and a variant of the MVDR beamformer, namely a minimum power distortionless response beamformer, into a single convolutional beamformer, and we optimize it based on a single unified optimization criterion. The proposed beamformer is referred to as a Weighted Power minimization Distortionless response beamformer. Experiments show that the proposed method substantially improves the speech enhancement performance in terms of both objective speech enhancement measures and automatic speech recognition performance.",0.1666666667],["a polarized nanodiamond absorbs energy form laser beams and causes","Optical Levitation of Nanodiamonds by Doughnut Beams in Vacuum","summarize: Optically levitated nanodiamonds with nitrogen-vacancy centers promise a high-quality hybrid spin-optomechanical system. However, the trapped nanodiamond absorbs energy form laser beams and causes thermal damage in vacuum. We propose to solve the problem by trapping a composite particle at the center of strongly focused doughnut-shaped laser beams. Systematical study on the trapping stability, heat absorption, and oscillation frequency concludes that the azimuthally polarized Gaussian beam and the linearly polarized Laguerre-Gaussian beam ",0.3333333333],["a task reward function can find a variety of policies for a task. the","Learning Novel Policies For Tasks","summarize: In this work, we present a reinforcement learning algorithm that can find a variety of policies for a task that is given by a task reward function. Our method does this by creating a second reward function that recognizes previously seen state sequences and rewards those by novelty, which is measured using autoencoders that have been trained on state sequences from previously discovered policies. We present a two-objective update technique for policy gradient algorithms in which each update of the policy is a compromise between improving the task reward and improving the novelty reward. Using this method, we end up with a collection of policies that solves a given task as well as carrying out action sequences that are distinct from one another. We demonstrate this method on maze navigation tasks, a reaching task for a simulated robot arm, and a locomotion task for a hopper. We also demonstrate the effectiveness of our approach on deceptive tasks in which policy gradient methods often get stuck.",0.1621621622],["the exact lattice results for the partition function of the free neutral scalar","Finite size effects in the thermodynamics of a free neutral scalar field","summarize: The exact analytical lattice results for the partition function of the free neutral scalar field in one spatial dimension in both the configuration and the momentum space were obtained in the framework of the path integral method. The symmetric square matrices of the bilinear forms on the vector space of fields in both configuration space and momentum space were found explicitly. The exact lattice results for the partition function were generalized to the three-dimensional spatial momentum space and the main thermodynamic quantities were derived both on the lattice and in the continuum limit. The thermodynamic properties and the finite volume corrections to the thermodynamic quantities of the free real scalar field were studied. We found that on the finite lattice the exact lattice results for the free massive neutral scalar field agree with the continuum limit only in the region of small values of temperature and volume. However, at these temperatures and volumes the continuum physical quantities for both massive and massless scalar field deviate essentially from their thermodynamic limit values and recover them only at high temperatures or\/and large volumes in the thermodynamic limit.",0.5263157895],["we present results of high-resolution numerical simulations of compressible 2D turbul","Energy Transfer and Spectra in Simulations of Two-dimensional Compressible Turbulence","summarize: We present results of high-resolution numerical simulations of compressible 2D turbulence forced at intermediate spatial scales with a solenoidal white-in-time external acceleration. A case with an isothermal equation of state, low energy injection rate, and turbulent Mach number ",0.1538461538],["we introduce a new concept of canonical convolution operator. we show that it","A new convolution theorem associated with the linear canonical transform","summarize: In this paper, we first introduce a new notion of canonical convolution operator, and show that it satisfies the commutative, associative, and distributive properties, which may be quite useful in signal processing. Moreover, it is proved that the generalized convolution theorem and generalized Young's inequality are also hold for the new canonical convolution operator associated with the LCT. Finally, we investigate the sufficient and necessary conditions for solving a class of convolution equations associated with the LCT.",0.4736842105],["testing samples from unseen classes disjointed with testing samples from unseen classes","Transfer feature generating networks with semantic classes structure for zero-shot learning","summarize: Feature generating networks face to the most important question, which is the fitting difference of the distribution between the generated feature and the real data. This inconsistence further influence the performance of the networks model, because training samples from seen classes is disjointed with testing samples from unseen classes in zero-shot learning . In generalization zero-shot learning , testing samples come from not only seen classes but also unseen classes for closer to the practical situation. Therefore, most of feature generating networks difficultly obtain satisfactory performance for the challenging GZSL by adversarial learning the distribution of semantic classes. To alleviate the negative influence of this inconsistence for ZSL and GZSL, transfer feature generating networks with semantic classes structure is proposed to construct networks model for improving the performance of ZSL and GZSL. TFGNSCS can not only consider the semantic structure relationship between seen and unseen classes, but also learn the difference of generating features by transferring classification model information from seen to unseen classes in networks. The proposed method can integrate the transfer loss, the classification loss and the Wasserstein distance loss to generate enough CNN features, on which softmax classifiers are trained for ZSL and GZSL. Experiments demonstrate that the performance of TFGNSCS outperforms that of the state of the arts on four challenging datasets, which are CUB,FLO,SUN, AWA in GZSL.",0.1363636364],["a sequence tagging framework is applied to word segmentation for a wide range of","Universal Word Segmentation: Implementation and Interpretation","summarize: Word segmentation is a low-level NLP task that is non-trivial for a considerable number of languages. In this paper, we present a sequence tagging framework and apply it to word segmentation for a wide range of languages with different writing systems and typological characteristics. Additionally, we investigate the correlations between various typological factors and word segmentation accuracy. The experimental results indicate that segmentation accuracy is positively related to word boundary markers and negatively to the number of unique non-segmental terms. Based on the analysis, we design a small set of language-specific settings and extensively evaluate the segmentation system on the Universal Dependencies datasets. Our model obtains state-of-the-art accuracies on all the UD languages. It performs substantially better on languages that are non-trivial to segment, such as Chinese, Japanese, Arabic and Hebrew, when compared to previous work.",0.3846153846],["the largest dataset of optimized molecular geometries and electronic properties calculated by the PM","PubChemQC PM6: A dataset of 221 million molecules with optimized molecular geometries and electronic properties","summarize: We report on the largest dataset of optimized molecular geometries and electronic properties calculated by the PM6 method for 92.9% of the 91.2 million molecules cataloged in PubChem Compounds retrieved on Aug. 29, 2016. In addition to neutral states, we also calculated those for cationic, anionic, and spin flipped electronic states of 56.2%, 49.7%, and 41.3% of the molecules, respectively. Thus, the grand total calculated is 221 million molecules. The dataset is available at http:\/\/pubchemqc.riken.jp\/pm6_dataset.html under the Creative Commons Attribution 4.0 International license.",0.5237228136],["the main object of the present paper is to introduce the. class of meromorphic","On a Subclass of Meromorphic Univalent Functions Involving Hypergeometric Function","summarize: The main object of the present paper is to, introduce the. class of meromorphic univalent functions Involving! hypergeomatrc function .We obtain~ some interesting geometric properties according to coefficient inequality , growth and distortion theorems , radii of starlikeness and convexity for the functions in our subclass.",0.1363636364],["we examine the inverse problem of parameter identification in non-coercive variational problems","Contingent derivatives and regularization for noncoercive inverse problems","summarize: We study the inverse problem of parameter identification in non-coercive variational problems that commonly appear in applied models. We examine the differentiability of the set-valued parameter-to-solution map by using the first-order and the second-order contingent derivatives. We explore the inverse problem by using the output least-squares and the modified output least-squares objectives. By regularizing the non-coercive variational problem, we obtain a single-valued regularized parameter-to-solution map and investigate its smoothness and boundedness. We also consider optimization problems using the output least-squares and the modified output least-squares objectives for the regularized variational problem. We give a complete convergence analysis showing that for the output least-squares and the modified output least-squares, the regularized minimization problems approximate the original optimization problems suitably. We also provide the first-order and the second-order adjoint method for the computation of the first-order and the second-order derivatives of the output least-squares objective. We provide discrete formulas for the gradient and the Hessian calculation and present numerical results.",0.3333333333],["this paper introduces an active learning approach to the fitting of interatomic potentials. the","Active learning of linearly parametrized interatomic potentials","summarize: This paper introduces an active learning approach to the fitting of machine learning interatomic potentials. Our approach is based on the D-optimality criterion for selecting atomic configurations on which the potential is fitted. It is shown that the proposed active learning approach is highly efficient in training potentials on the fly, ensuring that no extrapolation is attempted and leading to a completely reliable atomistic simulation without any significant decrease in accuracy. We apply our approach to molecular dynamics and structure relaxation, and we argue that it can be applied, in principle, to any other type of atomistic simulation. The software, test cases, and examples of usage are published at http:\/\/gitlab.skoltech.ru\/shapeev\/mlip\/.",0.2352941176],["ancient history relies on disciplines such as epigraphy, the study of ancient inscribed texts,","Restoring ancient text using deep learning: a case study on Greek epigraphy","summarize: Ancient history relies on disciplines such as epigraphy, the study of ancient inscribed texts, for evidence of the recorded past. However, these texts, inscriptions, are often damaged over the centuries, and illegible parts of the text must be restored by specialists, known as epigraphists. This work presents Pythia, the first ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks. Its architecture is carefully designed to handle long-term context information, and deal efficiently with missing or corrupted character and word representations. To train it, we wrote a non-trivial pipeline to convert PHI, the largest digital corpus of ancient Greek inscriptions, to machine actionable text, which we call PHI-ML. On PHI-ML, Pythia's predictions achieve a 30.1% character error rate, compared to the 57.3% of human epigraphists. Moreover, in 73.5% of cases the ground-truth sequence was among the Top-20 hypotheses of Pythia, which effectively demonstrates the impact of this assistive method on the field of digital epigraphy, and sets the state-of-the-art in ancient text restoration.",0.3125],["the machine learning method is used instead of density function theory method. we then trained an","Accelerating inverse crystal structure prediction by machine learning: a case study of carbon allotropes","summarize: Based on structure prediction method, the machine learning method is used instead of the density function theory method to predict the material properties, thereby accelerating the material search process. In this paper, we established a data set of carbon materials by high-throughput calculation with available carbon structures obtained from the Samara Carbon Allotrope Database. We then trained an ML model that specifically predicts the elastic modulus and confirmed that the accuracy is better than that of AFLOW-ML in predicting the elastic modulus of a carbon allotrope. We further combined our ML model with the CALYPSO code to search for new carbon structures with a high Young's modulus. A new carbon allotrope not included in the Samara Carbon Allotrope Database, named Cmcm-C24, which exhibits a hardness greater than 80 GPa, was firstly revealed. The Cmcm-C24 phase was identified as a semiconductor with a direct bandgap. The structural stability, elastic modulus, and electronic properties of the new carbon allotrope were systematically studied, and the obtained results demonstrate the feasibility of ML methods accelerating the material search process.",0.15],["e-commerce search is not directly applicable to e-commerce search because of differences","Globally Optimized Mutual Influence Aware Ranking in E-Commerce Search","summarize: In web search, mutual influences between documents have been studied from the perspective of search result diversification. But the methods in web search is not directly applicable to e-commerce search because of their differences. And little research has been done on the mutual influences between items in e-commerce search. We propose a global optimization framework for mutual influence aware ranking in e-commerce search. Our framework directly optimizes the Gross Merchandise Volume for ranking, and decomposes ranking into two tasks. The first task is mutual influence aware purchase probability estimation. We propose a global feature extension method to incorporate mutual influences into the features of an item. We also use Recurrent Neural Network to capture influences related to ranking orders in purchase probability estimation. The second task is to find the best ranking order based on the purchase probability estimations. We treat the second task as a sequence generation problem and solved it using the beam search algorithm. We performed online A\/B test on a large e-commerce search engine. The results show that our method brings a 5% increase in GMV for the search engine over a strong baseline.",0.0],["layered semiconductors are a new class of layered semiconductors. they feature","Raman spectroscopy of GaSe and InSe post-transition metal chalcogenides layers","summarize: III-VI post-transition metal chalcogenides are a new class of layered semiconductors, which feature a strong variation of size and type of their band gaps as a function of number of layers . Here, we investigate exfoliated layers of InSe and GaSe ranging from bulk crystals down to monolayer, encapsulated in hexagonal boron nitride, using Raman spectroscopy. We present the N-dependence of both intralayer vibrations within each atomic layer, as well as of the interlayer shear and layer breathing modes. A linear chain model can be used to describe the evolution of the peak positions as a function of N, consistent with first principles calculations.",0.35],["financial intelligence is the core technology of the AI 2.0 era. it has elicit","FinBrain: When Finance Meets AI 2.0","summarize: Artificial intelligence is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a financial brain. In this work, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field.",0.1176470588],["researchers working on mathematical foundations of quantum physics, quantum computing, and related areas.","Proceedings of the 15th International Conference on Quantum Physics and Logic","summarize: Quantum Physics and Logic is an annual conference that brings together researchers working on mathematical foundations of quantum physics, quantum computing, and related areas, with a focus on structural perspectives and the use of logical tools, ordered algebraic and category-theoretic structures, formal languages, semantical methods, and other computer science techniques applied to the study of physical behaviour in general. Work that applies structures and methods inspired by quantum theory to other fields is also welcome.",0.25],["inequalities in a real Hilbert space are governed by a strongly monoton","Outer Approximation Methods for Solving Variational Inequalities in Hilbert Space","summarize: In this paper we study variational inequalities in a real Hilbert space, which are governed by a strongly monotone and Lipschitz continuous operator ",0.652173913],["in this paper, we prove a strong connection to the u.s.","A Kulikov-Type Classification Theorem for a One Parameter Family of K3-Surfaces Over a p-ADIC Field and a Good Reduction Criterion","summarize: In this paper, we prove a ",0.0735388613],["paper provides a simple method to extract the zeros of some weight two Eisenstein series of","A simple method to extract the zeros of some Eisenstein series of level ","summarize: This paper provides a simple method to extract the zeros of some weight two Eisenstein series of level ",0.7],["spatial population dynamics typically focus on interplay between dispersal events and birth\/death","Spatial memory and taxis-driven pattern formation in model ecosystems","summarize: Mathematical models of spatial population dynamics typically focus on the interplay between dispersal events and birth\/death processes. However, for many animal communities, significant arrangement in space can occur on shorter timescales, where births and deaths are negligible. This phenomenon is particularly prevalent in populations of larger, vertebrate animals who often reproduce only once per year or less. To understand spatial arrangements of animal communities on such timescales, we use a class of diffusion-taxis equations for modelling inter-population movement responses between ",0.1538461538],["SRs involve significant effort to reduce manual tedious tasks. a novel method that reduce","Reducing the Effort for Systematic Reviews in Software Engineering","summarize: Context. Systematic Reviews are means for collecting and synthesizing evidence from the identification and analysis of relevant studies from multiple sources. To this aim, they use a well-defined methodology meant to mitigate the risks of biases and ensure repeatability for later updates. SRs, however, involve significant effort. Goal. The goal of this paper is to introduce a novel methodology that reduces the amount of manual tedious tasks involved in SRs while taking advantage of the value provided by human expertise. Method. Starting from current methodologies for SRs, we replaced the steps of keywording and data extraction with an automatic methodology for generating a domain ontology and classifying the primary studies. This methodology has been applied in the Software Engineering sub-area of Software Architecture and evaluated by human annotators. Results. The result is a novel Expert-Driven Automatic Methodology, EDAM, for assisting researchers in performing SRs. EDAM combines ontology-learning techniques and semantic technologies with the human-in-the-loop. The first fosters scalability, objectivity, reproducibility and granularity of the studies; the second allows tailoring to the specific focus of the study at hand and knowledge reuse from domain experts. We evaluated EDAM on the field of Software Architecture against six senior researchers. As a result, we found that the performance of the senior researchers in classifying papers was not statistically significantly different from EDAM. Conclusions. Thanks to automation of the less-creative steps in SRs, our methodology allows researchers to skip the tedious tasks of keywording and manually classifying primary studies, thus freeing effort for the analysis and the discussion.",0.0952380952],["a zero-dimensional ideal I in a polynomial ring is a","Computing and Using Minimal Polynomials","summarize: Given a zero-dimensional ideal I in a polynomial ring, many computations start by finding univariate polynomials in I. Searching for a univariate polynomial in I is a particular case of considering the minimal polynomial of an element in P\/I. It is well known that minimal polynomials may be computed via elimination, therefore this is considered to be a resolved problem. But being the key of so many computations, it is worth investigating its meaning, its optimization, its applications . We present efficient algorithms for computing the minimal polynomial of an element of P\/I. For the specific case where the coefficients are in Q, we show how to use modular methods to obtain a guaranteed result. We also present some applications of minimal polynomials, namely algorithms for computing radicals and primary decompositions of zero-dimensional ideals, and also for testing radicality and maximality.",0.4230769231],["Fe","Molecular beam epitaxy preparation and in situ characterization of FeTe thin films","summarize: We have synthesized Fe",0.0000167017],["a structured approach to the construction of linear BGK-type collision operators ensures that","A Structured Approach to the Construction of Stable Linear Lattice-Boltzmann Collision Operators","summarize: We introduce a structured approach to the construction of linear BGK-type collision operators ensuring that the resulting Lattice-Boltzmann methods are stable with respect to a weighted ",0.5263157895],["Paul ion trap design with integrated optical fibre cavity. the trap is designed to be","Precise positioning of an ion in an integrated Paul trap-cavity system using radiofrequency signals","summarize: We report a novel miniature Paul ion trap design with an integrated optical fibre cavity which can serve as a building block for a fibre-linked quantum network. In such cavity quantum electrodynamic set-ups, the optimal coupling of the ions to the cavity mode is of vital importance and this is achieved by moving the ion relative to the cavity mode. The trap presented herein features an endcap-style design complemented with extra electrodes on which additional radiofrequency voltages are applied to fully control the pseudopotential minimum in three dimensions. This method lifts the need to use three-dimensional translation stages for moving the fibre cavity with respect to the ion and achieves high integrability, mechanical rigidity and scalability. Not based on modifying the capacitive load of the trap, this method leads to precise control of the pseudopotential minimum allowing the ion to be moved with precisions limited only by the ion's position spread. We demonstrate this by coupling the ion to the fibre cavity and probing the cavity mode profile.",0.3888888889],["study reveals a relation between performance and success in soccer competitions. results of","Quantifying the relation between performance and success in soccer","summarize: The availability of massive data about sports activities offers nowadays the opportunity to quantify the relation between performance and success. In this study, we analyze more than 6,000 games and 10 million events in six European leagues and investigate this relation in soccer competitions. We discover that a team's position in a competition's final ranking is significantly related to its typical performance, as described by a set of technical features extracted from the soccer data. Moreover we find that, while victory and defeats can be explained by the team's performance during a game, it is difficult to detect draws by using a machine learning approach. We then simulate the outcomes of an entire season of each league only relying on technical data, i.e. excluding the goals scored, exploiting a machine learning model trained on data from past seasons. The simulation produces a team ranking which is close to the actual ranking, suggesting that a complex systems' view on soccer has the potential of revealing hidden patterns regarding the relation between performance and success.",0.6470588235],["the superMALT survey is observing 76 MALT90 clumps at different","SuperMALT: Physical and Chemical Properties of Massive and Dense Clumps","summarize: The SuperMALT survey is observing 76 MALT90 clumps at different evolutionary stages in high excitation molecular lines and key isotopomers using the Apex 12m telescope with an angular resolution of ",0.0],["pinnerSage is an end-to-end recommender system that represents each user via","PinnerSage: Multi-Modal User Embedding Framework for Recommendations at Pinterest","summarize: Latent user representations are widely adopted in the tech industry for powering personalized recommender systems. Most prior work infers a single high dimensional embedding to represent a user, which is a good starting point but falls short in delivering a full understanding of the user's interests. In this work, we introduce PinnerSage, an end-to-end recommender system that represents each user via multi-modal embeddings and leverages this rich representation of users to provides high quality personalized recommendations. PinnerSage achieves this by clustering users' actions into conceptually coherent clusters with the help of a hierarchical clustering method and summarizes the clusters via representative pins for efficiency and interpretability. PinnerSage is deployed in production at Pinterest and we outline the several design decisions that makes it run seamlessly at a very large scale. We conduct several offline and online A\/B experiments to show that our method significantly outperforms single embedding methods.",0.0],["the IS Success Model includes diversified indicators along with their measures. the constructs are Information","A Conceptual Framework for Successful E-commerce Smartphone Applications: The Context of GCC","summarize: Rapid expansion of online business has engulfed the GCC region. Such expansion causes competition among business entities, causing the need to identify the factors that the customers use to choose a suitable mobile business application. Instead of just focusing on the visitors\/users of the application, a shift in focus towards transforming casual customers to loyal customers is needed. The IS Success Model, whose main constructs are Information Quality, Quality Systems, Service Quality, User Satisfaction, Intention to Use and Net Benefits, includes diversified indicators along with their measures. This research considers User Satisfaction, Intention to Use and Net Benefits constructs as it is, but modified System Quality, Information Quality and Service Quality constructs based on previous state-of-the-art literature. The developed theoretical model was further tested surveying 803 GCC participants. Responses were analyzed using exploratory and confirmatory factor analysis. Study reveals the significance of Service Quality over Information Quality and System Quality, impacting the importance of User Satisfaction over the other constructs.",0.0526315789],["new robust model predictive control and safety guarantees are essential in modern robotics. we propose","Safe and Fast Tracking on a Robot Manipulator: Robust MPC and Neural Network Control","summarize: Fast feedback control and safety guarantees are essential in modern robotics. We present an approach that achieves both by combining novel robust model predictive control with function approximation via neural networks . The result is a new approach for complex tasks with nonlinear, uncertain, and constrained dynamics as are common in robotics. Specifically, we leverage recent results in MPC research to propose a new robust setpoint tracking MPC algorithm, which achieves reliable and safe tracking of a dynamic setpoint while guaranteeing stability and constraint satisfaction. The presented robust MPC scheme constitutes a one-layer approach that unifies the often separated planning and control layers, by directly computing the control command based on a reference and possibly obstacle positions. As a separate contribution, we show how the computation time of the MPC can be drastically reduced by approximating the MPC law with a NN controller. The NN is trained and validated from offline samples of the MPC, yielding statistical guarantees, and used in lieu thereof at run time. Our experiments on a state-of-the-art robot manipulator are the first to show that both the proposed robust and approximate MPC schemes scale to real-world robotic systems.",0.1333333333],["spectro-polarimetric measurements are often employed for remote sensing of aerosol","IPRT polarized radiative transfer model intercomparison project - Three-dimensional test cases ","summarize: Initially unpolarized solar radiation becomes polarized by scattering in the Earth's atmosphere. In particular molecular scattering polarizes electromagnetic radiation, but also scattering of radiation at aerosols, cloud droplets and ice crystals polarizes. Each atmospheric constituent produces a characteristic polarization signal, thus spectro-polarimetric measurements are frequently employed for remote sensing of aerosol and cloud properties. Retrieval algorithms require efficient radiative transfer models. Usually, these apply the plane-parallel approximation, assuming that the atmosphere consists of horizontally homogeneous layers. For remote sensing applications, the radiance is considered constant over the instantaneous field-of-view of the instrument and each sensor element is treated independently in plane-parallel approximation, neglecting horizontal radiation transport between adjacent pixels. In order to estimate the errors due to the IPA approximation, three-dimensional vector radiative transfer models are required. So far, only a few such models exist. Therefore, the International Polarized Radiative Transfer working group of the International Radiation Commission has initiated a model intercomparison project in order to provide benchmark results for polarized radiative transfer. The group has already performed an intercomparison for one-dimensional multi-layer test cases . This paper presents the continuation of the intercomparison project for 2D and 3D test cases: a step cloud, a cubic cloud, and a more realistic scenario including a 3D cloud field generated by a Large Eddy Simulation model and typical background aerosols. The commonly established benchmark results for 3D polarized radiative transfer are available at the IPRT website .",0.0],["Defining the","A geometric second-order-rectifiable stratification for closed subsets of Euclidean space","summarize: Defining the ",0.0],["proposed algorithm is based on a semi-discretization in time based on","Energy-based operator splitting approach for the time discretization of coupled systems of partial and ordinary differential equations for fluid flows: The Stokes case","summarize: The goal of this work is to develop a novel splitting approach for the numerical solution of multiscale problems involving the coupling between Stokes equations and ODE systems, as often encountered in blood flow modeling applications. The proposed algorithm is based on a semi-discretization in time based on operator splitting, whose design is guided by the rationale of ensuring that the physical energy balance is maintained at the discrete level. As a result, unconditional stability with respect to the time step choice is ensured by the implicit treatment of interface conditions within the Stokes substeps, whereas the coupling between Stokes and ODE substeps is enforced via appropriate initial conditions for each substep. Notably, unconditional stability is attained without the need of subiterating between Stokes and ODE substeps. Stability and convergence properties of the proposed algorithm are tested on three specific examples for which analytical solutions are derived.",0.2137615335],["magnetic van der Waals heterostructures of mn exhibit controllable magnetic properties while","Natural van der Waals heterostructural single crystals with both magnetic and topological properties","summarize: Heterostructures having both magnetism and topology are promising materials for the realization of exotic topological quantum states while challenging in synthesis and engineering. Here, we report natural magnetic van der Waals heterostructures of mn that exhibit controllable magnetic properties while maintaining their topological surface states. The interlayer antiferromagnetic exchange coupling is gradually weakened as the separation of magnetic layers increases, and an anomalous Hall effect that is well coupled with magnetization and shows ferromagnetic hysteresis was observed below 5 K. The obtained homogeneous heterostructure with atomically sharp interface and intrinsic magnetic properties will be an ideal platform for studying the quantum anomalous Hall effect, axion insulator states, and the topological magnetoelectric effect.",0.3943047491],["this paper introduces an expressive class of quotient-inductive types. the","Constructing Infinitary Quotient-Inductive Types","summarize: This paper introduces an expressive class of quotient-inductive types, called QW-types. We show that in dependent type theory with uniqueness of identity proofs, even the infinitary case of QW-types can be encoded using the combination of inductive-inductive definitions involving strictly positive occurrences of Hofmann-style quotient types, and Abel's size types. The latter, which provide a convenient constructive abstraction of what classically would be accomplished with transfinite ordinals, are used to prove termination of the recursive definitions of the elimination and computation properties of our encoding of QW-types. The development is formalized using the Agda theorem prover.",0.0],["we focus on three different cases: an infinite stack where each layer is rotated with respect to","Twists and The Electronic Structure of Graphitic Materials","summarize: We analyze the effect of twists on the electronic structure of configurations of infinite stacks of graphene layers. We focus on three different cases: an infinite stack where each layer is rotated with respect to the previous one by a fixed angle, two pieces of semi-infinite graphite rotated with respect to each other, and finally a single layer of graphene rotated with respect to a graphite surface. In all three cases we find a rich structure, with sharp resonances and flat bands for small twist angles. The method used can be easily generalized to more complex arrangements and stacking sequences.",0.1764705882],["we prove a PBW type result for universal enveloping dendri","Endofunctors and Poincar\\'e-Birkhoff-Witt theorems","summarize: We determine what appears to be the bare-bones categorical framework for Poincar\\'e-Birkhoff-Witt type theorems about universal enveloping algebras of various algebraic structures. Our language is that of endofunctors; we establish that a natural transformation of monads enjoys a Poincar\\'e-Birkhoff-Witt property only if that transformation makes its codomain a free right module over its domain. We conclude with a number of applications to show how this unified approach proves various old and new Poincar\\'e-Birkhoff-Witt type theorems. In particular, we prove a PBW type result for universal enveloping dendriform algebras of pre-Lie algebras, answering a question of Loday.",0.1818181818],["memristor can be used as non volatile memory and for emulating neuron","Impact of Integrated Circuit Packaging on Synaptic Dynamics of Memristive Devices","summarize: The memristor can be used as non volatile memory and for emulating neuron behavior. It has the ability to switch between low resistance ",0.0],["remote sensing image scene classification has been receiving remarkable attention. a systematic review of literature","Remote Sensing Image Scene Classification: Benchmark and State of the Art","summarize: Remote sensing image scene classification plays an important role in a wide range of applications and hence has been receiving remarkable attention. During the past years, significant efforts have been made to develop various datasets or present a variety of approaches for scene classification from remote sensing images. However, a systematic review of the literature concerning datasets and methods for scene classification is still lacking. In addition, almost all existing datasets have a number of limitations, including the small scale of scene classes and the image numbers, the lack of image variations and diversity, and the saturation of accuracy. These limitations severely limit the development of new approaches especially deep learning-based methods. This paper first provides a comprehensive review of the recent progress. Then, we propose a large-scale dataset, termed NWPU-RESISC45, which is a publicly available benchmark for REmote Sensing Image Scene Classification , created by Northwestern Polytechnical University . This dataset contains 31,500 images, covering 45 scene classes with 700 images in each class. The proposed NWPU-RESISC45 is large-scale on the scene classes and the total image number, holds big variations in translation, spatial resolution, viewpoint, object pose, illumination, background, and occlusion, and has high within-class diversity and between-class similarity. The creation of this dataset will enable the community to develop and evaluate various data-driven algorithms. Finally, several representative methods are evaluated using the proposed dataset and the results are reported as a useful baseline for future research.",0.2916666667],["a novel method for extracting highly discriminative characteristic sets is applied. the FN","Improving in-home appliance identification using fuzzy-neighbors-preserving analysis based QR-decomposition","summarize: This paper proposes a new appliance identification scheme by introducing a novel approach for extracting highly discriminative characteristic sets that can considerably distinguish between various appliance footprints. In this context, a precise and powerful characteristic projection technique depending on fuzzy-neighbors-preserving analysis based QR-decomposition is applied on the extracted energy consumption time-domain features. The FNPA-QR aims to diminish the distance among the between class features and increase the gap among features of dissimilar categories. Following, a novel bagging decision tree classifier is also designed to further improve the classification accuracy. The proposed technique is then validated on three appliance energy consumption datasets, which are collected at both low and high frequency. The practical results obtained point out the outstanding classification rate of the time-domain based FNPA-QR and BDT.",0.35],["this paper generalizes the definition of a multilinear map to arbitrary groups","Multilinear Cryptography using Nilpotent Groups","summarize: In this paper we generalize the definition of a multilinear map to arbitrary groups and develop a novel idea of multilinear cryptosystem using nilpotent group identities.",0.1666666667],["this paper addresses the problem of voltage regulation in a power distribution grid. we discuss how","Fully Distributed Peer-to-Peer Optimal Voltage Control with Minimal Model Requirements","summarize: This paper addresses the problem of voltage regulation in a power distribution grid using the reactive power injections of grid-connected power inverters. We first discuss how purely local voltage control schemes cannot regulate the voltages within a desired range under all circumstances and may even yield detrimental control decisions. Communication and, through that, coordination are therefore needed. On the other hand, short-range peer-to-peer communication and knowledge of electric distances between neighbouring controllers are sufficient for this task. We implement such a peer-to-peer controller and test it on a 400~V distribution feeder with asynchronous communication channels, confirming its viability on real-life systems. Finally, we analyze the scalability of this approach with respect to the number of agents on the feeder that participate in the voltage regulation task.",0.1904761905],["the whitehead group of Thompson's group T is infinitely generated. this leads to","On Thompson's group T and algebraic K-theory","summarize: Using a theorem of L\\uck-Reich-Rognes-Varisco, we show that the Whitehead group of Thompson's group T is infinitely generated, even when tensored with the rationals. To this end we describe the structure of the centralizers and normalizers of the finite cyclic subgroups of T, via a direct geometric approach based on rotation numbers. This also leads to an explicit computation of the source of the Farrell-Jones assembly map for the rationalized higher algebraic K-theory of the integral group ring of T.",0.3529411765],["the linear inverse problem has various well-designed network architectures. deep learning provides","Deep Learning Methods for Solving Linear Inverse Problems: Research Directions and Paradigms","summarize: The linear inverse problem is fundamental to the development of various scientific areas. Innumerable attempts have been carried out to solve different variants of the linear inverse problem in different applications. Nowadays, the rapid development of deep learning provides a fresh perspective for solving the linear inverse problem, which has various well-designed network architectures results in state-of-the-art performance in many applications. In this paper, we present a comprehensive survey of the recent progress in the development of deep learning for solving various linear inverse problems. We review how deep learning methods are used in solving different linear inverse problems, and explore the structured neural network architectures that incorporate knowledge used in traditional methods. Furthermore, we identify open challenges and potential future directions along this research line.",0.0],["we describe a novel type of weak cryptographic private key that can exist in any discrete","Removable Weak Keys for Discrete Logarithm Based Cryptography","summarize: We describe a novel type of weak cryptographic private key that can exist in any discrete logarithm based public-key cryptosystem set in a group of prime order ",0.2173913043],["electrochemical properties of vapor-phase grown MoS_ have been investigated. electrochemical","Dielectric Properties and Ion Transport in Layered MoS_ Grown by Vapor-Phase Sulfurization","summarize: Electronic and dielectric properties of vapor-phase grown MoS_ have been investigated in metal\/MoS_\/silicon capacitor structures by capacitance-voltage and conductancevoltage techniques. Analytical methods confirm the MoS_ layered structure, the presence of interfacial silicon oxide and the composition of the films. Electrical characteristics in combination with theoretical considerations quantify the concentration of electron states at the interface between Si and a 2.5 - 3 nm thick silicon oxide interlayer between Si and MoS_. Measurements under electric field stress indicate the existence of mobile ions in MoS_ that interact with interface states. Based on time-offlight secondary ion mass spectrometry, we propose OH^ ions as probable candidates responsible for the observations. The dielectric constant of the vapor-phase grown MoS_ extracted from CV measurements at 100 KHz is in the range of 2.6 to 2.9. The present study advances the understanding of defects and interface states in MoS_. It also indicates opportunities for ion-based plasticity in 2D material devices for neuromorphic computing applications.",0.0682275628],["conformal immersions are a must-see for conformal immersions.","Dirac Tori","summarize: We consider conformal immersions ",0.0588235294],["generalized model describes spherocylindrical micelles, which are simultaneously","Analytical modeling of micelle growth. 2. Molecular thermodynamics of mixed aggregates and scission energy in wormlike micelles","summarize: Hypotheses: Quantitative molecular-thermodynamic theory of the growth of giant wormlike micelles in mixed nonionic surfactant solutions can be developed on the basis of a generalized model, which includes the classical phase separation and mass action models as special cases. The generalized model describes spherocylindrical micelles, which are simultaneously multicomponent and polydisperse in size. Theory: The model is based on explicit analytical expressions for the four components of the free energy of mixed nonionic micelles: interfacial-tension, headgroup-steric, chain-conformation components and free energy of mixing. The radii of the cylindrical part and the spherical endcaps, as well as the chemical composition of the endcaps, are determined by minimization of the free energy. Findings: In the case of multicomponent micelles, an additional term appears in the expression for the micelle growth parameter , which takes into account the fact that the micelle endcaps and cylindrical part have different compositions. The model accurately predicts the mean mass aggregation number of wormlike micelles in mixed nonionic surfactant solutions without using any adjustable parameters. The endcaps are enriched in the surfactant with smaller packing parameter that is better accommodated in regions of higher mean surface curvature. The model can be further extended to mixed solutions of nonionic, ionic and zwitterionic surfactants used in personal-care and house-hold detergency.",0.0405815584],["BGP is the de-facto Internet routing protocol for exchanging prefix reachability information","Inferring Catchment in Internet Routing","summarize: BGP is the de-facto Internet routing protocol for exchanging prefix reachability information between Autonomous Systems . It is a dynamic, distributed, path-vector protocol that enables rich expressions of network policies . In this regime, where complexity is interwoven with information hiding, answering questions such as what is the expected catchment of the anycast sites of a content provider on the AS-level, if new sites are deployed?, or how will load-balancing behave if an ISP changes its routing policy for a prefix?, is a hard challenge. In this work, we present a formal model and methodology that takes into account policy-based routing and topological properties of the Internet graph, to predict the routing behavior of networks. We design algorithms that provide new capabilities for informative route inference . We analyze the properties of these inference algorithms, and evaluate them using publicly available routing datasets and real-world experiments. The proposed framework can be useful in a number of applications: measurements, traffic engineering, network planning, Internet routing models, etc. As a use case, we study the problem of selecting a set of measurement vantage points to maximize route inference. Our methodology is general and can capture standard valley-free routing, as well as more complex topological and routing setups appearing in practice.",0.0769230769],["we propose four indicators of answer quality: answer length, number of code lines and hyperlinks to","Dynamics of Content Quality in Collaborative Knowledge Production","summarize: We explore the dynamics of user performance in collaborative knowledge production by studying the quality of answers to questions posted on Stack Exchange. We propose four indicators of answer quality: answer length, the number of code lines and hyperlinks to external web content it contains, and whether it is accepted by the asker as the most helpful answer to the question. Analyzing millions of answers posted over the period from 2008 to 2014, we uncover regular short-term and long-term changes in quality. In the short-term, quality deteriorates over the course of a single session, with each successive answer becoming shorter, with fewer code lines and links, and less likely to be accepted. In contrast, performance improves over the long-term, with more experienced users producing higher quality answers. These trends are not a consequence of data heterogeneity, but rather have a behavioral origin. Our findings highlight the complex interplay between short-term deterioration in performance, potentially due to mental fatigue or attention depletion, and long-term performance improvement due to learning and skill acquisition, and its impact on the quality of user-generated content.",0.0869565217],["the TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of","TADPOLE Challenge: Accurate Alzheimer's disease prediction through crowdsourced forecasting of future data","summarize: The TADPOLE Challenge compares the performance of algorithms at predicting the future evolution of individuals at risk of Alzheimer's disease. TADPOLE Challenge participants train their models and algorithms on historical data from the Alzheimer's Disease Neuroimaging Initiative study. Participants are then required to make forecasts of three key outcomes for ADNI-3 rollover participants: clinical diagnosis, ADAS-Cog 13, and total volume of the ventricles -- which are then compared with future measurements. Strong points of the challenge are that the test data did not exist at the time of forecasting , and that it focuses on the challenging problem of cohort selection for clinical trials by identifying fast progressors. The submission phase of TADPOLE was open until 15 November 2017; since then data has been acquired until April 2019 from 219 subjects with 223 clinical visits and 150 Magnetic Resonance Imaging scans, which was used for the evaluation of the participants' predictions. Thirty-three teams participated with a total of 92 submissions. No single submission was best at predicting all three outcomes. For diagnosis prediction, the best forecast , which was based on gradient boosting, obtained a multiclass area under the receiver-operating curve of 0.931, while for ventricle prediction the best forecast , which was based on disease progression modelling and spline regression, obtained mean absolute error of 0.41% of total intracranial volume . For ADAS-Cog 13, no forecast was considerably better than the benchmark mixed effects model , provided to participants before the submission deadline. Further analysis can help understand which input features and algorithms are most suitable for Alzheimer's disease prediction and for aiding patient stratification in clinical trials.",0.2727272727],["a varactor-based tunable bandstop filter has been proposed. the filter","Continuously Tunable Dual-mode Bandstop Filter","summarize: A varactor-based tunable bandstop filter has been proposed in this article. The proposed filter is based on a dualmode circuit developed by introducing inductive and capacitive couplings into a notch filter. The frequency tunability is achieved by using varactor diodes instead of the lumped capacitors in the circuit. Next, the equivalent circuit model has been implemented in planar microstrip technology using thin inductive traces and varactor diodes. The fabricated filter prototype shows a continuous center frequency tuning range of 0.66 - 0.99 GHz with a compact size of 0.12lg*0:16lg, where lg is the guided wavelength at the middle frequency of the tuning range.",0.1666666667],["parallel to operated algebras built on top of planar rooted trees via the grafting","Hopf algebras of planar binary trees: an operated algebra approach","summarize: Parallel to operated algebras built on top of planar rooted trees via the grafting operator ",0.3333333333],["expository article revolves around the question to find short presentations of finite simple groups.","Two Generation of Finite Simple Groups","summarize: This expository article revolves around the question to find short presentations of finite simple groups. This subject is one of the most active research areas of group theory in recent times. We bring together several known results on two-generation and ",0.0666666667],["a new population of close dual active galactic nuclei is being investigated. the","Hubble Space Telescope Wide Field Camera 3 Identifies an ","summarize: Kiloparsec-scale dual active galactic nuclei are active supermassive black hole pairs co-rotating in galaxies with separations of less than a few kpc. Expected to be a generic outcome of hierarchical galaxy formation, their frequency and demographics remain uncertain. We have carried out an imaging survey with the Hubble Space Telescope Wide Field Camera 3 of AGNs with double-peaked narrow emission lines. HST\/WFC3 offers high image quality in the near-infrared to resolve the two stellar nuclei, and in the optical to resolve from ionized gas in the narrow-line regions. This combination has proven to be key in sorting out alternative scenarios. With HST\/WFC3 we are able to explore a new population of close dual AGNs at more advanced merger stages than can be probed from the ground. Here we show that the AGN SDSS J0924+0510, which had previously shown two stellar bulges, contains two spatially distinct regions consistent with a dual AGN. While we cannot completely exclude cross-ionization from a single central engine, the nearly equal ratios of strongly suggest a dual AGN with a projected angular separation of 0.4, corresponding to a projected physical separation of ",0.2105263158],["graphs embeddable on fixed surface can be embedded on a fixed surface.","Defective colouring of graphs excluding a subgraph or minor","summarize: Archdeacon proved that graphs embeddable on a fixed surface can be ",0.16],["inference algorithm used in most popular first-order differentiable Probabilistic Programming Languages","Hamiltonian Monte Carlo for Probabilistic Programs with Discontinuities","summarize: Hamiltonian Monte Carlo is arguably the dominant statistical inference algorithm used in most popular first-order differentiable Probabilistic Programming Languages . However, the fact that HMC uses derivative information causes complications when the target distribution is non-differentiable with respect to one or more of the latent variables. In this paper, we show how to use extensions to HMC to perform inference in probabilistic programs that contain discontinuities. To do this, we design a Simple first-order Probabilistic Programming Language that contains a sufficient set of language restrictions together with a compilation scheme. This enables us to preserve both the statistical and syntactic interpretation of if-else statements in the probabilistic program, within the scope of first-order PPLs. We also provide a corresponding mathematical formalism that ensures any joint density denoted in such a language has a suitably low measure of discontinuities.",0.1538461538],["justification Logics are special kinds of modal logics. they provide a","A note on strong axiomatization of G\\odel-Justification Logic","summarize: Justification Logics are special kinds of modal logics which provide a framework for reasoning about epistemic justification. For this, they extend classical boolean propositional logic by a family of necessity-style modal operators ",0.3333333333],["a paper aims to extend their approach to non-projectivity by providing the first practical","Global Transition-based Non-projective Dependency Parsing","summarize: Shi, Huang, and Lee obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH_4 algorithm, an ",0.1739130435],["miquel dynamics relies on the six circles theorem of the square grid.","Miquel dynamics for circle patterns","summarize: We study a new discrete-time dynamical system on circle patterns with the combinatorics of the square grid. This dynamics, called Miquel dynamics, relies on Miquel's six circles theorem. We provide a coordinatization of the appropriate space of circle patterns on which the dynamics acts and use it to derive local recurrence formulas. Isoradial circle patterns arise as periodic points of Miquel dynamics. Furthermore, we prove that certain signed sums of intersection angles are preserved by the dynamics. Finally, when the initial circle pattern is spatially biperiodic with a fundamental domain of size two by two, we show that the appropriately normalized motion of intersection points of circles takes place along an explicit quartic curve.",0.0625],["pilot allocation in massive MIMO is a hard combinatorial problem. it depends on","Location-Aware Pilot Allocation in Multi-Cell Multi-User Massive MIMO Networks","summarize: We propose a location-aware pilot allocation algorithm for a massive multiple-input multiple-output network with high-mobility users, where the wireless channels are subject to Rician fading. Pilot allocation in massive MIMO is a hard combinatorial problem and depends on the locations of users. As such, it is highly complex to achieve the optimal pilot allocation in real-time for a network with high-mobility users. Against this background, we propose a low-complexity pilot allocation algorithm, which exploits the behavior of line-of-sight interference among the users and allocate the same pilot sequence to the users with small LOS interference. Our examination demonstrates that our proposed algorithm significantly outperforms the existing algorithms, even with localization errors. Specifically, for the system considered in this work, our proposed algorithm provides up to 37.26% improvement in sum spectral efficiency and improves the sum SE of the worst interference-affected users by up to 2.57 bits\/sec\/Hz, as compared to the existing algorithms.",0.380952381],["a system that uses the fundamental features of topological photonics and synthetic dimensions to force","Mode-Locked Topological Insulator Laser Utilizing Synthetic Dimensions","summarize: We propose a system that exploits the fundamental features of topological photonics and synthetic dimensions to force many semiconductor laser resonators to synchronize, mutually lock, and under suitable modulation emit a train of transform-limited mode-locked pulses. These lasers exploit the Floquet topological edge states in a 1D array of ring resonators, which corresponds to a 2D topological system with one spatial dimension and one synthetic frequency dimension. We show that the lasing state of the multi-element laser system possesses the distinct characteristics of spatial topological edge states while exhibiting topologically protected transport. The topological synthetic-space edge mode imposes a constant-phase difference between the multi-frequency modes on the edges, and together with modulation of the individual elements forces the ensemble of resonators to mode-lock and emit short pulses, robust to disorder in the multi-resonator system. Our results offer a proof-of-concept mechanism to actively mode-lock a laser diode array of many lasing elements, which is otherwise extremely difficult due to the presence of many spatial modes of the array. The topological synthetic-space concepts proposed here offer an avenue to overcome this major technological challenge, and open new opportunities in laser physics.",0.2083333333],["domain adaptation investigates cross-domain knowledge transfer. source domain and target domain have distinctive data","Cycle-consistent Conditional Adversarial Transfer Networks","summarize: Domain adaptation investigates the problem of cross-domain knowledge transfer where the labeled source domain and unlabeled target domain have distinctive data distributions. Recently, adversarial training have been successfully applied to domain adaptation and achieved state-of-the-art performance. However, there is still a fatal weakness existing in current adversarial models which is raised from the equilibrium challenge of adversarial training. Specifically, although most of existing methods are able to confuse the domain discriminator, they cannot guarantee that the source domain and target domain are sufficiently similar. In this paper, we propose a novel approach named to handle this issue. Our approach takes care of the domain alignment by leveraging adversarial training. Specifically, we condition the adversarial networks with the cross-covariance of learned features and classifier predictions to capture the multimodal structures of data distributions. However, since the classifier predictions are not certainty information, a strong condition with the predictions is risky when the predictions are not accurate. We, therefore, further propose that the truly domain-invariant features should be able to be translated from one domain to the other. To this end, we introduce two feature translation losses and one cycle-consistent loss into the conditional adversarial domain adaptation networks. Extensive experiments on both classical and large-scale datasets verify that our model is able to outperform previous state-of-the-arts with significant improvements.",0.0],["we propose a pool-based non-parametric active learning algorithm. we give prediction","Active Nearest-Neighbor Learning in Metric Spaces","summarize: We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor , which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. MARMANN is based on a generalized sample compression scheme, and a new label-efficient active model-selection procedure.",0.1578947368],["few-shot Learning aims to learn from few labeled training data. the cost","Augmented Bi-path Network for Few-shot Learning","summarize: Few-shot Learning which aims to learn from few labeled training data is becoming a popular research topic, due to the expensive labeling cost in many real-world applications. One kind of successful FSL method learns to compare the testing image and training image by simply concatenating the features of two images and feeding it into the neural network. However, with few labeled data in each class, the neural network has difficulty in learning or comparing the local features of two images. Such simple image-level comparison may cause serious mis-classification. To solve this problem, we propose Augmented Bi-path Network for learning to compare both global and local features on multi-scales. Specifically, the salient patches are extracted and embedded as the local features for every image. Then, the model learns to augment the features for better robustness. Finally, the model learns to compare global and local features separately, i.e., in two paths, before merging the similarities. Extensive experiments show that the proposed ABNet outperforms the state-of-the-art methods. Both quantitative and visual ablation studies are provided to verify that the proposed modules lead to more precise comparison results.",0.0769230769],["we define an infinite graded graph of ordered pairs and acanonical action of","Universal adic approximation, invariant measures and scaled entropy","summarize: We define an infinite graded graph of ordered pairs and a~canonical action of the group ",0.1764705882],["we will demonstrate this using the canonical quantization procedure. this is valid irrespective","Affine Connections in Quantum Gravity and New Scalar Fields","summarize: In this manuscript, we will discuss the construction of covariant derivative operator in quantum gravity. We will find it is more perceptive alternative to use affine connections more general than metric compatible connections in quantum gravity. We will demonstrate this using the canonical quantization procedure. This is valid irrespective of the presence and nature of sources. The conventional Palatini and metric-affine formalisms, where the actions are linear in the scalar curvature with metric and affine connections being the the independent variables, are not much suitable to construct a source-free theory of gravity with general affine connections. This is also valid for many minimally coupled interacting theories where sources only couple with metric by using the Levi-Civita connections exclusively. We will discuss potential formalism of affine connections to introduce affine connections more general than metric compatible connections in gravity. We will also discuss possible extensions of the actions for this purpose. General affine connections introduce new fields in gravity besides metric. In this article, we will consider a simple potential formalism with symmetric affine connections and symmetric Ricci tensor. Corresponding affine connections introduce only two massless scalar fields. One of these fields contributes a stress-tensor with opposite sign to the sources of Einstein's equation when we state the equation using the Levi-Civita connections. This means we have a massless scalar field with negative stress-tensor in Einstein's equation. These scalar fields can be useful to explain dark energy and inflation. These fields bring us beyond strict local Minkowski geometries.",0.0],["an IRS is deployed to adjust its surface reflecting elements to ensure secure communication of multiple legitimate users in","Deep Reinforcement Learning Based Intelligent Reflecting Surface for Secure Wireless Communications","summarize: In this paper, we study an intelligent reflecting surface -aided wireless secure communication system for physical layer security, where an IRS is deployed to adjust its surface reflecting elements to guarantee secure communication of multiple legitimate users in the presence of multiple eavesdroppers. Aiming to improve the system secrecy rate, a design problem for jointly optimizing the base station 's beamforming and the IRS's reflecting beamforming is formulated given the different quality of service requirements and time-varying channel condition. As the system is highly dynamic and complex, and it is challenging to address the non-convex optimization problem, a novel deep reinforcement learning -based secure beamforming approach is firstly proposed to achieve the optimal beamforming policy against eavesdroppers in dynamic environments. Furthermore, post-decision state and prioritized experience replay schemes are utilized to enhance the learning efficiency and secrecy performance. Specifically, PDS is capable of tracing the environment dynamic characteristics and adjust the beamforming policy accordingly. Simulation results demonstrate that the proposed deep PDS-PER learning-based secure beamforming approach can significantly improve the system secrecy rate and QoS satisfaction probability in IRS-aided secure communication systems.",0.0909090909],["small-signal models of DC-DC converters often based on state-space","A Building-Block Approach to State-Space Modeling of DC-DC Converter Systems","summarize: Small-signal models of DC-DC converters are often based on a state-space averaging approach, from which both control-oriented and other frequency-domain characteristics, such as input or output impedance, can be derived. Updating these models when extending the converter by filters or non-trivial loads, or adding control loops, can become a tedious task, however. To simplify this potentially error-prone process, a modular modeling approach is being proposed in this article. It consists of small state-space models for certain building blocks of a converter system on the one hand, and standardized operations for connecting these subsystem models to an overall converter system model on the other hand. The resulting state-space system model builds upon a two-port converter description and allows the extraction of control-oriented and impedance characteristics at any modeling stage, be it open loop or closed loop, single converter or series connections of converters. The ease of creating more complex models enabled by the proposed approach is also demonstrated with examples comprising multiple control loops or cascaded converters.",0.2440470864],["layered transition-metal trichalcogenides are a new frontier as two","Pressure-Induced Structural Phase Transition and a Special Amorphization Phase of Two-Dimensional Ferromagnetic Semiconductor Cr2Ge2Te6","summarize: Layered transition-metal trichalcogenides have become one of the research frontiers as two-dimensional magnets and candidate materials used for phase-change memory devices. Herein we report the high-pressure synchrotron X-ray diffraction and resistivity measurements on Cr2Ge2Te6 single crystal by using diamond anvil cell techniques, which reveal a mixture of crystalline-to-crystalline and crystalline-to-amorphous transitions taking place concurrently at 18.3-29.2 GPa. The polymorphic transition could be interpreted by atomic layer reconstruction and the amorphization could be understood in connection with randomly flipping atoms into van der Waals gaps. The amorphous phase is quenchable to ambient conditions. The electrical resistance of CGT shows a bouncing point at ~ 18 GPa, consistent with the polymorphism phase transition. Interestingly, the high-pressure AM phase exhibits metallic resistance with the magnitude comparable to that of high-pressure crystalline phases, whereas the resistance of the AM phase at ambient pressure fails to exceed that of the crystalline phase, indicating that the AM phase of CGT appeared under high pressure is quite unique and similar behavior has never been observed in other phase-change materials. The results definitely would have significant implications for the design of new functional materials.",0.3060018244],["devoted surveys have been operated for data acquisition and processing. present discovery of 18.000 objects","NEARBY Platform: Algorithm for Automated Asteroids Detection in Astronomical Images","summarize: In the past two decades an increasing interest in discovering Near Earth Objects has been noted in the astronomical community. Dedicated surveys have been operated for data acquisition and processing, resulting in the present discovery of over 18.000 objects that are closer than 30 million miles of Earth. Nevertheless, recent events have shown that there still are many undiscovered asteroids that can be on collision course to Earth. This article presents an original NEO detection algorithm developed in the NEARBY research object, that has been integrated into an automated MOPS processing pipeline aimed at identifying moving space objects based on the blink method. Proposed solution can be considered an approach of Big Data processing and analysis, implementing visual analytics techniques for rapid human data validation.",0.0666666667],["2D ferromagnetic-ferroelectric multiferroics have","Two-Dimensional Hyperferroelectric Metals: a Different Route to Ferromagnetic-Ferroelectric Multiferroics","summarize: Recently, two-dimensional multiferroics have attracted numerous attention due to their fascinating properties and promising applications. Although the ferroelectric -ferroelastic and ferromagnetic -ferroelastic multiferroics have been observed\/predicted in 2D systems, 2D ferromagnetic-ferroelectric multiferroics remain to be discovered since FM insulators are very rare. Here, we proposed for the first time the concept of 2D hyperferroelectric metals, with which the insulating prerequisite for the FM-FE multiferroic is no longer required in 2D systems. We validate the concept of 2D hyperferroelectric metals and 2D metallic FM-FE multiferroics by performing first-principle calculations on 2D CrN and CrB2 systems. The 2D buckled monolayer CrN is found to be a hyperferroelectic metal with the FM ground state, i.e., a 2D FM-FE multiferroic. With the global optimization approach, we find the 2D CrB2 system has an antiferromagnetic \/planar ground state and a FM\/FE metastable state, suggesting that it can be used to realize electric field control of magnetism. Our analysis demonstrates that the spin-phonon coupling and metal-metal interaction are two new mechanisms for stabilizing the out-of-plane electric polarization in 2D systems. Our work not only extends the concept of FE to metallic systems, but also paves a new way to search the long-sought high temperature FM-FE multiferroics.",0.0],["soldiers have a limited vision to check around the entire land mass. they have built","Illegal Border Cross Detection and Warning System Using IR Sensor and Node MCU","summarize: We often find illegal immigrants moving from one country to another. By means of land these illegal immigrants move over the fence cut the border wires and moves to the other part of the land. What do you think our soldiers are not doing their job? It is not that actually. It is very difficult to watch consistently over the border. Soldiers have a limited vision to check around the whole land mass. So for them we have come up with a solution that we have built a device that will sense the presence of an intruder . This device will be installed over the fences. When an intruder passes over the fence this device will transmit the signal to the soldiers Smartphone app . The soldier will be notified with the signal and after receiving the signal, the soldier can switch on the alarm and the emergency lights via the app. By this the soldiers in the camp will be alerted and can take their respective positions and arrest the intruder which was passing the border illegally. By this device we can alert the soldiers in the border to take more safety precautions to keep our country safe. Index Terms-Node MCU ESP32, infrared sensor, Blynk app.",0.1739130435],["mobile network operators are trying to expand wireless network capacity. they are deploying wireless local area","A Deep Reinforcement Learning Based Approach for Cost- and Energy-Aware Multi-Flow Mobile Data Offloading","summarize: With the rapid increase in demand for mobile data, mobile network operators are trying to expand wireless network capacity by deploying wireless local area network hotspots on to which they can offload their mobile traffic. However, these network-centric methods usually do not fulfill the interests of mobile users . Taking into consideration many issues such as different applications' deadlines, monetary cost and energy consumption, how the MU decides whether to offload their traffic to a complementary wireless LAN is an important issue. Previous studies assume the MU's mobility pattern is known in advance, which is not always true. In this paper, we study the MU's policy to minimize his monetary cost and energy consumption without known MU mobility pattern. We propose to use a kind of reinforcement learning technique called deep Q-network for MU to learn the optimal offloading policy from past experiences. In the proposed DQN based offloading algorithm, MU's mobility pattern is no longer needed. Furthermore, MU's state of remaining data is directly fed into the convolution neural network in DQN without discretization. Therefore, not only does the discretization error present in previous work disappear, but also it makes the proposed algorithm has the ability to generalize the past experiences, which is especially effective when the number of states is large. Extensive simulations are conducted to validate our proposed offloading algorithms.",0.08],["collimated jets create cavities with dense walls in the slowly-expanding envelope","Shaping the envelope of the asymptotic giant branch star W43A with a collimated fast jet","summarize: One of the major puzzles in the study of stellar evolution is the formation process of bipolar and multi-polar planetary nebulae. There is growing consensus that collimated jets create cavities with dense walls in the slowly-expanding envelope ejected in previous evolutionary phases, leading to the observed morphologies. However, the launching of the jet and the way it interacts with the circumstellar material to create such asymmetric morphologies have remained poorly known. Here we present for the first time CO emission from the asymptotic giant branch star W43A that traces the whole stream of a jet, from the vicinity of its driving stellar system out to the regions where it shapes the circumstellar envelope. We found that the jet has a launch velocity of 175~km~s",0.2896433035],["fusion performance degradation is directly affected by the different features and processing methods undertaken. a","Infrared and Visible Image Fusion with ResNet and zero-phase component analysis","summarize: Feature extraction and processing tasks play a key role in Image Fusion, and the fusion performance is directly affected by the different features and processing methods undertaken. By contrast, most of deep learning-based methods use deep features directly without feature extraction or processing. This leads to the fusion performance degradation in some cases. To solve these drawbacks, we propose a deep features and zero-phase component analysis based novel fusion framework is this paper. Firstly, the residual network is used to extract deep features from source images. Then ZCA is utilized to normalize the deep features and obtain initial weight maps. The final weight maps are obtained by employing a soft-max operation in association with the initial weight maps. Finally, the fused image is reconstructed using a weighted-averaging strategy. Compared with the existing fusion methods, experimental results demonstrate that the proposed framework achieves better performance in both objective assessment and visual quality. The code of our fusion algorithm is available at https:\/\/github.com\/hli1221\/imagefusion_resnet50",0.4090909091],["multi-scroll circuit is complex due to irregular breakpoints and slopes.","Generating Multi-Scroll Chua's Attractors via Simplified Piecewise-Linear Chua's Diode","summarize: High implementation complexity of multi-scroll circuit is a bottleneck problem in real chaos-based communication. Especially, in multi-scroll Chua's circuit, the simplified implementation of piecewise-linear resistors with multiple segments is difficult due to their intricate irregular breakpoints and slopes. To solve the challenge, this paper presents a systematic scheme for synthesizing a Chua's diode with multi-segment piecewise-linearity, which is achieved by cascading even-numbered passive nonlinear resistors with odd-numbered ones via a negative impedance converter. The traditional voltage mode op-amps are used to implement nonlinear resistors. As no extra DC bias voltage is employed, the scheme can be implemented by much simpler circuits. The voltage-current characteristics of the obtained Chua's diode are analyzed theoretically and verified by numerical simulations. Using the Chua's diode and a second-order active Sallen-Key high-pass filter, a new inductor-free Chua's circuit is then constructed to generate multi-scroll chaotic attractors. Different number of scrolls can be generated by changing the number of passive nonlinear resistor cells or adjusting two coupling parameters. Besides, the system can be scaled by using different power supplies, satisfying the low-voltage low-power requirement of integrated circuit design. The circuit simulations and hardware experiments both confirmed the feasibility of the designed system.",0.2],["the trifocal essential matrix is a generalization of the ordinary essential matrix.","On Some Properties of Calibrated Trifocal Tensors","summarize: In two-view geometry, the essential matrix describes the relative position and orientation of two calibrated images. In three views, a similar role is assigned to the calibrated trifocal tensor. It is a particular case of the trifocal tensor and thus it inherits all its properties but, due to the smaller degrees of freedom, satisfies a number of additional algebraic constraints. Some of them are described in this paper. More specifically, we define a new notion --- the trifocal essential matrix. On the one hand, it is a generalization of the ordinary essential matrix, and, on the other hand, it is closely related to the calibrated trifocal tensor. We prove the two necessary and sufficient conditions that characterize the set of trifocal essential matrices. Based on these characterizations, we propose three necessary conditions on a calibrated trifocal tensor. They have a form of 15 quartic and 99 quintic polynomial equations. We show that in the practically significant real case the 15 quartic constraints are also sufficient.",0.1538461538],["the aim of this paper is to characterize the notion of internal category in the category of Le","Actions of internal groupoids in the category of Leibniz algebras","summarize: The aim of this paper is to characterize the notion of internal category in the category of Leibniz algebras and investigate the properties of well-known notions such as covering groupoid and groupoid operations in this category. Further, for a fixed internal groupoid ",0.4411764706],["the much-used trace distance of coherence was shown to be not a proper measure","The modified trace distance of coherence is constant on most pure states","summarize: Recently, the much-used trace distance of coherence was shown to not be a proper measure of coherence, so a modification of it was proposed. We derive an explicit formula for this modified trace distance of coherence on pure states. Our formula shows that, despite satisfying the axioms of proper coherence measures, it is likely not a good measure to use, since it is maximal on all except for an exponentially-small fraction of pure states.",0.4444444444],["proposed scheme is optimal with 1 or 2 sets of data. it is suboptimal with","Energy-Minimizing Bit Allocation For Powerline OFDM With Multiple Delay Constraints","summarize: We propose a bit-allocation scheme for powerline orthogonal frequency-division multiplexing that minimizes total transmit energy subject to total-bit and delay constraints. Multiple delay requirements stem from different sets of data that a transmitter must time-multiplex and transmit to a receiver. The proposed bit allocation takes into account the channel power-to-noise density ratio of subchannels as well as statistic of narrowband interference and impulsive noise that is pervasive in powerline communication channels. The proposed scheme is optimal with 1 or 2 sets of data, and is suboptimal with more than 2 sets of data. However, numerical examples show that the proposed scheme performs close to the optimum. Also, it is less computationally complex than the optimal scheme especially when minimizing total energy over large number of data sets. We also compare the proposed scheme with some existing schemes and find that our scheme requires less total transmit energy when the number of delay constraints is large.",0.1363636364],["we show the convergence results for iterates of Bregman projections onto closed linear sub","On the rate of convergence of iterated Bregman projections and of the alternating algorithm","summarize: We study the alternating algorithm for the computation of the metric projection onto the closed sum of two closed subspaces in uniformly convex and uniformly smooth Banach spaces. For Banach spaces which are convex and smooth of power type, we exhibit a condition which implies linear convergence of this method. We show these convergence results for iterates of Bregman projections onto closed linear subspaces. Using an intimate connection between the metric projection onto a closed linear subspace and the Bregman projection onto its annihilator, we deduce the convergence rate results for the alternating algorithm from the corresponding results for the iterated Bregman projection method.",0.3571428571],["encoding experiments can deliver single-scan MR images without folding complications. the","Multiple-coil k-space interpolation enhances resolution in single-shot spatiotemporal MRI","summarize: Purpose: Spatio-temporal encoding experiments can deliver single-scan MR images without folding complications and with robustness to chemical shift and susceptibility artifacts. It is here shown that further resolution improvements can arise by relying on multiple receivers, to interpolate the sampled data along the low-bandwidth dimension. The ensuing multiple-sensor interpolation is akin to recently introduced SPEN interleaving procedures, albeit without requiring multiple shots. Methods: By casting SPEN's spatial rasterization in k-space, it becomes evident that local k-data interpolations enabled by multiple receivers are akin to real-space interleaving of SPEN images. The practical implementation of such resolution-enhancing procedure becomes similar to those normally used in SMASH or SENSE, yet relaxing these methods' fold-over constraints. Results: Experiments validating the theoretical expectations were carried out on phantoms and human volunteers on a 3T scanner. The experiments showed the expected resolution enhancement, at no cost in the sequence's complexity. With the addition of multibanding and stimulated echo procedures, 48-slices full-brain coverage could be recorded free from distortions at sub-mm resolution, in 3 sec. Conclusion: Super-resolved SPEN with SENSE achieves the goals of multi-shot SPEN interleaving within one single scan, delivering single-shot sub-mm in-plane resolutions in scanners equipped with suitable multiple sensors.",0.0833333333],["249 bright stars have metallicities, effective temperatures, and ages between -0.7 and","High-resolution Spectroscopic Study of Dwarf Stars in the Northern Sky: Lithium, Carbon, and Oxygen Abundances","summarize: Abundances of lithium, carbon, and oxygen have been derived using spectral synthesis for a sample of 249 bright F, G, and K Northern Hemisphere dwarf stars from the high-resolution spectra acquired with the VUES spectrograph at the Moletai Astronomical Observatory of Vilnius University. The sample stars have metallicities, effective temperatures, and ages between -0.7 and 0.4 dex; 5000 and 6900 K; 1 and 12 Gyr, accordingly. We confirm a so far unexplained lithium abundance decrease at supersolar metallicities - ",0.1112572547],["proposed NMPC formulation allows for a fully parametric obstacle trajectory. the trajectory calculation","Nonlinear MPC for Collision Avoidance and Controlof UAVs With Dynamic Obstacles","summarize: This article proposes a Novel Nonlinear Model Predictive Control for navigation and obstacle avoidance of an Unmanned Aerial Vehicle . The proposed NMPC formulation allows for a fully parametric obstacle trajectory, while in this article we apply a classification scheme to differentiate between different kinds of trajectories to predict future obstacle positions. The trajectory calculation is done from an initial condition, and fed to the NMPC as an additional input. The solver used is the nonlinear, non-convex solver Proximal Averaged Newton for Optimal Control and its associated software OpEn , in which we apply a penalty method to properly consider the obstacles and other constraints during navigation. The proposed NMPC scheme allows for real-time solutions using a sampling time of 50 ms and a two second prediction of both the obstacle trajectory and the NMPC problem, which implies that the scheme can be considered as a local path-planner. This paper will present the NMPC cost function and constraint formulation, as well as the methodology of dealing with the dynamic obstacles. We include multiple laboratory experiments to demonstrate the efficacy of the proposed control architecture, and to show that the proposed method delivers fast and computationally stable solutions to the dynamic obstacle avoidance scenarios.",0.24],["fgivenx is a Python package for functional posterior plotting. it will","fgivenx: A Python package for functional posterior plotting","summarize: fgivenx is a Python package for functional posterior plotting, currently used in astronomy, but will be of use to scientists performing any Bayesian analysis which has predictive posteriors that are functions. The source code for fgivenx is available on GitHub at https:\/\/github.com\/williamjameshandley\/fgivenx",0.6428571429],["entropy games and matrix multiplication games have been introduced. they model the situation","The operator approach to entropy games","summarize: Entropy games and matrix multiplication games have been recently introduced by Asarin et al. They model the situation in which one player wishes to minimize the growth rate of a matrix product, whereas the other player wishes to maximize it. We develop an operator approach to entropy games. This allows us to show that entropy games can be cast as stochastic mean payoff games in which some action spaces are simplices and payments are given by a relative entropy . In this way, we show that entropy games with a fixed number of states belonging to Despot can be solved in polynomial time. This approach also allows us to solve these games by a policy iteration algorithm, which we compare with the spectral simplex algorithm developed by Protasov.",0.1875],["performance is measured by network's square of H-2 norm. it is shown that it","Performance Improvement in Noisy Linear Consensus Networks with Time-Delay","summarize: We analyze performance of a class of time-delay first-order consensus networks from a graph topological perspective and present methods to improve it. The performance is measured by network's square of H-2 norm and it is shown that it is a convex function of Laplacian eigenvalues and the coupling weights of the underlying graph of the network. First, we propose a tight convex, but simple, approximation of the performance measure in order to achieve lower complexity in our design problems by eliminating the need for eigen-decomposition. The effect of time-delay reincarnates itself in the form of non-monotonicity, which results in nonintuitive behaviors of the performance as a function of graph topology. Next, we present three methods to improve the performance by growing, re-weighting, or sparsifying the underlying graph of the network. It is shown that our suggested algorithms provide near-optimal solutions with lower complexity with respect to existing methods in literature.",0.2222222222],["the method is adapted to the particular challenges of the eddy current problem. it","Parallel-In-Time Simulation of Eddy Current Problems Using Parareal","summarize: In this contribution the usage of the Parareal method is proposed for the time-parallel solution of the eddy current problem. The method is adapted to the particular challenges of the problem that are related to the differential algebraic character due to non-conducting regions. It is shown how the necessary modification can be automatically incorporated by using a suitable time stepping method. The paper closes with a first demonstration of a simulation of a realistic four-pole induction machine model using Parareal.",0.05],["Given an arbitrary closed set A of the arbitrary set A of the arbitrary closed set","Fine properties of the curvature of arbitrary closed sets","summarize: Given an arbitrary closed set A of ",0.3888888889],["roll2Rail project aims to develop key technologies and remove existing blocking points for radical","The Wireless Train Communication Network: Roll2Rail vision","summarize: This paper explains the main results obtained from the research carried out in the work package 2 of the Roll2Rail project. This project aims to develop key technologies and to remove already identified blocking points for radical innovation in the field of railway vehicles, to increase their operational reliability and to reduce life-cycle costs. This project started in May 2015 and has been funded by the Horizon 2020 program of the European Commission. The goal for WP2 is to research on both technologies and architectures to develop a new wireless Train Communication Network within IEC61375 standard series. This TCN is today entirely wired and is used for Train Control and Monitoring System functions , operator-oriented services and customer-oriented services. This paradigm shift from wired to wireless means a removal of wirings implies, among other benefits, a significant reduction of life cycle costs due to the removal of cables, and the simplification of the train coupling procedure, among others.",0.0],["the theory of dependency graphs is a powerful toolbox to prove asymptotic","Weighted dependency graphs","summarize: The theory of dependency graphs is a powerful toolbox to prove asymptotic normality of sums of random variables. In this article, we introduce a more general notion of weighted dependency graphs and give normality criteria in this context. We also provide generic tools to prove that some weighted graph is a weighted dependency graph for a given family of random variables. To illustrate the power of the theory, we give applications to the following objects: uniform random pair partitions, the random graph model ",0.1764705882],["a molecule is a non-unit element of an integral domain having a unique","On the molecules of numerical semigroups, Puiseux monoids, and Puiseux algebras","summarize: A molecule is a nonzero non-unit element of an integral domain having a unique factorization into irreducibles . Here we study the molecules of Puiseux monoids as well as the molecules of their corresponding semigroup algebras, which we call Puiseux algebras. We begin by presenting, in the context of numerical semigroups, some results on the possible cardinalities of the sets of molecules and the sets of reducible molecules . Then we study the molecules in the more general context of Puiseux monoids. We construct infinitely many non-isomorphic atomic Puiseux monoids all whose molecules are atoms. In addition, we characterize the molecules of Puiseux monoids generated by rationals with prime denominators. Finally, we turn to investigate the molecules of Puiseux algebras. We provide a characterization of the molecules of the Puiseux algebras corresponding to root-closed Puiseux monoids. Then we use such a characterization to find an infinite class of Puiseux algebras with infinitely many non-associated reducible molecules.",0.5161290323],["the problem consists of determining the initial conditions and other parameters of an orbit from some observations","Orbit determination for standard-like maps: asymptotic expansion of the confidence region in regular zones","summarize: We deal with the orbit determination problem for a class of maps of the cylinder generalizing the Chirikov standard map. The problem consists of determining the initial conditions and other parameters of an orbit from some observations. A solution to this problem goes back to Gauss and leads to the least squares method. Since the observations admit errors, the solution comes with a confidence region describing the uncertainty of the solution itself. We study the behavior of the confidence region in the case of a simultaneous increase of the number of observations and the time span over which they are performed. More precisely, we describe the geometry of the confidence region for solutions in regular zones. We prove an estimate of the trend of the uncertainties in a set of positive measure of the phase space, made of invariant curve. Our result gives an analytical proof of some known numerical evidences.",0.2916666667],["ad publishers and ad networks are constantly trying to pursue new strategies to keep up","Measuring Abuse in Web Push Advertising","summarize: The rapid growth of online advertising has fueled the growth of ad-blocking software, such as new ad-blocking and privacy-oriented browsers or browser extensions. In response, both ad publishers and ad networks are constantly trying to pursue new strategies to keep up their revenues. To this end, ad networks have started to leverage the Web Push technology enabled by modern web browsers. As web push notifications are relatively new, their role in ad delivery has not been yet studied in depth. Furthermore, it is unclear to what extent WPN ads are being abused for malvertising . In this paper, we aim to fill this gap. Specifically, we propose a system called PushAdMiner that is dedicated to automatically registering for and collecting a large number of web-based push notifications from publisher websites, finding WPN-based ads among these notifications, and discovering malicious WPN-based ad campaigns. Using PushAdMiner, we collected and analyzed 21,541 WPN messages by visiting thousands of different websites. Among these, our system identified 572 WPN ad campaigns, for a total of 5,143 WPN-based ads that were pushed by a variety of ad networks. Furthermore, we found that 51% of all WPN ads we collected are malicious, and that traditional ad-blockers and malicious URL filters are remarkably ineffective against WPN-based malicious ads, leaving a significant abuse vector unchecked.",0.0],["general model contains many existing plane-based clustering methods. the general model is a","A general model for plane-based clustering with loss function","summarize: In this paper, we propose a general model for plane-based clustering. The general model contains many existing plane-based clustering methods, e.g., k-plane clustering , proximal plane clustering , twin support vector clustering and its extensions. Under this general model, one may obtain an appropriate clustering method for specific purpose. The general model is a procedure corresponding to an optimization problem, where the optimization problem minimizes the total loss of the samples. Thereinto, the loss of a sample derives from both within-cluster and between-cluster. In theory, the termination conditions are discussed, and we prove that the general model terminates in a finite number of steps at a local or weak local optimal point. Furthermore, based on this general model, we propose a plane-based clustering method by introducing a new loss function to capture the data distribution precisely. Experimental results on artificial and public available datasets verify the effectiveness of the proposed method.",0.375],["a powerful, computationally inexpensive CNN is implemented to estimate the number of lesion vo","Modelling brain lesion volume in patches with CNN-based Poisson Regression","summarize: Monitoring the progression of lesions is important for clinical response. Summary statistics such as lesion volume are objective and easy to interpret, which can help clinicians assess lesion growth or decay. CNNs are commonly used in medical image segmentation for their ability to produce useful features within large contexts and their associated efficient iterative patch-based training. Many CNN architectures require hundreds of thousands parameters to yield a good segmentation. In this work, an efficient, computationally inexpensive CNN is implemented to estimate the number of lesion voxels in a predefined patch size from magnetic resonance images. The output of the CNN is interpreted as the conditional Poisson parameter over the patch, allowing standard mini-batch gradient descent to be employed. The ISLES2015 data is used to train and evaluate the model, which by estimating lesion volume from raw features, accurately identified the lesion image with the larger lesion volume for 86% of paired sample patches. An argument for the development and use of estimating lesion volumes to also aid in model selection for segmentation is made.",0.4117647059],["a challenge in addressing this problem in urban traffic scenes is attributed to the unpredictable behavior","Recognition and 3D Localization of Pedestrian Actions from Monocular Video","summarize: Understanding and predicting pedestrian behavior is an important and challenging area of research for realizing safe and effective navigation strategies in automated and advanced driver assistance technologies in urban scenes. This paper focuses on monocular pedestrian action recognition and 3D localization from an egocentric view for the purpose of predicting intention and forecasting future trajectory. A challenge in addressing this problem in urban traffic scenes is attributed to the unpredictable behavior of pedestrians, whereby actions and intentions are constantly in flux and depend on the pedestrians pose, their 3D spatial relations, and their interaction with other agents as well as with the environment. To partially address these challenges, we consider the importance of pose toward recognition and 3D localization of pedestrian actions. In particular, we propose an action recognition framework using a two-stream temporal relation network with inputs corresponding to the raw RGB image sequence of the tracked pedestrian as well as the pedestrian pose. The proposed method outperforms methods using a single-stream temporal relation network based on evaluations using the JAAD public dataset. The estimated pose and associated body key-points are also used as input to a network that estimates the 3D location of the pedestrian using a unique loss function. The evaluation of our 3D localization method on the KITTI dataset indicates the improvement of the average localization error as compared to existing state-of-the-art methods. Finally, we conduct qualitative tests of action recognition and 3D localization on HRI's H3D driving dataset.",0.1785714286],["this work introduces the class of generalized linear-quadratic functions. we consider","Epiconvergence, the Moreau envelope and generalized linear-quadratic functions","summarize: This work introduces the class of generalized linear-quadratic functions, constructed using maximally monotone symmetric linear relations. Calculus rules and properties of the Moreau envelope for this class of functions are developed. In finite dimensions, on a metric space defined by Moreau envelopes, we consider the epigraphical limit of a sequence of quadratic functions and categorize the results. We explore the question of when a quadratic function is a Moreau envelope of a generalized linear-quadratic function; characterizations involving nonexpansiveness and Lipschitz continuity are established. This work generalizes some results by Hiriart-Urruty and by Rockafellar and Wets.",0.2727272727],["two filters have been demonstrated at 4.5 GHz with sharp roll-off, flat in-","4.5 GHz Lithium Niobate MEMS Filters with 10% Fractional Bandwidth for 5G Front-ends","summarize: This paper presents a new class of micro-electro-mechanical system C-band filters for 5G front-ends. The filter is comprised of resonators based on the first-order asymmetric Lamb wave mode in thin film lithium niobate. Two filters have been demonstrated at 4.5 GHz with sharp roll-off, flat in-band group delay, and spurious-free response over a wide frequency range. The first design shows a fractional bandwidth of 10%, an insertion loss of 1.7 dB, an out-of-band rejection of -13 dB, and a compact footprint of 0.36 mm2, while the second design shows an FBW of 8.5%, an IL of 2.7 dB, an OoB rejection of -25 dB, and a footprint of 0.9 mm^2. The demonstrations herein mark the largest fractional bandwidth achieved for acoustic-only filters at 5G frequencies.",0.2666666667],["a recent cross-modal hashing method has sparked a great revolution in","MTFH: A Matrix Tri-Factorization Hashing Framework for Efficient Cross-Modal Retrieval","summarize: Hashing has recently sparked a great revolution in cross-modal retrieval because of its low storage cost and high query speed. Recent cross-modal hashing methods often learn unified or equal-length hash codes to represent the multi-modal data and make them intuitively comparable. However, such unified or equal-length hash representations could inherently sacrifice their representation scalability because the data from different modalities may not have one-to-one correspondence and could be encoded more efficiently by different hash codes of unequal lengths. To mitigate these problems, this paper exploits a related and relatively unexplored problem: encode the heterogeneous data with varying hash lengths and generalize the cross-modal retrieval in various challenging scenarios. To this end, a generalized and flexible cross-modal hashing framework, termed Matrix Tri-Factorization Hashing , is proposed to work seamlessly in various settings including paired or unpaired multi-modal data, and equal or varying hash length encoding scenarios. More specifically, MTFH exploits an efficient objective function to flexibly learn the modality-specific hash codes with different length settings, while synchronously learning two semantic correlation matrices to semantically correlate the different hash representations for heterogeneous data comparable. As a result, the derived hash codes are more semantically meaningful for various challenging cross-modal retrieval tasks. Extensive experiments evaluated on public benchmark datasets highlight the superiority of MTFH under various retrieval scenarios and show its competitive performance with the state-of-the-arts.",0.6],["in the presence of additive Gaussian noise, the statistics of the nonlinear Fourier","Statistics of the Nonlinear Discrete Spectrum of a Noisy Pulse","summarize: In the presence of additive Gaussian noise, the statistics of the nonlinear Fourier transform of a pulse are not yet completely known in closed form. In this paper, we propose a novel approach to study this problem. Our contributions are twofold: first, we extend the existing Fourier Collocation method to compute the whole discrete spectrum . We show numerically that the accuracy of FC is comparable to the state-of-the-art NFT algorithms. Second, we apply perturbation theory of linear operators to derive analytic expressions for the joint statistics of the eigenvalues and the spectral amplitudes when a pulse is contaminated by additive Gaussian noise. Our analytic expressions closely match the empirical statistics obtained through simulations.",0.3636363636],["vision-based localization has demonstrated superior performance to other localization methods. a scheme","Cooperative Vision-based Localization Networks with Communication Constraints","summarize: Accurate location information is indispensable for the emerging applications of \\ac, such as automatic driving and formation control. In the real scenario, vision-based localization has demonstrated superior performance to other localization methods for its stability and flexibility. In this paper, a scheme of cooperative vision-based localization with communication constraints is proposed. Vehicles collect images of the environment and distance measurements between each other. Then vehicles transmit the coordinates of feature points and distances with constrained bits to the edge to estimate their positions. The \\ac for absolute localization is first obtained, based on which we derive the relative \\ac through subspace projection. Furthermore, we formulate the corresponding bit allocation problem for relative localization. Finally, a \\ac algorithm is developed by considering the influence of photographing, distance measurements and quantization noises. Compared with conventional bit allocation methods, numerical results demonstrate the localization performance gain of our proposed algorithm with higher computational efficiency.",0.2727272727],["we define the notion of a separable element in a finite Weyl","Separable elements in Weyl groups","summarize: We define the notion of a separable element in a finite Weyl group, generalizing the well-studied class of separable permutations. We prove that the upper and lower order ideals in weak Bruhat order generated by a separable element are rank-symmetric and rank-unimodal, and that the product of their rank generating functions gives that of the whole group, answering an open problem of Fan Wei. We also prove that separable elements are characterized by pattern avoidance in the sense of Billey and Postnikov.",0.35],["document ontology captures general purpose semantic structure and domain specific semantic concepts from large, structured","Understanding and representing the semantics of large structured documents","summarize: Understanding large, structured documents like scholarly articles, requests for proposals or business reports is a complex and difficult task. It involves discovering a document's overall purpose and subject, understanding the function and meaning of its sections and subsections, and extracting low level entities and facts about them. In this research, we present a deep learning based document ontology to capture the general purpose semantic structure and domain specific semantic concepts from a large number of academic articles and business documents. The ontology is able to describe different functional parts of a document, which can be used to enhance semantic indexing for a better understanding by human beings and machines. We evaluate our models through extensive experiments on datasets of scholarly articles from arXiv and Request for Proposal documents.",0.3333333333],["a quintessence model is associated with non-Abelian gauge fields","Gaugessence: a dark energy model with early time radiation-like equation of state","summarize: In this work, we study a new quintessence model associated with non-Abelian gauge fields, minimally coupled to Einstein gravity. This gauge theory has been recently introduced and studied as an inflationary model, called gauge-flation. Here, however, we are interested in the late time cosmology of the model in the presence of matter and radiation to explain the present time accelerating Universe. During the radiation and matter eras, the gauge field tracks radiation and basically acts like a dark radiation sector. As we approach lower redshifts, the dark component takes the form of a dark energy source which eventually becomes the dominate part of the energy budget of the Universe. Due to the tracking feature of our model, solutions with different initial values are attracted to a common trajectory. The existence of early dark radiation is a robust prediction of our model which contributes to the effective number of relativistic species, ",0.3858245518],["a gauge field for two attractive particles moves on a curved surface. this indicates","Emergent Gauge Field for a Chiral Bound State on Curved Surface","summarize: In this letter we show that there emerges a gauge field for two attractive particles moving on a curved surface when they form a chiral bound state. By solving a two-body problem on a sphere, we show explicitly that the center-of-mass wave functions of such deeply bound states are monopole harmonics instead of spherical harmonics. This indicates that the bound state experiences a gauge field identical to a magnetic monopole at the center of the sphere, with the monopole charge equal to the quantized relative angular momentum of this bound state. We show that this emergent gauge field is due to the coupling between the center-of-mass and the relative motion on curved surfaces. Our results can be generalized to an arbitrary curved surface where the emergent magnetic field is exactly the local Gaussian curvature. This result establishes an intriguing connection between space curvature and gauge field, paves an alternative way to engineer topological state with space curvature, and may be observed in cold atom system.",0.4285714286],["we calculate the shape and velocity of a bubble rising in an infinitely large and closed He","Theoretical analysis for flattening of a rising bubble in a Hele-Shaw cell","summarize: We calculate the shape and the velocity of a bubble rising in an infinitely large and closed Hele-Shaw cell using Park and Homsy's boundary condition which accounts for the change of the three dimensional structure in the perimeter zone. We first formulate the problem in the form of a variational problem, and discuss the shape change assuming that the bubble takes elliptic shape. We calculate the shape and the velocity of the bubble as a function of the bubble size, gap distance and the inclination angle of the cell. We show that the bubble is flattened as it rises. This result is in agreement with experiments for large Hele-Shaw cells.",0.4838709677],["we measured mid-infrared polarization of protoplanetary discs. we","Mid-Infrared Polarization of Herbig Ae\/Be Discs","summarize: We measured mid-infrared polarization of protoplanetary discs to gain new insight into their magnetic fields. Using CanariCam at the 10.4 m Gran Telescopio Canarias, we detected linear polarization at 8.7, 10.3, and 12.5 ",0.1],["the paper is focused on the preparation of the flexible and freestanding polymer electrolyte","Optimization of salt concentration and Explanation of Two Peak Percolation in Blend Solid Polymer Nanocomposite Films","summarize: The present paper report is focused toward the preparation of the flexible and freestanding blend solid polymer electrolyte films based on PEO-PVP complexed with NaPF6 by solution cast technique. The structural\/morphological features of the synthesized polymer nanocomposite films have been investigated in detail using X-ray diffraction, Fourier transform infra-red spectroscopy, field emission scanning electron microscope, and atomic force microscopy techniques. The film PEO-PVP+NaPF6 O\/Na:8 exhibits highest ionic conductivity ~5.92x10-6 S cm-1 at 40 oC and ~2.46x10-4 S cm-1 at 100 oC. The temperature dependent conductivity shows Arrhenius type behavior and activation energy decreases with the addition of salt. The high temperature conductivity monitoring is done for the optimized PEO-PVP+NaPF6 O\/Na:8 highly conductive system and the conductivity is still maintained stable up to 160 h . The thermal transitions parameters were measured by the differential scanning calorimetry measurements. The prepared polymer electrolyte film displays the smoother surface in addition of salt and a thermal stability up to 300 oC............",0.1576141636],["the near bed transport region is represented by the lower layer which has an arbitrarily constant,","Quasi-two-layer morphodynamic model for bedload-dominated problems: bed slope-induced morphological diffusion","summarize: We derive a two-layer depth-averaged model of sediment transport and morphological evolution for application to bedload-dominated problems. The near bed transport region is represented by the lower layer which has an arbitrarily constant, vanishing thickness , and whose average sediment concentration is free to vary. Sediment is allowed to enter the upper layer, and so total load may also be simulated, provided that concentrations of suspended sediment remain low. The model conforms with established theories of bedload, and is validated satisfactorily against empirical expressions for sediment transport rates and the morphodynamic experiment of a migrating mining pit by Lee et al. . Investigation into the effect of a local bed gradient on bedload leads to derivation of an analytical, physically meaningful expression for morphological diffusion induced by a non-zero local bed slope. Incorporation of the proposed morphological diffusion into a conventional morphodynamic model improves model predictions when applied to the evolution of a mining pit, without the need either to resort to special numerical treatment of the equations or to use additional tuning parameters.",0.1],["Hitomi carries two Hard X-ray Telescopes that can focus","Inorbit Performance of the Hard X-ray Telescope on board the Hitomi satellite","summarize: Hitomi carries two Hard X-ray Telescopes that can focus X-rays up to 80 keV. Combined with the Hard X-ray Imagers that detect the focused X-rays, imaging spectroscopy in the high-energy band from 5 keV to 80 keV is made possible. We studied characteristics of HXTs after the launch such as the encircled energy function and the effective area using the data of a Crab observation. The half power diameters in the 5--80 keV band evaluated from the EEFs are 1.59 arcmin for HXT-1 and 1.65 arcmin for HXT-2. Those are consistent with the HPDs measured with ground experiments when uncertainties are taken into account. We can conclude that there is no significant change in the characteristics of the HXTs before and after the launch. The off-axis angle of the aim point from the optical axis is evaluated to be less than 0.5 arcmin for both HXT-1 and HXT-2. The best-fit parameters for the Crab spectrum obtained with the HXT-HXI system are consistent with the canonical values.",0.2388437702],["the Fermi-LAT GeV excess is known as the Fermi-LA","The Fermi-LAT GeV Excess Traces Stellar Mass in the Galactic Bulge","summarize: An anomalous emission component at energies of a few GeV and located towards the inner Galaxy is present in the Fermi-LAT data. It is known as the Fermi-LAT GeV excess. Using almost 8 years of data we reanalyze the characteristics of this excess with SkyFACT, a novel tool that combines image reconstruction with template fitting techniques. We find that an emission profile that traces stellar mass in the boxy and nuclear bulge provides the best description of the excess emission, providing strong circumstantial evidence that the excess is due to a stellar source population in the Galactic bulge. We find a luminosity to stellar mass ratio of ",0.4003687015],["a manipulation planner needs to generate a trajectory of the manipulator arm. the manipul","Manipulation Trajectory Optimization with Online Grasp Synthesis and Selection","summarize: In robot manipulation, planning the motion of a robot manipulator to grasp an object is a fundamental problem. A manipulation planner needs to generate a trajectory of the manipulator arm to avoid obstacles in the environment and plan an end-effector pose for grasping. While trajectory planning and grasp planning are often tackled separately, how to efficiently integrate the two planning problems remains a challenge. In this work, we present a novel method for joint motion and grasp planning. Our method integrates manipulation trajectory optimization with online grasp synthesis and selection, where we apply online learning techniques to select goal configurations for grasping, and introduce a new grasp synthesis algorithm to generate grasps online. We evaluate our planning approach and demonstrate that our method generates robust and efficient motion plans for grasping in cluttered scenes. Our video can be found at https:\/\/www.youtube.com\/watch?v=LIcACf8YkGU .",0.375],["algorithm devised to perform orthogonal range searching in static databases with multiple dimensions.","The n-dimensional k-vector and its application to orthogonal range searching","summarize: This work focuses on the definition and study of the n-dimensional k-vector, an algorithm devised to perform orthogonal range searching in static databases with multiple dimensions. The methodology first finds the order in which to search the dimensions, and then, performs the search using a modified projection method. In order to determine the dimension order, the algorithm uses the k-vector, a range searching technique for one dimension that identifies the number of elements contained in the searching range. Then, using this information, the algorithm predicts and selects the best approach to deal with each dimension. The algorithm has a worst case complexity of ",0.3571428571],["proposed epBRM is built with sequence of small networks and is computationally lightweight","epBRM: Improving a Quality of 3D Object Detection using End Point Box Regression Module","summarize: We present an endpoint box regression module, which is designed for predicting precise 3D bounding boxes using raw LiDAR 3D point clouds. The proposed epBRM is built with sequence of small networks and is computationally lightweight. Our approach can improve a 3D object detection performance by predicting more precise 3D bounding box coordinates. The proposed approach requires 40 minutes of training to improve the detection performance. Moreover, epBRM imposes less than 12ms to network inference time for up-to 20 objects. The proposed approach utilizes a spatial transformation mechanism to simplify the box regression task. Adopting spatial transformation mechanism into epBRM makes it possible to improve the quality of detection with a small sized network. We conduct in-depth analysis of the effect of various spatial transformation mechanisms applied on raw LiDAR 3D point clouds. We also evaluate the proposed epBRM by applying it to several state-of-the-art 3D object detection systems. We evaluate our approach on KITTI dataset, a standard 3D object detection benchmark for autonomous vehicles. The proposed epBRM enhances the overlaps between ground truth bounding boxes and detected bounding boxes, and improves 3D object detection. Our proposed method evaluated in KITTI test server outperforms current state-of-the-art approaches.",0.1234614772],["NILM is unidentifiable and therefore a challenge problem because the infer","Transfer Learning for Non-Intrusive Load Monitoring","summarize: Non-intrusive load monitoring is a technique to recover source appliances from only the recorded mains in a household. NILM is unidentifiable and thus a challenge problem because the inferred power value of an appliance given only the mains could not be unique. To mitigate the unidentifiable problem, various methods incorporating domain knowledge into NILM have been proposed and shown effective experimentally. Recently, among these methods, deep neural networks are shown performing best. Arguably, the recently proposed sequence-to-point learning is promising for NILM. However, the results were only carried out on the same data domain. It is not clear if the method could be generalised or transferred to different domains, e.g., the test data were drawn from a different country comparing to the training data. We address this issue in the paper, and two transfer learning schemes are proposed, i.e., appliance transfer learning and cross-domain transfer learning . For ATL, our results show that the latent features learnt by a `complex' appliance, e.g., washing machine, can be transferred to a `simple' appliance, e.g., kettle. For CTL, our conclusion is that the seq2point learning is transferable. Precisely, when the training and test data are in a similar domain, seq2point learning can be directly applied to the test data without fine tuning; when the training and test data are in different domains, seq2point learning needs fine tuning before applying to the test data. Interestingly, we show that only the fully connected layers need fine tuning for transfer learning. Source code can be found at https:\/\/github.com\/MingjunZhong\/transferNILM.",0.1875],["the field of learning analytics needs to adopt a more rigorous approach for predictive model evaluation.","Dropout Model Evaluation in MOOCs","summarize: The field of learning analytics needs to adopt a more rigorous approach for predictive model evaluation that matches the complex practice of model-building. In this work, we present a procedure to statistically test hypotheses about model performance which goes beyond the state-of-the-practice in the community to analyze both algorithms and feature extraction methods from raw data. We apply this method to a series of algorithms and feature sets derived from a large sample of Massive Open Online Courses . While a complete comparison of all potential modeling approaches is beyond the scope of this paper, we show that this approach reveals a large gap in dropout prediction performance between forum-, assignment-, and clickstream-based feature extraction methods, where the latter is significantly better than the former two, which are in turn indistinguishable from one another. This work has methodological implications for evaluating predictive or AI-based models of student success, and practical implications for the design and targeting of at-risk student models and interventions.",0.0833333333],["this paper studies the possibility of detecting and isolating topology failures of","Generic Detectability and Isolability of Topology Failures in Networked Linear Systems","summarize: This paper studies the possibility of detecting and isolating topology failures of a networked system from subsystem measurements, in which subsystems are of fixed high-order linear dynamics, and the exact interaction weights among them are unknown. We prove that in such class of networked systems with the same network topologies, the detectability and isolability of a given topology failure are generic properties, indicating that it is the network topology that dominates the property of being detectable or isolable for a failure . We first give algebraic conditions for detectability and isolability of arbitrary parameter perturbations for a lumped plant, and then derive graph-theoretical necessary and sufficient conditions for generic detectability and isolability of topology failures for the networked systems. On the basis of these results, we consider the problems of deploying the smallest set of sensors for generic detectability and isolability. We reduce the associated sensor placement problems to the hitting set problems, which can be effectively solved by greedy algorithms with guaranteed approximation performances.",0.2142857143],["multi-scroll circuit is complex due to irregular breakpoints and slopes.","Generating Multi-Scroll Chua's Attractors via Simplified Piecewise-Linear Chua's Diode","summarize: High implementation complexity of multi-scroll circuit is a bottleneck problem in real chaos-based communication. Especially, in multi-scroll Chua's circuit, the simplified implementation of piecewise-linear resistors with multiple segments is difficult due to their intricate irregular breakpoints and slopes. To solve the challenge, this paper presents a systematic scheme for synthesizing a Chua's diode with multi-segment piecewise-linearity, which is achieved by cascading even-numbered passive nonlinear resistors with odd-numbered ones via a negative impedance converter. The traditional voltage mode op-amps are used to implement nonlinear resistors. As no extra DC bias voltage is employed, the scheme can be implemented by much simpler circuits. The voltage-current characteristics of the obtained Chua's diode are analyzed theoretically and verified by numerical simulations. Using the Chua's diode and a second-order active Sallen-Key high-pass filter, a new inductor-free Chua's circuit is then constructed to generate multi-scroll chaotic attractors. Different number of scrolls can be generated by changing the number of passive nonlinear resistor cells or adjusting two coupling parameters. Besides, the system can be scaled by using different power supplies, satisfying the low-voltage low-power requirement of integrated circuit design. The circuit simulations and hardware experiments both confirmed the feasibility of the designed system.",0.2],["spectrographs used NEMESIS radiative transfer code and retrieval suite to","Venus upper clouds and the UV-absorber from MESSENGER\/MASCS observations","summarize: One of the most intriguing, long-standing questions regarding Venus' atmosphere is the origin and distribution of the unknown UV-absorber, responsible for the absorption band detected at the near-UV and blue range of Venus' spectrum. In this work, we use data collected by MASCS spectrograph on board the MESSENGER mission during its second Venus flyby in June 2007 to address this issue. Spectra range from 0.3 m to 1.5 m including some gaseous H2O and CO2 bands, as well as part of the SO2 absorption band and the core of the UV absorption. We used the NEMESIS radiative transfer code and retrieval suite to investigate the vertical distribution of particles in the Equatorial atmosphere and to retrieve the imaginary refractive indices of the UV-absorber, assumed to be well mixed with Venus' small mode-1 particles. The results show an homogeneous Equatorial atmosphere, with cloud tops at 75+\/-2 km above surface. The UV absorption is found to be centered at 0.34+\/-0.03 m with a full width half maximum of 0.14+\/-0.01 m. Our values are compared with previous candidates for the UV aerosol absorber, among which disulfur oxide and dioxide disulfur provide the best agreement with our results.",0.1],["the ubiquitous biomacromolecule DNA has an axial rigidity persistence length of","Double Helical Conformation and Extreme Rigidity in a Rodlike Polyelectrolyte","summarize: The ubiquitous biomacromolecule DNA has an axial rigidity persistence length of ~50 nm, driven by its elegant double helical structure. While double and multiple helix structures appear widely in nature, only rarely are these found in synthetic non-chiral macromolecules. Here we describe a double helical conformation in the densely charged aromatic polyamide poly or PBDT. This double helix macromolecule represents one of the most rigid simple molecular structures known, exhibiting an extremely high axial persistence length . We present X-ray diffraction, NMR spectroscopy, and molecular dynamics simulations that reveal and confirm the double helical conformation. The discovery of this extreme rigidity in combination with high charge density gives insight into the self-assembly of molecular ionic composites with high mechanical modulus yet with liquid-like ion motions inside, and provides fodder for formation of new 1D-reinforced composites.",0.0909090909],["a main limitation of the computing paradigm is the low throughput induced by the intrinsic serial","Stochastic Computing with Integrated Optics","summarize: Stochastic computing allows reducing hardware complexity and improving energy efficiency of error resilient applications. However, a main limitation of the computing paradigm is the low throughput induced by the intrinsic serial computing of bit-streams. In this paper, we address the implementation of SC in the optical domain, with the aim to improve the computation speed. We implement a generic optical architecture allowing the execution of polynomial functions. We propose design methods to explore the design space in order to optimize key metrics such as circuit robustness and power consumption. We show that a circuit implementing a 2 nd order polynomial degree function and operating at 1Ghz leads to 20.1pJ laser consumption per computed bit.",0.0740740741],["the stability of iterations of affine linear maps of affine linear maps","Stability of perpetuities in Markovian environment","summarize: The stability of iterations of affine linear maps ",0.125],["empathetic interaction is already challenging in in-person communication. but computer-mediated","Computer-mediated Empathy","summarize: While novel social networks and emerging technologies help us transcend the spatial and temporal constraints inherent to in-person communication, the trade-off is a loss of natural expressivity. While empathetic interaction is already challenging in in-person communication, computer-mediated communication makes such empathetically rich communication even more difficult. Are technology and intelligent systems opportunities or threats to more empathic interpersonal communication? Realizing empathy is suggested not only as a way to communicate with others but also to design products for users and facilitate creativity. In this position paper, I suggest a framework to breakdown empathy, introduce each element, and show how computing, technologies, and algorithms can support certain elements of the empathy framework.",0.0],["the star V2775 Ori is a very young, pre-main sequence object","The ALMA Early Science view of FUor\/EXor objects. I. Through the looking-glass of V2775 Ori","summarize: As part of an ALMA survey to study the origin of episodic accretion in young eruptive variables, we have observed the circumstellar environment of the star V2775 Ori. This object is a very young, pre-main sequence object which displays a large amplitude outburst characteristic of the FUor class. We present Cycle-2 band 6 observations of V2775 Ori with a continuum and CO isotopologue resolution of 0.25\\as . We report the detection of a marginally resolved circumstellar disc in the ALMA continuum with an integrated flux of ",0.3208356593],["system identification is a major advancement in the evolution of engineering. it is by default the","Investigating Data-driven systems as digital twins: Numerical behavior of Ho-Kalman method for order estimation","summarize: System identification has been a major advancement in the evolution of engineering. As it is by default the first step towards a significant set of adaptive control techniques, it is imperative for engineers to apply it in order to practice control. Given that system identification could be useful in creating a digital twin, this work focuses on the initial stage of the procedure by discussing simplistic system order identification. Through specific numerical examples, this study constitutes an investigation on the most \\natural method for estimating the order from responses in a convenient and seamless way in time-domain. The method itself, originally proposed by Ho and Kalman and utilizing linear algebra, is an intuitive tool retrieving information out of the data themselves. Finally, with the help of the limitations of the methods, the potential future outlook is discussed, under the prism of forming a digital twin.",0.4074074074],["c","Detection of Gas Molecule using C","summarize: C",0.006737947],["the machine learning method is used instead of density function theory method. we then trained an","Accelerating inverse crystal structure prediction by machine learning: a case study of carbon allotropes","summarize: Based on structure prediction method, the machine learning method is used instead of the density function theory method to predict the material properties, thereby accelerating the material search process. In this paper, we established a data set of carbon materials by high-throughput calculation with available carbon structures obtained from the Samara Carbon Allotrope Database. We then trained an ML model that specifically predicts the elastic modulus and confirmed that the accuracy is better than that of AFLOW-ML in predicting the elastic modulus of a carbon allotrope. We further combined our ML model with the CALYPSO code to search for new carbon structures with a high Young's modulus. A new carbon allotrope not included in the Samara Carbon Allotrope Database, named Cmcm-C24, which exhibits a hardness greater than 80 GPa, was firstly revealed. The Cmcm-C24 phase was identified as a semiconductor with a direct bandgap. The structural stability, elastic modulus, and electronic properties of the new carbon allotrope were systematically studied, and the obtained results demonstrate the feasibility of ML methods accelerating the material search process.",0.15],["a modified parametric bootstrap method is based on a Gaussian error model","Uncertainty Quantification Under Group Sparsity","summarize: Quantifying the uncertainty in penalized regression under group sparsity is an important open question. We establish, under a high-dimensional scaling, the asymptotic validity of a modified parametric bootstrap method for the group lasso, assuming a Gaussian error model and mild conditions on the design matrix and the true coefficients. Simulation of bootstrap samples provides simultaneous inferences on large groups of coefficients. Through extensive numerical comparisons, we demonstrate that our bootstrap method performs much better than popular competitors, highlighting its practical utility. The theoretical result is generalized to other block norm penalization and sub-Gaussian errors, which further broadens the potential applications.",0.3461538462],["the double descent risk curve was proposed to qualitatively describe the out-of-sample","Two models of double descent for weak features","summarize: The double descent risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares\/least norm predictor. Specifically, it is shown that the risk peaks when the number of features ",0.1428571429],["we shed new light on the textit of optimization problems arising in prediction error parameter","On the smoothness of nonlinear system identification","summarize: We shed new light on the \\textit of optimization problems arising in prediction error parameter estimation of linear and nonlinear systems. We show that for regions of the parameter space where the model is not contractive, the Lipschitz constant and ",0.2777777778],["the local droop control is a centralized dispatch based on optimal power flow","Experimental Validation of Feedback Optimization in Power Distribution Grids","summarize: We consider the problem of controlling the voltage of a distribution feeder using the reactive power capabilities of inverters. On a real distribution grid, we compare the local Volt\/VAr droop control recommended in recent grid codes, a centralized dispatch based on optimal power flow programming, and a feedback optimization controller that we propose. The local droop control yields suboptimal regulation, as predicted analytically. The OPF-based dispatch strategy requires an accurate grid model and measurement of all loads on the feeder in order to achieve proper voltage regulation. However, in the experiment, the OPF-based strategy violates voltage constraints due to inevitable model mismatch and uncertainties. Our proposed FO controller, on the other hand, satisfies the constraints and does not require load measurements or any grid state estimation. The only needed model knowledge is the sensitivity of the voltages with respect to reactive power, which can be obtained from data. As we show, an approximation of these sensitivities is also sufficient, which makes the approach essentially model-free, easy to tune, compatible with the current sensing and control infrastructure, and remarkably robust to measurement noise. We expect these properties to be fundamental features of FO for power systems and not specific to Volt\/VAr regulation or to distribution grids.",0.4],["the majority of the pore sizes are less than 50 nm. the most fluid","Modeling nanoconfinement effects using active learning","summarize: Predicting the spatial configuration of gas molecules in nanopores of shale formations is crucial for fluid flow forecasting and hydrocarbon reserves estimation. The key challenge in these tight formations is that the majority of the pore sizes are less than 50 nm. At this scale, the fluid properties are affected by nanoconfinement effects due to the increased fluid-solid interactions. For instance, gas adsorption to the pore walls could account for up to 85% of the total hydrocarbon volume in a tight reservoir. Although there are analytical solutions that describe this phenomenon for simple geometries, they are not suitable for describing realistic pores, where surface roughness and geometric anisotropy play important roles. To describe these, molecular dynamics simulations are used since they consider fluid-solid and fluid-fluid interactions at the molecular level. However, MD simulations are computationally expensive, and are not able to simulate scales larger than a few connected nanopores. We present a method for building and training physics-based deep learning surrogate models to carry out fast and accurate predictions of molecular configurations of gas inside nanopores. Since training deep learning models requires extensive databases that are computationally expensive to create, we employ active learning . AL reduces the overhead of creating comprehensive sets of high-fidelity data by determining where the model uncertainty is greatest, and running simulations on the fly to minimize it. The proposed workflow enables nanoconfinement effects to be rigorously considered at the mesoscale where complex connected sets of nanopores control key applications such as hydrocarbon recovery and CO2 sequestration.",0.0],["wind tunnel is a three-dimensional, cartesian geometry hydrodynamic simulation setup. we","Common Envelope Wind Tunnel: Coefficients of Drag and Accretion in a Simplified Context for Studying Flows Around Objects Embedded Within Stellar Envelopes","summarize: This paper examines the properties of flows around objects embedded within common envelopes in the simplified context of a wind tunnel. We establish characteristic relationships between key common envelope flow parameters like the Mach number and density scale height. Our wind tunnel is a three-dimensional, cartesian geometry hydrodynamic simulation setup that includes the gravity of the primary and secondary stars and allows us to study the coefficients of drag and accretion experienced by the embedded object. Accretion and drag lead to a transformation of an embedded object and its orbit during a common envelope phase. We present two suites of simulations spanning a range of density gradients and Mach numbers -- relevant for flow near the limb of a stellar envelope to the deep interior. In one suite, we adopt an ideal gas adiabatic exponent of ",0.0919698603],["paper presents multi-scale approach to predict paper hygro-mechanical behaviour.","Hygro-mechanical properties of paper fibrous networks through asymptotic homogenization and comparison with idealized models","summarize: This paper presents a multi-scale approach to predict the effective hygro-mechanical behaviour of paper sheets based on the properties of the underlying fibrous network. Despite the vast amount of literature on paper hygro-expansion, the functional dependence of the effective material properties on the micro-structural features remains yet unclear. In this work, a micro-structural model of the paper fibrous network is first developed by random deposition of the fibres within a planar region according to an orientation probability density function. Asymptotic homogenization is used to determine its effective properties numerically. Alternatively, two much more idealized micro-structural models are considered, one based on a periodic lattice structure with a regular network of perpendicular fibres and one based on the Voigt average. Despite their simplicity, they reproduce representative micro-structural features, such as the orientation anisotropy and network level hygro-elastic properties. These alternative models can be solved analytically, providing closed-form expressions that explicitly reveal the influence of the individual micro-scale parameters on the effective hygro-mechanical response. The trend predicted by the random network model is captured reasonably well by the two idealized models. The resulting hygro-mechanical properties are finally compared with experimental data reported in the literature, revealing an adequate quantitative agreement.",0.1564782057],["quantum field theory is known to exhibit an important number of fundamental physical features. this state could","The large-scale structure of vacuum","summarize: The vacuum state in quantum field theory is known to exhibit an important number of fundamental physical features. In this work we explore the possibility that this state could also present a non-trivial space-time structure on large scales. In particular, we will show that by imposing the renormalized vacuum energy-momentum tensor to be conserved and compatible with cosmological observations, the vacuum energy of sufficiently heavy fields behaves at late times as non-relativistic matter rather than as a cosmological constant. In this limit, the vacuum state supports perturbations whose speed of sound is negligible and accordingly allows the growth of structures in the vacuum energy itself. This large-scale structure of vacuum could seed the formation of galaxies and clusters very much in the same way as cold dark matter does.",0.05],["theorems are used by the u.s. and uk to describe","Thompson's theorem for compact operators and diagonals of unitary operators","summarize: As applications of Kadison's Pythageorean and carpenter's theorems, the Schur-Horn theorem, and Thompson's theorem, we obtain an extension of Thompsons theorem to compact operators and use these ideas to give a characterization of diagonals of unitary operators. Thompson's mysterious inequality concerning the last terms of the diagonal and singular value sequences plays a central role.",0.2727272727],["a concurrent pandemic of information has spread across the world. the pandemic","The COVID19 infodemic. The role and place of academics in science communication","summarize: As the COVID19 pandemic has spread across the world, a concurrent pandemic of information has spread with it. Deemed an infodemic by the World Health Organization, and described as an overabundance of information, some accurate, some not, that occurs during an epidemic, this proliferation of data, research and opinions provides both opportunities and challenges for academics. Academics and scientists have a key role to play in the solutions to the infodemic challenge: as educators, influences and communicators, even where their expertise and experience does not align precisely with the SARS-Cov2 virus and its impacts. Successful communication requires a better understanding of how the public seeks, understands and processes scientific information, however, in order to maximise the ways in which experts engage with traditional and social media and to make sure that such engagement does not add to confusion and misinformation alongside efforts to counter or challenge it. This paper will outline the key advantages to be had from greater engagement with COVID19 discussions, the popular channels through which such discussions take place and through which information is disseminated. It also warns against the common pitfalls those who choose to engage might encounter, whilst stressing that the disadvantages of doing so are far outweighed by the advantages such engagement offers.",0.2727272727],["the main focus of this work is the reconstruction of the signals.","Disjoint sparsity for signal separation and applications to hybrid inverse problems in medical imaging","summarize: The main focus of this work is the reconstruction of the signals ",0.0403086536],["Feynman's time-slicing construction approximates the path integral by","Path integrals, SUSY QM and the Atiyah-Singer index theorem for twisted Dirac","summarize: Feynman's time-slicing construction approximates the path integral by a product, determined by a partition of a finite time interval, of approximate propagators. This paper formulates general conditions to impose on a short-time approximation to the propagator in a general class of imaginary-time quantum mechanics on a Riemannian manifold which ensure these products converge. The limit defines a path integral which agrees pointwise with the heat kernel for a generalized Laplacian. The result is a rigorous construction of the propagator for supersymmetric quantum mechanics, with potential, as a path integral. Further, the class of Laplacians includes the square of the twisted Dirac operator, which corresponds to an extension of N=1\/2 supersymmetric quantum mechanics. General results on the rate of convergence of the approximate path integrals suffice in this case to derive the local version of the Atiyah-Singer index theorem.",0.1516326649],["harmonic functions associated to systems of stochastic differential equations of the form.","Regularity of solutions to anisotropic nonlocal equations","summarize: We study harmonic functions associated to systems of stochastic differential equations of the form ",0.2666666667],["Electride materials offer attractive physical properties due to their loosely bound electrons.","Mechanical, optoelectronic and transport properties of single-layer Ca2N and Sr2N electrides","summarize: Electride materials offer attractive physical properties due to their loosely bound electrons. Ca2N, an electride in the two-dimensional form was successfully recently synthesized. We conducted extensive first-principles calculations to explore the mechanical, electronic, optical and transport response of single-layer and free-standing Ca2N and Sr2N electrides to external strain. We show that Ca2N and Sr2N sheets present isotropic elastic properties with positive Poisson's ratios, however, they yield around 50% higher tensile strength along the zigzag direction as compared with armchair. We also showed that the strain has negligible effect on the conductivity of the materials; the current in the system reduces by less than 32% for the structure under ultimate uniaxial strain along the armchair direction. Compressive strain always increases the electronic transport in the systems due to stronger overlap of the atomic orbitals. Our results show that the optical spectra are anisotropic for light polarization parallel and perpendicular to the plane. Interband transition contributions along in-plane polarization are not negligible, by considering this effect the optical properties of Ca2N and Sr2N sheets in the low frequency regime significantly changed. The insight provided by this study can be useful for the future application of Ca2N and Sr2N in nanodevices.",0.1666666667],["the link uses an active telecommunication fiber network with parallel data traffic. it is equipped","First industrial-grade coherent fiber link for optical frequency standard dissemination","summarize: We report on a fully bi-directional 680~km fiber link connecting two cities for which the equipment, the set up and the characterization are managed for the first time by an industrial consortium. The link uses an active telecommunication fiber network with parallel data traffic and is equipped with three repeater laser stations and four remote double bi-directional Erbium-doped fiber amplifiers. We report a short term stability at 1~s integration time of ",0.25],["the automated generation of Japanese fonts is in high demand. a Japanese font requires over","Automatic Generation of Typographic Font from a Small Font Subset","summarize: This paper addresses the automatic generation of a typographic font from a subset of characters. Specifically, we use a subset of a typographic font to extrapolate additional characters. Consequently, we obtain a complete font containing a number of characters sufficient for daily use. The automated generation of Japanese fonts is in high demand because a Japanese font requires over 1,000 characters. Unfortunately, professional typographers create most fonts, resulting in significant financial and time investments for font generation. The proposed method can be a great aid for font creation because designers do not need to create the majority of the characters for a new font. The proposed method uses strokes from given samples for font generation. The strokes, from which we construct characters, are extracted by exploiting a character skeleton dataset. This study makes three main contributions: a novel method of extracting strokes from characters, which is applicable to both standard fonts and their variations; a fully automated approach for constructing characters; and a selection method for sample characters. We demonstrate our proposed method by generating 2,965 characters in 47 fonts. Objective and subjective evaluations verify that the generated characters are similar to handmade characters.",0.2307692308],["we define a simplicial complex called the non-kissing complex. we","Non-kissing and non-crossing complexes for locally gentle algebras","summarize: Starting from a locally gentle bound quiver, we define on the one hand a simplicial complex, called the non-kissing complex. On the other hand, we construct a punctured, marked, oriented surface with boundary, endowed with a pair of dual dissections. From those geometric data, we define two simplicial complexes: the accordion complex, and the slalom complex, generalizing work of A. Garver and T. McConville in the case of a disk. We show that all three simplicial complexes are isomorphic, and that they are pure and thin. In particular, there is a notion of mutation on their facets, akin to ",0.3125],["complexity of many of these systems poses accountability challenges. data provenance methods show much promise as","Decision Provenance: Harnessing data flow for accountable systems","summarize: Demand is growing for more accountability regarding the technological systems that increasingly occupy our world. However, the complexity of many of these systems - often systems-of-systems - poses accountability challenges. A key reason for this is because the details and nature of the information flows that interconnect and drive systems, which often occur across technical and organisational boundaries, tend to be invisible or opaque. This paper argues that data provenance methods show much promise as a technical means for increasing the transparency of these interconnected systems. Specifically, given the concerns regarding ever-increasing levels of automated and algorithmic decision-making, and so-called 'algorithmic systems' in general, we propose decision provenance as a concept showing much promise. Decision provenance entails using provenance methods to provide information exposing decision pipelines: chains of inputs to, the nature of, and the flow-on effects from the decisions and actions taken throughout systems. This paper introduces the concept of decision provenance, and takes an interdisciplinary exploration into its potential for assisting accountability in algorithmic systems. We argue that decision provenance can help facilitate oversight, audit, compliance, risk mitigation, and user empowerment, and we also indicate the implementation considerations and areas for research necessary for realising its vision. More generally, we make the case that considerations of data flow, and systems more broadly, are important to discussions of accountability, and complement the considerable attention already given to algorithmic specifics.",0.1111111111],["the paper examines the difference between the two. the results are known in the literature","Jarzynski's equality, fluctuation theorems, and variance reduction: Mathematical analysis and numerical algorithms","summarize: In this paper, we study Jarzynski's equality and fluctuation theorems for diffusion processes. While some of the results considered in the current work are known in the literature, we review and generalize these nonequilibrium theorems using mathematical arguments, therefore enabling further investigations in the mathematical community. On the numerical side, variance reduction approaches such as importance sampling method are studied in order to compute free energy differences based on Jarzynski's equality.",0.2777777778],["the convNet analyzes a pair of fixed and moving images. the conv","End-to-End Unsupervised Deformable Image Registration with a Convolutional Neural Network","summarize: In this work we propose a deep learning network for deformable image registration . The DIRNet consists of a convolutional neural network regressor, a spatial transformer, and a resampler. The ConvNet analyzes a pair of fixed and moving images and outputs parameters for the spatial transformer, which generates the displacement vector field that enables the resampler to warp the moving image to the fixed image. The DIRNet is trained end-to-end by unsupervised optimization of a similarity metric between input image pairs. A trained DIRNet can be applied to perform registration on unseen image pairs in one pass, thus non-iteratively. Evaluation was performed with registration of images of handwritten digits and cardiac cine MR scans . The results demonstrate that registration with DIRNet is as accurate as a conventional deformable image registration method with substantially shorter execution times.",0.3],["a molecule is a non-unit element of an integral domain having a unique","On the molecules of numerical semigroups, Puiseux monoids, and Puiseux algebras","summarize: A molecule is a nonzero non-unit element of an integral domain having a unique factorization into irreducibles . Here we study the molecules of Puiseux monoids as well as the molecules of their corresponding semigroup algebras, which we call Puiseux algebras. We begin by presenting, in the context of numerical semigroups, some results on the possible cardinalities of the sets of molecules and the sets of reducible molecules . Then we study the molecules in the more general context of Puiseux monoids. We construct infinitely many non-isomorphic atomic Puiseux monoids all whose molecules are atoms. In addition, we characterize the molecules of Puiseux monoids generated by rationals with prime denominators. Finally, we turn to investigate the molecules of Puiseux algebras. We provide a characterization of the molecules of the Puiseux algebras corresponding to root-closed Puiseux monoids. Then we use such a characterization to find an infinite class of Puiseux algebras with infinitely many non-associated reducible molecules.",0.5161290323],["only a small number of papers introduced the above-mentioned models in health Economics","Spatial Discrete Choice and Spatial Limited Dependent Variable Models: a review with an emphasis on the use in Regional Health Economics","summarize: Despite spatial econometrics is now considered a consolidated discipline, only in recent years we have experienced an increasing attention to the possibility of applying it to the field of discrete choices and limited dependent variable models. In particular, only a small number of papers introduced the above-mentioned models in Health Economics. The main purpose of the present paper is to review the different methodological solutions in spatial discrete choice models as they appeared in several applied fields by placing an emphasis on the health economics applications.",0.2401924429],["the geodesic flow on the unit tangent bundle to a 2-orbif","Which geodesic flows are left-handed ?","summarize: We prove that the geodesic flow on the unit tangent bundle to a hyperbolic 2-orbifold is left-handed if and only if the orbifold is a sphere with three conic points. As a consequence, on the unit tangent bundle to a 3-conic sphere, the lift of every finite collection of closed geodesics that is zero in integral homology is the binding of an open book decomposition.",0.2857142857],["the center for Planetary Science conducted 200 observations in the radio spectrum. the results of this","Hydrogen Line Observations of Cometary Spectra at 1420 Mhz","summarize: In 2016, the Center for Planetary Science proposed a hypothesis arguing a comet and\/or its hydrogen cloud were a strong candidate for the source of the Wow! Signal. From 27 November 2016 to 24 February 2017, the Center for Planetary Science conducted 200 observations in the radio spectrum to validate the hypothesis. The investigation discovered that comet 266\/P Christensen emitted a radio signal at 1420.25 MHz. The results of this investigation, therefore, conclude that cometary spectra are detectable at 1420 MHz and, more importantly, that the 1977 Wow! Signal was a natural phenomenon from a Solar System body.",0.0909090909],["approach for semi-automatic annotation of object instances mimics how most current datasets have been","Annotating Object Instances with a Polygon-RNN","summarize: We propose an approach for semi-automatic annotation of object instances. While most current methods treat object segmentation as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object. This allows a human annotator to interfere at any time and correct a vertex if needed, producing as accurate segmentation as desired by the annotator. We show that our approach speeds up the annotation process by a factor of 4.7 across all classes in Cityscapes, while achieving 78.4% agreement in IoU with original ground-truth, matching the typical agreement between human annotators. For cars, our speed-up factor is 7.3 for an agreement of 82.2%. We further show generalization capabilities of our approach to unseen datasets.",0.0],["current measures for evaluating text simplification focus on evaluating lexical text aspects.","Semantic Structural Evaluation for Text Simplification","summarize: Current measures for evaluating text simplification systems focus on evaluating lexical text aspects, neglecting its structural aspects. In this paper we propose the first measure to address structural aspects of text simplification, called SAMSA. It leverages recent advances in semantic parsing to assess simplification quality by decomposing the input based on its semantic structure and comparing it to the output. SAMSA provides a reference-less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA's substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.",0.1764705882],["fall detection is an important problem from both the health and machine learning perspective. a fall","Spatio-Temporal Adversarial Learning for Detecting Unseen Falls","summarize: Fall detection is an important problem from both the health and machine learning perspective. A fall can lead to severe injuries, long term impairments or even death in some cases. In terms of machine learning, it presents a severely class imbalance problem with very few or no training data for falls owing to the fact that falls occur rarely. In this paper, we take an alternate philosophy to detect falls in the absence of their training data, by training the classifier on only the normal activities and identifying a fall as an anomaly. To realize such a classifier, we use an adversarial learning framework, which comprises of a spatio-temporal autoencoder for reconstructing input video frames and a spatio-temporal convolution network to discriminate them against original video frames. 3D convolutions are used to learn spatial and temporal features from the input video frames. The adversarial learning of the spatio-temporal autoencoder will enable reconstructing the normal activities of daily living efficiently; thus, rendering detecting unseen falls plausible within this framework. We tested the performance of the proposed framework on camera sensing modalities that may preserve an individual's privacy , such as thermal and depth camera. Our results on three publicly available datasets show that the proposed spatio-temporal adversarial framework performed better than other baseline frame based adversarial learning methods.",0.2142857143],["ed miliband says the call for fair play for machines is an early and often","Value Alignment, Fair Play, and the Rights of Service Robots","summarize: Ethics and safety research in artificial intelligence is increasingly framed in terms of alignment with human values and interests. I argue that Turing's call for fair play for machines is an early and often overlooked contribution to the alignment literature. Turing's appeal to fair play suggests a need to correct human behavior to accommodate our machines, a surprising inversion of how value alignment is treated today. Reflections on fair play motivate a novel interpretation of Turing's notorious imitation game as a condition not of intelligence but instead of value alignment: a machine demonstrates a minimal degree of alignment when it can go undetected when interrogated by a human. I carefully distinguish this interpretation from the Moral Turing Test, which is not motivated by a principle of fair play, but instead depends on imitation of human moral behavior. Finally, I consider how the framework of fair play can be used to situate the debate over robot rights within the alignment literature. I argue that extending rights to service robots operating in public spaces is fair in precisely the sense that it encourages an alignment of interests between humans and machines.",0.15],["MAP perturbation models provide a way to efficiently sample from the Gibbs distribution","Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time","summarize: MAP perturbation models have emerged as a powerful framework for inference in structured prediction. Such models provide a way to efficiently sample from the Gibbs distribution and facilitate predictions that are robust to random noise. In this paper, we propose a provably polynomial time randomized algorithm for learning the parameters of perturbed MAP predictors. Our approach is based on minimizing a novel Rademacher-based generalization bound on the expected loss of a perturbed MAP predictor, which can be computed in polynomial time. We obtain conditions under which our randomized learning algorithm can guarantee generalization to unseen examples.",0.25],["the Kremer-Grest polymer model is a standard model for studying generic polymer","Kremer-Grest models for commodity polymer melts: Linking theory, experiment and simulation at the Kuhn scale","summarize: The Kremer-Grest polymer model is a standard model for studying generic polymer properties in Molecular Dynamics simulations. It owes its popularity to its simplicity and computational efficiency, rather than its ability to represent specific polymers species and conditions. Here we show, that by tuning the chain stiffness it is possible to adapt the KG model to model melts of real polymers. In particular, we provide mapping relations from KG to SI units for a wide range of commodity polymers. The connection between the experimental and the KG melts is made at the Kuhn scale, i.e. at the crossover from chemistry-specific small scale to the universal large scale behavior. We expect Kuhn scale-mapped KG models to faithfully represent universal properties dominated by the large scale conformational statistics and dynamics of flexible polymers. In particular, we observe very good agreement between entanglement moduli of our KG models and the experimental moduli of the target polymers.",0.4326671017],["we show that any type of a person is a sexy person.","Singular MASAs in type III factors and Connes' Bicentralizer Property","summarize: We show that any type ",0.45],["Gamma-ray bursts were confirmed to be of extragalactic origin","Testing the anisotropy in the angular distribution of ","summarize: Gamma-ray bursts were confirmed to be of extragalactic origin due to their isotropic angular distribution, combined with the fact that they exhibited an intensity distribution that deviated strongly from the ",0.1111111111],["the computational bottleneck is because of the similarity between a read and candidate locations in that","GateKeeper: A New Hardware Architecture for Accelerating Pre-Alignment in DNA Short Read Mapping","summarize: Motivation: High throughput DNA sequencing technologies generate an excessive number of small DNA segments -- called short reads -- that cause significant computational burden. To analyze the entire genome, each of the billions of short reads must be mapped to a reference genome based on the similarity between a read and candidate locations in that reference genome. The similarity measurement, called alignment, formulated as an approximate string matching problem, is the computational bottleneck because: it is implemented using quadratic-time dynamic programming algorithms, and the majority of candidate locations in the reference genome do not align with a given read due to high dissimilarity. Calculating the alignment of such incorrect candidate locations consumes an overwhelming majority of a modern read mapper's execution time. Therefore, it is crucial to develop a fast and effective filter that can detect incorrect candidate locations and eliminate them before invoking computationally costly alignment operations. Results: We propose GateKeeper, a new hardware accelerator that functions as a pre-alignment step that quickly filters out most incorrect candidate locations. GateKeeper is the first design to accelerate pre-alignment using Field-Programmable Gate Arrays , which can perform pre-alignment much faster than software. GateKeeper can be integrated with any mapper that performs sequence alignment for verification. When implemented on a single FPGA chip, GateKeeper maintains high accuracy while providing up to 90-fold and 130-fold speedup over the state-of-the-art software pre-alignment techniques, Adjacency Filter and Shifted Hamming Distance , respectively. The addition of GateKeeper as a pre-alignment step can reduce the verification time of the mrFAST mapper by a factor of 10. Availability: https:\/\/github.com\/BilkentCompGen\/GateKeeper",0.2413793103],["customer lifetime value prediction system deployed at ASOS.com. system provides daily estimates of","Customer Lifetime Value Prediction Using Embeddings","summarize: We describe the Customer LifeTime Value prediction system deployed at ASOS.com, a global online fashion retailer. CLTV prediction is an important problem in e-commerce where an accurate estimate of future value allows retailers to effectively allocate marketing spend, identify and nurture high value customers and mitigate exposure to losses. The system at ASOS provides daily estimates of the future value of every customer and is one of the cornerstones of the personalised shopping experience. The state of the art in this domain uses large numbers of handcrafted features and ensemble regressors to forecast value, predict churn and evaluate customer loyalty. Recently, domains including language, vision and speech have shown dramatic advances by replacing handcrafted features with features that are learned automatically from data. We detail the system deployed at ASOS and show that learning feature representations is a promising extension to the state of the art in CLTV modelling. We propose a novel way to generate embeddings of customers, which addresses the issue of the ever changing product catalogue and obtain a significant improvement over an exhaustive set of handcrafted features.",0.0],["gait data are collected using color sensors, such as a CCD camera, depth sensors","Robust Gait Recognition by Integrating Inertial and RGBD Sensors","summarize: Gait has been considered as a promising and unique biometric for person identification. Traditionally, gait data are collected using either color sensors, such as a CCD camera, depth sensors, such as a Microsoft Kinect, or inertial sensors, such as an accelerometer. However, a single type of sensors may only capture part of the dynamic gait features and make the gait recognition sensitive to complex covariate conditions, leading to fragile gait-based person identification systems. In this paper, we propose to combine all three types of sensors for gait data collection and gait recognition, which can be used for important identification applications, such as identity recognition to access a restricted building or area. We propose two new algorithms, namely EigenGait and TrajGait, to extract gait features from the inertial data and the RGBD data, respectively. Specifically, EigenGait extracts general gait dynamics from the accelerometer readings in the eigenspace and TrajGait extracts more detailed sub-dynamics by analyzing 3D dense trajectories. Finally, both extracted features are fed into a supervised classifier for gait recognition and person identification. Experiments on 50 subjects, with comparisons to several other state-of-the-art gait-recognition approaches, show that the proposed approach can achieve higher recognition accuracy and robustness.",0.1818181818],["research has opened up new directions of research for identification and containment of fake news. research","Combating Fake News: A Survey on Identification and Mitigation Techniques","summarize: The proliferation of fake news on social media has opened up new directions of research for timely identification and containment of fake news, and mitigation of its widespread impact on public opinion. While much of the earlier research was focused on identification of fake news based on its contents or by exploiting users' engagements with the news on social media, there has been a rising interest in proactive intervention strategies to counter the spread of misinformation and its impact on society. In this survey, we describe the modern-day problem of fake news and, in particular, highlight the technical challenges associated with it. We discuss existing methods and techniques applicable to both identification and mitigation, with a focus on the significant advances in each method and their advantages and limitations. In addition, research has often been limited by the quality of existing datasets and their specific application contexts. To alleviate this problem, we comprehensively compile and summarize characteristic features of available datasets. Furthermore, we outline new directions of research to facilitate future development of effective and interdisciplinary solutions.",0.04],["this paper reflects on a number of trends towards a more open and reproducible approach","Opening practice: supporting Reproducibility and Critical spatial data science","summarize: This paper reflects on a number of trends towards a more open and reproducible approach to geographic and spatial data science over recent years. In particular it considers trends towards Big Data, and the impacts this is having on spatial data analysis and modelling. It identifies a turn in academia towards coding as a core analytic tool, and away from proprietary software tools offering 'black boxes' where the internal workings of the analysis are not revealed. It is argued that this closed form software is problematic, and considers a number of ways in which issues identified in spatial data analysis could be overlooked when working with closed tools, leading to problems of interpretation and possibly inappropriate actions and policies based on these. In addition, this paper and considers the role that reproducible and open spatial science may play in such an approach, taking into account the issues raised. It highlights the dangers of failing to account for the geographical properties of data, now that all data are spatial , the problems of a desire for n=all observations in data science and it identifies the need for a critical approach. This is one in which openness, transparency, sharing and reproducibility provide a mantra for defensible and robust spatial data science.",0.5555555556],["heuristic based approaches have been proposed in literature. a new approach for","QoS constrained Large Scale Web Service Composition using Abstraction Refinement","summarize: Efficient service composition in real time while providing necessary Quality of Service guarantees has been a challenging research problem with ever growing complexity. Several heuristic based approaches with diverse proposals for taming the scale and complexity of web service composition, have been proposed in literature. In this paper, we present a new approach for efficient service composition based on abstraction refinement. Instead of considering individual services during composition, we propose several abstractions to form service groups and the composition is done on these abstract services. Abstraction reduces the search space significantly and thereby can be done reasonably fast. While this can expedite solution construction to a great extent, this also entails a possibility that it may fail to generate any solution satisfying the QoS constraints, though the individual services construct a valid solution. Hence, we propose to refine an abstraction to generate the composite solution with desired QoS values. A QoS satisfying solution, if one exists, can be constructed with multiple iterations of abstraction refinement. While in the worst case, this approach may end up exploring the complete composition graph constructed on individual services, on an average, the solution can be achieved on the abstract graph. The abstraction refinement techniques give a significant speed-up compared to the traditional composition techniques. Experimental results on real benchmarks show the efficiency of our proposed mechanism in terms of time and the number of services considered for composition.",0.25],["the Wiener-Hopf equation for this case is derived. it involves two","Diffraction by a quarter-plane. Analytical continuation of spectral functions","summarize: The problem of diffraction by a Dirichlet quarter-plane in a 3D space is studied. The Wiener-Hopf equation for this case is derived and involves two unknown functions depending on two complex variables. The aim of the present work is to build an analytical continuation of these functions onto a well-described Riemann manifold and to study their behaviour and singularities on this manifold. In order to do so, integral formulae for analytical continuation of the spectral functions are derived and used. It is shown that the Wiener-Hopf problem can be reformulated using the concept of additive crossing of branch lines introduced in the paper. Both the integral formulae and the additive crossing reformulation are novel and represent the main results of this work.",0.0],["paper considers extractive summarisation in a comparative setting. goal is to","Comparative Document Summarisation via Classification","summarize: This paper considers extractive summarisation in a comparative setting: given two or more document groups , the goal is to select a small number of documents that are representative of each group, and also maximally distinguishable from other groups. We formulate a set of new objective functions for this problem that connect recent literature on document summarisation, interpretable machine learning, and data subset selection. In particular, by casting the problem as a binary classification amongst different groups, we derive objectives based on the notion of maximum mean discrepancy, as well as a simple yet effective gradient-based optimisation strategy. Our new formulation allows scalable evaluations of comparative summarisation as a classification task, both automatically and via crowd-sourcing. To this end, we evaluate comparative summarisation methods on a newly curated collection of controversial news topics over 13 months. We observe that gradient-based optimisation outperforms discrete and baseline approaches in 14 out of 24 different automatic evaluation settings. In crowd-sourced evaluations, summaries from gradient optimisation elicit 7% more accurate classification from human workers than discrete optimisation. Our result contrasts with recent literature on submodular data subset selection that favours discrete optimisation. We posit that our formulation of comparative summarisation will prove useful in a diverse range of use cases such as comparing content sources, authors, related topics, or distinct view points.",0.4],["rheological model based on real parameter.","A generalization of the Becker model in linear viscoelasticity: Creep, relaxation and internal friction","summarize: We present a new rheological model depending on a real parameter ",0.0878657127],["a quantum dot is modeled using an infinite potential well and a two-dimensional","Energy levels in a single-electron quantum dot with hydrostatic pressure","summarize: In this article we present a study of the effects of hydrostatic pressure on the energy levels of a quantum dot with an electron. A quantum dot is modeled using an infinite potential well and a two-dimensional harmonic oscillator and solved through the formalism of second quantization. A scheme for the implementation of a quantum NOT gate controlled with hydrostatic pressure is proposed.",0.3333333333],["algorithms for strain tomography from energy-resolved neutron transmission measurements have been","Neutron Transmission Strain Tomography for Non-Constant Stress-Free Lattice Spacing","summarize: Recently, several algorithms for strain tomography from energy-resolved neutron transmission measurements have been proposed. These methods assume that the stress-free lattice spacing ",0.0909090909],["an industrial indoor environment is harsh for wireless communications compared to an office environment. the prevalent","An efficient genetic algorithm for large-scale planning of robust industrial wireless networks","summarize: An industrial indoor environment is harsh for wireless communications compared to an office environment, because the prevalent metal easily causes shadowing effects and affects the availability of an industrial wireless local area network . On the one hand, it is costly, time-consuming, and ineffective to perform trial-and-error manual deployment of wireless nodes. On the other hand, the existing wireless planning tools only focus on office environments such that it is hard to plan IWLANs due to the larger problem size and the deployed IWLANs are vulnerable to prevalent shadowing effects in harsh industrial indoor environments. To fill this gap, this paper proposes an overdimensioning model and a genetic algorithm based over-dimensioning algorithm for deploying large-scale robust IWLANs. As a progress beyond the state-of-the-art wireless planning, two full coverage layers are created. The second coverage layer serves as redundancy in case of shadowing. Meanwhile, the deployment cost is reduced by minimizing the number of access points ; the hard constraint of minimal inter-AP spatial paration avoids multiple APs covering the same area to be simultaneously shadowed by the same obstacle. The computation time and occupied memory are dedicatedly considered in the design of GAOD for large-scale optimization. A greedy heuristic based over-dimensioning algorithm and a random OD algorithm are taken as benchmarks. In two vehicle manufacturers with a small and large indoor environment, GAOD outperformed GHOD with up to 20% less APs, while GHOD outputted up to 25% less APs than a random OD algorithm. Furthermore, the effectiveness of this model and GAOD was experimentally validated with a real deployment system.",0.25],["a covering alignment asks for two paths to be used as basis of sequence analysis.","Hardness of Covering Alignment: Phase Transition in Post-Sequence Genomics","summarize: Covering alignment problems arise from recent developments in genomics; so called pan-genome graphs are replacing reference genomes, and advances in haplotyping enable full content of diploid genomes to be used as basis of sequence analysis. In this paper, we show that the computational complexity will change for natural extensions of alignments to pan-genome representations and to diploid genomes. More broadly, our approach can also be seen as a minimal extension of sequence alignment to labelled directed acyclic graphs . Namely, we show that finding a \\emph of two labeled DAGs is NP-hard even on binary alphabets. A covering alignment asks for two paths ",0.2083333333],["game theoretic behavior prediction model achieves state of the art prediction accuracy. we evaluate","Multi-Fidelity Recursive Behavior Prediction","summarize: Predicting the behavior of surrounding vehicles is a critical problem in automated driving. We present a novel game theoretic behavior prediction model that achieves state of the art prediction accuracy by explicitly reasoning about possible future interaction between agents. We evaluate our approach on the NGSIM vehicle trajectory data set and demonstrate lower root mean square error than state-of-the-art methods.",0.0],["co-clustering is a method to identify rows and columns of a data matrix","Non-Exhaustive, Overlapping Co-Clustering: An Extended Analysis","summarize: The goal of co-clustering is to simultaneously identify a clustering of rows as well as columns of a two dimensional data matrix. A number of co-clustering techniques have been proposed including information-theoretic co-clustering and the minimum sum-squared residue co-clustering method. However, most existing co-clustering algorithms are designed to find pairwise disjoint and exhaustive co-clusters while many real-world datasets contain not only a large overlap between co-clusters but also outliers which should not belong to any co-cluster. In this paper, we formulate the problem of Non-Exhaustive, Overlapping Co-Clustering where both of the row and column clusters are allowed to overlap with each other and outliers for each dimension of the data matrix are not assigned to any cluster. To solve this problem, we propose intuitive objective functions, and develop an an efficient iterative algorithm which we call the NEO-CC algorithm. We theoretically show that the NEO-CC algorithm monotonically decreases the proposed objective functions. Experimental results show that the NEO-CC algorithm is able to effectively capture the underlying co-clustering structure of real-world data, and thus outperforms state-of-the-art clustering and co-clustering methods. This manuscript includes an extended analysis of .",0.3043478261],["direct nanoimprinting of crystalline metals is a simple and high-throughput","One-step fabrication of metal nanostructures by high-throughput imprinting","summarize: Direct nanoimprinting provides a simple and high-throughput route for producing uniform nanopatterns at great precision and at low costs. However, applying this technique to crystalline metals has been considered as impossible due to intrinsic limitation from grain size effect. Here we demonstrate direct superplastic nanoimprinting of crystalline metals well below their melting temperatures , generating ordered nanowire arrays with aspect ratio up to ~2000. Our investigations of replicating metal hierarchical nanostructures show the capability of imprinting features as small as 8 nm, far smaller than the grain size of bulk metals. Most surprisingly, the prepared metal hierarchical nanostructures were found possessing perfect monocrystalline structures. These findings indicate that nanoimprinting of crystalline metals below Tm might be from lattice diffusion. SPNI as a one-step and highly controlled high-throughput fabrication method, could facilitate the applications of metal nanostructures in bio-sensing, diagnostic imaging, catalysis, food industry and environmental conservation.",0.4285714286],["ARM big.LITTLE architecture is at the heart of prevalent commercial edge devices.","High-Throughput CNN Inference on Embedded ARM big.LITTLE Multi-Core Processors","summarize: IoT Edge intelligence requires Convolutional Neural Network inference to take place in the edge devices itself. ARM big.LITTLE architecture is at the heart of prevalent commercial edge devices. It comprises of single-ISA heterogeneous cores grouped into multiple homogeneous clusters that enable power and performance trade-offs. All cores are expected to be simultaneously employed in inference to attain maximal throughput. However, high communication overhead involved in parallelization of computations from convolution kernels across clusters is detrimental to throughput. We present an alternative framework called Pipe-it that employs pipelined design to split convolutional layers across clusters while limiting parallelization of their respective kernels to the assigned cluster. We develop a performance-prediction model that utilizes only the convolutional layer descriptors to predict the execution time of each layer individually on all permitted core configurations . Pipe-it then exploits the predictions to create a balanced pipeline using an efficient design space exploration algorithm. Pipe-it on average results in a 39% higher throughput than the highest antecedent throughput.",0.1666666667],["we prove the existence and uniqueness of curved traveling fronts. the ad","Curved fronts in a shear flow: case of combustion nonlinearities","summarize: We prove the existence and uniqueness, up to a shift in time, of curved traveling fronts for a reaction-advection-diffusion equation with a combustion-type nonlinearity. The advection is through a shear flow ",0.1428571429],["unsupervised near-duplicate detection has many practical applications. it involves running a","Benchmarking unsupervised near-duplicate image detection","summarize: Unsupervised near-duplicate detection has many practical applications ranging from social media analysis and web-scale retrieval, to digital image forensics. It entails running a threshold-limited query on a set of descriptors extracted from the images, with the goal of identifying all possible near-duplicates, while limiting the false positives due to visually similar images. Since the rate of false alarms grows with the dataset size, a very high specificity is thus required, up to ",0.3684210526],["proposed method is compared with other iterative regularization methods. it is used to","The Averaged Kaczmarz Iteration for Solving Inverse Problems","summarize: We introduce a new iterative regularization method for solving inverse problems that can be written as systems of linear or non-linear equations in Hilbert spaces. The proposed averaged Kaczmarz method can be seen as a hybrid method between the Landweber and the Kaczmarz method. As the Kaczmarz method, the proposed method only requires evaluation of one direct and one adjoint sub-problem per iterative update. On the other, similar to the Landweber iteration, it uses an average over previous auxiliary iterates which increases stability. We present a convergence analysis of the AVEK iteration. Further, detailed numerical studies are presented for a tomographic image reconstruction problem, namely the limited data problem in photoacoustic tomography. Thereby, the AVEK is compared with other iterative regularization methods including standard Landweber and Kaczmarz iterations, as well as recently proposed accelerated versions based on error minimizing relaxation strategies.",0.0],["in this paper, we prove a method of linearization of a nonlinear equation","A linearized stability theorem for nonlinear delay fractional differential equations","summarize: In this paper, we prove a theorem of linearized asymptotic stability for fractional differential equations with a time delay. More precisely, using the method of linearization of a nonlinear equation along an orbit , we show that an equilibrium of a nonlinear Caputo fractional differential equation with a time delay is asymptotically stable if its linearization at the equilibrium is asymptotically stable. Our approach based on a technique which converts the linear part of the equation into a diagonal one. Then using properties of generalized Mittag-Leffler functions, the construction of an associated Lyapunov--Perron operator and the Banach contraction mapping theorem, we obtain the desired result.",0.6206896552],["the approach is validated with respect to compression rate and storage requirement. it is observed that","On identification of self-similar characteristics using the Tensor Train decomposition method with application to channel turbulence flow","summarize: A study on the application of the Tensor Train decomposition method to 3D direct numerical simulation data of channel turbulence flow is presented. The approach is validated with respect to compression rate and storage requirement. In tests with synthetic data, it is found that grid-aligned self-similar patterns are well captured, and also the application to non grid-aligned self-similarity yields satisfying results. It is observed that the shape of the input Tensor significantly affects the compression rate. Applied to data of channel turbulent flow, the Tensor Train format allows for surprisingly high compression rates whilst ensuring low relative errors.",0.328794572],["a series of experiments have been carried out in the past. we compared and","A Comparative Study on Transformer vs RNN in Speech Applications","summarize: Sequence-to-sequence models have been widely used in end-to-end speech processing, for example, automatic speech recognition , speech translation , and text-to-speech . This paper focuses on an emergent sequence-to-sequence model called Transformer, which achieves state-of-the-art performance in neural machine translation and other natural language processing applications. We undertook intensive studies in which we experimentally compared and analyzed Transformer and conventional recurrent neural networks in a total of 15 ASR, one multilingual ASR, one ST, and two TTS benchmarks. Our experiments revealed various training tips and significant performance benefits obtained with Transformer for each task including the surprising superiority of Transformer in 13\/15 ASR benchmarks in comparison with RNN. We are preparing to release Kaldi-style reproducible recipes using open source and publicly available datasets for all the ASR, ST, and TTS tasks for the community to succeed our exciting outcomes.",0.2631578947],["screening methods are often used as a preprocessing step for reducing variables used by","Are screening methods useful in feature selection? An empirical study","summarize: Filter or screening methods are often used as a preprocessing step for reducing the number of variables used by a learning algorithm in obtaining a classification or regression model. While there are many such filter methods, there is a need for an objective evaluation of these methods. Such an evaluation is needed to compare them with each other and also to answer whether they are at all useful, or a learning algorithm could do a better job without them. For this purpose, many popular screening methods are partnered in this paper with three regression learners and five classification learners and evaluated on ten real datasets to obtain accuracy criteria such as R-square and area under the ROC curve . The obtained results are compared through curve plots and comparison tables in order to find out whether screening methods help improve the performance of learning algorithms and how they fare with each other. Our findings revealed that the screening methods were useful in improving the prediction of the best learner on two regression and two classification datasets out of the ten datasets evaluated.",0.2],["magnetic van der Waals heterostructures of mn exhibit controllable magnetic properties while","Natural van der Waals heterostructural single crystals with both magnetic and topological properties","summarize: Heterostructures having both magnetism and topology are promising materials for the realization of exotic topological quantum states while challenging in synthesis and engineering. Here, we report natural magnetic van der Waals heterostructures of mn that exhibit controllable magnetic properties while maintaining their topological surface states. The interlayer antiferromagnetic exchange coupling is gradually weakened as the separation of magnetic layers increases, and an anomalous Hall effect that is well coupled with magnetization and shows ferromagnetic hysteresis was observed below 5 K. The obtained homogeneous heterostructure with atomically sharp interface and intrinsic magnetic properties will be an ideal platform for studying the quantum anomalous Hall effect, axion insulator states, and the topological magnetoelectric effect.",0.3943047491],["the near-sensor hardware implementation of LSTM is challenged due to large parallelism and","Design of CMOS-memristor Circuits for LSTM architecture","summarize: Long Short-Term memory architecture is a well-known approach for building recurrent neural networks useful in sequential processing of data in application to natural language processing. The near-sensor hardware implementation of LSTM is challenged due to large parallelism and complexity. We propose a 0.18 m CMOS, GST memristor LSTM hardware architecture for near-sensor processing. The proposed system is validated in a forecasting problem based on Keras model.",0.2857142857],["radar search pattern optimization can be approximated as a set cover problem. the algorithm is","Branch-and-Bound Method for Just-in-Time Optimization of Radar Search Patterns","summarize: Electronic phased-array radars offer new possibilities for radar search pattern optimization by using bi-dimensional beam-forming and beam-steering. Radar search pattern optimization can be approximated as a set cover problem and solved using integer programming, while accounting for localized clutter and terrain masks in detection constraints. We present a set cover problem approximation for time-budget minimization of radar search patterns, under constraints of range, detection probability and direction-specific scan update rates. Branch\\&Bound is a classical optimization procedure for solving combinatorial problems. It is known mainly as an exact algorithm, but features interesting characteristics, making it particularly fit for solving optimization problems in real-time applications and producing just-in-time solutions.",0.28],["we consider the H'enon problem in the unit disc with Dirichlet boundary conditions","The H\\'enon problem with large exponent in the disc","summarize: In this paper we consider the H\\'enon problem in the unit disc with Dirichlet boundary conditions. We study the asymptotic profile of least energy and nodal least energy radial solutions and then deduce the exact computation of their Morse index for large values of the exponent p. As a consequence of this computation a multiplicity result for positive and nodal solutions is obtained.",0.4],["stress tensor is adapted to the local flow conditions. it includes extensional","A theoretical framework for steady-state rheometry in generic flow conditions","summarize: We introduce a general decomposition of the stress tensor for incompressible fluids in terms of its components on a tensorial basis adapted to the local flow conditions, which include extensional flows, simple shear flows, and any type of mixed flows. Such a basis is determined solely by the symmetric part of the velocity gradient and allows for a straightforward interpretation of the non-Newtonian response in any local flow conditions. In steady homogeneous flows, the material functions that represent the components of the stress on the adapted basis generalize and complete the classical set of viscometric functions used to characterize the response in simple shear flows. Such a general decomposition of the stress is effective in coherently organizing and interpreting rheological data from laboratory measurements and computational studies in non-viscometric steady flows of great importance for practical applications. The decomposition of the stress in terms with clearly distinct roles is also useful in developing constitutive models.",0.2307692308],["bounded stochastic processes can be induced by overdamped approximation","Boundedness vs Unboundedness of A Noise Linked to Tsallis q-Statistics: The Role of The Overdamped Approximation","summarize: An apparently ideal way to generate continuous bounded stochastic processes is to consider the stochastically perturbed motion of a point of small mass in an infinite potential well, under overdamped approximation. Here, however, we show that the aforementioned procedure can be fallacious and lead to incorrect results. We indeed provide a counter-example concerning one of the most employed bounded noises, hereafter called Tsallis-Stariolo-Borland noise, which admits the well known Tsallis q-statistics as stationary density. In fact, we show that for negative values of the Tsallis parameter q , the motion resulting from the overdamped approximation is unbounded. We then investigate the cause of the failure of Kramers first type approximation, and we formally show that the solutions of the full Newtonian non-approximated model are bounded, following the physical intuition. Finally, we provide a new family of bounded noises extending the TSB noise, the boundedness of whose solutions we formally show.",0.0510473138],["churn prediction is a challenge common to a variety of sectors. it","Games and Big Data: A Scalable Multi-Dimensional Churn Prediction Model","summarize: The emergence of mobile games has caused a paradigm shift in the video-game industry. Game developers now have at their disposal a plethora of information on their players, and thus can take advantage of reliable models that can accurately predict player behavior and scale to huge datasets. Churn prediction, a challenge common to a variety of sectors, is particularly relevant for the mobile game industry, as player retention is crucial for the successful monetization of a game. In this article, we present an approach to predicting game abandon based on survival ensembles. Our method provides accurate predictions on both the level at which each player will leave the game and their accumulated playtime until that moment. Further, it is robust to different data distributions and applicable to a wide range of response variables, while also allowing for efficient parallelization of the algorithm. This makes our model well suited to perform real-time analyses of churners, even for games with millions of daily active users.",0.4210526316],["spectral analysis of Chandra\/HETGS and NuSTAR observations. X","Relativistic components of the Ultra-Fast Outflow in the Quasar PDS 456 from Chandra\/HETGS, NuSTAR and XMM-Newton observations","summarize: We present the spectral analysis of Chandra\/HETGS and NuSTAR observations of the quasar PDS 456 from 2015, and XMM-Newton and NuSTAR archival data from 2013-2014, together with Chandra\/HETGS data from 2003. We analyzed these three different epochs in a consistent way, looking for absorption features corresponding to highly ionized blueshifted absorption lines from H-like and He-like ions of iron , as well as of other elements in the soft band. We confirm the presence of a persistent ultra-fast outflow with a velocity of v_out=-0.24 - -0.29c, previously detected. We also report the detection of an additional faster component of the UFO with a relativistic velocity of v_out=-0.48c. We implemented photoionization modeling, using XSTAR analytic model warmabs, to characterize the physical properties of the different kinematic components of the ultra-fast outflow and of the partial covering absorber detected in PDS 456. These two relativistic components of the ultra-fast outflow observed in the three epochs analyzed in this paper are powerful enough to impact the host galaxy of PDS 456 through AGN feedback.",0.2164349782],["autonomous, vision-based flight brings up fundamental challenges in robotics. our approach combines","Deep Drone Racing: Learning Agile Flight in Dynamic Environments","summarize: Autonomous agile flight brings up fundamental challenges in robotics, such as coping with unreliable state estimation, reacting optimally to dynamically changing environments, and coupling perception and action in real time under severe resource constraints. In this paper, we consider these challenges in the context of autonomous, vision-based drone racing in dynamic environments. Our approach combines a convolutional neural network with a state-of-the-art path-planning and control system. The CNN directly maps raw images into a robust representation in the form of a waypoint and desired speed. This information is then used by the planner to generate a short, minimum-jerk trajectory segment and corresponding motor commands to reach the desired goal. We demonstrate our method in autonomous agile flight scenarios, in which a vision-based quadrotor traverses drone-racing tracks with possibly moving gates. Our method does not require any explicit map of the environment and runs fully onboard. We extensively test the precision and robustness of the approach in simulation and in the physical world. We also evaluate our method against state-of-the-art navigation approaches and professional human drone pilots.",0.2142857143],["polarimetric analysis of white-light images makes use of polarized sequences","Coronal Photopolarimetry with the LASCO-C2 Coronagraph over 24 Years -- Application to the K\/F Separation and to the Determination of the Electron Density","summarize: We present an in-depth characterization of the polarimetric channel of the Large-Angle Spectrometric COronagraph LASCO-C2 onboard the Solar and Heliospheric Observatory . The polarimetric analysis of the white-light images makes use of polarized sequences composed of three images obtained though three polarizers oriented at +60, 0 and -60, complemented by a neighboring unpolarized image, and relies on the formalism of Mueller. The Mueller matrix characterizing the C2 instrument was obtained through extensive ground-based calibrations of the optical components and global laboratory tests. Additional critical corrections were derived from in-flight tests relying prominently on roll sequences and on consistency criteria, mainly the tangential direction of polarization. Our final results encompass the characterization of the polarization of the white-light corona, of its polarized radiance, of the two-dimensional electron density, and of the K-corona over two solar cycles. They are in excellent agreement with measurements obtained at several solar eclipses except for slight discrepancies affecting the innermost part of the C2 field-of-view, probably resulting from an imperfect removal of the bright diffraction fringe surrounding the occulter.",0.0454219655],["the time series data of the healthcare sector of india for the period January 2010 till December 2016 is","A Time Series Analysis-Based Forecasting Framework for the Indian Healthcare Sector","summarize: Designing efficient and robust algorithms for accurate prediction of stock market prices is one of the most exciting challenges in the field of time series analysis and forecasting. With the exponential rate of development and evolution of sophisticated algorithms and with the availability of fast computing platforms, it has now become possible to effectively and efficiently extract, store, process and analyze high volume of stock market data with diversity in its contents. Availability of complex algorithms which can execute very fast on parallel architecture over the cloud has made it possible to achieve higher accuracy in forecasting results while reducing the time required for computation. In this paper, we use the time series data of the healthcare sector of India for the period January 2010 till December 2016. We first demonstrate a decomposition approach of the time series and then illustrate how the decomposition results provide us with useful insights into the behavior and properties exhibited by the time series. Further, based on the structural analysis of the time series, we propose six different methods of forecasting for predicting the time series index of the healthcare sector. Extensive results are provided on the performance of the forecasting methods to demonstrate their effectiveness.",0.1851851852],["theorem describes the convergence of an infinite array of variants of SGD.","SGD: General Analysis and Improved Rates","summarize: We propose a general yet simple theorem describing the convergence of SGD under the arbitrary sampling paradigm. Our theorem describes the convergence of an infinite array of variants of SGD, each of which is associated with a specific probability law governing the data selection rule used to form mini-batches. This is the first time such an analysis is performed, and most of our variants of SGD were never explicitly considered in the literature before. Our analysis relies on the recently introduced notion of expected smoothness and does not rely on a uniform bound on the variance of the stochastic gradients. By specializing our theorem to different mini-batching strategies, such as sampling with replacement and independent sampling, we derive exact expressions for the stepsize as a function of the mini-batch size. With this we can also determine the mini-batch size that optimizes the total complexity, and show explicitly that as the variance of the stochastic gradient evaluated at the minimum grows, so does the optimal mini-batch size. For zero variance, the optimal mini-batch size is one. Moreover, we prove insightful stepsize-switching rules which describe when one should switch from a constant to a decreasing stepsize regime.",0.1],["we consider the Caputo fractional derivative and say that a function is Caputo","Local density of Caputo-stationary functions in the space of smooth functions","summarize: We consider the Caputo fractional derivative and say that a function is Caputo-stationary if its Caputo derivative is zero. We then prove that any ",0.3913043478],["the classifier is a significant goal in space situational awareness. the aim is to","Development of a High Fidelity Simulator for Generalised Photometric Based Space Object Classification using Machine Learning","summarize: This paper presents the initial stages in the development of a deep learning classifier for generalised Resident Space Object characterisation that combines high-fidelity simulated light curves with transfer learning to improve the performance of object characterisation models that are trained on real data. The classification and characterisation of RSOs is a significant goal in Space Situational Awareness in order to improve the accuracy of orbital predictions. The specific focus of this paper is the development of a high-fidelity simulation environment for generating realistic light curves. The simulator takes in a textured geometric model of an RSO as well as the objects ephemeris and uses Blender to generate photo-realistic images of the RSO that are then processed to extract the light curve. Simulated light curves have been compared with real light curves extracted from telescope imagery to provide validation for the simulation environment. Future work will involve further validation and the use of the simulator to generate a dataset of realistic light curves for the purpose of training neural networks.",0.4173856554],["fractional Fokker-Planck system with multiple internal states is derived in.","Numerical algorithm for the space-time fractional Fokker-Planck system with two internal states","summarize: The fractional Fokker-Planck system with multiple internal states is derived in , where the space derivative is Laplace operator. If the jump length distribution of the particles is power law instead of Gaussian, the space derivative should be replaced with fractional Laplacian. This paper focuses on solving the two state Fokker-Planck system with fractional Laplacian. We first provide a priori estimate for this system under different regularity assumptions on the initial data. Then we use ",0.5210104792],["a non-perturbative formulation leads to a vacuum created gravitational pair","Emergent D-instanton as a source of Dark Energy","summarize: We revisit a non-perturbative formulation leading to a vacuum created gravitational pair of -brane by a Poincare dual higher form U gauge theory on a D4 -brane. In particular, the analysis has revealed a dynamical geometric torsion H 3 for an on-shell Neveu-Schwarz form on a fat 4-brane. We argue that a D-instanton can be a viable candidate to incorporate the quintessence correction hidden to an emergent -dimensional brane universe. It is shown that a dynamical non-perturbative correction may be realized with an axionic scalar QFT on an emergent anti 3-brane within a gravitational pair. The theoretical tool provokes thought to believe for an extra instantaneous dimension transverse to our classical brane-universe in an emergent scenario. Interestingly a D-instanton correction, sourced by an axion on an anti 3-brane, may serve as a potential candidate to explain the accelerated rate of expansion of our 3-brane universe and may provide a clue to the origin of dark energy.",0.3],["Let us know what you think about it!","The hit problem of five variables in the generic degree and its application","summarize: Let ",0.0],["fusion performance degradation is directly affected by the different features and processing methods undertaken. a","Infrared and Visible Image Fusion with ResNet and zero-phase component analysis","summarize: Feature extraction and processing tasks play a key role in Image Fusion, and the fusion performance is directly affected by the different features and processing methods undertaken. By contrast, most of deep learning-based methods use deep features directly without feature extraction or processing. This leads to the fusion performance degradation in some cases. To solve these drawbacks, we propose a deep features and zero-phase component analysis based novel fusion framework is this paper. Firstly, the residual network is used to extract deep features from source images. Then ZCA is utilized to normalize the deep features and obtain initial weight maps. The final weight maps are obtained by employing a soft-max operation in association with the initial weight maps. Finally, the fused image is reconstructed using a weighted-averaging strategy. Compared with the existing fusion methods, experimental results demonstrate that the proposed framework achieves better performance in both objective assessment and visual quality. The code of our fusion algorithm is available at https:\/\/github.com\/hli1221\/imagefusion_resnet50",0.4090909091],["flowrate is determined directly using a gravimetric method. the flowrate is determined","Acoustic prediction of flowrate: varying liquid jet stream onto a free surface","summarize: Information on liquid jet stream flow is crucial in many real world applications. In a large number of cases, these flows fall directly onto free surfaces , creating a splash with accompanying splashing sounds. The sound produced is supplied by energy interactions between the liquid jet stream and the passive free surface. In this investigation, we collect the sound of a water jet of varying flowrate falling into a pool of water, and use this sound to predict the flowrate and flowrate trajectory involved. Two approaches are employed: one uses machine-learning models trained using audio features extracted from the collected sound to predict the flowrate . In contrast, the second method directly uses acoustic parameters related to the spectral energy of the liquid-liquid interaction to estimate the flowrate trajectory. The actual flowrate, however, is determined directly using a gravimetric method: tracking the change in mass of the pooling liquid over time. We show here that the two methods agree well with the actual flowrate and offer comparable performance in accurately predicting the flowrate trajectory, and accordingly offer insights for potential real-life applications using sound.",0.2857142857],["tech and automobile companies investing huge amounts of capital in research and development of SAVs.","Coverage based testing for V&V and Safety Assurance of Self-driving Autonomous Vehicles: A Systematic Literature Review","summarize: Self-driving Autonomous Vehicles are gaining more interest each passing day by the industry as well as the general public. Tech and automobile companies are investing huge amounts of capital in research and development of SAVs to make sure they have a head start in the SAV market in the future. One of the major hurdles in the way of SAVs making it to the public roads is the lack of confidence of public in the safety aspect of SAVs. In order to assure safety and provide confidence to the public in the safety of SAVs, researchers around the world have used coverage-based testing for Verification and Validation and safety assurance of SAVs. The objective of this paper is to investigate the coverage criteria proposed and coverage maximizing techniques used by researchers in the last decade up till now, to assure safety of SAVs. We conduct a Systematic Literature Review for this investigation in our paper. We present a classification of existing research based on the coverage criteria used. Several research gaps and research directions are also provided in this SLR to enable further research in this domain. This paper provides a body of knowledge in the domain of safety assurance of SAVs. We believe the results of this SLR will be helpful in the progression of V&V and safety assurance of SAVs.",0.26728771],["a new classification approach for EEG time series based on Recurrent Neural Networks","Deep Classification of Epileptic Signals","summarize: Electrophysiological observation plays a major role in epilepsy evaluation. However, human interpretation of brain signals is subjective and prone to misdiagnosis. Automating this process, especially seizure detection relying on scalp-based Electroencephalography and intracranial EEG, has been the focus of research over recent decades. Nevertheless, its numerous challenges have inhibited a definitive solution. Inspired by recent advances in deep learning, we propose a new classification approach for EEG time series based on Recurrent Neural Networks via the use of Long-Short Term Memory networks. The proposed deep network effectively learns and models discriminative temporal patterns from EEG sequential data. Especially, the features are automatically discovered from the raw EEG data without any pre-processing step, eliminating humans from laborious feature design task. We also show that, in the epilepsy scenario, simple architectures can achieve competitive performance. Using simple architectures significantly benefits in the practical scenario considering their low computation complexity and reduced requirement for large training datasets. Using a public dataset, a multi-fold cross-validation scheme exhibited an average validation accuracy of 95.54\\% and an average AUC of 0.9582 of the ROC curve among all sets defined in the experiment. This work reinforces the benefits of deep learning to be further attended in clinical applications and neuroscientific research.",0.2],["we develop an efficient and high order panel method with applications in airfoil design.","A general high order two-dimensional panel method","summarize: We develop an efficient and high order panel method with applications in airfoil design. Through the use of analytic work and careful considerations near singularities our approach is quadrature-free. The resulting method is examined with respect to accuracy and efficiency and we discuss the different trade-offs in approximation order and computational complexity. A reference implementation within a package for a two-dimensional fast multipole method is distributed freely.",0.3125],["a number of works have studied different aspects of drone package delivery service by a supplier","Joint Ground and Aerial Package Delivery Services: A Stochastic Optimization Approach","summarize: Unmanned aerial vehicles , also known as drones, have emerged as a promising mode of fast, energy-efficient, and cost-effective package delivery. A considerable number of works have studied different aspects of drone package delivery service by a supplier, one of which is delivery planning. However, existing works addressing the planning issues consider a simple case of perfect delivery without service interruption, e.g., due to accident which is common and realistic. Therefore, this paper introduces the joint ground and aerial delivery service optimization and planning framework. The framework explicitly incorporates uncertainty of drone package delivery, i.e., takeoff and breakdown conditions. The GADOP framework aims to minimize the total delivery cost given practical constraints, e.g., traveling distance limit. Specifically, we formulate the GADOP framework as a three-stage stochastic integer programming model. To deal with the high complexity issue of the problem, a decomposition method is adopted. Then, the performance of the GADOP framework is evaluated by using two data sets including Solomon benchmark suite and the real data from one of the Singapore logistics companies. The performance evaluation clearly shows that the GADOP framework can achieve significantly lower total payment than that of the baseline methods which do not take uncertainty into account.",0.4285714286],["a low-precision arithmetic representation can enable improved energy efficiency.","ProbLP: A framework for low-precision probabilistic inference","summarize: Bayesian reasoning is a powerful mechanism for probabilistic inference in smart edge-devices. During such inferences, a low-precision arithmetic representation can enable improved energy efficiency. However, its impact on inference accuracy is not yet understood. Furthermore, general-purpose hardware does not natively support low-precision representation. To address this, we propose ProbLP, a framework that automates the analysis and design of low-precision probabilistic inference hardware. It automatically chooses an appropriate energy-efficient representation based on worst-case error-bounds and hardware energy-models. It generates custom hardware for the resulting inference network exploiting parallelism, pipelining and low-precision operation. The framework is validated on several embedded-sensing benchmarks.",0.2307692308],["only a handful of sources have been confirmed to lie at z>5 and only two","A dusty star-forming galaxy at z=6 revealed by strong gravitational lensing","summarize: Since their discovery, submillimetre-selected galaxies have revolutionized the field of galaxy formation and evolution. From the hundreds of square degrees mapped at submillimetre wavelengths, only a handful of sources have been confirmed to lie at z>5 and only two at z>6. All of these SMGs are rare examples of extreme starburst galaxies with star formation rates of >1000 M_sun\/yr and therefore are not representative of the general population of dusty star-forming galaxies. Consequently, our understanding of the nature of these sources, at the earliest epochs, is still incomplete. Here we report the spectroscopic identification of a gravitationally amplified dusty star-forming galaxy at z=6.027. After correcting for gravitational lensing, we derive an intrinsic less-extreme SFR of 380 +\/- 50 M_sun\/yr for this source and find that its gas and dust properties are similar to those measured for local Ultra Luminous Infrared Galaxies , extending the local trends to a poorly explored territory in the early Universe. The star-formation efficiency of this galaxy is similar to those measured in its local analogues, despite a ~12 Gyr difference in cosmic time.",0.2727272727],["a system based on global features and deep neural networks. the best approach combines","The Medico-Task 2018: Disease Detection in the Gastrointestinal Tract using Global Features and Deep Learning","summarize: In this paper, we present our approach for the 2018 Medico Task classifying diseases in the gastrointestinal tract. We have proposed a system based on global features and deep neural networks. The best approach combines two neural networks, and the reproducible experimental results signify the efficiency of the proposed model with an accuracy rate of 95.80%, a precision of 95.87%, and an F1-score of 95.80%.",0.4876995513],["we discuss existence and regularity results for multi-channel images.","Convex Regularization of Multi-Channel Images Based on Variants of the TV-Model","summarize: We discuss existence and regularity results for multi-channel images in the setting of isotropic and anisotropic variants of the TV-model.",0.0],["the discrete versions of the well known Borg type theorem are considered discrete","A note on discrete Borg-type theorems","summarize: We consider the discrete versions of the well known Borg theorem and use simple linear algebraic techniques to obtain new versions of the discrete Borg type theorems. To be precise, we prove that the periodic potential of a discrete Schrodinger operator is almost a constant if and only if the possible spectral gaps of the operator are of small width. This result is further extended to more general settings and the connection to the well known Ten Martini problem is also discussed.",0.3684210526],["NISP will be tested at Laboratoire d'Astrophysique de","3D metrology with a laser tracker inside a vacuum chamber for NISP test campaign","summarize: In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille in ERIOS chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the EUCLID object plane, a telescope simulator with a very well know focal distance will be installed in front of NISP into ERIOS chamber. We need to measure at cold and vacuum the position of reflectors installed on NISP instrument and the telescope simulator. From these measurements, we will provide at operational temperature the measurement of references frames set on the telescope simulator and NISP, the knowledge of the coordinates of the object point source provided by the telescope simulator and the measurement of the angle between the telescope simulator optical axis and NISP optical axis. In this context, we have developed a metrology method based on the use of a laser tracker to measure the position of the reflectors inside ERIOS. The laser tracker is installed outside the vacuum chamber and measure through a curved window reflectors put inside the chamber either at ambient pressure or vacuum pressure. Several tests campaigns have been done at LAM to demonstrate the measurement performance with this configuration. Using a well know reflectors configuration, we show that it is possible to correct the laser tracker measurement from the window disturbances and from the vacuum impact. A corrective term is applied to the data and allows retrieving the real coordinates of the reflectors with a bias lower than 30",0.1574555176],["k.p-based interpolation scheme can extend eigenvalues and momentum","Enabling accurate first-principle calculations of electronic properties with a corrected k.p scheme","summarize: A computationally inexpensive k.p-based interpolation scheme is developed that can extend the eigenvalues and momentum matrix elements of a sparsely sampled k-point grid into a densely sampled one. Dense sampling, often required to accurately describe transport and optical properties of bulk materials, can be demanding to compute, for instance, in combination with hybrid functionals in density functional theory or with perturbative expansions beyond DFT such as the ",0.0758163325],["deposition of ultrathin tantalum on the NiFe top electrode reversed the","Nanoscale Tantalum Layer Controlling the Magnetic Coupling between Two Ferromagnetic Electrodes via Insulator of a Magnetic Tunnel Junction","summarize: Ability to tailor the nature of the magnetic coupling between two ferromagnetic electrodes can enable the realization of new spintronics device systems. This paper discusses our finding that deposition of an ultrathin tantalum on the NiFe top electrode reversed the nature of inter-ferromagnetic electrode coupling. We observed that the deposition of ~ 5 nm Ta on the top of a magnetic tunnel junction with Ta\/Co\/NiFe \/AlOx\/NiFe configuration changed the magnetic coupling between two ferromagnetic electrodes from antiferromagnetic to ferromagnetic. We investigated Ta effect using multiple magnetic characterizations like ferromagnetic resonance, magnetometry, and polarized neutron reflectometry. Ferromagnetic resonance characterization was very sensitive for detecting the changes in magnetic coupling via the insulating spacer. This simple approach of adding Ta film to alter the magnetic coupling can impact the other burgeoning areas like molecular spintronics. We found that preexisting magnetic coupling between two ferromagnetic electrodes impacted the resultant magnetic properties of magnetic tunnel junctions based molecular spintronics devices.",0.1890047648],["researchers have studied various types of calendar events to predict smartphone user behavior for incoming mobile communications","CalBehav: A Machine Learning based Personalized Calendar Behavioral Model using Time-Series Smartphone Data","summarize: The electronic calendar is a valuable resource nowadays for managing our daily life appointments or schedules, also known as events, ranging from professional to highly personal. Researchers have studied various types of calendar events to predict smartphone user behavior for incoming mobile communications. However, these studies typically do not take into account behavioral variations between individuals. In the real world, smartphone users can differ widely from each other in how they respond to incoming communications during their scheduled events. Moreover, an individual user may respond the incoming communications differently in different contexts subject to what type of event is scheduled in her personal calendar. Thus, a static calendar-based behavioral model for individual smartphone users does not necessarily reflect their behavior to the incoming communications. In this paper, we present a machine learning based context-aware model that is personalized and dynamically identifies individual's dominant behavior for their scheduled events using logged time-series smartphone data, and shortly name as ``CalBehav''. The experimental results based on real datasets from calendar and phone logs, show that this data-driven personalized model is more effective for intelligently managing the incoming mobile communications compared to existing calendar-based approaches.",0.0],["two-qubit quantum gates play an essential role in quantum computing. the entang","Performance Assessment of Resonantly Driven Silicon Two-Qubit Quantum Gate","summarize: Two-qubit quantum gates play an essential role in quantum computing, whose operation critically depends on the entanglement between two qubits. Resonantly driven controlled-NOT gates based on silicon double quantum dots are studied theoretically. The physical mechanisms for effective gate modulation of the exchange coupling between two qubits are elucidated. Scaling behaviors of the singlet-triplet energy split, gate-switching speed, and gate fidelity are investigated as a function of the quantum dot spacing and modulation gate voltage. It is shown that the entanglement strength and gate-switching speed exponentially depend on the quantum dot spacing. A small spacing of ~10nm can promise a CNOT gate delay of <1 ns and reliable gate switching in the presence of decoherence. The results show promising performance potential of the resonantly driven two-qubit quantum gates based on aggressively scaled silicon DQDs.",0.1666666667],["a logistic regression model is able to classify disinformation vs mainstream networks","A multi-layer approach to disinformation detection on Twitter","summarize: We tackle the problem of classifying news articles pertaining to disinformation vs mainstream news by solely inspecting their diffusion mechanisms on Twitter. Our technique is inherently simple compared to existing text-based approaches, as it allows to by-pass the multiple levels of complexity which are found in news content . We employ a multi-layer representation of Twitter diffusion networks, and we compute for each layer a set of global network features which quantify different aspects of the sharing process. Experimental results with two large-scale datasets, corresponding to diffusion cascades of news shared respectively in the United States and Italy, show that a simple Logistic Regression model is able to classify disinformation vs mainstream networks with high accuracy , also when considering the political bias of different sources in the classification task. We also highlight differences in the sharing patterns of the two news domains which appear to be country-independent. We believe that our network-based approach provides useful insights which pave the way to the future development of a system to detect misleading and harmful information spreading on social media.",0.3684210526],["social media changed the way we consume content favoring disintermediated access. this scenario has","The Limited Reach of Fake News on Twitter during 2019 European Elections","summarize: The advent of social media changed the way we consume content favoring a disintermediated access and production. This scenario has been matter of critical discussion about its impact on society. Magnified in the case of Arab Spring or heavily criticized in the Brexit and 2016 U.S. elections. In this work we explore information consumption on Twitter during the last European electoral campaign by analyzing the interaction patterns of official news sources, fake news sources, politicians, people from the showbiz and many others. We extensively explore interactions among different classes of accounts in the months preceding the last European elections, held between 23rd and 26th of May, 2019. We collected almost 400,000 tweets posted by 863 accounts having different roles in the public society. Through a thorough quantitative analysis we investigate the information flow among them, also exploiting geolocalized information. Accounts show the tendency to confine their interaction within the same class and the debate rarely crosses national borders. Moreover, we do not find any evidence of an organized network of accounts aimed at spreading disinformation. Instead, disinformation outlets are largely ignored by the other actors and hence play a peripheral role in online political discussions.",0.0],["low thermal conductivity is favorable for preserving the temperature gradient between the two ends of a","Ultralow thermal conductivity from transverse acoustic phonon suppression in distorted crystalline -MgAgSb","summarize: Low thermal conductivity is favorable for preserving the temperature gradient between the two ends of a thermoelectric material in order to ensure continuous electron current generation. In high-performance thermoelectric materials, there are two main low thermal conductivity mechanisms: the phonon anharmonic in PbTe and SnSe and phonon scattering resulting from the dynamic disorder in AgCrSe2 and CuCrSe2, which have been successfully revealed by inelastic neutron scattering. Using neutron scattering and ab initio calculations, we report here a mechanism of static local structure distortion combined with phonon-anharmonic-induced ultralow lattice thermal conductivity in -MgAgSb. Since the transverse acoustic phonons are almost fully scattered by the compound's intrinsic distorted rocksalt sublattice, the heat is mainly transported by the longitudinal acoustic phonons. The ultralow thermal conductivity in -MgAgSb is attributed to its atomic dynamics being altered by the structure distortion, which presents a possible microscopic route to enhance the performance of similar thermoelectric materials.",0.44],["existing methods in the literature are based on elaborated representations of a correlation matrix","A fast Metropolis-Hastings method for generating random correlation matrices","summarize: We propose a novel Metropolis-Hastings algorithm to sample uniformly from the space of correlation matrices. Existing methods in the literature are based on elaborated representations of a correlation matrix, or on complex parametrizations of it. By contrast, our method is intuitive and simple, based the classical Cholesky factorization of a positive definite matrix and Markov chain Monte Carlo theory. We perform a detailed convergence analysis of the resulting Markov chain, and show how it benefits from fast convergence, both theoretically and empirically. Furthermore, in numerical experiments our algorithm is shown to be significantly faster than the current alternative approaches, thanks to its simple yet principled approach.",0.4],["we propose a possible solution to these challenges: secure federated learning. we propose","Federated Machine Learning: Concept and Applications","summarize: Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.",0.2],["the KdV equation can be derived in the shallow water limit of the Euler equation","Superposition solutions to the extended KdV equation for water surface waves","summarize: The KdV equation can be derived in the shallow water limit of the Euler equations. Over the last few decades, this equation has been extended to include higher order effects. Although this equation has only one conservation law, exact periodic and solitonic solutions exist. Khare and Saxena \\cite demonstrated the possibility of generating new exact solutions by combining known ones for several fundamental equations . Here we find that this construction can be repeated for higher order, non-integrable extensions of these equations. Contrary to many statements in the literature, there seems to be no correlation between integrability and the number of nonlinear one variable wave solutions.",0.3043478261],["the authors describe a case study of the migration of an interactive diagramming tool written in","Lessons Learned in Migrating from Swing to JavaFX","summarize: The authors describe a case study of the migration of an interactive diagramming tool written in Java from the Swing Graphical User Interface framework to the more recent JavaFX framework. The study distills the authors' experience identifying what information was needed to support the migration effort, and how the information was ultimately discovered. The outcome is presented in a set of five lessons about the discrepancies between expectations and reality in the search for information when migrating software between major frameworks.",0.2413793103],["two evanescent mode waveguide filters of order five have been designed using a low","Design and Implementation of Evanescent Mode Waveguide Filters Using Dielectrics and Additive Manufacturing Techniques","summarize: In this contribution, we describe the design of bandpass filters using evanescent mode waveguides and dielectric resonators implemented with additive manufacturing techniques. Two C-band Chebyshev evanescent mode waveguide filters of order five have been designed using a low cost commercial dielectric material , widely used by Fused Deposition Modeling 3D printers. The housings of the filters have been manufactured using traditional computer numerical control machining techniques. Practical manufacturing considerations are also discussed, including the integration of dielectric and metallic parts. We first discuss two breadboards using two different resonator geometries. We then demonstrate how different transfer functions can be easily implemented by changing the 3D printed parts in the same metallic housing. Breadboards show fractional bandwidths between 3% and 4.6% with return losses better than RL=18dB, and spurious free ranges of SFR=1GHz. Insertion losses are better than IL=4.3dB. Even though dielectric losses from the plastic material are shown to be high, the measured results are quite satisfactory, thereby clearly showing that this strategy maybe useful for the fast production of low cost microwave filters implementing complex geometries.",0.2941176471],["the modification of the normalization process in the multi-criteria decision-making algorithm is still","A VIKOR and TOPSIS focused reanalysis of the MADM methods based on logarithmic normalization","summarize: Decision and policy-makers in multi-criteria decision-making analysis take into account some strategies in order to analyze outcomes and to finally make an effective and more precise decision. Among those strategies, the modification of the normalization process in the multiple-criteria decision-making algorithm is still a question due to the confrontation of many normalization tools. Normalization is the basic action in defining and solving a MADM problem and a MADM model. Normalization is the first, also necessary, step in solving, i.e. the application of a MADM method. It is a fact that the selection of normalization methods has a direct effect on the results. One of the latest normalization methods introduced is the Logarithmic Normalization method. This new method has a distinguished advantage, reflecting in that a sum of the normalized values of criteria always equals 1. This normalization method had never been applied in any MADM methods before. This research study is focused on the analysis of the classical MADM methods based on logarithmic normalization. VIKOR and TOPSIS, as the two famous MADM methods, were selected for this reanalysis research study. Two numerical examples were checked in both methods, based on both the classical and the novel ways based on the LN. The results indicate that there are differences between the two approaches. Eventually, a sensitivity analysis is also designed to illustrate the reliability of the final results.",0.2645603082],["the Kalman filter and its extensions are used in a vast number of aerospace and navigation applications","On Computational Complexity Reduction Methods for Kalman Filter Extensions","summarize: The Kalman filter and its extensions are used in a vast number of aerospace and navigation applications for nonlinear state estimation of time series. In the literature, different approaches have been proposed to exploit the structure of the state and measurement models to reduce the computational demand of the algorithms. In this tutorial, we survey existing code optimization methods and present them using unified notation that allows them to be used with various Kalman filter extensions. We develop the optimization methods to cover a wider range of models, show how different structural optimizations can be combined, and present new applications for the existing optimizations. Furthermore, we present an example that shows that the exploitation of the structure of the problem can lead to improved estimation accuracy while reducing the computational load. This tutorial is intended for persons who are familiar with Kalman filtering and want to get insights for reducing the computational demand of different Kalman filter extensions.",0.1612903226],["carsharing is a model of renting vehicles for short periods of time. payment is made","Proposal of a Carsharing System to Improve Urban Mobility","summarize: Carsharing is a model of renting vehicles for short periods of time, where the payment is made according to the time and distance effectively traveled. Carsharing offers a simple, economical and smart alternative to urban mobility, that is already being adopted in the major cities in the world. The proposed methodology consisted in the development of a decision support system that simplifies the process of choosing carsharing services. Adopting the AHP method, the user can indicate their preferences in the choice of vehicles, and the system returns an ordered list of the most suitable available vehicles based on their geographic location. The findings of the project indicate that the use of this system encourage and simplify the use of carsharing services, which will allow to enhance the financial, mobility and environment advantages inherent to their use.",0.3043478261]]}