{"columns":["predict_title","actual_title","actual_abstract","bleu"],"index":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,622,623,624,625,626,627,628,629,630,631,632,633,634,635,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,662,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,740,741,742,743,744,745,746,747,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,768,769,770,771,772,773,774,775,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,792,793,794,795,796,797,798,799,800,801,802,803,804,805,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831,832,833,834,835,836,837,838,839,840,841,842,843,844,845,846,847,848,849,850,851,852,853,854,855,856,857,858,859,860,861,862,863,864,865,866,867,868,869,870,871,872,873,874,875,876,877,878,879,880,881,882,883,884,885,886,887,888,889,890,891,892,893,894,895,896,897,898,899,900,901,902,903,904,905,906,907,908,909,910,911,912,913,914,915,916,917,918,919,920,921,922,923,924,925,926,927,928,929,930,931,932,933,934,935,936,937,938,939,940,941,942,943,944,945,946,947,948,949,950,951,952,953,954,955,956,957,958,959,960,961,962,963,964,965,966,967,968,969,970,971,972,973,974,975,976,977,978,979,980,981,982,983,984,985,986,987,988,989,990,991,992,993,994,995,996,997,998,999,1000,1001],"data":[["a newly developed 1-bit local most powerful test detector requires 3.3Q sensors to achieve the","Distributed Detection of Sparse Stochastic Signals via Fusion of 1-bit Local Likelihood Ratios","summarize: In this letter, we consider the detection of sparse stochastic signals with sensor networks , where the fusion center collects 1-bit data from the local sensors and then performs global detection. For this problem, a newly developed 1-bit locally most powerful test detector requires 3.3Q sensors to asymptotically achieve the same detection performance as the centralized LMPT detector with Q sensors. This 1-bit LMPT detector is based on 1-bit quantized observations without any additional processing at the local sensors. However, direct quantization of observations is not the most efficient processing strategy at the sensors since it incurs unnecessary information loss. In this letter, we propose an improved-1-bit LMPT detector that fuses local 1-bit quantized likelihood ratios instead of directly quantized local observations. In addition, we design the quantization thresholds at the local sensors to ensure asymptotically optimal detection performance of the proposed detector. It is shown theoretically and numerically that, with the designed quantization thresholds, the proposed Im-1-bit LMPT detector for the detection of sparse signals requires less number of sensor nodes to compensate for the performance loss caused by 1-bit quantization.",0.2777777778],["mMTC applications are designed for human-driven services. but legacy networks cannot meet","Applying Device-to-Device Communication to Enhance IoT Services","summarize: Massive Machine Type Communication to serve billions of IoT devices is considered to open a potential new market for the next generation cellular network. Legacy cellular networks cannot meet the requirements of emerging mMTC applications, since they were designed for human-driven services. In order to provide supports for mMTC services, current research and standardization work focus on the improvement and adaptation of legacy networks. However, these solutions face challenges to enhance the service availability and improve the battery life of mMTC devices simultaneously. In this article, we propose to exploit a network controlled sidelink communication scheme to enable cellular network with better support for mMTC services. Moreover, a context-aware algorithm is applied to ensure the efficiency of the proposed scheme and multiple context information of devices are taken into account. Correspondingly, signaling schemes are also designed and illustrated in this work to facilitate the proposed technology. The signaling schemes enable the network to collect required context information with light signaling effort and thus network can derive a smart configuration for both the sidelink and cellular link. In order to demonstrate the improvements brought by our scheme, a system-level simulator is implemented and numerical results show that our scheme can simultaneously enhance both the service availability and battery life of sensors.",0.0],["branch-and-bound is a decision space search method. the branching is performed","Branch-and-bound for biobjective mixed-integer linear programming","summarize: We present a generic branch-and-bound algorithm for finding all the Pareto solutions of a biobjective mixed-integer linear program. The main contributions are new algorithms for obtaining dual bounds at a node, for checking node fathoming, presolve and duality gap measurement. Our branch-and-bound is predominantly a decision space search method since the branching is performed on the decision variables, akin to single objective problems, although we also sometimes split gaps and branch in the objective space. The various algorithms are implemented using a data structure for storing Pareto sets. Computational experiments are carried out on literature instances and also on a new set of instances that we generate using the MIPLIB benchmark library for single objective problems. We also perform comparisons against the triangle splitting method from literature, which is an objective space search algorithm.",0.2],["rate-control algorithms rely on bit allocation strategies to appropriately distribute bits. intra frames are","Estimation of Rate Control Parameters for Video Coding Using CNN","summarize: Rate-control is essential to ensure efficient video delivery. Typical rate-control algorithms rely on bit allocation strategies, to appropriately distribute bits among frames. As reference frames are essential for exploiting temporal redundancies, intra frames are usually assigned a larger portion of the available bits. In this paper, an accurate method to estimate number of bits and quality of intra frames is proposed, which can be used for bit allocation in a rate-control scheme. The algorithm is based on deep learning, where networks are trained using the original frames as inputs, while distortions and sizes of compressed frames after encoding are used as ground truths. Two approaches are proposed where either local or global distortions are predicted.",0.1176470588],["the proposed algorithm is an enabler of real-time softwarized control of large-scale","Generic Dijkstra for optical networks","summarize: We present the generic Dijkstra shortest path algorithm: an efficient algorithm for finding a shortest path in an optical network, both in a wavelength-division multiplexed network, and an elastic optical network . The proposed algorithm is an enabler of real-time softwarized control of large-scale networks, and is not limited to optical networks. The Dijkstra algorithm is a generalization of the breadth-first search, and we generalize the Dijkstra algorithm further to resolve the continuity and contiguity constraints of the frequency slot units required in EONs. Specifically, we generalize the notion of a label, change what we iterate with, and reformulate the edge relaxation so that vertices are revisited, loops avoided, and worse labels discarded. We also used the typical constriction during edge relaxation to take care of the signal modulation constraints. The algorithm can be used with various spectrum allocation policies. We motivate and discuss the algorithm design, and provide our free, reliable, and generic implementation using the Boost Graph Library. We carried out 85000 simulation runs for realistic and random networks of 75 vertices with about a billion shortest-path searches, and found that the proposed algorithm outperforms considerably three other competing optimal algorithms that are frequently used.",0.0],["paper tackles the Toeplitz systems directly for the elliptic PDEs","Uniform convergence of V-cycle multigrid algorithms for two-dimensional fractional Feynman-Kac equation","summarize: In this paper we derive new uniform convergence estimates for the V-cycle MGM applied to symmetric positive definite Toeplitz block tridiagonal matrices, by also discussing few connections with previous results. More concretely, the contributions of this paper are as follows: It tackles the Toeplitz systems directly for the elliptic PDEs. Simple restriction operator and prolongation operator are employed in order to handle general Toeplitz systems at each level of the recursion. Such a technique is then applied to systems of algebraic equations generated by the difference scheme of the two-dimensional fractional Feynman-Kac equation, which describes the joint probability density function of non-Brownian motion. In particular, we consider the two coarsening strategies, i.e., doubling the mesh size and Galerkin approach , which lead to the distinct coarsening stiffness matrices in the general case: however, several numerical experiments show that the two algorithms produce almost the same error behaviour.",0.0754031182],["scalar field and cosmological constant behaviour analysed in this article.","Scalar field and time varying Cosmological constant in ","summarize: In this article, we have analysed the behaviour of scalar field and cosmological constant in ",0.4],["fine-tuned transductively, this outperforms the current state-of","A Baseline for Few-Shot Image Classification","summarize: Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the hardness of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.",0.0],["chromosome conformation capture experiments provide partial information on the chromatin organization in","Computing 3D chromatin configurations from contact probability maps by Inverse Brownian Dynamics","summarize: The three-dimensional organization of chromatin, on the length scale of a few genes, is crucial in determining the functional state - accessibility and the amount of gene expression - of the chromatin. Recent advances in chromosome conformation capture experiments provide partial information on the chromatin organization in a cell population, namely the contact count between any segment pairs, but not on the interaction strength that leads to these contact counts. However, given the contact matrix, determining the complete 3D organization of the whole chromatin polymer is an inverse problem. In the present work, a novel Inverse Brownian Dynamics method based on a coarse grained bead-spring chain model has been proposed to compute the optimal interaction strengths between different segments of chromatin such that the experimentally measured contact count probability constraints are satisfied. Applying this method to the -globin gene locus in two different cell types, we predict the 3D organizations corresponding to active and repressed states of chromatin at the locus. We show that the average distance between any two segments of the region has a broad distribution and cannot be computed as a simple inverse relation based on the contact probability alone. The results presented for multiple normalization methods suggest that all measurable quantities may crucially depend on the nature of normalization. We argue that by experimentally measuring predicted quantities, one may infer the appropriate form of normalization.",0.3333333333],["paper presents augmented Markovian system model for non-Markovian quantum systems.","Modelling and Filtering for Non-Markovian Quantum Systems","summarize: This paper presents an augmented Markovian system model for non-Markovian quantum systems. In this augmented system model, ancillary systems are introduced to play the role of internal modes of the non-Markovian environment converting white noise to colored noise. Consequently, non-Markovian dynamics are represented as resulting from direct interaction of the principal system with the ancillary system. To demonstrate the utility of the proposed augmented system model, it is applied to design whitening quantum filters for non-Markovian quantum systems. Examples are presented to illustrate how whitening quantum filters can be utilized for estimating non-Markovian linear quantum systems and qubit systems. In particular, we showed that the augmented Markovian formulation can be used to theoretically model the environment for an observed non-Markovian behavior in a recent experiment on quantum dots ",0.1666666667],["the animation industry outsources large animation workloads to foreign countries where labor is inexpensive and long","Artist-Guided Semiautomatic Animation Colorization","summarize: There is a delicate balance between automating repetitive work in creative domains while staying true to an artist's vision. The animation industry regularly outsources large animation workloads to foreign countries where labor is inexpensive and long hours are common. Automating part of this process can be incredibly useful for reducing costs and creating manageable workloads for major animation studios and outsourced artists. We present a method for automating line art colorization by keeping artists in the loop to successfully reduce this workload while staying true to an artist's vision. By incorporating color hints and temporal information to an adversarial image-to-image framework, we show that it is possible to meet the balance between automation and authenticity through artist's input to generate colored frames with temporal consistency.",0.1111111111],["a finite family of continuously differentiable positive definite functions is a Lya","Max-Min Lyapunov Functions for Switched Systems and Related Differential Inclusions","summarize: Starting from a finite family of continuously differentiable positive definite functions, we study conditions under which a function obtained by max-min combinations is a Lyapunov function, establishing stability for two kinds of nonlinear dynamical systems: a) Differential inclusions where the set-valued right-hand-side comprises the convex hull of a finite number of vector fields, and b) Autonomous switched systems with a state-dependent switching signal. We investigate generalized notions of directional derivatives for these max-min functions, and use them in deriving stability conditions with various degrees of conservatism, where more conservative conditions are numerically more tractable. The proposed constructions also provide nonconvex Lyapunov functions, which are shown to be useful for systems with state-dependent switching that do not admit a convex Lyapunov function. Several examples are included to illustrate the results.",0.5238095238],["re-identification requires models to capture diverse features. but with continuous training, models","ES-Net: Erasing Salient Parts to Learn More in Re-Identification","summarize: As an instance-level recognition problem, re-identification requires models to capture diverse features. However, with continuous training, re-ID models pay more and more attention to the salient areas. As a result, the model may only focus on few small regions with salient representations and ignore other important information. This phenomenon leads to inferior performance, especially when models are evaluated on small inter-identity variation data. In this paper, we propose a novel network, Erasing-Salient Net , to learn comprehensive features by erasing the salient areas in an image. ES-Net proposes a novel method to locate the salient areas by the confidence of objects and erases them efficiently in a training batch. Meanwhile, to mitigate the over-erasing problem, this paper uses a trainable pooling layer P-pooling that generalizes global max and global average pooling. Experiments are conducted on two specific re-identification tasks . Our ES-Net outperforms state-of-the-art methods on three Person re-ID benchmarks and two Vehicle re-ID benchmarks. Specifically, mAP \/ Rank-1 rate: 88.6% \/ 95.7% on Market1501, 78.8% \/ 89.2% on DuckMTMC-reID, 57.3% \/ 80.9% on MSMT17, 81.9% \/ 97.0% on Veri-776, respectively. Rank-1 \/ Rank-5 rate: 83.6% \/ 96.9% on VehicleID , 79.9% \/ 93.5% on VehicleID , 76.9% \/ 90.7% on VehicleID , respectively. Moreover, the visualized salient areas show human-interpretable visual explanations for the ranking results.",0.0714285714],["uncloneable encryption allows the encryption of a classical message. this allows the","Uncloneable Quantum Encryption via Oracles","summarize: Quantum information is well-known to achieve cryptographic feats that are unattainable using classical information alone. Here, we add to this repertoire by introducing a new cryptographic functionality called uncloneable encryption. This functionality allows the encryption of a classical message such that two collaborating but isolated adversaries are prevented from simultaneously recovering the message, even when the encryption key is revealed. Clearly, such functionality is unattainable using classical information alone. We formally define uncloneable encryption, and show how to achieve it using Wiesner's conjugate coding, combined with a quantum-secure pseudorandom function . Modelling the qPRF as a quantum random oracle, we show security by adapting techniques from the quantum one-way-to-hiding lemma, as well as using bounds from quantum monogamy-of-entanglement games.",0.1666666667],["the diffusion of chiral active Brownian particles in three-dimensional space is studied","Diffusion of active chiral particles","summarize: The diffusion of chiral active Brownian particles in three-dimensional space is studied analytically, by consideration of the corresponding Fokker-Planck equation for the probability density of finding a particle at position ",0.3333333333],["fluorescein-a fluorescent marker used as a proxy for drug molecules.","Quantitative Kinetic Models from Intravital Microcopy: A Case Study Using Hepatic Transport","summarize: The liver performs critical physiological functions, including metabolizing and removing substances, such as toxins and drugs, from the bloodstream. Hepatotoxicity itself is intimately linked to abnormal hepatic transport and hepatotoxicity remains the primary reason drugs in development fail and approved drugs are withdrawn from the market. For this reason, we propose to analyze, across liver compartments, the transport kinetics of fluorescein-a fluorescent marker used as a proxy for drug molecules-using intravital microscopy data. To resolve the transport kinetics quantitatively from fluorescence data, we account for the effect that different liver compartments have on fluorescein's emission rate. To do so, we develop ordinary differential equation transport models from the data where the kinetics are related to the observable fluorescence levels by measurement parameters that vary across different liver compartments. On account of the steep non-linearities in the kinetics and stochasticity inherent to the model, we infer kinetic and measurement parameters by generalizing the method of parameter cascades. For this application, the method of parameter cascades ensures fast and precise parameter estimates from noisy time traces.",0.3148964435],["in 1964, he began working in the Physics Department of the Advanced Studies Center of the National","Brief Notes and History Computing in Mexico during 50 years","summarize: The history of computing in Mexico can not be thought without the name of Prof. Harold V. McIntosh . For almost 50 years, in Mexico he contributed to the development of computer science with wide international recognition. Approximately in 1964, McIntosh began working in the Physics Department of the Advanced Studies Center of the National Polytechnic Institute , now called CINVESTAV. In 1965, at the National Center of Calculus , he was a founding member of the Master in Computing, first in Latin America. With the support of Mario Baez Camargo and Enrique Melrose, McIntosh continues his research of Martin-Baltimore Computer Center and University of Florida at IBM 709.",0.1875],["new code implements the HLLD approximate Riemann solver and the hyperbolic","Magnetohydrodynamic Simulation Code CANS+: Assessments and Applications","summarize: We present a new magnetohydrodynamic simulation code with the aim of providing accurate numerical solutions to astrophysical phenomena where discontinuities, shock waves, and turbulence are inherently important. The code implements the HLLD approximate Riemann solver, the fifth-order-monotonicity-preserving interpolation scheme, and the hyperbolic divergence cleaning method for a magnetic field. This choice of schemes significantly improved numerical accuracy and stability, and saved computational costs in multidimensional problems. Numerical tests of one- and two-dimensional problems showed the advantages of using the high-order scheme by comparing with results from a standard second-order TVD MUSCL scheme. The present code enabled us to explore long-term evolution of a three-dimensional accretion disk around a black hole, in which compressible MHD turbulence caused continuous mass accretion via nonlinear growth of the magneto-rotational instability . Numerical tests with various computational cell sizes exhibited a convergent picture of the early nonlinear growth of the MRI in a global model, and indicated that the MP5 scheme has more than twice the resolution of the MUSCL scheme in practical applications.",0.0769230769],["the socket is critical in a prosthetic system for a person with limb a","Multi-Material 3-D Viscoelastic Model of a Transtibial Residuum from In-vivo Indentation and MRI Data","summarize: Although the socket is critical in a prosthetic system for a person with limb amputation, the methods of its design are largely artisanal. A roadblock for a repeatable and quantitative socket design process is the lack of predictive and patient specific biomechanical models of the residuum. This study presents the evaluation of such a model using a combined experimental-numerical approach. The model geometry and tissue boundaries are derived from MRI. The soft tissue non-linear elastic and viscoelastic mechanical behavior was evaluated using inverse finite element analysis of in-vivo indentation experiments. A custom designed robotic in-vivo indentation system was used to provide a rich experimental data set of force versus time at 18 sites across a limb. During FEA, the tissues were represented by two layers, namely the skin-adipose layer and an underlying muscle-soft tissue complex. The non-linear elastic behavior was modeled using 2nd order Ogden hyperelastic formulations, and viscoelasticity was modeled using the quasi-linear theory of viscoelasticity. To determine the material parameters for each tissue, an inverse FEA based optimization routine was used that minimizes the combined mean of the squared force differences between the numerical and experimental force-time curves for indentations at 4 distinct anatomical regions on the residuum. The optimization provided the following material parameters for the skin-adipose layer: c=5.22 kPa, m=4.79, gamma=3.57 MPa, tau=0.32 s and for the muscle-soft tissue complex: c=5.20 kPa, m=4.78, gamma=3.47 MPa, tau=0.34 s. These parameters were evaluated to predict the force-time curves for the remaining 14 anatomical locations. The mean percentage error for these predictions was 7 +\/- 3%. The mean percentage error at the 4 sites used for the optimization was 4%.",0.5416666667],["MUltiple SIgnal Classification algorithm is used to identify the locations of small","Interpretation of MUSIC for location detecting of small inhomogeneities surrounded by random scatterers","summarize: In this paper, we consider the MUltiple SIgnal Classification algorithm for identifying the locations of small electromagnetic inhomogeneities surrounded by random scatterers. For this purpose, we rigorously analyze the structure of MUSIC-type imaging function by establishing a relationship with zero-order Bessel function of the first kind. This relationship shows certain properties of the MUSIC algorithm, explains some unexplained phenomena, and provides a method for improvements.",0.1533407358],["the spectral radius of a graph is the largest eigenvalue of its adj","Forbidden subgraphs for graphs of bounded spectral radius, with applications to equiangular lines","summarize: The spectral radius of a graph is the largest eigenvalue of its adjacency matrix. Let ",0.5217391304],["the Accountability Principle of the GDPR requires an organisation to demonstrate compliance with the regulations.","Design Challenges for GDPR RegTech","summarize: The Accountability Principle of the GDPR requires that an organisation can demonstrate compliance with the regulations. A survey of GDPR compliance software solutions shows significant gaps in their ability to demonstrate compliance. In contrast, RegTech has recently brought great success to financial compliance, resulting in reduced risk, cost saving and enhanced financial regulatory compliance. It is shown that many GDPR solutions lack interoperability features such as standard APIs, meta-data or reports and they are not supported by published methodologies or evidence to support their validity or even utility. A proof of concept prototype was explored using a regulator based self-assessment checklist to establish if RegTech best practice could improve the demonstration of GDPR compliance. The application of a RegTech approach provides opportunities for demonstrable and validated GDPR compliance, notwithstanding the risk reductions and cost savings that RegTech can deliver. This paper demonstrates a RegTech approach to GDPR compliance can facilitate an organisation meeting its accountability obligations.",0.0434782609],["innovative behaviour is dependent on individual and contextual factors. the literature classifies this phenomenon as innovative","The Innovative Behaviour of Software Engineers: Findings from a Pilot Case Study","summarize: Context: In the workplace, some individuals engage in the voluntary and intentional generation, promotion, and realization of new ideas for the benefit of individual performance, group effectiveness, or the organization. The literature classifies this phenomenon as innovative behaviour. Despite its importance to the development of innovation, innovative behaviour has not been fully investigated in software engineering. Objective: To understand the factors that support or inhibit innovative behaviour in software engineering practice. Method: We conducted a pilot case study in a Canadian software company using interviews and observations as data collection techniques. Using qualitative analysis, we identified relevant factors and relationships not addressed by studies from other areas. Results: Individual innovative behaviour is influenced by individual attitudes and also by situational factors such as relationships in the workplace, organizational characteristics, and project type. We built a model to express the interacting effects of these factors. Conclusions: Innovative behaviour is dependent on individual and contextual factors. Our results contribute to relevant impacts on research and practice, and to topics that deserve further study.",0.0454545455],["the information acquisition problem is based on a planning algorithm on known models. the proposed","Learning Q-network for Active Information Acquisition","summarize: In this paper, we propose a novel Reinforcement Learning approach for solving the Active Information Acquisition problem, which requires an agent to choose a sequence of actions in order to acquire information about a process of interest using on-board sensors. The classic challenges in the information acquisition problem are the dependence of a planning algorithm on known models and the difficulty of computing information-theoretic cost functions over arbitrary distributions. In contrast, the proposed framework of reinforcement learning does not require any knowledge on models and alleviates the problems during an extended training stage. It results in policies that are efficient to execute online and applicable for real-time control of robotic systems. Furthermore, the state-of-the-art planning methods are typically restricted to short horizons, which may become problematic with local minima. Reinforcement learning naturally handles the issue of planning horizon in information problems as it maximizes a discounted sum of rewards over a long finite or infinite time horizon. We discuss the potential benefits of the proposed framework and compare the performance of the novel algorithm to an existing information acquisition method for multi-target tracking scenarios.",0.2413793103],["we reconstruct posterior distributions for the position of a simulated set of binary neutron-","Dirichlet Process Gaussian-mixture model: An application to localizing coalescing binary neutron stars with gravitational-wave observations","summarize: We reconstruct posterior distributions for the position of a simulated set of binary neutron-star gravitational-waves signals observed with Advanced LIGO and Advanced Virgo. We use a Dirichlet Process Gaussian-mixture model, a fully Bayesian non-parametric method that can be used to estimate probability density functions with a flexible set of assumptions. The ability to reliably reconstruct the source position is important for multimessenger astronomy, as recently demonstrated with GW170817. We show that for detector networks comparable to the early operation of Advanced LIGO and Advanced Virgo, typical localization volumes are ",0.2069028399],["the existing models rely on Sommerfeld's free-electron theory. the","Fractional Fowler-Nordheim Law for Field Emission from Rough Surface with Nonparabolic Energy Dispersion","summarize: The theories of field electron emission from perfectly planar and smooth canonical surfaces are well understood, but they are not suitable for describing emission from rough, irregular surfaces arising in modern nanoscale electron sources. Moreover, the existing models rely on Sommerfeld's free-electron theory for the description of electronic distribution which is not a valid assumption for modern materials with nonparabolic energy dispersion. In this paper, we derive analytically a generalized Fowler-Nordheim type equation that takes into account the reduced space-dimensionality seen by the quantum mechanically tunneling electron at a rough, irregular emission surface. We also consider the effects of non-parabolic energy dispersion on field-emission from narrow-gap semiconductors and few-layer graphene using Kane's band model. The traditional FN equation is shown to be a limiting case of our model in the limit of a perfectly flat surface of a material with parabolic dispersion. The fractional-dimension parameter used in this model can be experimentally calculated from appropriate current-voltage data plot. By applying this model to experimental data, the standard field-emission parameters can be deduced with better accuracy than by using the conventional FN equation.",0.0915971983],["filtering for hidden Markov models is linked to the notion of duality. the filter","Optimal filtering and the dual process","summarize: We link optimal filtering for hidden Markov models to the notion of duality for Markov processes. We show that when the signal is dual to a process that has two components, one deterministic and one a pure death process, and with respect to functions that define changes of measure conjugate to the emission density, the filtering distributions evolve in the family of finite mixtures of such measures and the filter can be computed at a cost that is polynomial in the number of observations. Special cases of our framework include the Kalman filter, and computable filters for the Cox-Ingersoll-Ross process and the one-dimensional Wright-Fisher process, which have been investigated before. The dual we obtain for the Cox-Ingersoll-Ross process appears to be new in the literature.",0.2352941176],["membranes of biological cells display melting transitions of their lipids at a temperature of","Phase transitions in biological membranes","summarize: Native membranes of biological cells display melting transitions of their lipids at a temperature of 10-20 degrees below body temperature. Such transitions can be observed in various bacterial cells, in nerves, in cancer cells, but also in lung surfactant. It seems as if the presence of transitions slightly below physiological temperature is a generic property of most cells. They are important because they influence many physical properties of the membranes. At the transition temperature, membranes display a larger permeability that is accompanied by ion-channel-like phenomena even in the complete absence of proteins. Membranes are softer, which implies that phenomena such as endocytosis and exocytosis are facilitated. Mechanical signal propagation phenomena related to nerve pulses are strongly enhanced. The position of transitions can be affected by changes in temperature, pressure, pH and salt concentration or by the presence of anesthetics. Thus, even at physiological temperature, these transitions are of relevance. There position and thereby the physical properties of the membrane can be controlled by changes in the intensive thermodynamic variables. Here, we review some of the experimental findings and the thermodynamics that describes the control of the membrane function.",0.2592592593],["no theoretical origin for GNNs has been proposed. we propose subspace power iter","Understanding the Message Passing in Graph Neural Networks via Power Iteration Clustering","summarize: The mechanism of message passing in graph neural networks is still mysterious. Apart from convolutional neural networks, no theoretical origin for GNNs has been proposed. To our surprise, message passing can be best understood in terms of power iteration. By fully or partly removing activation functions and layer weights of GNNs, we propose subspace power iteration clustering models that iteratively learn with only one aggregator. Experiments show that our models extend GNNs and enhance their capability to process random featured networks. Moreover, we demonstrate the redundancy of some state-of-the-art GNNs in design and define a lower limit for model evaluation by a random aggregator of message passing. Our findings push the boundaries of the theoretical understanding of neural networks.",0.0666666667],["generative model of point clouds in form of energy-based model. energy function learns","Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification","summarize: We propose a generative model of unordered point sets, such as point clouds, in the form of an energy-based model, where the energy function is parameterized by an input-permutation-invariant bottom-up neural network. The energy function learns a coordinate encoding of each point and then aggregates all individual point features into an energy for the whole point cloud. We call our model the Generative PointNet because it can be derived from the discriminative PointNet. Our model can be trained by MCMC-based maximum likelihood learning , without the help of any assisting networks like those in GANs and VAEs. Unlike most point cloud generators that rely on hand-crafted distance metrics, our model does not require any hand-crafted distance metric for the point cloud generation, because it synthesizes point clouds by matching observed examples in terms of statistical properties defined by the energy function. Furthermore, we can learn a short-run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpolation. The learned point cloud representation can be useful for point cloud classification. Experiments demonstrate the advantages of the proposed generative model of point clouds.",0.0902530441],["CS methods always recover the scene images in pixel level. this causes the smoothness","Perceptual Compressive Sensing","summarize: Compressive sensing works to acquire measurements at sub-Nyquist rate and recover the scene images. Existing CS methods always recover the scene images in pixel level. This causes the smoothness of recovered images and lack of structure information, especially at a low measurement rate. To overcome this drawback, in this paper, we propose perceptual CS to obtain high-level structured recovery. Our task no longer focuses on pixel level. Instead, we work to make a better visual effect. In detail, we employ perceptual loss, defined on feature level, to enhance the structure information of the recovered images. Experiments show that our method achieves better visual results with stronger structure information than existing CS methods at the same measurement rate.",0.0625],["the thermomechanical process route was designed to be industrially translatable. it consists","Design of a High Strength, High Ductility 12 wt% Mn Medium Manganese Steel With Hierarchical Deformation Behaviour","summarize: A novel medium Mn steel of composition Fe-12Mn-4.8Al-2Si-0.32C-0.3V was manufactured with 1.09 GPa yield strength, 1.26 GPa tensile strength and 54% elongation. The thermomechanical process route was designed to be industrially translatable and consists of hot and then warm rolling before a 30 min intercritical anneal. The resulting microstructure comprised of coarse elongated austenite grains in the rolling direction surrounded by necklace layers of fine austenite and ferrite grains. The tensile behaviour was investigated by in-situ neutron diffraction and the evolution of microstructure studied with Electron Backscattered Diffraction . It was found that the coarse austenite grains contributed to the first stage of strain hardening by transforming into martensite and the fine austenite necklace grains contributed to the second stage of strain hardening by a mixture of twinning and transformation induced plasticity mechanisms. This hierarchical deformation behaviour contributed to the exceptional ductility of this steel.",0.0507108177],["cite has proposed the use of the maximum loss over random structured outputs. this","Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms","summarize: Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \\cite. In natural language processing, recent work \\cite has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \\cite. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \\cite, our theoretical results show that more general techniques are possible.",0.0],["plasmonic internal photoemission detectors can be used to detect ultra-broadband","Silicon-plasmonic integrated circuits for terahertz signal generation and coherent detection","summarize: Optoelectronic signal processing offers great potential for generation and detection of ultra-broadband waveforms in the THz range, so-called T-waves. However, fabrication of the underlying high-speed photodiodes and photoconductors still relies on complex processes using dedicated III-V semiconductor substrates. This severely limits the application potential of current T-wave transmitters and receivers, in particular when it comes to highly integrated systems that combine photonic signal processing with optoelectronic conversion to THz frequencies. In this paper, we demonstrate that these limitations can be overcome by plasmonic internal photoemission detectors . PIPED can be realized on the silicon photonic platform and hence allow to leverage the enormous opportunities of the associated device portfolio. In our experiments, we demonstrate both T-wave signal generation and coherent detection at frequencies of up to 1 THz. To proof the viability of our concept, we monolithically integrate a PIPED transmitter and a PIPED receiver on a common silicon photonic chip and use them for measuring the complex transfer impedance of an integrated T-wave device.",0.1538461538],["the rigid existing Internet of things architecture blocks current traffic management technology to provide a real differentiated","Dynamic Load-Balancing Vertical Control for Large-Scale Software-Defined Internet of Things","summarize: As the global Internet of things increasingly is popular with consumers and business environment, network flow management has become an important topic to optimize the performance on Internet of Things. The rigid existing Internet of things architecture blocks current traffic management technology to provide a real differentiated service for large-scale IoT. Software-defined Internet of Things is a new computing paradigm that separates control plane and data plane, and enables centralized logic control. In this paper, we first present a general framework for SD-IoT, which consists of two main components: SD-IoT controllers and SD-IoT switches. The controllers of SD-IoT uses resource pooling technology, and the pool is responsible for the centralized control of the entire network. The switches of SD-IoT integrate with the gateway functions, which is responsible for data access and forwarding. The SD-IoT controller pool is designed as a vertical control architecture, which includes the main control layer and the base control layer. The controller of the main control layer interacts upward with the application layer, interacts with the base control layer downwards, and the controller of the basic control layer interacts with the data forwarding layer. We propose a dynamic balancing algorithm of the main controller based on election mechanism and a dynamic load balancing algorithm of the basic controller based on the balanced delay, respectively. The experimental results show that the dynamic balancing algorithm based on the election mechanism can ensure the consistency of the messages between the main controllers, and the dynamic load balancing algorithm based on the balanced delay can balance between these different workloads in the basic controllers.",0.3913043478],["simultaneous observations from a doppler weather radar and an instrumented micrometeor","Unravelling the turbulent structures of temperature variations during a wind gust event: a case study","summarize: The simultaneous observations from a Doppler weather radar and an instrumented micrometeorological tower, offer an opportunity to dissect the effects of a gust front on the surface layer turbulence in a tropical convective boundary layer. We present a case study where a sudden drop in temperature was noted at heights within the surface layer during the passage of a gust front in the afternoon time. Consequently, this temperature drop created an interface which separated two different turbulent regimes. In one regime the turbulent temperature fluctuations were large and energetic, whereas in the other regime they were weak and quiescent. Given its uniqueness, we investigated the size distribution and aggregation properties of the turbulent structures related to these two regimes. We found that, the size distributions of the turbulent structures for both of these regimes displayed a clear power-law signature. Since power-laws are synonymous with scale-invariance, this indicated the passing of the gust front initiated a scale-free response which governed the turbulent characteristics of the temperature fluctuations. We propose a hypothesis to link such behaviour with the self organized criticality as observed in the complex systems. However, the temporal organization of the turbulent structures, as indicated by their clustering tendencies, differed between these two regimes. For the regime, corresponding to large temperature fluctuations, the turbulent structures were significantly clustered, whose clustering properties changed with height. Contrarily, for the other regime where the temperature fluctuations were weak, the turbulent structures remained less clustered with no discernible change being observed with height.",0.2433003749],["the aim of this paper is to extend the concept of regional exponential general observability to the","Regional exponential general observability in diffusion problem","summarize: The aim of this paper is to extend the concept of regional exponential general observability to the case of Neumann boundary conditions problem in diffusion system. More precisely, for linear distributed parameter diffusion systems, we show that the sensors characterizations allow to reconstruct the regional exponential state in a sub-region of the considered systems domain. Moreover, various interesting results associated with the choice of sensors are given and illustrated in specific situations in order regional exponential general observability notion to be achieved. Finally, we also show that, there exists a dynamical system for diffusion system is not exponential general observable in the usual sense, but it may be regional exponential general observable.",0.1071428571],["four test images are designed to facilitate effective examination and comparison of image processing algorithms. the images","A Canonical Image Set for Examining and Comparing Image Processing Algorithms","summarize: The purpose of this paper is to introduce a set of four test images containing features and structures that can facilitate effective examination and comparison of image processing algorithms. More specifically, the images are designed to more explicitly expose the characteristic properties of algorithms for image compression, virtual resolution adjustment, and enhancement. This set was developed at the Naval Research Laboratory in the late 1990s as a more rigorous alternative to Lena and other images that have come into common use for purely ad hoc reasons with little or no rigorous consideration of their suitability. The increasing number of test images appearing in the literature not only makes it more difficult to compare results from different papers, it also introduces the potential for cherry-picking to influence results. The key contribution of this paper is the proposal to establish canonical set to ensure that published results can be analyzed and compared in a rigorous way from one paper to another, and consideration of the four NRL images is proposed for this purpose.",0.0476190476],["we study the correctness of automatic differentiation in a higher-order language. we study","Automatic Differentiation in PCF","summarize: We study the correctness of automatic differentiation in the context of a higher-order, Turing-complete language , both in forward and reverse mode. Our main result is that, under mild hypotheses on the primitive functions included in the language, AD is almost everywhere correct, that is, it computes the derivative or gradient of the program under consideration except for a set of Lebesgue measure zero. Stated otherwise, there are inputs on which AD is incorrect, but the probability of randomly choosing one such input is zero. Our result is in fact more precise, in that the set of failure points admits a more explicit description: for example, in case the primitive functions are just constants, addition and multiplication, the set of points where AD fails is contained in a countable union of zero sets of non-identically-zero polynomials.",0.1363636364],["Tajweed is a set of rules to read the Quran in a correct Pron","Smartajweed Automatic Recognition of Arabic Quranic Recitation Rules","summarize: Tajweed is a set of rules to read the Quran in a correct Pronunciation of the letters with all its Qualities, while Reciting the Quran. which means you have to give every letter in the Quran its due of characteristics and apply it to this particular letter in this specific situation while reading, which may differ in other times. These characteristics include melodic rules, like where to stop and for how long, when to merge two letters in pronunciation or when to stretch some, or even when to put more strength on some letters over other. Most of the papers focus mainly on the main recitation rules and the pronunciation but not which give different rhythm and different melody to the pronunciation with every different rule of . Which is also considered very important and essential in Reading the Quran as it can give different meanings to the words. In this paper we discuss in detail full system for automatic recognition of Quran Recitation Rules by using support vector machine and threshold scoring system",0.5909090909],["internal categories in the category of the crossed modules are characterized. the equival","Categories internal to crossed modules","summarize: In this study, internal categories in the category of the crossed modules are characterized and it has been shown that there is a natural equivalence between the category of the crossed modules over crossed modules, i.e. crossed squares, and the category of the internal categories within the category of crossed modules. Finally, we obtain examples of crossed squares using this equivalence.",0.2],["a half-century journey of great complexity has been a journey of great complexity.","Core-Collapse Supernova Explosion Theory","summarize: Most supernova explosions accompany the death of a massive star. These explosions give birth to neutron stars and black holes and eject solar masses of heavy elements. However, determining the mechanism of explosion has been a half-century journey of great complexity. In this paper, we present our perspective of the status of this theoretical quest and the physics and astrophysics upon which its resolution seems to depend. The delayed neutrino-heating mechanism is emerging as a robust solution, but there remain many issues to address, not the least of which involves the chaos of the dynamics, before victory can unambiguously be declared. It is impossible to review in detail all aspects of this multi-faceted, more-than-half-century-long theoretical quest. Rather, we here map out the major ingredients of explosion and the emerging systematics of the observables with progenitor mass, as we currently see them. Our discussion will of necessity be speculative in parts, and many of the ideas may not survive future scrutiny. Some statements may be viewed as informed predictions concerning the numerous observables that rightly exercise astronomers witnessing and diagnosing the supernova Universe. Importantly, the same explosion in the inside, by the same mechanism, can look very different in photons, depending upon the mass and radius of the star upon explosion. A 10",0.1290322581],["paper is concerned with the existence and uniqueness of the solutions of nonlinear impulsive","Analysis of Impulsive ","summarize: This paper is concerned with the existence and uniqueness, and Ulam--Hyers stabilities of solutions of nonlinear impulsive ",0.1578947368],["a cosmological model of spatially homogeneous and isotropic accelerating","FRW dark energy cosmological model with hybrid expansion law","summarize: In this work, we study a cosmological model of spatially homogeneous and isotropic accelerating universe which exhibits a transition from deceleration to acceleration. For this, Friedmann Robertson Walker metric is taken and Hybrid expansion law ",0.4],["first-principles calculations show that all compounds are very stable. most of compounds present","Atomically thin binary V-V compound semiconductor: a first-principles study","summarize: Searching the novel 2D semiconductor is crucial to develop the next-generation low-dimensional electronic device. Using first-principles calculations, we propose a class of unexplored binary V-V compound semiconductor with monolayer black phosphorene and blue phosphorene structure. Our phonon spectra and room-temperature molecular dynamics calculations indicate that all compounds are very stable. Moreover, most of compounds are found to present a moderate energy gap in the visible frequency range, which can be tuned gradually by in-plane strain. Especially, ",0.1333333333],["mobile traffic offloading scheme in heterogeneous network. the scheme is designed to","Mobile Traffic Offloading with Forecasting using Deep Reinforcement Learning","summarize: With the explosive growth in demand for mobile traffic, one of the promising solutions is to offload cellular traffic to small base stations for better system efficiency. Due to increasing system complexity, network operators are facing severe challenges and looking for machine learning-based solutions. In this work, we propose an energy-aware mobile traffic offloading scheme in the heterogeneous network jointly apply deep Q network decision making and advanced traffic demand forecasting. The base station control model is trained and verified on an open dataset from a major telecom operator. The performance evaluation shows that DQN-based methods outperform others at all levels of mobile traffic demand. Also, the advantage of accurate traffic prediction is more significant under higher traffic demand.",0.1333333333],["the optimal harvesting strategy is determined for the generic configuration of a flexible cable fixed at both","Optimal Energy Harvesting from Vortex-Induced Vibrations of Cables","summarize: Vortex-induced vibrations of flexible cables are an example of flow-induced vibrations that can act as energy harvesting systems by converting energy associated with the spontaneous cable motion into electricity. This work investigates the optimal positioning of the harvesting devices along the cable, using numerical simulations with a wake oscillator model to describe the unsteady flow forcing. Using classical gradient-based optimization, the optimal harvesting strategy is determined for the generic configuration of a flexible cable fixed at both ends, including the effect of flow forces and gravity on the cable's geometry. The optimal strategy is found to consist systematically in a concentration of the harvesting devices at one of the cable's ends, relying on deformation waves along the cable to carry the energy toward this harvesting site. Furthermore, we show that the performance of systems based on VIV of flexible cables is significantly more robust to flow velocity variations, in comparison with a rigid cylinder device. This results from two passive control mechanisms inherent to the cable geometry : the adaptability to the flow velocity of the fundamental frequencies of cables through the flow-induced tension and the selection of successive vibration modes by the flow velocity for cables with gravity-induced tension.",0.2222222222],["resulting network architecture for image classification becomes provably scale covariant. the approach","Scale-covariant and scale-invariant Gaussian derivative networks","summarize: This paper presents a hybrid approach between scale-space theory and deep learning, where a deep learning architecture is constructed by coupling parameterized scale-space operations in cascade. By sharing the learnt parameters between multiple scale channels, and by using the transformation properties of the scale-space primitives under scaling transformations, the resulting network becomes provably scale covariant. By in addition performing max pooling over the multiple scale channels, a resulting network architecture for image classification also becomes provably scale invariant. We investigate the performance of such networks on the MNISTLargeScale dataset, which contains rescaled images from original MNIST over a factor of 4 concerning training data and over a factor of 16 concerning testing data. It is demonstrated that the resulting approach allows for scale generalization, enabling good performance for classifying patterns at scales not present in the training data.",0.25],["contract verification aims to guarantee all or most of these properties ahead of time. existing methods","Soft Contract Verification for Higher-Order Stateful Programs","summarize: Software contracts allow programmers to state rich program properties using the full expressive power of an object language. However, since they are enforced at runtime, monitoring contracts imposes significant overhead and delays error discovery. So contract verification aims to guarantee all or most of these properties ahead of time, enabling valuable optimizations and yielding a more general assurance of correctness. Existing methods for static contract verification satisfy the needs of more restricted target languages, but fail to address the challenges unique to those conjoining untyped, dynamic programming, higher-order functions, modularity, and statefulness. Our approach tackles all these features at once, in the context of the full Racket system---a mature environment for stateful, higher-order, multi-paradigm programming with or without types. Evaluating our method using a set of both pure and stateful benchmarks, we are able to verify 99.94% of checks statically . Stateful, higher-order functions pose significant challenges for static contract verification in particular. In the presence of these features, a modular analysis must permit code from the current module to escape permanently to an opaque context that may be stateful and therefore store a reference to the escaped closure. Also, contracts themselves, being predicates wri en in unrestricted Racket, may exhibit stateful behavior; a sound approach must be robust to contracts which are arbitrarily expressive and interwoven with the code they monitor. In this paper, we present and evaluate our solution based on higher-order symbolic execution, explain the techniques we used to address such thorny issues, formalize a notion of behavioral approximation, and use it to provide a mechanized proof of soundness.",0.1666666667],["quantum gravity is a reasonable form of quantum gravity. cosmologies with an average","The rotation problem","summarize: Any reasonable form of quantum gravity can explain why on a large scale, inertial frames seem not to rotate relative to the average matter distribution in the universe without the need for absolute space, finely tuned initial conditions, or without giving up independent degrees of freedom for the gravitational field. A simple saddlepoint approximation to a path-integral calculation for a perfect fluid cosmology shows that only cosmologies with an average present relative rotation rate smaller than about ",0.0357142857],["this paper deals with the theory of the braids from chromatic configuration spaces. this","Crossing-changeable braids from chromatic configuration spaces","summarize: Motivated by the work in , this paper deals with the theory of the braids from chromatic configuration spaces. This kind of braids possess the property that some strings of each braid may intersect together and can also be untangled, so they are quite different from the ordinary braids in the sense of Artin. This enriches and extends the theory of ordinary braids.",0.2],["randomized methods for numerical linear algebra have received growing interest as a general approach to large-","A Bootstrap Method for Error Estimation in Randomized Matrix Multiplication","summarize: In recent years, randomized methods for numerical linear algebra have received growing interest as a general approach to large-scale problems. Typically, the essential ingredient of these methods is some form of randomized dimension reduction, which accelerates computations, but also creates random approximation error. In this way, the dimension reduction step encodes a tradeoff between cost and accuracy. However, the exact numerical relationship between cost and accuracy is typically unknown, and consequently, it may be difficult for the user to precisely know how accurate a given solution is, or how much computation is needed to achieve a given level of accuracy. In the current paper, we study randomized matrix multiplication as a prototype setting for addressing these general problems. As a solution, we develop a bootstrap method for \\emph the accuracy as a function of the reduced dimension . From a computational standpoint, the proposed method does not substantially increase the cost of standard sketching methods, and this is made possible by an extrapolation technique. In addition, we provide both theoretical and empirical results to demonstrate the effectiveness of the proposed method.",0.2222222222],["patching is a common activity in software development. it is generally performed on a","FixMiner: Mining Relevant Fix Patterns for Automated Program Repair","summarize: Patching is a common activity in software development. It is generally performed on a source code base to address bugs or add new functionalities. In this context, given the recurrence of bugs across projects, the associated similar patches can be leveraged to extract generic fix actions. While the literature includes various approaches leveraging similarity among patches to guide program repair, these approaches often do not yield fix patterns that are tractable and reusable as actionable input to APR systems. In this paper, we propose a systematic and automated approach to mining relevant and actionable fix patterns based on an iterative clustering strategy applied to atomic changes within patches. The goal of FixMiner is thus to infer separate and reusable fix patterns that can be leveraged in other patch generation systems. Our technique, FixMiner, leverages Rich Edit Script which is a specialized tree structure of the edit scripts that captures the AST-level context of the code changes. FixMiner uses different tree representations of Rich Edit Scripts for each round of clustering to identify similar changes. These are abstract syntax trees, edit actions trees, and code context trees. We have evaluated FixMiner on thousands of software patches collected from open source projects. Preliminary results show that we are able to mine accurate patterns, efficiently exploiting change information in Rich Edit Scripts. We further integrated the mined patterns to an automated program repair prototype, PARFixMiner, with which we are able to correctly fix 26 bugs of the Defects4J benchmark. Beyond this quantitative performance, we show that the mined fix patterns are sufficiently relevant to produce patches with a high probability of correctness: 81% of PARFixMiner's generated plausible patches are correct.",0.4137931034],["fix some prime number.","The 1-eigenspace for matrices in ","summarize: Fix some prime number ",0.0],["Narvis is a slideshow authoring tool designed for introducing data visualizations to non","Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs","summarize: Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the non-experts. However, few if any presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end-users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end-users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshows.",0.5],["this study uses deep learning based multiple regression algorithm. we predict the TCWV with","Deep Learning based Multiple Regression to Predict Total Column Water Vapor from Physical Parameters in West Africa by using Keras Library","summarize: Total column water vapor is an important factor for the weather and climate. This study apply deep learning based multiple regression to map the TCWV with elements that can improve spatiotemporal prediction. In this study, we predict the TCWV with the use of ERA5 that is the fifth generation ECMWF atmospheric reanalysis of the global climate. We use an appropriate deep learning based multiple regression algorithm using Keras library to improve nonlinear prediction between Total Column water vapor and predictors as Mean sea level pressure, Surface pressure, Sea surface temperature, 100 metre U wind component, 100 metre V wind component, 10 metre U wind component, 10 metre V wind component, 2 metre dew point temperature, 2 metre temperature. The results obtained permit to build a predictor which modelling TCWV with a mean abs error equal to 3.60 kg\/m2 and a coefficient of determination R2 equal to 0.90.",0.0433236186],["the master wants to distribute the data to untrusted workers who have volunteered or are in","Minimizing Latency for Secure Coded Computing Using Secret Sharing via Staircase Codes","summarize: We consider the setting of a Master server, M, who possesses confidential data and wants to run intensive computations on it, as part of a machine learning algorithm for example. The Master wants to distribute these computations to untrusted workers who have volunteered or are incentivized to help with this task. However, the data must be kept private and not revealed to the individual workers. Some of the workers may be stragglers, e.g., slow or busy, and will take a random time to finish the task assigned to them. We are interested in reducing the delays experienced by the Master. We focus on linear computations as an essential operation in many iterative algorithms such as principal component analysis, support vector machines and other gradient-descent based algorithms. A classical solution is to use a linear secret sharing scheme, such as Shamir's scheme, to divide the data into secret shares on which the workers can perform linear computations. However, classical codes can provide straggler mitigation assuming a worst-case scenario of a fixed number of stragglers. We propose a solution based on new secure codes, called Staircase codes, introduced previously by two of the authors. Staircase codes allow flexibility in the number of stragglers up to a given maximum, and universally achieve the information theoretic limit on the download cost by the Master, leading to latency reduction. Under the shifted exponential model, we find upper and lower bounds on the Master's mean waiting time. We derive the distribution of the Master's waiting time, and its mean, for systems with up to two stragglers. For systems with any number of stragglers, we derive an expression that can give the exact distribution, and the mean, of the waiting time of the Master. We show that Staircase codes always outperform classical secret sharing codes.",0.0952380952],["bio-inspired workpiece structural optimization approach is presented in this paper. aim of this method","Bio-inspired method based on bone architecture to optimize the structure of mechanical workspieces","summarize: Nowadays, additive manufacturing processes greatly simplify the production of openwork workpiece providing new opportunities for workpieces design. Based on Nature knowledge, a new bio-inspired workpiece structural optimization approach is presented in this paper. This approach is derived from bones structure. The aim of this method is to reduce the workpiece weight maintaining an acceptable resistance. Like in bones, the porosity of the part to optimize was controlled by a bio-inspired method as function of the local stress field. Shape, size and orientation of the porosities were derived from bone structure; two main strategies were used: one inspired of avian species and other inspired of terrestrial mammalian. Subsequently, to validate this method, an experimental test was carried out for comparing a topological optimization and the proposed bio-inspired designs. This test was conducted on a beam part in 2.5D subjected to a static three-point bending with 65% of density. Three beams were manufactured by 3D metal printing: two bio-inspired beams and the last designed using a topological optimization method. Experimental test results demonstrated the usefulness of the proposed method. This bio-inspired structural optimization approach opens up new prospects in design of openwork workpiece.",0.1578947368],["the COVID-19 pandemic has been a major challenge to humanity. 1","Considerations, Good Practices, Risks and Pitfalls in Developing AI Solutions Against COVID-19","summarize: The COVID-19 pandemic has been a major challenge to humanity, with 12.7 million confirmed cases as of July 13th, 2020 . In previous work, we described how Artificial Intelligence can be used to tackle the pandemic with applications at the molecular, clinical, and societal scales . In the present follow-up article, we review these three research directions, and assess the level of maturity and feasibility of the approaches used, as well as their potential for operationalization. We also summarize some commonly encountered risks and practical pitfalls, as well as guidelines and best practices for formulating and deploying AI applications at different scales.",0.3759826479],["the same degree of control has not yet been achieved for few-cycle extreme ultraviolet pulses generated","Phase Control of Attosecond Pulses in a Train","summarize: Ultrafast processes in matter can be captured and even controlled by using sequences of few-cycle optical pulses, which need to be well characterized, both in amplitude and phase. The same degree of control has not yet been achieved for few-cycle extreme ultraviolet pulses generated by high-order harmonic generation in gases, with duration in the attosecond range. Here, we show that by varying the spectral phase and carrier-envelope phase of a high-repetition rate laser, using dispersion in glass, we achieve a high degree of control of the relative phase and CEP between consecutive attosecond pulses. The experimental results are supported by a detailed theoretical analysis based upon the semiclassical three-step model for high-order harmonic generation.",0.125],["new model-based reinforcement learning algorithm uses supervision to constrain exploration and learn efficiently while handling complex constraints","Safety Augmented Value Estimation from Demonstrations : Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","summarize: Reinforcement learning for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations , which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations. Code and supplementary material is available at https:\/\/tinyurl.com\/saved-rl.",0.0],["black hole candidate 1E 1740.7-2942 is one of the strongest hard X","Tandem Swift and INTEGRAL Data to Revisit the Orbital and Superorbital Periods of 1E 1740.7-2942","summarize: The black hole candidate 1E 1740.7-2942 is one of the strongest hard X-ray sources in the Galactic Center region. No counterparts in longer wavelengths have been identified for this object yet. The presence of characteristic timing signatures in the flux history of X-ray sources has been shown to be an important diagnostic tool for the properties of these systems. Using simultaneous data from NASA's Swift and ESA's INTEGRAL missions, we have found two periodic signatures at 12.61 ",0.3245003263],["a long baseline interferometer. where two distantly located radio telescopes receive simultaneous","Radio VLBI and the quantum interference paradox","summarize: We address here the question of interference of radio signals from astronomical sources like distant quasars, in a very long baseline interferometer , where two distantly located radio telescopes , receive simultaneous signal from the sky. In an equivalent optical two-slit experiment, it is generally argued that for the photons involved in the interference pattern on the screen, it is not possible, even in principle, to know which of the two slits a particular photon went through and that any procedure to ascertain this destroys the interference pattern. But in the case of the modern radio VLBI, it is a routine matter to record the phase and amplitude of the voltage outputs from the two radio antennas on a recording media separately and then do the correlation between the two recorded signals later in an offline manner. Does this not violate the quantum interference principle? We provide a resolution of this problem here.",0.2941176471],["the structure analysis of atomic liquids is questionable for soft particles. the liquid is","Structure of the simple harmonic-repulsive system in liquid and glassy states studied by the triple correlation function","summarize: An efficient description of the structures of liquids and, in particular, the structural changes that happen with liquids on supercooling remains to be a challenge. The systems composed of soft particles are especially interesting in this context because they often demonstrate non-trivial local orders that do not allow to introduce the concept of the nearest-neighbor shell. For this reason, the use of some methods, developed for the structure analysis of atomic liquids, is questionable for the soft-particle systems. Here we report about our investigations of the structure of the simple harmonic-repulsive liquid in 3D using the triple correlation function , i.e., the method that does not rely on the nearest neighbor concept. The liquid is considered at reduced pressure at which it exhibits remarkable stability against crystallization on cooling. It is demonstrated that the TCF allows addressing the development of the orientational correlations in the structures that do not allow drawing definite conclusions from the studies of the bond-orientational order parameters. Our results demonstrate that the orientational correlations, if measured by the heights of the peaks in the TCF, significantly increase on cooling. This rise in the orientational ordering is not captured properly by the Kirkwood's superposition approximation. Detailed considerations of the peaks' shapes in the TCF suggest the existence of a link between the orientational ordering and the slowdown of the system's dynamics. Our findings support the view that the development of the orientational correlations in liquids may play a significant role in the liquids' dynamics and that the considerations of the pair distribution function may not be sufficient to understand intuitively all the structural changes that happen with liquids on supercooling.",0.2201230219],["the newman-watts model is given by taking a cycle graph of","First passage percolation on the Newman-Watts small world model","summarize: The Newman-Watts model is given by taking a cycle graph of n vertices and then adding each possible edge ",0.4666666667],["the scientific and industrial community has focused on the astonishing properties of Fe-Mn-Al","A General Perspective of Fe-Mn-Al-C Steels","summarize: During the last years, the scientific and industrial community has focused on the astonishing properties of Fe-Mn-Al-C steels. These high advanced steels allow high-density reductions about ~15% lighter than conventional steels, high corrosion resistance, high strength ~1 Gpa) and at the same time ductilities above 60%. The increase of the tensile or yield strength and the ductility at the same time is almost a special feature of this kind of new steels, which makes them so interesting for many applications such as in the automotive, armor and mining industry. The control of these properties depends on a complex relationship between the chemical composition of the steel, the test temperature, the external loads and the processing parameters of the steel. This review has been conceived to tried to elucidate these complex relations and gather the most important aspects of Fe-Mn-Al-C steels developed so far.",0.125],["an analytic approach is initiated by Fayolle, Iasno","Analytic approach for reflected Brownian motion in the quadrant","summarize: Random walks in the quarter plane are an important object both of combinatorics and probability theory. Of particular interest for their study, there is an analytic approach initiated by Fayolle, Iasnogorodski and Malyshev, and further developed by the last two authors of this note. The outcomes of this method are explicit expressions for the generating functions of interest, asymptotic analysis of their coefficients, etc. Although there is an important literature on reflected Brownian motion in the quarter plane , an analogue of the analytic approach has not been fully developed to that context. The aim of this note is twofold: it is first an extended abstract of two recent articles of the authors of this paper, which propose such an approach; we further compare various aspects of the discrete and continuous analytic approaches.",0.2941656342],["a successful conversational Recommender System requires proper handling of interactions between conversation and recommendation","Estimation-Action-Reflection: Towards Deep Interaction Between Conversational and Recommender Systems","summarize: Recommender systems are embracing conversational technologies to obtain user preferences dynamically, and to overcome inherent limitations of their static models. A successful Conversational Recommender System requires proper handling of interactions between conversation and recommendation. We argue that three fundamental problems need to be solved: 1) what questions to ask regarding item attributes, 2) when to recommend items, and 3) how to adapt to the users' online feedback. To the best of our knowledge, there lacks a unified framework that addresses these problems. In this work, we fill this missing interaction framework gap by proposing a new CRS framework named Estimation-Action-Reflection, or EAR, which consists of three stages to better converse with users. Estimation, which builds predictive models to estimate user preference on both items and item attributes; Action, which learns a dialogue policy to determine whether to ask attributes or recommend items, based on Estimation stage and conversation history; and Reflection, which updates the recommender model when a user rejects the recommendations made by the Action stage. We present two conversation scenarios on binary and enumerated questions, and conduct extensive experiments on two datasets from Yelp and LastFM, for each scenario, respectively. Our experiments demonstrate significant improvements over the state-of-the-art method CRM , corresponding to fewer conversation turns and a higher level of recommendation hits.",0.3913043478],["emph is a novel context-assisted single shot face detector.","PyramidBox: A Context-assisted Single Shot Face Detector","summarize: Face detection has been well studied for many years and one of remaining challenges is to detect small, blurred and partially occluded faces in uncontrolled environment. This paper proposes a novel context-assisted single shot face detector, named \\emph to handle the hard face detection problem. Observing the importance of the context, we improve the utilization of contextual information in the following three aspects. First, we design a novel context anchor to supervise high-level contextual feature learning by a semi-supervised method, which we call it PyramidAnchors. Second, we propose the Low-level Feature Pyramid Network to combine adequate high-level context semantic feature and Low-level facial feature together, which also allows the PyramidBox to predict faces of all scales in a single shot. Third, we introduce a context-sensitive structure to increase the capacity of prediction network to improve the final accuracy of output. In addition, we use the method of Data-anchor-sampling to augment the training samples across different scales, which increases the diversity of training data for smaller faces. By exploiting the value of context, PyramidBox achieves superior performance among the state-of-the-art over the two common face detection benchmarks, FDDB and WIDER FACE. Our code is available in PaddlePaddle: \\href}.",0.3333333333],["a novel protocol to solve this rarely investigated question. we call this phenomenon 'correct","Correctness Attraction: A Study of Stability of Software Behavior Under Runtime Perturbation","summarize: Can the execution of a software be perturbed without breaking the correctness of the output? In this paper, we devise a novel protocol to answer this rarely investigated question. In an experimental study, we observe that many perturbations do not break the correctness in ten subject programs. We call this phenomenon ``correctness attraction''. The uniqueness of this protocol is that it considers a systematic exploration of the perturbation space as well as perfect oracles to determine the correctness of the output. To this extent, our findings on the stability of software under execution perturbations have a level of validity that has never been reported before in the scarce related work. A qualitative manual analysis enables us to set up the first taxonomy ever of the reasons behind correctness attraction.",0.2],["we consider the problem of pointwise stabilization of a one-dimensional wave equation.","Rapid exponential stabilization of a 1-D transmission wave equation with in-domain anti-damping","summarize: We consider the problem of pointwise stabilization of a one-dimensional wave equation with an internal spatially varying anti-damping term. We design a feedback law based on the backstepping method and prove exponential stability of the closed-loop system with a desired decay rate.",0.5789473684],["horizontal FL is a new method of perturbed local embedding. it is","VAFL: a Method of Vertical Asynchronous Federated Learning","summarize: Horizontal Federated learning handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochastic gradient algorithms without coordination with other clients, so it is suitable for intermittent connectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.",0.3125],["we define non-commutative versions of the vertex packing polytope, thet","Sandwich theorems and capacity bounds for non-commutative graphs","summarize: We define non-commutative versions of the vertex packing polytope, the theta convex body and the fractional vertex packing polytope of a graph, and establish a quantum version of the Sandwich Theorem of Gr\\tschel, Lov\\'sz and Schrijver. We define new non-commutative versions of the Lov\\'sz number of a graph which lead to an upper bound of the zero-error capacity of the corresponding quantum channel that can be genuinely better than the one established previously by Duan, Severini and Winter. We define non-commutative counterparts of widely used classical graph parameters and establish their interrelation.",0.1818181818],["a new dynamic of scientific articles dissemination was initiated. the view profiles along time tend to","Classification of abrupt changes along viewing profiles of scientific articles","summarize: With the expansion of electronic publishing, a new dynamics of scientific articles dissemination was initiated. Nowadays, many works are widely disseminated even before publication, in the form of preprints. Another important new element concerns the views of published articles. Thanks to the availability of respective data by some journals, such as PLoS ONE, it became possible to develop investigations on how scientific works are viewed along time, often before the first citations appear. This provides the main theme of the present work. More specifically, our research was motivated by preliminary observations that the view profiles along time tend to present a piecewise linear nature. A methodology was then delineated in order to identify the main segments in the view profiles, which allowed several related measurements to be derived. In particular, we focused on the inclination and length of each subsequent segment. Basic statistics indicated that the inclination can vary substantially along subsequent segments, while the segment lengths resulted more stable. Complementary joint statistics analysis, considering pairwise correlations, provided further information about the properties of the views. In order to better understand the view profiles, we performed respective multivariate statistical analysis, including principal component analysis and hierarchical clustering. The results suggest that a portion of the polygonal views are organized into clusters or groups. These groups were characterized in terms of prototypes indicating the relative increase or decrease along subsequent segments. Four respective distinct models were then developed for representing the observed segments. It was found that models incorporating joint dependencies between the properties of the segments provided the most accurate results among the considered alternatives.",0.5652173913],["quantum Internet requires knowledge about link qualities used for optimal route selection. ruleSet-based protocol","Simulation of a Dynamic, RuleSet-based Quantum Network","summarize: Similar to the classical Internet, the quantum Internet will require knowledge regarding link qualities used for purposes such as optimal route selection. This is commonly accomplished by performing link-level tomography with or without purification -- a.k.a. quantum link bootstrapping. Meanwhile, the gate selection and the resource selection for a task must be coordinated beforehand. This thesis introduces the RuleSet-based communication protocol aimed for supporting the autonomous coordination of quantum operations among distant nodes, with minimal classical packet transmission. This thesis also discusses the RuleSet-based quantum link bootstrapping protocol, which consists of recurrent purifications and link-level tomography, evaluated over a Markov-Chain Monte-Carlo simulation with noisy systems modeled on real world quality hardware. Given a 10km MeetInTheMiddle based two-node system, each with 100 memory qubits ideally connected to the optical fiber, the Recurrent Single selection - Single error purification protocol is capable of improving the fidelity from an average input ",0.0],["Let us know what you think about it!","Unitary Subgroups of commutative group algebras of characteristic two","summarize: Let ",0.0],["two new series for the logarithm of the logarithm of the loga","Two series expansions for the logarithm of the gamma function involving Stirling numbers and containing only rational coefficients for certain arguments related to ","summarize: In this paper, two new series for the logarithm of the ",0.1992680306],["no theoretical origin for GNNs has been proposed. we propose subspace power iter","Understanding the Message Passing in Graph Neural Networks via Power Iteration Clustering","summarize: The mechanism of message passing in graph neural networks is still mysterious. Apart from convolutional neural networks, no theoretical origin for GNNs has been proposed. To our surprise, message passing can be best understood in terms of power iteration. By fully or partly removing activation functions and layer weights of GNNs, we propose subspace power iteration clustering models that iteratively learn with only one aggregator. Experiments show that our models extend GNNs and enhance their capability to process random featured networks. Moreover, we demonstrate the redundancy of some state-of-the-art GNNs in design and define a lower limit for model evaluation by a random aggregator of message passing. Our findings push the boundaries of the theoretical understanding of neural networks.",0.0666666667],["soft robotic system employs the inherent qualities of soft fluidic actuators. it establishe","Design and integration of a parallel, soft robotic end-effector for extracorporeal ultrasound","summarize: Objective: In this work we address limitations in state-of-the-art ultrasound robots by designing and integrating a novel soft robotic system for ultrasound imaging. It employs the inherent qualities of soft fluidic actuators to establish safe, adaptable interaction between ultrasound probe and patient. Methods: We acquire clinical data to determine the movement ranges and force levels required in prenatal foetal ultrasound imaging and design the soft robotic end-effector accordingly. We verify its mechanical characteristics, derive and validate a kinetostatic model and demonstrate controllability and imaging capabilities on an ultrasound phantom. Results: The soft robot exhibits the desired stiffness characteristics and is able to reach 100% of the required workspace when no external force is present, and 95% of the workspace when considering its compliance. The model can accurately predict the end-effector pose with a mean error of 1.18+\/-0.29mm in position and 0.92+\/-0.47deg in orientation. The derived controller is, with an average position error of 0.39mm, able to track a target pose efficiently without and with externally applied loads. Ultrasound images acquired with the system are of equally good quality compared to a manual sonographer scan. Conclusion: The system is able to withstand loads commonly applied during foetal ultrasound scans and remains controllable with a motion range similar to manual scanning. Significance: The proposed soft robot presents a safe, cost-effective solution to offloading sonographers in day-to-day scanning routines. The design and modelling paradigms are greatly generalizable and particularly suitable for designing soft robots for physical interaction tasks.",0.2777777778],["the multi-sensory setups consisting of the laser scanners and cameras are popular as","Spatiotemporal Calibration of Camera and 3D Laser Scanner","summarize: The multi-sensory setups consisting of the laser scanners and cameras are popular as the measurements complement each other and provide necessary robustness for applications. Under dynamic conditions or when in motion, a direct transformation and time offset between sensors is needed to determine the correspondence between measurements. We propose an open-source spatiotemporal calibration framework for a camera and a 3D laser scanner. Our solution is based on commonly available chessboard markers requiring one-minute calibration before the operation that offers accurate and repeatable results. The framework is based on batch optimization of point-to-plane constraints with a time offset calibration possible by a novel continuous representation of the plane equations based on a minimal representation in the Lie algebra and the use of B-splines. The framework's properties are evaluated in simulation while correctness is verified with two distinct sensory setups with Velodyne VLP-16 and SICK MRS6124 3D laser scanners.",0.1764705882],["the central idea is that after reduction the twisting motion is apparent in a body frame","Twisting Somersault","summarize: A complete description of twisting somersaults is given using a reduction to a time-dependent Euler equation for non-rigid body dynamics. The central idea is that after reduction the twisting motion is apparent in a body frame, while the somersaulting is recovered by a combination of dynamic and geometric phase. In the simplest kick-model the number of somersaults ",0.1333333333],["the goal of this small note is to give a more concise proof of a result due","A new proof of a vanishing result due to Berthelot, Esnault, and R\\ulling","summarize: The goal of this small note is to give a more concise proof of a result due to Berthelot, Esnault, and R\\ulling. For a regular, proper, and flat scheme ",0.6206896552],["compression is used to reduce communication costs in distributed data-parallel training of deep neural networks","On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning","summarize: Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layer-wise and entire-model compression.",0.1944829598],["inverse problem is usually solved through a regularized least-squares approach.","Automatic alignment for three-dimensional tomographic reconstruction","summarize: In tomographic reconstruction, the goal is to reconstruct an unknown object from a collection of line integrals. Given a complete sampling of such line integrals for various angles and directions, explicit inverse formulas exist to reconstruct the object. Given noisy and incomplete measurements, the inverse problem is typically solved through a regularized least-squares approach. A challenge for both approaches is that in practice the exact directions and offsets of the x-rays are only known approximately due to, e.g. calibration errors. Such errors lead to artifacts in the reconstructed image. In the case of sufficient sampling and geometrically simple misalignment, the measurements can be corrected by exploiting so-called consistency conditions. In other cases, such conditions may not apply and we have to solve an additional inverse problem to retrieve the angles and shifts. In this paper we propose a general algorithmic framework for retrieving these parameters in conjunction with an algebraic reconstruction technique. The proposed approach is illustrated by numerical examples for both simulated data and an electron tomography dataset.",0.25],["a compact object orbits a larger object, like a solar-massive","Understanding the importance of transient resonances in extreme mass ratio inspirals","summarize: Extreme mass ratio inspirals occur when a compact object orbits a much larger one, like a solar-mass black hole around a supermassive black hole. The orbit has 3 frequencies which evolve through the inspiral. If the orbital radial frequency and polar frequency become commensurate, the system passes through a transient resonance. Evolving through resonance causes a jump in the evolution of the orbital parameters. We study these jumps and their impact on EMRI gravitational-wave detection. Jumps are smaller for lower eccentricity orbits; since most EMRIs have small eccentricities when passing through resonances, we expect that the impact on detection will be small. Neglecting the effects of transient resonances leads to a loss of ~4% of detectable signals for an astrophysically motivated population of EMRIs.",0.6552270958],["Madden has shown that in contrast to the situation with frames, the smallest dense quot","A special class of congruences on ","summarize: Madden has shown that in contrast to the situation with frames, the smallest dense quotient of a ",0.0],["generative adversarial networks are used to generate realistic data. the generator of the G","Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks","summarize: Chest X-ray is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",0.1176470588],["the information acquisition problem is based on a planning algorithm on known models. the proposed","Learning Q-network for Active Information Acquisition","summarize: In this paper, we propose a novel Reinforcement Learning approach for solving the Active Information Acquisition problem, which requires an agent to choose a sequence of actions in order to acquire information about a process of interest using on-board sensors. The classic challenges in the information acquisition problem are the dependence of a planning algorithm on known models and the difficulty of computing information-theoretic cost functions over arbitrary distributions. In contrast, the proposed framework of reinforcement learning does not require any knowledge on models and alleviates the problems during an extended training stage. It results in policies that are efficient to execute online and applicable for real-time control of robotic systems. Furthermore, the state-of-the-art planning methods are typically restricted to short horizons, which may become problematic with local minima. Reinforcement learning naturally handles the issue of planning horizon in information problems as it maximizes a discounted sum of rewards over a long finite or infinite time horizon. We discuss the potential benefits of the proposed framework and compare the performance of the novel algorithm to an existing information acquisition method for multi-target tracking scenarios.",0.2413793103],["reinforcement learning is a popular paradigm for addressing sequential decision tasks. learning in many domain","Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey","summarize: Reinforcement learning is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.",0.3235294118],["we propose a power and data transfer network on a conductive fabric material based on","Inter-IC for Wearables : Power and Data Transfer over Double-sided Conductive Textile","summarize: We propose a power and data transfer network on a conductive fabric material based on an existing serial communication protocol, Inter-Integrated Circuit . We call the proposed network Inter-IC for Wearables . Continuous dc power and I2C-formatted data are simultaneously transferred to tiny sensor nodes distributed on a double-sided conductive textile. The textile has two conductive sides isolated from each other and is used as a single planar transmission line. I2C data are transferred along with dc power supply based on frequency division multiplexing . Two carriers are modulated with the clock and the data signals of I2C. A modulation and demodulation circuit is designed to enable using off-the-shelf I2C-interfaced sensor ICs. One significant originality of this work is that a special filter to enable passive modulation is designed by locating its impedance poles and zeros at appropriate frequencies. The proposed scheme enables flexible implementation of wearable sensor systems in which multiple off-the-shelf tiny sensors are distributed all over a wear.",0.4210526316],["human-subject study aims to explore two facets of human mental models of robots","Robot Capability and Intention in Trust-based Decisions across Tasks","summarize: In this paper, we present results from a human-subject study designed to explore two facets of human mental models of robots---inferred capability and intention---and their relationship to overall trust and eventual decisions. In particular, we examine delegation situations characterized by uncertainty, and explore how inferred capability and intention are applied across different tasks. We develop an online survey where human participants decide whether to delegate control to a simulated UAV agent. Our study shows that human estimations of robot capability and intent correlate strongly with overall self-reported trust. However, overall trust is not independently sufficient to determine whether a human will decide to trust a given task to a robot. Instead, our study reveals that estimations of robot intention, capability, and overall trust are integrated when deciding to delegate. From a broader perspective, these results suggest that calibrating overall trust alone is insufficient; to make correct decisions, humans need multi-faceted mental models when collaborating with robots across multiple contexts.",0.0],["the unique information measure quantifies a deviation from the blackwell order. the quantity is","Unique Information and Secret Key Decompositions","summarize: The unique information is an information measure that quantifies a deviation from the Blackwell order. We have recently shown that this quantity is an upper bound on the one-way secret key rate. In this paper, we prove a triangle inequality for the ",0.0769230769],["static analysis can predict uses of invokedynamic while also cooperating with extra rules to handle runtime","Deep Static Modeling of invokedynamic","summarize: Java 7 introduced programmable dynamic linking in the form of the invokedynamic framework. Static analysis of code containing programmable dynamic linking has often been cited as a significant source of unsoundness in the analysis of Java programs. For example, Java lambdas, introduced in Java 8, are a very popular feature, which is, however, resistant to static analysis, since it mixes invokedynamic with dynamic code generation. These techniques invalidate static analysis assumptions: programmable linking breaks reasoning about method resolution while dynamically generated code is, by definition, not available statically. In this paper, we show that a static analysis can predictively model uses of invokedynamic while also cooperating with extra rules to handle the runtime code generation of lambdas. Our approach plugs into an existing static analysis and helps eliminate all unsoundness in the handling of lambdas and generic invokedynamic uses. We evaluate our technique on a benchmark suite of our own and on third-party benchmarks, uncovering all code previously unreachable due to unsoundness, highly efficiently.",0.125],["cooperation in repeated public goods game is hardly achieved unless contingent behavior is present.","Cooperation in public goods games: stay, but not for too long","summarize: Cooperation in repeated public goods game is hardly achieved, unless contingent behavior is present. Surely, if mechanisms promoting positive assortment between cooperators are present, then cooperators may beat defectors, because cooperators would collect greater payoffs. In the context of evolutionary game theory, individuals that always cooperate cannot win the competition against defectors in well-mixed populations. Here, we study the evolution of a population where fitness is obtained in repeated public goods games and players have a fixed probability of playing the next round. As a result, the group size decreases during the game. The population is well-mixed and there are only two available strategies: always cooperate or always defect . Through numerical calculation and analytical approximations we show that cooperation can emerge if the players stay playing the game, but not for too long. The essential mechanism is the interaction between the transition from strong to weak altruism, as the group size decreases, and the existence of an upper limit to the number of rounds representing limited time availability.",0.2352941176],["a self-supervised framework is proposed to address this problem. it uses surveillance video sequence","AED-Net: An Abnormal Event Detection Network","summarize: It is challenging to detect the anomaly in crowded scenes for quite a long time. In this paper, a self-supervised framework, abnormal event detection network , which is composed of PCAnet and kernel principal component analysis , is proposed to address this problem. Using surveillance video sequences of different scenes as raw data, PCAnet is trained to extract high-level semantics of crowd's situation. Next, kPCA,a one-class classifier, is trained to determine anomaly of the scene. In contrast to some prevailing deep learning methods,the framework is completely self-supervised because it utilizes only video sequences in a normal situation. Experiments of global and local abnormal event detection are carried out on UMN and UCSD datasets, and competitive results with higher EER and AUC compared to other state-of-the-art methods are observed. Furthermore, by adding local response normalization layer, we propose an improvement to original AED-Net. And it is proved to perform better by promoting the framework's generalization capacity according to the experiments.",0.0526315789],["underwater image enhancement algorithms have been proposed in the last few years. however, these algorithms are","An Underwater Image Enhancement Benchmark Dataset and Beyond","summarize: Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an underwater image enhancement network trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEB for training Convolutional Neural Networks . The benchmark evaluations and the proposed Water-Net demonstrate the performance and limitations of state-of-the-art algorithms, which shed light on future research in underwater image enhancement. The dataset and code are available at https:\/\/li-chongyi.github.io\/proj_benchmark.html.",0.0],["the Argonne v18 and Urbana IX nuclear potentials are constructed for core","Nuclear equation of state for core-collapse supernova simulations with realistic nuclear forces","summarize: A new table of the nuclear equation of state based on realistic nuclear potentials is constructed for core-collapse supernova numerical simulations. Adopting the EOS of uniform nuclear matter constructed by two of the present authors with the cluster variational method starting from the Argonne v18 and Urbana IX nuclear potentials, the Thomas-Fermi calculation is performed to obtain the minimized free energy of a Wigner-Seitz cell in non-uniform nuclear matter. As a preparation for the Thomas-Fermi calculation, the EOS of uniform nuclear matter is modified so as to remove the effects of deuteron cluster formation in uniform matter at low densities. Mixing of alpha particles is also taken into account following the procedure used by Shen et al. . The critical densities with respect to the phase transition from non-uniform to uniform phase with the present EOS are slightly higher than those with the Shen EOS at small proton fractions. The critical temperature with respect to the liquid-gas phase transition decreases with the proton fraction in a more gradual manner than in the Shen EOS. Furthermore, the mass and proton numbers of nuclides appearing in non-uniform nuclear matter with small proton fractions are larger than those of the Shen EOS. These results are consequences of the fact that the density derivative coefficient of the symmetry energy of our EOS is smaller than that of the Shen EOS.",0.25],["a full study of an improved nonlinear plasmonic slot waveguides.","Improved nonlinear plasmonic slot waveguide: a full study","summarize: We present a full study of an improved nonlinear plasmonic slot waveguides in which buffer linear dielectric layers are added between the Kerr type nonlinear dielectric core and the two semi-infinite metal regions. For TM polarized waves, the inclusion of these supplementary layers have two consequences. First, they reduced the overall losses. Secondly, they modify the types of solutions that propagate in the NPSWs adding new profiles enlarging the possibilities offered by these nonlinear waveguides. Our structure also provides longer propagation length due to the decrease of the losses compared to the simple nonlinear slot waveguide and exhibits, for well-chosen refractive index or thickness of the buffer layer, a spatial transition of its main modes that can be controlled by the power. We provide a full phase diagram of the TM wave operating regimes of these improved NPSWs. The stability of the main TM modes is then demonstrated numerically using the FDTD. We also demonstrate the existence of TE waves for both linear and nonlinear cases in which the maximum intensity is located in the middle of the waveguide. This kind of structures could be fabricated and characterized experimentally due to the realistic parameters chosen to model them.",0.6428571429],["we consider convergence of alternating projections between non-convex sets. we obtain","Alternating projections with applications to Gerchberg-Saxton error reduction","summarize: We consider convergence of alternating projections between non-convex sets and obtain applications to convergence of the Gerchberg-Saxton error reduction method, of the Gaussian expectation-maximization algorithm, and of Cadzow's algorithm.",0.0666666667],["rural building mapping is paramount to support demographic studies. but quality and quantity are not sufficient for","Correcting rural building annotations in OpenStreetMap using convolutional neural networks","summarize: Rural building mapping is paramount to support demographic studies and plan actions in response to crisis that affect those areas. Rural building annotations exist in OpenStreetMap , but their quality and quantity are not sufficient for training models that can create accurate rural building maps. The problems with these annotations essentially fall into three categories: most commonly, many annotations are geometrically misaligned with the updated imagery; some annotations do not correspond to buildings in the images ; and some annotations are missing for buildings in the images . First, we propose a method based on Markov Random Field to align the buildings with their annotations. The method maximizes the correlation between annotations and a building probability map while enforcing that nearby buildings have similar alignment vectors. Second, the annotations with no evidence in the building probability map are removed. Third, we present a method to detect non-annotated buildings with predefined shapes and add their annotation. The proposed methodology shows considerable improvement in accuracy of the OSM annotations for two regions of Tanzania and Zimbabwe, being more accurate than state-of-the-art baselines.",0.1764705882],["branch-and-bound is a decision space search method. the branching is performed","Branch-and-bound for biobjective mixed-integer linear programming","summarize: We present a generic branch-and-bound algorithm for finding all the Pareto solutions of a biobjective mixed-integer linear program. The main contributions are new algorithms for obtaining dual bounds at a node, for checking node fathoming, presolve and duality gap measurement. Our branch-and-bound is predominantly a decision space search method since the branching is performed on the decision variables, akin to single objective problems, although we also sometimes split gaps and branch in the objective space. The various algorithms are implemented using a data structure for storing Pareto sets. Computational experiments are carried out on literature instances and also on a new set of instances that we generate using the MIPLIB benchmark library for single objective problems. We also perform comparisons against the triangle splitting method from literature, which is an objective space search algorithm.",0.2],["black hole candidate 1E 1740.7-2942 is one of the strongest hard X","Tandem Swift and INTEGRAL Data to Revisit the Orbital and Superorbital Periods of 1E 1740.7-2942","summarize: The black hole candidate 1E 1740.7-2942 is one of the strongest hard X-ray sources in the Galactic Center region. No counterparts in longer wavelengths have been identified for this object yet. The presence of characteristic timing signatures in the flux history of X-ray sources has been shown to be an important diagnostic tool for the properties of these systems. Using simultaneous data from NASA's Swift and ESA's INTEGRAL missions, we have found two periodic signatures at 12.61 ",0.3245003263],["algorithm predicts the final grade of each student in a class. it issues a","Predicting Grades","summarize: To increase efficacy in traditional classroom courses as well as in Massive Open Online Courses , automated systems supporting the instructor are needed. One important problem is to automatically detect students that are going to do poorly in a course early enough to be able to take remedial actions. Existing grade prediction systems focus on maximizing the accuracy of the prediction while overseeing the importance of issuing timely and personalized predictions. This paper proposes an algorithm that predicts the final grade of each student in a class. It issues a prediction for each student individually, when the expected accuracy of the prediction is sufficient. The algorithm learns online what is the optimal prediction and time to issue a prediction based on past history of students' performance in a course. We derive a confidence estimate for the prediction accuracy and demonstrate the performance of our algorithm on a dataset obtained based on the performance of approximately 700 UCLA undergraduate students who have taken an introductory digital signal processing over the past 7 years. We demonstrate that for 85% of the students we can predict with 76% accuracy whether they are going do well or poorly in the class after the 4th course week. Using data obtained from a pilot course, our methodology suggests that it is effective to perform early in-class assessments such as quizzes, which result in timely performance prediction for each student, thereby enabling timely interventions by the instructor when necessary.",0.1071428571],["our CACHEFIX framework verifies the cache side-channel freedom of a program","Symbolic Verification of Cache Side-channel Freedom","summarize: Cache timing attacks allow third-party observers to retrieve sensitive information from program executions. But, is it possible to automatically check the vulnerability of a program against cache timing attacks and then, automatically shield program executions against these attacks? For a given program, a cache configuration and an attack model, our CACHEFIX framework either verifies the cache side-channel freedom of the program or synthesizes a series of patches to ensure cache side-channel freedom during program execution. At the core of our framework is a novel symbolic verification technique based on automated abstraction refinement of cache semantics. The power of such a framework is to allow symbolic reasoning over counterexample traces and to combine it with runtime monitoring for eliminating cache side channels during program execution. Our evaluation with routines from OpenSSL, libfixedtimefixedpoint, GDK and FourQlib libraries reveals that our CACHEFIX approach proves cache sidechannel freedom within an average of 75 seconds. Besides, in all except one case, CACHEFIX synthesizes all patches within 20 minutes to ensure cache side-channel freedom of the respective routines during execution.",0.2666666667],["the concept of type in programming languages is not the same as it was perceived in the sixties","Several types of types in programming languages","summarize: Types are an important part of any modern programming language, but we often forget that the concept of type we understand nowadays is not the same it was perceived in the sixties. Moreover, we conflate the concept of type in programming languages with the concept of the same name in mathematical logic, an identification that is only the result of the convergence of two different paths, which started apart with different aims. The paper will present several remarks on the subject, as a basis for a further investigation. The thesis we will argue is that there are three different characters at play in programming languages, all of them now called types: the technical concept used in language design to guide implementation; the general abstraction mechanism used as a modelling tool; the classifying tool inherited from mathematical logic. We will suggest three possible dates ad quem for their presence in the programming language literature, suggesting that the emergence of the concept of type in computer science is relatively independent from the logical tradition, until the Curry-Howard isomorphism will make an explicit bridge between them.",0.275862069],["present a visualization framework for annotating and comparing colonoscopy videos.","Visualization Framework for Colonoscopy Videos","summarize: We present a visualization framework for annotating and comparing colonoscopy videos, where these annotations can then be used for semi-automatic report generation at the end of the procedure. Currently, there are approximately 14 million colonoscopies performed every year in the US. In this work, we create a visualization tool to deal with the deluge of colonoscopy videos in a more effective way. We present an interactive visualization framework for the annotation and tagging of colonoscopy videos in an easy and intuitive way. These annotations and tags can later be used for report generation for electronic medical records and for comparison at an individual as well as group level. We also present important use cases and medical expert feedback for our visualization framework.",0.2352941176],["the master wants to distribute the data to untrusted workers who have volunteered or are in","Minimizing Latency for Secure Coded Computing Using Secret Sharing via Staircase Codes","summarize: We consider the setting of a Master server, M, who possesses confidential data and wants to run intensive computations on it, as part of a machine learning algorithm for example. The Master wants to distribute these computations to untrusted workers who have volunteered or are incentivized to help with this task. However, the data must be kept private and not revealed to the individual workers. Some of the workers may be stragglers, e.g., slow or busy, and will take a random time to finish the task assigned to them. We are interested in reducing the delays experienced by the Master. We focus on linear computations as an essential operation in many iterative algorithms such as principal component analysis, support vector machines and other gradient-descent based algorithms. A classical solution is to use a linear secret sharing scheme, such as Shamir's scheme, to divide the data into secret shares on which the workers can perform linear computations. However, classical codes can provide straggler mitigation assuming a worst-case scenario of a fixed number of stragglers. We propose a solution based on new secure codes, called Staircase codes, introduced previously by two of the authors. Staircase codes allow flexibility in the number of stragglers up to a given maximum, and universally achieve the information theoretic limit on the download cost by the Master, leading to latency reduction. Under the shifted exponential model, we find upper and lower bounds on the Master's mean waiting time. We derive the distribution of the Master's waiting time, and its mean, for systems with up to two stragglers. For systems with any number of stragglers, we derive an expression that can give the exact distribution, and the mean, of the waiting time of the Master. We show that Staircase codes always outperform classical secret sharing codes.",0.0952380952],["we study inverse problems of reconstructing static and dynamic discrete structures from tomographic data","On the Reconstruction of Static and Dynamic Discrete Structures","summarize: We study inverse problems of reconstructing static and dynamic discrete structures from tomographic data . The main emphasis is on recent mathematical developments and new applications, which emerge in scientific areas such as physics and materials science, but also in inner mathematical fields such as number theory, optimization, and imaging. Along with a concise introduction to the field of discrete tomography, we give pointers to related aspects of computerized tomography in order to contrast the worlds of continuous and discrete inverse problems.",0.1428571429],["we study incremental stability and convergence of switched Filippov systems via contraction analysis. we then apply","Contraction analysis of switched Filippov systems via regularization","summarize: We study incremental stability and convergence of switched Filippov systems via contraction analysis. In particular, by using results on regularization of switched dynamical systems, we derive sufficient conditions for convergence of any two trajectories of the Filippov system between each other within some region of interest. We then apply these conditions to the study of different classes of Filippov systems including piecewise smooth systems, piecewise affine systems and relay feedback systems. We show that contrary to previous approaches, our conditions allow the system to be studied in metrics other than the Euclidean norm. The theoretical results are illustrated by numerical simulations on a set of representative examples that confirm their effectiveness and ease of application.",0.3333333333],["quantum networks based on optical quantum technologies. we use dual-rail photonic qubit","Robust quantum network architectures and topologies for entanglement distribution","summarize: Entanglement distribution is a prerequisite for several important quantum information processing and computing tasks, such as quantum teleportation, quantum key distribution, and distributed quantum computing. In this work, we focus on two-dimensional quantum networks based on optical quantum technologies using dual-rail photonic qubits for the building of a fail-safe quantum internet. We lay out a quantum network architecture for entanglement distribution between distant parties using a Bravais lattice topology, with the technological constraint that quantum repeaters equipped with quantum memories are not easily accessible. We provide a robust protocol for simultaneous entanglement distribution between two distant groups of parties on this network. We also discuss a memory-based quantum network architecture that can be implemented on networks with an arbitrary topology. We examine networks with bow-tie lattice and Archimedean lattice topologies and use percolation theory to quantify the robustness of the networks. In particular, we provide figures of merit on the loss parameter of the optical medium that depend only on the topology of the network and quantify the robustness of the network against intermittent photon loss and intermittent failure of nodes. These figures of merit can be used to compare the robustness of different network topologies in order to determine the best topology in a given real-world scenario, which is critical in the realization of the quantum internet.",0.2],["double-diffusive convection driven by both thermal and compositional buoyancy in","The onset of thermo-compositional convection in rotating spherical shells","summarize: Double-diffusive convection driven by both thermal and compositional buoyancy in a rotating spherical shell can exhibit a rather large number of behaviours often distinct from that of the single diffusive system. In order to understand how the differences in thermal and compositional molecular diffusivities determine the dynamics of thermo-compositional convection we investigate numerically the linear onset of convective instability in a double-diffusive setup. We construct an alternative equivalent formulation of the non-dimensional equations where the linearised double-diffusive problem is described by an effective Rayleigh number, ",0.3],["a triage session involves deciding what to do with unhandled emails. users","Characterizing and Predicting Email Deferral Behavior","summarize: Email triage involves going through unhandled emails and deciding what to do with them. This familiar process can become increasingly challenging as the number of unhandled email grows. During a triage session, users commonly defer handling emails that they cannot immediately deal with to later. These deferred emails, are often related to tasks that are postponed until the user has more time or the right information to deal with them. In this paper, through qualitative interviews and a large-scale log analysis, we study when and what enterprise email users tend to defer. We found that users are more likely to defer emails when handling them involves replying, reading carefully, or clicking on links and attachments. We also learned that the decision to defer emails depends on many factors such as user's workload and the importance of the sender. Our qualitative results suggested that deferring is very common, and our quantitative log analysis confirms that 12% of triage sessions and 16% of daily active users had at least one deferred email on weekdays. We also discuss several deferral strategies such as marking emails as unread and flagging that are reported by our interviewees, and illustrate how such patterns can be also observed in user logs. Inspired by the characteristics of deferred emails and contextual factors involved in deciding if an email should be deferred, we train a classifier for predicting whether a recently triaged email is actually deferred. Our experimental results suggests that deferral can be classified with modest effectiveness. Overall, our work provides novel insights about how users handle their emails and how deferral can be modeled.",0.3125],["decomposition of a non-empty simple graph.","Coloring decompositions of complete geometric graphs","summarize: A decomposition of a non-empty simple graph ",0.5714285714],["the thermodynamic model is based on a simplified particle model and thermodynamic equations of","A multi-phase ferrofluid flow model with equation of state for thermomagnetic pumping and heat transfer","summarize: A one-dimensional multi-phase flow model for thermomagnetically pumped ferrofluid with heat transfer is proposed. The thermodynamic model is a combination of a simplified particle model and thermodynamic equations of state for the base fluid. The magnetization model is based on statistical mechanics, taking into account non-uniform particle size distributions. An implementation of the proposed model is validated against experiments from the literature, and found to give good predictions for the thermomagnetic pumping performance. However, the results reveal a very large sensitivity to uncertainties in heat transfer coefficient predictions.",0.4482894865],["plasmonic internal photoemission detectors can be used to detect ultra-broadband","Silicon-plasmonic integrated circuits for terahertz signal generation and coherent detection","summarize: Optoelectronic signal processing offers great potential for generation and detection of ultra-broadband waveforms in the THz range, so-called T-waves. However, fabrication of the underlying high-speed photodiodes and photoconductors still relies on complex processes using dedicated III-V semiconductor substrates. This severely limits the application potential of current T-wave transmitters and receivers, in particular when it comes to highly integrated systems that combine photonic signal processing with optoelectronic conversion to THz frequencies. In this paper, we demonstrate that these limitations can be overcome by plasmonic internal photoemission detectors . PIPED can be realized on the silicon photonic platform and hence allow to leverage the enormous opportunities of the associated device portfolio. In our experiments, we demonstrate both T-wave signal generation and coherent detection at frequencies of up to 1 THz. To proof the viability of our concept, we monolithically integrate a PIPED transmitter and a PIPED receiver on a common silicon photonic chip and use them for measuring the complex transfer impedance of an integrated T-wave device.",0.1538461538],["VSSS-RL is a traditional league in the Latin American robotics competition.","Learning to Play Soccer by Reinforcement and Applying Sim-to-Real to Compete in the Real World","summarize: This work presents an application of Reinforcement Learning for the complete control of real soccer robots of the IEEE Very Small Size Soccer , a traditional league in the Latin American Robotics Competition . In the VSSS league, two teams of three small robots play against each other. We propose a simulated environment in which continuous or discrete control policies can be trained, and a Sim-to-Real method to allow using the obtained policies to control a robot in the real world. The results show that the learned policies display a broad repertoire of behaviors that are difficult to specify by hand. This approach, called VSSS-RL, was able to beat the human-designed policy for the striker of the team ranked 3rd place in the 2018 LARC, in 1-vs-1 matches.",0.3271265545],["network science is a powerful tool for analyzing complex systems. we propose two generative","Measuring and Modeling Bipartite Graphs with Community Structure","summarize: Network science is a powerful tool for analyzing complex systems in fields ranging from sociology to engineering to biology. This paper is focused on generative models of large-scale bipartite graphs, also known as two-way graphs or two-mode networks. We propose two generative models that can be easily tuned to reproduce the characteristics of real-world networks, not just qualitatively, but quantitatively. The characteristics we consider are the degree distributions and the metamorphosis coefficient. The metamorphosis coefficient, a bipartite analogue of the clustering coefficient, is the proportion of length-three paths that participate in length-four cycles. Having a high metamorphosis coefficient is a necessary condition for close-knit community structure. We define edge, node, and degreewise metamorphosis coefficients, enabling a more detailed understanding of the bipartite connectivity that is not explained by degree distribution alone. Our first model, bipartite Chung-Lu , is able to reproduce real-world degree distributions, and our second model, bipartite block two-level Erd\\os-R\\'enyi , reproduces both the degree distributions as well as the degreewise metamorphosis coefficients. We demonstrate the effectiveness of these models on several real-world data sets.",0.2105263158],["magnetic flux dispersion in the undisturbed solar photosphere was explored. two","Dispersion of the solar magnetic flux in undisturbed photosphere as derived from SDO\/HMI data","summarize: To explore the magnetic flux dispersion in the undisturbed solar photosphere, magnetograms acquired by Helioseismic and Magnetic Imager onboard the Solar Dynamic Observatory were utilized. Two areas, a coronal hole area and an area of super-granulation pattern, SG, were analyzed. We explored the displacement and separation spectra and the behavior of the turbulent diffusion coefficient, ",0.4844638824],["Graphs as models are used in many areas of computer science and computer engineering. for","Proceedings Third Workshop on Graphs as Models","summarize: Graphs are used as models in many areas of computer science and computer engineering. For example graphs are used to represent syntax, control and data flow, dependency, state spaces, models such as UML and other types of domain-specific models, and social network graphs. In all of these examples, the graph serves as an intuitive yet mathematically precise foundation for many purposes, both in theory building as well as in practical applications. Graph-based models serve as an abstract communication medium and are used to describe various concepts and phenomena. Moreover, once such graph-based models are constructed, they can be analyzed and transformed to verify the correctness of static and dynamic properties, to discover new properties, to deeply study a particular domain of interest or to produce new equivalent and\/or optimized versions of graph-based models. The Graphs as Models workshop series combines the strengths of two pre-existing workshop series: GT-VMT and GRAPHITE , but also solicits research from other related areas, such as social network analysis. GaM offers a platform for exchanging new ideas and results for active researchers in these areas, with a particular aim of boosting inter- and transdisciplinary research exploiting new applications of graphs as models in any area of computational science. This year , the third edition of the GaM workshop was co-located with the European Joint Conferences on Theory and Practice of Software 2017 , held in Uppsala, Sweden.",0.1428571429],["the marginalized graph kernel is a kernel-based pipeline that can learn and predict mole","Prediction of Atomization Energy Using Graph Kernel and Active Learning","summarize: Data-driven prediction of molecular properties presents unique challenges to the design of machine learning methods concerning data structure\/dimensionality, symmetry adaption, and confidence management. In this paper, we present a kernel-based pipeline that can learn and predict the atomization energy of molecules with high accuracy. The framework employs Gaussian process regression to perform predictions based on the similarity between molecules, which is computed using the marginalized graph kernel. To apply the marginalized graph kernel, a spatial adjacency rule is first employed to convert molecules into graphs whose vertices and edges are labeled by elements and interatomic distances, respectively. We then derive formulas for the efficient evaluation of the kernel. Specific functional components for the marginalized graph kernel are proposed, while the effect of the associated hyperparameters on accuracy and predictive confidence are examined. We show that the graph kernel is particularly suitable for predicting extensive properties because its convolutional structure coincides with that of the covariance formula between sums of random variables. Using an active learning procedure, we demonstrate that the proposed method can achieve a mean absolute error of 0.62 +- 0.01 kcal\/mol using as few as 2000 training samples on the QM7 data set.",0.2173913043],["we propose a novel and principled method to learn a nonparametric graph model called","Learning Graphons via Structured Gromov-Wasserstein Barycenters","summarize: We propose a novel and principled method to learn a nonparametric graph model called graphon, which is defined in an infinite-dimensional space and represents arbitrary-size graphs. Based on the weak regularity lemma from the theory of graphons, we leverage a step function to approximate a graphon. We show that the cut distance of graphons can be relaxed to the Gromov-Wasserstein distance of their step functions. Accordingly, given a set of graphs generated by an underlying graphon, we learn the corresponding step function as the Gromov-Wasserstein barycenter of the given graphs. Furthermore, we develop several enhancements and extensions of the basic algorithm, ",0.3571428571],["averaging algorithm uses a novel snapshot scheme to approximate the accumulation of history gradients of","Distributionally Robust Federated Averaging","summarize: In this paper, we study communication efficient distributed algorithms for distributionally robust federated learning via periodic averaging with adaptive sampling. In contrast to standard empirical risk minimization, due to the minimax structure of the underlying optimization problem, a key difficulty arises from the fact that the global parameter that controls the mixture of local losses can only be updated infrequently on the global stage. To compensate for this, we propose a Distributionally Robust Federated Averaging algorithm that employs a novel snapshotting scheme to approximate the accumulation of history gradients of the mixing parameter. We analyze the convergence rate of DRFA in both convex-linear and nonconvex-linear settings. We also generalize the proposed idea to objectives with regularization on the mixture parameter and propose a proximal variant, dubbed as DRFA-Prox, with provable convergence rates. We also analyze an alternative optimization method for regularized cases in strongly-convex-strongly-concave and non-convex -strongly-concave settings. To the best of our knowledge, this paper is the first to solve distributionally robust federated learning with reduced communication, and to analyze the efficiency of local descent methods on distributed minimax problems. We give corroborating experimental evidence for our theoretical results in federated learning settings.",0.1111111111],["hybridization of atoms is the basis for the endless library of naturally occurring molecules.","Perspective on Coupled Colloidal Quantum Dot Molecules","summarize: Electronic coupling and hence hybridization of atoms serve as the basis for the rich properties of the endless library of naturally occurring molecules. Colloidal quantum dots manifesting quantum strong confinement, possess atomic like characteristics with s and p electronic levels, which popularized the notion of CQDs as artificial atoms. Continuing this analogy, when two atoms are close enough to form a molecule so that their orbitals start overlapping, the orbitals' energies start to split into bonding and anti-bonding states made out of hybridized orbitals. The same concept is also applicable for two fused core-shell nanocrystals in close proximity. Their band-edge states, which dictate the emitted photon energy, start to hybridize changing their electronic and optical properties. Thus, an exciting direction of artificial molecules emerges leading to a multitude of possibilities for creating a library of new hybrid nanostructures with novel optoelectronic properties with relevance towards diverse applications including quantum technologies. In a model fused core-shell homodimer molecule, the hybridization energy is strongly correlated with the extent of structural continuity, the delocalization of the exciton wavefunction, and the barrier thickness as calculated numerically. The hybridization impacts the emitted photon statistics manifesting a faster radiative decay rate, photon bunching effect, and modified Auger recombination pathway compared to the monomer artificial atoms. Future perspectives for the nanocrystals chemistry paradigm are highlighted.",0.0],["theory modelling is a two-step process. we have to identify the appropriate type of","Was Thebes Necessary? Contingency in Spatial Modelling","summarize: When data is poor we resort to theory modelling. This is a two-step process. We have first to identify the appropriate type of model for the system under consideration and then to tailor it to the specifics of the case. To understand settlement formation, which is the concern of this paper, this not only involves choosing input parameter values such as site separations but also input functions which characterises the ease of travel between sites. Although the generic behaviour of the model is understood, the details are not. Different choices will necessarily lead to different outputs . We can only proceed if choices that are close give outcomes are similar. Where there are local differences it suggests that there was no compelling reason for one outcome rather than the other. If these differences are important for the historic record we may interpret this as sensitivity to contingency. We re-examine the rise of Greek city states as first formulated by Rihll and Wilson in 1979, initially using the same retail gravity model. We suggest that, whereas cities like Athens owe their position to a combination of geography and proximity to other sites, the rise of Thebes is the most contingent, whose success reflects social forces outside the grasp of simple network modelling.",0.2222222222],["convection caused by Joule heating of electrolyte during charging or dischar","Thermal convection in a liquid metal battery","summarize: Generation of thermal convection flow in the liquid metal battery, a device recently proposed as a promising solution for the problem of the short-term energy storage, is analyzed using a numerical model. It is found that convection caused by Joule heating of electrolyte during charging or discharging is virtually unavoidable. It exists in laboratory prototypes larger than a few cm in size and should become much stronger in larger-scale batteries. The phenomenon needs further investigation in view of its positive and negative effects.",0.0909090909],["simulator imperfection is ubiquitous in practical data assimilation problems. we propose an","Ensemble-based kernel learning for a class of data assimilation problems with imperfect forward simulators","summarize: Simulator imperfection, often known as model error, is ubiquitous in practical data assimilation problems. Despite the enormous efforts dedicated to addressing this problem, properly handling simulator imperfection in data assimilation remains to be a challenging task. In this work, we propose an approach to dealing with simulator imperfection from a point of view of functional approximation that can be implemented through a certain machine learning method, such as kernel-based learning adopted in the current work. To this end, we start from considering a class of supervised learning problems, and then identify similarities between supervised learning and variational data assimilation. These similarities found the basis for us to develop an ensemble-based learning framework to tackle supervised learning problems, while achieving various advantages of ensemble-based methods over the variational ones. After establishing the ensemble-based learning framework, we proceed to investigate the integration of ensemble-based learning into an ensemble-based data assimilation framework to handle simulator imperfection. In the course of our investigations, we also develop a strategy to tackle the issue of multi-modality in supervised-learning problems, and transfer this strategy to data assimilation problems to help improve assimilation performance. For demonstration, we apply the ensemble-based learning framework and the integrated, ensemble-based data assimilation framework to a supervised learning problem and a data assimilation problem with an imperfect forward simulator, respectively. The experiment results indicate that both frameworks achieve good performance in relevant case studies, and that functional approximation through machine learning may serve as a viable way to account for simulator imperfection in data assimilation problems.",0.3527007187],["theory modelling is a two-step process. we have to identify the appropriate type of","Was Thebes Necessary? Contingency in Spatial Modelling","summarize: When data is poor we resort to theory modelling. This is a two-step process. We have first to identify the appropriate type of model for the system under consideration and then to tailor it to the specifics of the case. To understand settlement formation, which is the concern of this paper, this not only involves choosing input parameter values such as site separations but also input functions which characterises the ease of travel between sites. Although the generic behaviour of the model is understood, the details are not. Different choices will necessarily lead to different outputs . We can only proceed if choices that are close give outcomes are similar. Where there are local differences it suggests that there was no compelling reason for one outcome rather than the other. If these differences are important for the historic record we may interpret this as sensitivity to contingency. We re-examine the rise of Greek city states as first formulated by Rihll and Wilson in 1979, initially using the same retail gravity model. We suggest that, whereas cities like Athens owe their position to a combination of geography and proximity to other sites, the rise of Thebes is the most contingent, whose success reflects social forces outside the grasp of simple network modelling.",0.2222222222],["branch star is the star of the asymptotic giant branch.","The extended molecular envelope of the asymptotic giant branch star ","summarize: The S-type asymptotic giant branch star ",0.5089710476],["the fundamental solution of the Dirac equation for an electron in an electromagnetic field with harmonic dependence on","Ground state and the spin precession of the Dirac electron in counterpropagating plane electromagnetic waves","summarize: The fundamental solution of the Dirac equation for an electron in an electromagnetic field with harmonic dependence on space-time coordinates is obtained. The field is composed of three standing plane harmonic waves with mutually orthogonal phase planes and the same frequency. Each standing wave consists of two eigenwaves with different complex amplitudes and opposite directions of propagation. The fundamental solution is obtained in the form of the projection operator defining the subspace of solutions to the Dirac equation. It is illustrated by the analysis of the ground state and the spin precession of the Dirac electron in the field of two counterpropagating plane waves with left and right circular polarizations. Interrelations between the fundamental solution and approximate partial solutions is discussed and a criterion for evaluating accuracy of approximate solutions is suggested.",0.5769230769],["the animation industry outsources large animation workloads to foreign countries where labor is inexpensive and long","Artist-Guided Semiautomatic Animation Colorization","summarize: There is a delicate balance between automating repetitive work in creative domains while staying true to an artist's vision. The animation industry regularly outsources large animation workloads to foreign countries where labor is inexpensive and long hours are common. Automating part of this process can be incredibly useful for reducing costs and creating manageable workloads for major animation studios and outsourced artists. We present a method for automating line art colorization by keeping artists in the loop to successfully reduce this workload while staying true to an artist's vision. By incorporating color hints and temporal information to an adversarial image-to-image framework, we show that it is possible to meet the balance between automation and authenticity through artist's input to generate colored frames with temporal consistency.",0.1111111111],["the cubic galileon model previously investigated in cite. it is based on","Dynamical systems analysis of the cubic galileon beyond the exponential potential and the cosmological analogue of the vDVZ discontinuity","summarize: In this paper we generalize the dynamical systems analysis of the cubic galileon model previously investigated in \\cite by including self-interaction potentials beyond the exponential one. It will be shown that, consistently with the results of \\cite, the cubic self-interaction of the galileon vacuum appreciably modifies the late-time cosmic dynamics by the existence of a phantom-like attractor . In contrast, in the presence of background matter the late-time cosmic dynamics remains practically the same as in the standard quintessence scenario. This means that we can not recover the cubic galileon vacuum continuously from the more general cubic quintessence with background matter, by setting to zero the matter energy density . This happens to be a kind of cosmological vDVZ discontinuity that can be evaded by means of the cosmological version of the Vainshtein screening mechanism.",0.2976187444],["the equation is a stochastic forcing term which is a fractional Wiener noise","2D Navier-Stokes equation with cylindrical fractional Brownian noise","summarize: We consider the Navier-Stokes equation on the 2D torus, with a stochastic forcing term which is a cylindrical fractional Wiener noise of Hurst parameter ",0.6296296296],["regrasp control policy makes use of tactile sensing to plan local grasp adjustments","Tactile Regrasp: Grasp Adjustments via Simulated Tactile Transformations","summarize: This paper presents a novel regrasp control policy that makes use of tactile sensing to plan local grasp adjustments. Our approach determines regrasp actions by virtually searching for local transformations of tactile measurements that improve the quality of the grasp. First, we construct a tactile-based grasp quality metric using a deep convolutional neural network trained on over 2800 grasps. The quality of each grasp, a continuous value between 0 and 1, is determined experimentally by measuring its resistance to external perturbations. Second, we simulate the tactile imprints associated with robot motions relative to the initial grasp by performing rigid-body transformations of the given tactile measurements. The newly generated tactile imprints are evaluated with the learned grasp quality network and the regrasp action is chosen to maximize the grasp quality. Results show that the grasp quality network can predict the outcome of grasps with an average accuracy of 85% on known objects and 75% on a cross validation set of 12 objects. The regrasp control policy improves the success rate of grasp actions by an average relative increase of 70% on a test set of 8 objects.",0.0714285714],["a study of scan data shows that several prefixes do not accommodate any host of","Towards Better Internet Citizenship: Reducing the Footprint of Internet-wide Scans by Topology Aware Prefix Selection","summarize: Internet service discovery is an emerging topic to study the deployment of protocols. Towards this end, our community periodically scans the entire advertised IPv4 address space. In this paper, we question this principle. Being good Internet citizens means that we should limit scan traffic to what is necessary. We conducted a study of scan data, which shows that several prefixes do not accommodate any host of interest and the network topology is fairly stable. We argue that this allows us to collect representative data by scanning less. In our paper, we explore the idea to scan all prefixes once and then identify prefixes of interest for future scanning. Based on our analysis of the censys.io data set we found that we can reduce scan traffic between 25-90% and miss only 1-10% of the hosts, depending on desired trade-offs and protocols.",0.2],["fractional mean curvature is a fractional mean curvature. fractional","Complete stickiness of nonlocal minimal surfaces for small values of the fractional parameter","summarize: In this paper, we consider the asymptotic behavior of the fractional mean curvature when ",0.2653160228],["the joint distribution of a geometric Brownian motion and its time-integral was","Geometric Brownian motion with affine drift and its time-integral","summarize: The joint distribution of a geometric Brownian motion and its time-integral was derived in a seminal paper by Yor using Lamperti's transformation, leading to explicit solutions in terms of modified Bessel functions. In this paper, we revisit this classic result using the simple Laplace transform approach in connection to the Heun differential equation. We extend the methodology to the geometric Brownian motion with affine drift and show that the joint distribution of this process and its time-integral can be determined by a doubly-confluent Heun equation. Furthermore, the joint Laplace transform of the process and its time-integral is derived from the asymptotics of the solutions. In addition, we provide an application by using the results for the asymptotics of the double-confluent Heun equation in pricing Asian options. Numerical results show the accuracy and efficiency of this new method.",0.5625],["a time-variable photochemical model is used to study the distribution of stratos","Seasonal stratospheric photochemistry on Uranus and Neptune","summarize: A time-variable 1D photochemical model is used to study the distribution of stratospheric hydrocarbons as a function of altitude, latitude, and season on Uranus and Neptune. The results for Neptune indicate that in the absence of stratospheric circulation or other meridional transport processes, the hydrocarbon abundances exhibit strong seasonal and meridional variations in the upper stratosphere, but that these variations become increasingly damped with depth due to increasing dynamical and chemical time scales. At high altitudes, hydrocarbon mixing ratios are typically largest where the solar insolation is the greatest, leading to strong hemispheric dichotomies between the summer-to-fall hemisphere and winter-to-spring hemisphere. At mbar pressures and deeper, slower chemistry and diffusion lead to latitude variations that become more symmetric about the equator. On Uranus, the stagnant, poorly mixed stratosphere confines methane and its photochemical products to higher pressures, where chemistry and diffusion time scales remain large. Seasonal variations in hydrocarbons are therefore predicted to be more muted on Uranus, despite the planet's very large obliquity. Radiative-transfer simulations demonstrate that latitude variations in hydrocarbons on both planets are potentially observable with future JWST mid-infrared spectral imaging. Our seasonal model predictions for Neptune compare well with retrieved C2H2 and C2H6 abundances from spatially resolved ground-based observations , suggesting that stratospheric circulation -- which was not included in these models -- may have little influence on the large-scale meridional hydrocarbon distributions on Neptune, unlike the situation on Jupiter and Saturn.",0.4736842105],["Graphs as models are used in many areas of computer science and computer engineering. for","Proceedings Third Workshop on Graphs as Models","summarize: Graphs are used as models in many areas of computer science and computer engineering. For example graphs are used to represent syntax, control and data flow, dependency, state spaces, models such as UML and other types of domain-specific models, and social network graphs. In all of these examples, the graph serves as an intuitive yet mathematically precise foundation for many purposes, both in theory building as well as in practical applications. Graph-based models serve as an abstract communication medium and are used to describe various concepts and phenomena. Moreover, once such graph-based models are constructed, they can be analyzed and transformed to verify the correctness of static and dynamic properties, to discover new properties, to deeply study a particular domain of interest or to produce new equivalent and\/or optimized versions of graph-based models. The Graphs as Models workshop series combines the strengths of two pre-existing workshop series: GT-VMT and GRAPHITE , but also solicits research from other related areas, such as social network analysis. GaM offers a platform for exchanging new ideas and results for active researchers in these areas, with a particular aim of boosting inter- and transdisciplinary research exploiting new applications of graphs as models in any area of computational science. This year , the third edition of the GaM workshop was co-located with the European Joint Conferences on Theory and Practice of Software 2017 , held in Uppsala, Sweden.",0.1428571429],["we consider the algorithmic problem of generating each subset of subsets of subset","Trimming and gluing Gray codes","summarize: We consider the algorithmic problem of generating each subset of ",0.0],["this paper also includes some criteria used for the construction of this modeling platform. the power of","Basis to develop a platform for multiple-scale complex systems modeling and visualization: MoNet","summarize: This work presents some characteristics of MoNet, a digital platform for the modeling and visualization of complex systems. Emphasis is on the ideas that allowed the successful progressive development of this modeling platform, which goes along with the implementation of applications to the modeling of several studied systems. The platform can represent different aspects of systems modeled at different observation scales. This tool offers advantages in the sense of favoring the perception of the phenomenon of the emergence of information, associated with changes of scale. This paper also includes some criteria used for the construction of this modeling platform. The power of current computers has made practical representing graphic resources such as shapes, line thickness, overlaying-text tags, colors, and transparencies, in the graphical modeling of systems. By visualizing diagrams conveniently designed to highlight contrasts, these modeling platforms allow the recognition of patterns that drive our understanding of systems and their structure. Graphs reflecting the benefits of the tool regarding the visualization of systems at different scales of observation are presented to illustrate the application of the platform.",0.1666666667],["progress in superconductor electronics fabrication has enabled single-flux-quantum digital circuit","Superconductor Electronics Fabrication Process with MoN","summarize: Recent progress in superconductor electronics fabrication has enabled single-flux-quantum digital circuits with close to one million Josephson junctions on 1-cm",0.0],["the description extends earlier work to include the effects on the states of two or more interacting","Effect of number scaling on entangled states in quantum mechanics","summarize: A summary of number structure scaling is followed by a description of the effects of number scaling in nonrelativistic quantum mechanics. The description extends earlier work to include the effects on the states of two or more interacting particles. Emphasis is placed on the effects on entangled states. The resulting scaling field is generalized to describe the effects on these states. It is also seen that one can use fiber bundles with fibers associated with single locations of the underlying space to describe the effects of scaling on arbitrary numbers of particles.",0.1153846154],["amplitudes amplitudes in amplitudes.","Celestial amplitudes and conformal soft theorems","summarize: Scattering amplitudes in ",0.1819591979],["this paper presents a structured survey of mechanism and protocols to update computer networks in a fast","Survey of Consistent Software-Defined Network Updates","summarize: Computer networks have become a critical infrastructure. In fact, networks should not only meet strict requirements in terms of correctness, availability, and performance, but they should also be very flexible and support fast updates, e.g., due to policy changes, increasing traffic, or failures. This paper presents a structured survey of mechanism and protocols to update computer networks in a fast and consistent manner. In particular, we identify and discuss the different desirable consistency properties that should be provided throughout a network update, the algorithmic techniques which are needed to meet these consistency properties, and the implications on the speed and costs at which updates can be performed. We also explain the relationship between consistent network update problems and classic algorithmic optimization ones. While our survey is mainly motivated by the advent of Software-Defined Networks and their primary need for correct and efficient update techniques, the fundamental underlying problems are not new, and we provide a historical perspective of the subject as well.",0.2],["the IRC, equipped with a a planetary camera, obtained a low-","AKARI\/IRC Near-Infrared Spectral Atlas of Galactic Planetary Nebulae","summarize: Near-infrared low-resolution spectra of 72 Galactic planetary nebulae were obtained with the Infrared Camera in the post-helium phase. The IRC, equipped with a ",0.75],["gyrofluid model for kinetic Alfv'en waves in","Inverse cascade and magnetic vortices in kinetic Alfv\\'en-wave turbulence","summarize: A Hamiltonian two-field gyrofluid model for kinetic Alfv\\'en waves in a magnetized electron-proton plasma, retaining ion finite-Larmor-radius corrections and parallel magnetic field fluctuations, is used to study the inverse cascades that develop when turbulence is randomly driven at sub-ion scales. In the directions perpendicular to the ambient field, the dynamics of the cascade turns out to be nonlocal and the ratio ",0.2818039849],["experiments show a massive acceleration of the annealing of a monolayer of passive","Activity-controlled Annealing of Colloidal Monolayers","summarize: Molecular motors are essential to the living, they generate additional fluctuations that boost transport and assist assembly. Self-propelled colloids, that consume energy to move, hold similar potential for the man-made assembly of microparticles. Yet, experiments showing their use as a powerhouse in materials science lack. Our work explores the design of man-made materials controlled by fluctuations, arising from the internal forces generated by active colloids. Here we show a massive acceleration of the annealing of a monolayer of passive beads by moderate addition of self-propelled microparticles. We rationalize our observations with a model of collisions that drive active fluctuations to overcome kinetic barriers and activate the annealing. The experiment is quantitatively compared with Brownian dynamic simulations that further unveil a dynamical transition in the mechanism of annealing. Active dopants travel uniformly in the system or co-localize at the grain boundaries as a result of the persistence of their motion. Our findings uncover the potential of man-made materials controlled by internal activity and lay the groundwork for the rise of materials science beyond equilibrium.",0.2571428571],["patching is a common activity in software development. it is generally performed on a","FixMiner: Mining Relevant Fix Patterns for Automated Program Repair","summarize: Patching is a common activity in software development. It is generally performed on a source code base to address bugs or add new functionalities. In this context, given the recurrence of bugs across projects, the associated similar patches can be leveraged to extract generic fix actions. While the literature includes various approaches leveraging similarity among patches to guide program repair, these approaches often do not yield fix patterns that are tractable and reusable as actionable input to APR systems. In this paper, we propose a systematic and automated approach to mining relevant and actionable fix patterns based on an iterative clustering strategy applied to atomic changes within patches. The goal of FixMiner is thus to infer separate and reusable fix patterns that can be leveraged in other patch generation systems. Our technique, FixMiner, leverages Rich Edit Script which is a specialized tree structure of the edit scripts that captures the AST-level context of the code changes. FixMiner uses different tree representations of Rich Edit Scripts for each round of clustering to identify similar changes. These are abstract syntax trees, edit actions trees, and code context trees. We have evaluated FixMiner on thousands of software patches collected from open source projects. Preliminary results show that we are able to mine accurate patterns, efficiently exploiting change information in Rich Edit Scripts. We further integrated the mined patterns to an automated program repair prototype, PARFixMiner, with which we are able to correctly fix 26 bugs of the Defects4J benchmark. Beyond this quantitative performance, we show that the mined fix patterns are sufficiently relevant to produce patches with a high probability of correctness: 81% of PARFixMiner's generated plausible patches are correct.",0.4137931034],["we have developed an online radiative-transfer suite applicable to a broad range of","Planetary Spectrum Generator: an accurate online radiative transfer suite for atmospheres, comets, small bodies and exoplanets","summarize: We have developed an online radiative-transfer suite applicable to a broad range of planetary objects . The Planetary Spectrum Generator can synthesize planetary spectra for a broad range of wavelengths from any observatory , any orbiter , or any lander . This is achieved by combining several state-of-the-art radiative transfer models, spectroscopic databases and planetary databases . PSG has a 3D orbital calculator for most bodies in the solar system, and all confirmed exoplanets, while the radiative-transfer models can ingest billions of spectral signatures for hundreds of species from several spectroscopic repositories. It integrates the latest radiative-transfer and scattering methods in order to compute high resolution spectra via line-by-line calculations, and utilizes the efficient correlated-k method at moderate resolutions, while for computing cometary spectra, PSG handles non-LTE and LTE excitation processes. PSG includes a realistic noise calculator that integrates several telescope \/ instrument configurations and detector technologies . Such an integration of advanced spectroscopic methods into an online tool can greatly serve the planetary community, ultimately enabling the retrieval of planetary parameters from remote sensing data, efficient mission planning strategies, interpretation of current and future planetary data, calibration of spectroscopic data, and development of new instrument\/spacecraft concepts.",0.5292817719],["four-dimensional large-scale large-scales are considered asymptotically-","Spectral sum rules for confining large-N theories","summarize: We consider asymptotically-free four-dimensional large-",0.0],["abstract error analysis framework for approximation of linear partial differential equation problems in weak formulation","A third Strang lemma and an Aubin-Nitsche trick for schemes in fully discrete formulation","summarize: In this work, we present an abstract error analysis framework for the approximation of linear partial differential equation problems in weak formulation. We consider approximation methods in fully discrete formulation, where the discrete and continuous spaces are possibly not embedded in a common space. A proper notion of consistency is designed, and, under a classical inf-sup condition, it is shown to bound the approximation error. This error estimate result is in the spirit of Strang's first and second lemmas, but applicable in situations not covered by these lemmas . An improved estimate is also established in a weaker norm, using the Aubin--Nitsche trick. We then apply these abstract estimates to an anisotropic heterogeneous diffusion model and two classical families of schemes for this model: Virtual Element and Finite Volume methods. For each of these methods, we show that the abstract results yield new error estimates with a precise and mild dependency on the local anisotropy ratio. A key intermediate step to derive such estimates for Virtual Element Methods is proving optimal approximation properties of the oblique elliptic projector in weighted Sobolev seminorms. This is a result whose interest goes beyond the specific model and methods considered here. We also obtain, to our knowledge, the first clear notion of consistency for Finite Volume methods, which leads to a generic error estimate involving the fluxes and valid for a wide range of Finite Volume schemes. An important application is the first error estimate for Multi-Point Flux Approximation L and G methods. In the appendix, not included in the published version of this work, we show that classical estimates for discontinuous Galerkin methods can be obtained with simplified arguments using the abstract framework.",0.2941176471],["studies show that the acquisition of knowledge is the key to build competitive advantage of companies. we","Model of knowledge transfer within an organisation","summarize: Many studies show that the acquisition of knowledge is the key to build competitive advantage of companies. We propose a simple model of knowledge transfer within the organization and we implement the proposed model using cellular automata technique. In this paper the organisation is considered in the context of complex systems. In this perspective, the main role in organisation is played by the network of informal contacts and the distributed leadership. The goal of this paper is to check which factors influence the efficiency and effectiveness of knowledge transfer. Our studies indicate a significant role of initial concentration of chunks of knowledge for knowledge transfer process, and the results suggest taking action in the organisation to shorten the distance between people with different levels of knowledge, or working out incentives to share knowledge.",0.1818181818],["photoexcited graphene can act as the gain medium to produce coherent radiation at low T","Tunable Terahertz Amplification Based on Photoexcited Active Graphene Hyperbolic Metamaterials","summarize: The efficient amplification and lasing of electromagnetic radiation at terahertz frequencies is a non-trivial task achieved mainly by quantum cascade laser configurations with limited tunability and narrowband functionality. There is a strong need of compact and efficient THz electromagnetic sources with reconfigurable operation in a broad frequency range. Photoexcited graphene can act as the gain medium to produce coherent radiation at low THz frequencies but its response is very weak due to its ultrathin thickness. In this work, we demonstrate an alternative design to achieve efficient tunable and compact THz amplifiers and lasers with broadband operation based on active THz hyperbolic metamaterials designed by multiple stacked photoexcited graphene layers separated by thin dielectric sheets. The hyperbolic THz response of the proposed ultrathin active HMM is analytically and numerically studied and characterized. When the graphene-based HMM structure is periodically patterned, a broadband slow-wave propagation regime is identified, thanks to the hyperbolic dispersion. In this scenario, reconfigurable amplification of THz waves in a broad frequency range is obtained, which can be made tunable by varying the quasi-Fermi level of graphene. We demonstrate that the THz response of the presented tunable THz amplifiers or lasers is controlled by the incident optical pumping and the loaded dielectric materials in the HMM waveguide array, an interesting property that can have great potential for THz amplification, emission, and sensing applications.",0.2941176471],["this article examines the structure and spatial patterns of violent political organizations in the Sahel-","Wars Without Beginning or End: Violent Political Organizations and Irregular Warfare in the Sahel-Sahara","summarize: This article examines the structure and spatial patterns of violent political organizations in the Sahel-Sahara, a region characterized by growing political instability over the last 20 years. Drawing on a public collection of disaggregated data, the article uses network science to represent alliances and conflicts of 179 organizations that were involved in violent events between 1997 and 2014. To this end, we combine two spectral embedding techniques that have previously been considered separately: one for directed graphs , and one for signed graphs . Our result show that groups that are net attackers are indistinguishable at the level of their individual behavior, but clearly separate into pro- and anti-political violence based on the groups to which they are close. The second part of the article maps a series of 389 events related to nine Trans-Saharan Islamist groups between 2004 and 2014. Spatial analysis suggests that cross-border movement has intensified following the establishment of military bases by AQIM in Mali but reveals no evidence of a border sanctuary. Owing to the transnational nature of conflict, the article shows that national management strategies and foreign military interventions have profoundly affected the movement of Islamist groups.",0.3333333333],["outlier and noise detection processes are highly useful in the quality assessment of any kind of database","A Proposal for Outlier and Noise Detection in Public Officials' Affidavits","summarize: Outlier and noise detection processes are highly useful in the quality assessment of any kind of database. Such processes may have novel civic and public applications in the detection of anomalies in public data. The purpose of this work is to explore the possibilities of experimentation with, validation and application of hybrid outlier and noise detection procedures in public officials' affidavit systems currently available in Argentina.",0.1],["a detailed spectroscopic study. aims to probe single photon emission and","Spectroscopic Investigations of Negatively Charged Tin-Vacancy Centres in Diamond","summarize: The recently discovered negatively charged tin-vacancy centre in diamond is a promising candidate for applications in quantum information processing . We here present a detailed spectroscopic study encompassing single photon emission and polarisation properties, the temperature dependence of emission spectra as well as a detailed analysis of the phonon sideband and Debye-Waller factor. Using photoluminescence excitation spectroscopy we probe an energetically higher lying excited state and prove fully lifetime limited linewidths of single emitters at cryogenic temperatures. For these emitters we also investigate the stability of the charge state under resonant excitation. These results provide a detailed insight into the spectroscopic properties of the ",0.2666666667],["a device which can generate, manipulate, and analyse two-qubit entangled states","Silicon photonic processor of two-qubit entangling quantum logic","summarize: Entanglement is a fundamental property of quantum mechanics, and is a primary resource in quantum information systems. Its manipulation remains a central challenge in the development of quantum technology. In this work, we demonstrate a device which can generate, manipulate, and analyse two-qubit entangled states, using miniature and mass-manufacturable silicon photonics. By combining four photon-pair sources with a reconfigurable six-mode interferometer, embedding a switchable entangling gate, we generate two-qubit entangled states, manipulate their entanglement, and analyse them, all in the same silicon chip. Using quantum state tomography, we show how our source can produce a range of entangled and separable states, and how our switchable controlled-Z gate operates on them, entangling them or making them separable depending on its configuration.",0.15],["we observe the emergence of nonlinear localized structures that evolve on a stoch","Coexistence of solitons and extreme events in deep water surface waves","summarize: We study experimentally, in a large-scale basin, the propagation of unidirectional deep water gravity waves stochastically modulated in phase. We observe the emergence of nonlinear localized structures that evolve on a stochastic wave background. Such a coexistence is expected by the integrable turbulence theory for the nonlinear Schrdinger equation , and we report the first experimental observation in the context of hydrodynamic waves. We characterize the formation, the properties and the dynamics of these nonlinear coherent structures within the incoherent wave background. The extreme events result from the strong steepening of wave train fronts, and their emergence occurs after roughly one nonlinear length scale of propagation . Solitons arise when nonlinearity and dispersion are weak, and of the same order of magnitude as expected from NLSE. We characterize the statistical properties of this state. The number of solitons and extreme events is found to increase all along the propagation, the wave-field distribution has a heavy tail, and the surface elevation spectrum is found to scale as a frequency power-law with an exponent --4.5 ",0.3529411765],["paper addresses image fusion problem. multi-focus image fusion problem is to be combined","Image Fusion With Cosparse Analysis Operator","summarize: The paper addresses the image fusion problem, where multiple images captured with different focus distances are to be combined into a higher quality all-in-focus image. Most current approaches for image fusion strongly rely on the unrealistic noise-free assumption used during the image acquisition, and then yield limited robustness in fusion processing. In our approach, we formulate the multi-focus image fusion problem in terms of an analysis sparse model, and simultaneously perform the restoration and fusion of multi-focus images. Based on this model, we propose an analysis operator learning, and define a novel fusion function to generate an all-in-focus image. Experimental evaluations confirm the effectiveness of the proposed fusion approach both visually and quantitatively, and show that our approach outperforms state-of-the-art fusion methods.",0.1052631579],["a gap exists in our ability to systematically generate networks that adhere to theoretical guarantees for the","Designing Networks: A Mixed-Integer Linear Optimization Approach","summarize: Designing networks with specified collective properties is useful in a variety of application areas, enabling the study of how given properties affect the behavior of network models, the downscaling of empirical networks to workable sizes, and the analysis of network evolution. Despite the importance of the task, there currently exists a gap in our ability to systematically generate networks that adhere to theoretical guarantees for the given property specifications. In this paper, we propose the use of Mixed-Integer Linear Optimization modeling and solution methodologies to address this Network Generation Problem. We present a number of useful modeling techniques and apply them to mathematically express and constrain network properties in the context of an optimization formulation. We then develop complete formulations for the generation of networks that attain specified levels of connectivity, spread, assortativity and robustness, and we illustrate these via a number of computational case studies.",0.1333333333],["nano-based nano-based devices are able to be deposited on metal surfaces.","Molecular anchoring stabilizes low valence NiTPP on copper against thermally induced chemical changes","summarize: Many applications of molecular layers deposited on metal surfaces, ranging from single-atom catalysis to on-surface magnetochemistry and biosensing, rely on the use of thermal cycles to regenerate the pristine properties of the system. Thus, understanding the microscopic origin behind the thermal stability of organic\/metal interfaces is fundamental for engineering reliable organic-based devices. Here, we study nickel porphyrin molecules on a copper surface as an archetypal system containing a metal center whose oxidation state can be controlled through the interaction with the metal substrate. We demonstrate that the strong molecule-surface interaction, followed by charge transfer at the interface, plays a fundamental role in the thermal stability of the layer by rigidly anchoring the porphyrin to the substrate. Upon thermal treatment, the molecules undergo an irreversible transition at 420 K, which is associated with an increase of the charge transfer from the substrate, mostly localized on the phenyl substituents, and a downward tilting of the latters without any chemical modification",0.0641348399],["the BNDO demonstrates robust control performance against mismatched uncertainties. the SLDO","Feedback Linearization Control for Systems with Mismatched Uncertainties via Disturbance Observers","summarize: This paper focuses on a novel feedback linearization control law based on a self-learning disturbance observer to counteract mismatched uncertainties. The FLC based on BNDO demonstrates robust control performance only against mismatched time-invariant uncertainties while the FLC based on SLDO demonstrates robust control performance against mismatched time-invariant and -varying uncertainties, and both of them maintain the nominal control performance in the absence of mismatched uncertainties. In the estimation scheme for the SLDO, the BNDO is used to provide a conventional estimation law, which is used as being the learning error for the type-2 neuro-fuzzy system , and T2NFS learns mismatched uncertainties. Thus, the T2NFS takes the overall control of the estimation signal entirely in a very short time and gives unbiased estimation results for the disturbance. A novel learning algorithm established on sliding mode control theory is derived for an interval type-2 fuzzy logic system. The stability of the overall system is proven for a second-order nonlinear system with mismatched uncertainties. The simulation results show that the FLC-SLDO demonstrates better control performance than the traditional FLC, FLC with an integral action and FLC-BNDO.",0.0],["modern microprocessors are equipped with single instruction multiple data or vector instructions. programmers","Revec: Program Rejuvenation through Revectorization","summarize: Modern microprocessors are equipped with Single Instruction Multiple Data or vector instructions which expose data level parallelism at a fine granularity. Programmers exploit this parallelism by using low-level vector intrinsics in their code. However, once programs are written using vector intrinsics of a specific instruction set, the code becomes non-portable. Modern compilers are unable to analyze and retarget the code to newer vector instruction sets. Hence, programmers have to manually rewrite the same code using vector intrinsics of a newer generation to exploit higher data widths and capabilities of new instruction sets. This process is tedious, error-prone and requires maintaining multiple code bases. We propose Revec, a compiler optimization pass which revectorizes already vectorized code, by retargeting it to use vector instructions of newer generations. The transformation is transparent, happening at the compiler intermediate representation level, and enables performance portability of hand-vectorized code. Revec can achieve performance improvements in real-world performance critical kernels. In particular, Revec achieves geometric mean speedups of 1.160",0.125],["rule 22 elementary cellular automaton has a 3-cell neighborhood, binary cell states.","On patterns and dynamics of Rule 22 cellular automaton","summarize: Rule 22 elementary cellular automaton has a 3--cell neighborhood, binary cell states, where a cell takes state `1' if there is exactly one neighbor, including the cell itself, in state `1'. In Boolean terms the cell-state transition is a XOR function of three cell states. In physico--chemical terms the rule might be seen as describing propagation of self-inhibiting quantities\/species. Space-time dynamics of Rule 22 demonstrates non-trivial patterns and quasi-chaotic behavior. We characterize the phenomena observed in this rule using mean field theory, attractors, de Bruijn diagrams, subset diagrams, filters, fractals and memory.",0.4761904762],["inverse planning model relies on an incorrect application of the Charnes-Cooper transformation","Comment on `Linear energy transfer incorporated intensity modulated proton therapy optimization'","summarize: Cao et al published an article on inverse planning based on dose-averaged linear energy transfer . Their claim that the problem can be cast as a linear optimization model relies on an incorrect application of the Charnes-Cooper transformation. In this comment we show that their linear model is simlar to one where dose-averaged LET is multiplied with dose, explaining why their model was nonetheless able to improve the LET distribution.",0.3125],["transmission line theory is used to shed new light on the topology of a wired network","On the Exploitation of Admittance Measurements for Wired Network Topology Derivation","summarize: The knowledge of the topology of a wired network is often of fundamental importance. For instance, in the context of Power Line Communications networks it is helpful to implement data routing strategies, while in power distribution networks and Smart Micro Grids it is required for grid monitoring and for power flow management. In this paper, we use the transmission line theory to shed new light and to show how the topological properties of a wired network can be found exploiting admittance measurements at the nodes. An analytic proof is reported to show that the derivation of the topology can be done in complex networks under certain assumptions. We also analyze the effect of the network background noise on admittance measurements. In this respect, we propose a topology derivation algorithm that works in the presence of noise. We finally analyze the performance of the algorithm using values that are typical of power line distribution networks.",0.2857142857],["gerrymandering has long legal history. it has been a long legal","On partisan bias in redistricting: computational complexity meets the science of gerrymandering","summarize: The topic of this paper is gerrymandering, namely the curse of deliberate creations of district maps with highly asymmetric electoral outcomes to disenfranchise voters, and it has a long legal history. Measuring and eliminating gerrymandering has enormous implications to sustain the backbone of democratic principles of a society. Although there is no dearth of legal briefs involving gerrymandering over many years, it is only more recently that mathematicians and applied computational researchers have started to investigate this topic. However, it has received relatively little attention so far from the computational complexity researchers dealing with theoretical analysis of computational complexity issues, such as computational hardness, approximability issues, etc. There could be many reasons for this, such as descriptions of these problem non-CS non-math journals that theoretical CS people usually do not follow, or the lack of coverage of these topics in TCS publication venues. One of our modest goals in writing this article is to improve upon this situation by stimulating further interactions between the gerrymandering and TCS researchers. To this effect, our main contributions are twofold: we provide formalization of several models, related concepts, and corresponding problem statements using TCS frameworks from the descriptions of these problems as available in existing non-TCS venues, and we also provide computational complexity analysis of some versions of these problems, leaving other versions for future research. The goal of writing this article is not to have the final word on gerrymandering, but to introduce a series of concepts, models and problems to the TCS community and to show that science of gerrymandering involves an intriguing set of partitioning problems involving geometric and combinatorial optimization.",0.3320366241],["we derive point-wise and integral rigidity\/gap results for a closed","On Closed Manifolds with Harmonic Weyl Curvature","summarize: We derive point-wise and integral rigidity\/gap results for a closed manifold with harmonic Weyl curvature in any dimension. In particular, there is a generalization of Tachibana's theorem for non-negative curvature operator. The key ingredients are new Bochner-Weitzenb\\ock-Lichnerowicz type formulas for the Weyl tensor, which are generalizations of identities in dimension four.",0.2307692308],["a non convex least-squares fit criterion and a penalty","Rational Optimization for Nonlinear Reconstruction with Approximate ","summarize: Recovering nonlinearly degraded signal in the presence of noise is a challenging problem. In this work, this problem is tackled by minimizing the sum of a non convex least-squares fit criterion and a penalty term. We assume that the nonlinearity of the model can be accounted for by a rational function. In addition, we suppose that the signal to be sought is sparse and a rational approximation of the ",0.5263157895],["planning techniques have been developed to allow autonomous systems to act and make decisions based on their perception","Une approche totalement instanci\\'ee pour la planification HTN","summarize: Many planning techniques have been developed to allow autonomous systems to act and make decisions based on their perceptions of the environment. Among these techniques, HTN planning is one of the most used in practice. Unlike classical approaches of planning. HTN operates by decomposing task into sub-tasks until each of these sub-tasks can be achieved an action. This hierarchical representation provide a richer representation of planning problems and allows to better guide the plan search and provides more knowledge to the underlying algorithms. In this paper, we propose a new approach of HTN planning in which, as in conventional planning, we instantiate all planning operators before starting the search process. This approach has proven its effectiveness in classical planning and is necessary for the development of effective heuristics and encoding planning problems in other formalism such as CSP or SAT. The instantiation is actually used by most modern planners but has never been applied in an HTN based planning framework. We present in this article a generic instantiation algorithm which implements many simplification techniques to reduce the process complexity inspired from those used in classical planning. Finally we present some results obtained from an experimentation on a range of problems used in the international planning competitions with a modified version of SHOP planner using fully instantiated problems.",0.12],["the goal of this small note is to give a more concise proof of a result due","A new proof of a vanishing result due to Berthelot, Esnault, and R\\ulling","summarize: The goal of this small note is to give a more concise proof of a result due to Berthelot, Esnault, and R\\ulling. For a regular, proper, and flat scheme ",0.6206896552],["the images can be captured in such scenarios using Near-Infrared and Thermal cameras","PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks for Thermal and NIR to Visible Image Transformation","summarize: In many real world scenarios, it is difficult to capture the images in the visible light spectrum due to bad lighting conditions. However, the images can be captured in such scenarios using Near-Infrared and Thermal cameras. The NIR and THM images contain the limited details. Thus, there is a need to transform the images from THM\/NIR to VIS for better understanding. However, it is non-trivial task due to the large domain discrepancies and lack of abundant datasets. Nowadays, Generative Adversarial Network is able to transform the images from one domain to another domain. Most of the available GAN based methods use the combination of the adversarial and the pixel-wise losses as the objective function for training. The quality of transformed images in case of THM\/NIR to VIS transformation is still not up to the mark using such objective function. Thus, better objective functions are needed to improve the quality, fine details and realism of the transformed images. A new model for THM\/NIR to VIS image transformation called Perceptual Cyclic-Synthesized Generative Adversarial Network is introduced to address these issues. The PCSGAN uses the combination of the perceptual losses along with the pixel-wise and the adversarial losses. Both the quantitative and qualitative measures are used to judge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR scene datasets. The proposed PCSGAN outperforms the state-of-the-art image transformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in terms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is available at https:\/\/github.com\/KishanKancharagunta\/PCSGAN.",0.1984202311],["Let us know what you think about it!","On Monotonous Separately Continuous Functions","summarize: Let ",0.125],["chiral CFTs are generated by quantum fields depending on one light-ray coordinate","Operator algebras and vertex operator algebras","summarize: In two-dimensional conformal field theory the building blocks are given by chiral CFTs, i.e.~CFTs on the unit circle . They are generated by quantum fields depending on one light-ray coordinate only. There are two mathematical formulations of chiral CFT, the one based on vertex operator algebras and the one based on conformal nets. We describe some recent results which, for first time, gives a general construction of conformal nets from VOAs.",0.0],["the diffusion of chiral active Brownian particles in three-dimensional space is studied","Diffusion of active chiral particles","summarize: The diffusion of chiral active Brownian particles in three-dimensional space is studied analytically, by consideration of the corresponding Fokker-Planck equation for the probability density of finding a particle at position ",0.3333333333],["we review a method to construct a method.","Current progress on ","summarize: We review a method to construct ",0.0],["we compare the variance of work output per cycle to an average work output. the variable of","Work and power fluctuations in a critical heat engine","summarize: We investigate fluctuations of output work for a class of Stirling heat engines with working fluid composed of interacting units and compare these fluctuations to an average work output. In particular, we focus on engine performance close to a critical point where Carnot's efficiency may be attained at a finite power as reported in . We show that the variance of work output per cycle scales with the same critical exponent as the heat capacity of the working fluid. As a consequence, the relative work fluctuation diverges unless the output work obeys a rather strict scaling condition, which would be very hard to fulfill in practice. Even under this condition, the fluctuations of work and power do not vanish in the infinite system size limit. Large fluctuations of output work thus constitute inseparable and dominant element in performance of the macroscopic heat engines close to a critical point.",0.0769230769],["the largest body in the Main Belt is characterized by a large abundance of water ice","Thermal convection in the crust of the dwarf planet Ceres","summarize: Ceres is the largest body in the Main Belt, and it is characterized by a large abundance of water ice in its interior. This feature is suggested by its relatively low bulk density , while its partial differentiation into a rocky core and icy crust is suggested by several geological and geochemical features: minerals and salts produced by aqueous alteration, icy patches on the surface, lobate morphology interpreted as surface flows. In this work we explore how the composition can influence the characteristics of thermal convection in the crust of Ceres. Our results suggest that the onset of thermal convection is difficult and when it occurs it is short lived and this could imply that Ceres preserved deep liquid until present, as recent suggested by the work of Castillo-Rogez et al.. Moreover, cryovolcanism could be driven by diapirism rather than thermal convection.",0.3214285714],["astrophysics is a promising source for ground-based GW detectors.","Early Advanced LIGO binary neutron-star sky localization and parameter estimation","summarize: 2015 will see the first observations of Advanced LIGO and the start of the gravitational-wave advanced-detector era. One of the most promising sources for ground-based GW detectors are binary neutron-star coalescences. In order to use any detections for astrophysics, we must understand the capabilities of our parameter-estimation analysis. By simulating the GWs from an astrophysically motivated population of BNSs, we examine the accuracy of parameter inferences in the early advanced-detector era. We find that sky location, which is important for electromagnetic follow-up, can be determined rapidly , but that sky areas may be hundreds of square degrees. The degeneracy between component mass and spin means there is significant uncertainty for measurements of the individual masses and spins; however, the chirp mass is well measured .",0.2237098292],["a multiscale technique is able to reduce the complexity of a system composed of","Reducing complexity of multiagent systems with symmetry breaking: an application to opinion dynamics with polls","summarize: In this paper we investigate the possibility of reducing the complexity of a system composed of a large number of interacting agents, whose dynamics feature a symmetry breaking. We consider first order stochastic differential equations describing the behavior of the system at the particle level and we get its continuous counterpart via a kinetic description. However, the resulting continuous model alone fails to describe adequately the evolution of the system, due to the loss of granularity which prevents it from reproducing the symmetry breaking of the particle system. By suitably coupling the two models we are able to reduce considerably the necessary number of particles while still keeping the symmetry breaking and some of its large-scale statistical properties. We describe such a multiscale technique in the context of opinion dynamics, where the symmetry breaking is induced by the results of some opinion polls reported by the media.",0.5262528755],["the SIMPLS or NIPALS algorithm provides estimates on incomplete data. the algorithm is","Determining the Number of Components in PLS Regression on Incomplete Data","summarize: Partial least squares regression---or PLS---is a multivariate method in which models are estimated using either the SIMPLS or NIPALS algorithm. PLS regression has been extensively used in applied research because of its effectiveness in analysing relationships between an outcome and one or several components. Note that the NIPALS algorithm is able to provide estimates on incomplete data. Selection of the number of components used to build a representative model in PLS regression is an important problem. However, how to deal with missing data when using PLS regression remains a matter of debate. Several approaches have been proposed in the literature, including the ",0.1578947368],["supervised classification of remotely sensed images acquired over large geographical areas or at short time intervals","Recent Advances in Domain Adaptation for the Classification of Remote Sensing Data","summarize: The success of supervised classification of remotely sensed images acquired over large geographical areas or at short time intervals strongly depends on the representativity of the samples used to train the classification algorithm and to define the model. When training samples are collected from an image different from the one used for mapping, spectral shifts between the two distributions are likely to make the model fail. Such shifts are generally due to differences in acquisition and atmospheric conditions or to changes in the nature of the object observed. In order to design classification methods that are robust to data-set shifts, recent remote sensing literature has considered solutions based on domain adaptation approaches. Inspired by machine learning literature, several DA methods have been proposed to solve specific problems in remote sensing data classification. This paper provides a critical review of the recent advances in DA for remote sensing and presents an overview of methods divided into four categories: i) invariant feature selection; ii) representation matching; iii) adaptation of classifiers and iv) selective sampling. We provide an overview of recent methodologies, as well as examples of application of the considered techniques to real remote sensing images characterized by very high spatial and spectral resolution. Finally, we propose guidelines to the selection of the method to use in real application scenarios.",0.2222222222],["combined heat and power units require coordinated operation of power system and district heating system. the proposed","Reconfiguration of District Heating Network for Operational Flexibility Enhancement in Power System Unit Commitment","summarize: Massive adoptions of combined heat and power units necessitate the coordinated operation of power system and district heating system . Exploiting the reconfigurable property of district heating networks provides a cost-effective solution to enhance the flexibility of the power system by redistributing heat loads in DHS. In this paper, a unit commitment considering combined electricity and reconfigurable heating network is proposed to coordinate the day-ahead scheduling of power system and DHS. The DHS is formulated as a nonlinear and mixed-integer model with considering the reconfigurable DHN. Also, an auxiliary energy flow variable is introduced in the formed DHS model to make the commitment problem tractable, where the computational burdens are significantly reduced. Extensive case studies are presented to validate the effectiveness of the approximated model and illustrate the potential benefits of the proposed method with respect to congestion management and wind power accommodation. ",0.0416666667],["the current distribution is a linear cylindrical antenna center-driven by a delta-function generator","Alleviating Oscillatory Approximate-Kernel Solutions for Cylindrical Antennas Embedded in a Conducting Medium: a Numerical and Asymptotic Study","summarize: We alleviate the unnatural oscillations occurring in the current distribution along a linear cylindrical antenna center-driven by a delta-function generator and embedded in a conducting medium. The intensely fluctuating current arises as a small-",0.4200808461],["a study of the relation between the charge density at a point on a conducting surface","On the Dependence of Charge Density on Surface Curvature of an Isolated Conductor","summarize: A study of the relation between the electrostatic charge density at a point on a conducting surface and the curvature of the surface is presented. Two major scientific literature on this topic are reviewed and the apparent discrepancy between them is resolved. Hence, a step is taken towards obtaining a general analytic formula for relating the charge density with surface curvature of conductors. The merit of this formula and its limitations are discussed.",0.5641025641],["linguistic typology has been widely used to support multilingual NLP. no systematic survey","Survey on the Use of Typological Information in Natural Language Processing","summarize: In recent years linguistic typology, which classifies the world's languages according to their functional and structural properties, has been widely used to support multilingual NLP. While the growing importance of typological information in supporting multilingual tasks has been recognised, no systematic survey of existing typological resources and their use in NLP has been published. This paper provides such a survey as well as discussion which we hope will both inform and inspire future work in the area.",0.0],["co-clustering is a method to identify rows and columns of a data matrix","Non-Exhaustive, Overlapping Co-Clustering: An Extended Analysis","summarize: The goal of co-clustering is to simultaneously identify a clustering of rows as well as columns of a two dimensional data matrix. A number of co-clustering techniques have been proposed including information-theoretic co-clustering and the minimum sum-squared residue co-clustering method. However, most existing co-clustering algorithms are designed to find pairwise disjoint and exhaustive co-clusters while many real-world datasets contain not only a large overlap between co-clusters but also outliers which should not belong to any co-cluster. In this paper, we formulate the problem of Non-Exhaustive, Overlapping Co-Clustering where both of the row and column clusters are allowed to overlap with each other and outliers for each dimension of the data matrix are not assigned to any cluster. To solve this problem, we propose intuitive objective functions, and develop an an efficient iterative algorithm which we call the NEO-CC algorithm. We theoretically show that the NEO-CC algorithm monotonically decreases the proposed objective functions. Experimental results show that the NEO-CC algorithm is able to effectively capture the underlying co-clustering structure of real-world data, and thus outperforms state-of-the-art clustering and co-clustering methods. This manuscript includes an extended analysis of .",0.3043478261],["magnetic fluctuations can often have strong electromagnetic fluctuations. correlation scale is smaller than electron Larmor","Diffusion and Radiation in Magnetized Collisionless Plasmas with High-Frequency Small-Scale Turbulence","summarize: Magnetized high-energy-density plasmas can often have strong electromagnetic fluctuations whose correlation scale is smaller than the electron Larmor radius. Radiation from the electrons in such plasmas, which markedly differs from both synchrotron and cyclotron radiation, and their energy and pitch-angle diffusion are tightly related. In this paper, we present a comprehensive theoretical and numerical study of the particles' transport in both cold, small-scale Langmuir and Whistler-mode turbulence and its relation to the spectra of radiation simultaneously produced by these particles. We emphasize that this relation is a superb diagnostic tool of laboratory, astrophysical, interplanetary, and solar plasmas with a mean magnetic field and strong small-scale turbulence.",0.0555555556],["a new approach to search for first order invariants of rational second order ordinary differential equation","Dealing with Rational Second Order Ordinary Differential Equations where both Darboux and Lie Find It Difficult: The ","summarize: Here we present a new approach to search for first order invariants of rational second order ordinary differential equations. This method is an alternative to the Darbouxian and symmetry approaches. Our procedure can succeed in many cases where these two approaches fail. We also present here a Maple implementation of the theoretical results and methods, hereby introduced, in a computational package -- . The package is designed, apart from materializing the algorithms presented, to provide a set of tools to allow the user to analyse the intermediary steps of the process.",0.2521419722],["RL methods learn optimal decisions in the presence of a stationary environment. but the stationary","Reinforcement Learning in Non-Stationary Environments","summarize: Reinforcement learning methods learn optimal decisions in the presence of a stationary environment. However, the stationary assumption on the environment is very restrictive. In many real world problems like traffic signal control, robotic applications, one often encounters situations with non-stationary environments and in these scenarios, RL methods yield sub-optimal decisions. In this paper, we thus consider the problem of developing RL methods that obtain optimal decisions in a non-stationary environment. The goal of this problem is to maximize the long-term discounted reward achieved when the underlying model of the environment changes over time. To achieve this, we first adapt a change point algorithm to detect change in the statistics of the environment and then develop an RL algorithm that maximizes the long-run reward accrued. We illustrate that our change point method detects change in the model of the environment effectively and thus facilitates the RL algorithm in maximizing the long-run reward. We further validate the effectiveness of the proposed solution on non-stationary random Markov decision processes, a sensor energy management problem and a traffic signal control problem.",0.16],["the kinematic jerk index is estimated on the basis of gyr","A Smartphone-Based Acquisition System for Hips Rotation Fluency Assessment","summarize: The present contribution is motivated by recent studies on the assessment of the fluency of body movements during complex motor tasks. In particular, we focus on the estimation of the Cartesian kinematic jerk of the hips' orientation during a full three-dimensional movement. The kinematic jerk index is estimated on the basis of gyroscopic signals acquired through a smartphone. A specific free mobile application available for the Android mobile operating system, HyperIMU, is used to acquire the gyroscopic signals and to transmit them to a personal computer via a User Datagram Protocol through a wireless network. The personal computer elaborates the acquired data through a MATLAB script, either in real time or offline, and returns the kinematic jerk index associated to a motor task.",0.1428571429],["algorithm is designed to identify pre-modern coins minted from same dies. it","A simple numeric algorithm for ancient coin dies identification","summarize: A simple computer-based algorithm has been developed to identify pre-modern coins minted from the same dies, intending mainly coins minted by hand-made dies designed to be applicable to images taken from auction websites or catalogs. Though the method is not intended to perform a complete automatic classification, which would require more complex and intensive algorithms accessible to experts of computer vision its simplicity of use and lack of specific requirement about the quality of pictures can provide help and complementary information to the visual inspection, adding quantitative measurements of the distance between pairs of different coins. The distance metric is based on a number of pre-defined reference points that mark key features of the coin to identify the set of coins they have been minted from.",0.2307692308],["a characterizing non-local Stein operator boils down to classical Stein operators. we","On Stein's Method for Infinitely Divisible Laws With Finite First Moment","summarize: We present, in a unified way, a Stein methodology for infinitely divisible laws having finite first moment. Based on a correlation representation, we obtain a characterizing non-local Stein operator which boils down to classical Stein operators in specific examples. Thanks to this characterizing operator, we introduce various extensions of size bias and zero bias distributions and prove that these notions are closely linked to infinite divisibility. Combined with standard Fourier techniques, these extensions also allow obtaining explicit rates of convergence for compound Poisson approximation in particular towards the symmetric ",0.125],["convergence lattice is a lattice and coframe.","Convergence without points","summarize: We introduce a pointfree theory of convergence on lattices and coframes. A convergence lattice is a lattice ",0.0],["auroral records were also surveyed in historical documents. the red celestial sign","The Celestial Sign in the Anglo-Saxon Chronicle in the 770s: Insights on Contemporary Solar Activity","summarize: The anomalous concentration of radiocarbon in 774\/775 attracted intense discussion on its origin, including the possible extreme solar event exceeding any events in observational history. Anticipating such extreme solar events, auroral records were also surveyed in historical documents and those including the red celestial sign after sunset in the Anglo-Saxon Chronicle were subjected to consideration. Usoskin et al. interpreted this record as an aurora and suggested enhanced solar activity around 774\/775. Conversely, Neuhauser and Neuhauser interpreted after sunset as during sunset or twilight; they considered this sign as a halo display and suggested a solar minimum around 774. However, so far these records have not been discussed in comparison with eyewitness auroral records during the known extreme space-weather events, although they were discussed in relationship with potential extreme events in 774\/775. Therefore, we reconstruct the observational details based on the original records in the ASC and philological references, compare them with eyewitness auroral observations during known extreme space-weather events, and consider contemporary solar activity. We clarify the observation was indeed after sunset, reject the solar halo hypothesis, define the observational time span between 25 Mar. 775 and 25 Dec. 777, and note the parallel halo drawing in 806 in the ASC shown in N15b was not based on the original observation in England. We show examples of eyewitness auroral observations during twilight in known space-weather events, and this celestial sign does not contradict the observational evidence. Accordingly, we consider this event happened after the onset of the event in 774\/775, but shows relatively enhanced solar activity, with other historical auroral records in the mid-770s, as also confirmed by the Be data from ice cores.",0.1298001305],["we used the transverse Ising model to map the observed results of the quantum state onto","Deep Neural Network Detects Quantum Phase Transition","summarize: We detect the quantum phase transition of a quantum many-body system by mapping the observed results of the quantum state onto a neural network. In the present study, we utilized the simplest case of a quantum many-body system, namely a one-dimensional chain of Ising spins with the transverse Ising model. We prepared several spin configurations, which were obtained using repeated observations of the model for a particular strength of the transverse field, as input data for the neural network. Although the proposed method can be employed using experimental observations of quantum many-body systems, we tested our technique with spin configurations generated by a quantum Monte Carlo simulation without initial relaxation. The neural network successfully classified the strength of transverse field only from the spin configurations, leading to consistent estimations of the critical point of our model ",0.0],["SN 2019ein exhibited some of the highest measured expansion velocities of any","Constraining the Source of the High-velocity Ejecta in Type Ia SN 2019ein","summarize: We present multiwavelength photometric and spectroscopic observations of SN 2019ein, a high-velocity Type Ia supernova discovered in the nearby galaxy NGC 5353 with a two-day nondetection limit. SN 2019ein exhibited some of the highest measured expansion velocities of any SN Ia, with a Si II absorption minimum blueshifted by 24,000 km s",0.3571428571],["spectral method and semigroup theory are based on verified computations. the method","Rigorous numerical computations for 1D advection equations with variable coefficients","summarize: This paper provides a methodology of verified computing for solutions to 1-dimensional advection equations with variable coefficients. The advection equation is typical partial differential equations of hyperbolic type. There are few results of verified numerical computations to initial-boundary value problems of hyperbolic PDEs. Our methodology is based on the spectral method and semigroup theory. The provided method in this paper is regarded as an efficient application of semigroup theory in a sequence space associated with the Fourier series of unknown functions. This is a foundational approach of verified numerical computations for hyperbolic PDEs. Numerical examples show that the rigorous error estimate showing the well-posedness of the exact solution is given with high accuracy and high speed.",0.1875],["roll2Rail project aims to develop key technologies and remove existing blocking points for radical","The Wireless Train Communication Network: Roll2Rail vision","summarize: This paper explains the main results obtained from the research carried out in the work package 2 of the Roll2Rail project. This project aims to develop key technologies and to remove already identified blocking points for radical innovation in the field of railway vehicles, to increase their operational reliability and to reduce life-cycle costs. This project started in May 2015 and has been funded by the Horizon 2020 program of the European Commission. The goal for WP2 is to research on both technologies and architectures to develop a new wireless Train Communication Network within IEC61375 standard series. This TCN is today entirely wired and is used for Train Control and Monitoring System functions , operator-oriented services and customer-oriented services. This paradigm shift from wired to wireless means a removal of wirings implies, among other benefits, a significant reduction of life cycle costs due to the removal of cables, and the simplification of the train coupling procedure, among others.",0.0],["the dark asteroid Bennu studied by NASAtextquoteright s OS","Modeling optical roughness and first-order scattering processes from OSIRIS-REx color images of the rough surface of asteroid Bennu","summarize: The dark asteroid Bennu studied by NASA\\textquoteright s OSIRIS-REx mission has a boulder-rich and apparently dust-poor surface, providing a natural laboratory to investigate the role of single-scattering processes in rough particulate media. Our goal is to define optical roughness and other scattering parameters that may be useful for the laboratory preparation of sample analogs, interpretation of imaging data, and analysis of the sample that will be returned to Earth. We rely on a semi-numerical statistical model aided by digital terrain model shadow ray-tracing to obtain scattering parameters at the smallest surface element allowed by the DTM . Using a Markov Chain Monte Carlo technique, we solved the inversion problem on all four-band images of the OSIRIS-REx mission\\textquoteright s top four candidate sample sites, for which high-precision laser altimetry DTMs are available. We reconstructed the \\emph probability distribution for each parameter and distinguished primary and secondary solutions. Through the photometric image correction, we found that a mixing of low and average roughness slope best describes Bennu's surface for up to ",0.2341050989],["each year, member states deliver statements during the General Debate. these speeches provide invaluable information","Understanding State Preferences With Text As Data: Introducing the UN General Debate Corpus","summarize: Every year at the United Nations, member states deliver statements during the General Debate discussing major issues in world politics. These speeches provide invaluable information on governments' perspectives and preferences on a wide range of issues, but have largely been overlooked in the study of international politics. This paper introduces a new dataset consisting of over 7,701 English-language country statements from 1970-2016. We demonstrate how the UN General Debate Corpus can be used to derive country positions on different policy dimensions using text analytic methods. The paper provides applications of these estimates, demonstrating the contribution the UNGDC can make to the study of international politics.",0.1875],["agents pick items according to a policy. a policy is a simple and attractive","Welfare of Sequential Allocation Mechanisms for Indivisible Goods","summarize: Sequential allocation is a simple and attractive mechanism for the allocation of indivisible goods. Agents take turns, according to a policy, to pick items. Sequential allocation is guaranteed to return an allocation which is efficient but may not have an optimal social welfare. We consider therefore the relation between welfare and efficiency. We study the questions of what welfare is possible or necessary depending on the choice of policy. We also consider a novel control problem in which the chair chooses a policy to improve social welfare.",0.3513513514],["software organizations have relied on process and technology initiatives to compete in a highly globalized world","Universality of Egoless Behavior of Software Engineering Students","summarize: Software organizations have relied on process and technology initiatives to compete in a highly globalized world. Unfortunately, that has led to little or no success. We propose that the organizations start working on people initiatives, such as inspiring egoless behavior among software developers. This paper proposes a multi-stage approach to develop egoless behavior and discusses the universality of the egoless behavior by studying cohorts from three different countries, i.e., Japan, India, and Canada. The three stages in the approach are self-assessment, peer validation, and action plan development. The paper covers the first stage of self-assssment using an instrument based on Lamont Adams Ten commandments of egoless programming, seven of the factors are general, whereas three are related to coding behavior. We found traces of universality in the egoless behavior among the three cohorts such as there was no difference in egoless behaviours between Indian and Canadian cohorts and both Indian and Japanese cohorts had difficulties in behaving in egoless manner in coding activities than in general activities.",0.2],["in this paper, we study the interactions of electromagnetic waves with a non-dispersive","Electromagnetic fields in a time-varying medium: Exceptional points and operator symmetries","summarize: In this paper, we study the interactions of electromagnetic waves with a non-dispersive dynamic medium that is temporally dependent. Electromagnetic fields under material time-modulation conserve their momentum but not their energy. We assume a time-variation of the permittivity, permeability and conductivity and derive the appropriate time-domain solutions based on the causality state at a past observation time. We formulate a time-transitioning state matrix and connect the unusual energy transitions of electromagnetic fields in time-varying media with the exceptional point theory. This state-matrix approach allows us to analyze further the electromagnetic waves in terms of parity and time-reversal symmetries and signify parity-time symmetric wave-states without the presence of a spatially symmetric distribution of gain and loss, or any inhomogeneities and material periodicity. This paper provides a useful arsenal to study electromagnetic wave phenomena under time-varying media and points out novel physical insights connecting the resulting energy transitions and electromagnetic modes with exceptional point physics and operator symmetries.",0.3888888889],["theta characteristics on an algebraic curve are the specialization of the tropical curve.","Tropicalization of theta characteristics, double covers, and Prym varieties","summarize: We study the behavior of theta characteristics on an algebraic curve under the specialization map to a tropical curve. We show that each effective theta characteristic on the tropical curve is the specialization of ",0.35],["model performance overestimated by a model under distribution shift. we use domain-","Estimating Generalization under Distribution Shifts via Domain-Invariant Representations","summarize: When machine learning models are deployed on a test distribution different from the training distribution, they can perform poorly, but overestimate their performance. In this work, we aim to better estimate a model's performance under distribution shift, without supervision. To do so, we use a set of domain-invariant predictors as a proxy for the unknown, true target labels. Since the error of the resulting risk estimate depends on the target risk of the proxy model, we study generalization of domain-invariant representations and show that the complexity of the latent representation has a significant influence on the target risk. Empirically, our approach enables self-tuning of domain adaptation models, and accurately estimates the target error of given models under distribution shift. Other applications include model selection, deciding early stopping and error detection.",0.2941176471],["quasars can serve as precise cosmological probes. a number","Kernel regression estimates of time delays between gravitationally lensed fluxes","summarize: Strongly lensed variable quasars can serve as precise cosmological probes, provided that time delays between the image fluxes can be accurately measured. A number of methods have been proposed to address this problem. In this paper, we explore in detail a new approach based on kernel regression estimates, which is able to estimate a single time delay given several datasets for the same quasar. We develop realistic artificial data sets in order to carry out controlled experiments to test of performance of this new approach. We also test our method on real data from strongly lensed quasar Q0957+561 and compare our estimates against existing results.",0.2982797723],["photonic materials are emerging for exploring the interface between microscopic quantum dynamics and macroscopic","Synthetic Landau levels for photons","summarize: Synthetic photonic materials are an emerging platform for exploring the interface between microscopic quantum dynamics and macroscopic material properties. Photons experiencing a Lorentz force develop handedness, providing opportunities to study quantum Hall physics and topological quantum science. Here we present an experimental realization of a magnetic field for continuum photons. We trap optical photons in a multimode ring resonator to make a two-dimensional gas of massive bosons, and then employ a non-planar geometry to induce an image rotation on each round-trip. This results in photonic Coriolis\/Lorentz and centrifugal forces and so realizes the Fock-Darwin Hamiltonian for photons in a magnetic field and harmonic trap. Using spatial- and energy-resolved spectroscopy, we track the resulting photonic eigenstates as radial trapping is reduced, finally observing a photonic Landau level at degeneracy. To circumvent the challenge of trap instability at the centrifugal limit, we constrain the photons to move on a cone. Spectroscopic probes demonstrate flat space away from the cone tip. At the cone tip, we observe that spatial curvature increases the local density of states, and we measure fractional state number excess consistent with the Wen-Zee theory, providing an experimental test of this theory of electrons in both a magnetic field and curved space. This work opens the door to exploration of the interplay of geometry and topology, and in conjunction with Rydberg electromagnetically induced transparency, enables studies of photonic fractional quantum Hall fluids and direct detection of anyons.",0.2142857143],["we consider convergence of alternating projections between non-convex sets. we obtain","Alternating projections with applications to Gerchberg-Saxton error reduction","summarize: We consider convergence of alternating projections between non-convex sets and obtain applications to convergence of the Gerchberg-Saxton error reduction method, of the Gaussian expectation-maximization algorithm, and of Cadzow's algorithm.",0.0666666667],["abstract error analysis framework for approximation of linear partial differential equation problems in weak formulation","A third Strang lemma and an Aubin-Nitsche trick for schemes in fully discrete formulation","summarize: In this work, we present an abstract error analysis framework for the approximation of linear partial differential equation problems in weak formulation. We consider approximation methods in fully discrete formulation, where the discrete and continuous spaces are possibly not embedded in a common space. A proper notion of consistency is designed, and, under a classical inf-sup condition, it is shown to bound the approximation error. This error estimate result is in the spirit of Strang's first and second lemmas, but applicable in situations not covered by these lemmas . An improved estimate is also established in a weaker norm, using the Aubin--Nitsche trick. We then apply these abstract estimates to an anisotropic heterogeneous diffusion model and two classical families of schemes for this model: Virtual Element and Finite Volume methods. For each of these methods, we show that the abstract results yield new error estimates with a precise and mild dependency on the local anisotropy ratio. A key intermediate step to derive such estimates for Virtual Element Methods is proving optimal approximation properties of the oblique elliptic projector in weighted Sobolev seminorms. This is a result whose interest goes beyond the specific model and methods considered here. We also obtain, to our knowledge, the first clear notion of consistency for Finite Volume methods, which leads to a generic error estimate involving the fluxes and valid for a wide range of Finite Volume schemes. An important application is the first error estimate for Multi-Point Flux Approximation L and G methods. In the appendix, not included in the published version of this work, we show that classical estimates for discontinuous Galerkin methods can be obtained with simplified arguments using the abstract framework.",0.2941176471],["flow simulation was conducted in a 3D closed-loop heat pipe. flow flow","Nonlinear Analysis of Chaotic Flow in a Three-Dimensional Closed-Loop Pulsating Heat Pipe","summarize: Numerical simulation has been conducted for the chaotic flow in a 3D closed-loop pulsating heat pipe . Heat flux and constant temperature boundary conditions were applied for evaporator and condenser sections, respectively. Water and ethanol were used as working fluids. Volume of Fluid method has been employed for two-phase flow simulation. Spectral analysis of temperature time series was carried out using Power Spectrum Density method. Existence of dominant peak in PSD diagram indicated periodic or quasi-periodic behavior in temperature oscillations at particular frequencies. Correlation dimension values for ethanol as working fluid was found to be higher than that for water under the same operating conditions. Similar range of Lyapunov exponent values for the PHP with water and ethanol as working fluids indicated strong dependency of Lyapunov exponent to the structure and dimensions of the PHP. An O-ring structure pattern was obtained for reconstructed 3D attractor at periodic or quasi-periodic behavior of temperature oscillations. Minimum thermal resistance of 0.85 degree\/W and 0.88 degree\/W were obtained for PHP with water and ethanol, respectively. Simulation results showed good agreement with experimental results from other work under the same operating conditions.",0.2380952381],["random forests using conditional inference trees and subsampling have been found to be not","A Comparison of Resampling and Recursive Partitioning Methods in Random Forest for Estimating the Asymptotic Variance Using the Infinitesimal Jackknife","summarize: The infinitesimal jackknife has recently been applied to the random forest to estimate its prediction variance. These theorems were verified under a traditional random forest framework which uses classification and regression trees and bootstrap resampling. However, random forests using conditional inference trees and subsampling have been found to be not prone to variable selection bias. Here, we conduct simulation experiments using a novel approach to explore the applicability of the IJ to random forests using variations on the resampling method and base learner. Test data points were simulated and each trained using random forest on one hundred simulated training data sets using different combinations of resampling and base learners. Using CI trees instead of traditional CART trees as well as using subsampling instead of bootstrap sampling resulted in a much more accurate estimation of prediction variance when using the IJ. The random forest variations here have been incorporated into an open source software package for the R programming language.",0.1221448233],["pseudoachromatic index of finite affine space. pseudoachromatic index","On chromatic indices of finite affine spaces","summarize: The pseudoachromatic index of the finite affine space ",0.3333333333],["stochastic predictive controller for discrete time linear time invariant systems under incomplete state information","Output feedback stable stochastic predictive control with hard control constraints","summarize: We present a stochastic predictive controller for discrete time linear time invariant systems under incomplete state information. Our approach is based on a suitable choice of control policies, stability constraints, and employment of a Kalman filter to estimate the states of the system from incomplete and corrupt observations. We demonstrate that this approach yields a computationally tractable problem that should be solved online periodically, and that the resulting closed loop system is mean-square bounded for any positive bound on the control actions. Our results allow one to tackle the largest class of linear time invariant systems known to be amenable to stochastic stabilization under bounded control actions via output feedback stochastic predictive control.",0.1176470588],["numeric method is shown to compute an autonomous landing trajectory from any operating condition. the","Reachability as a Unifying Framework for Computing Helicopter Safe Operating Conditions and Autonomous Emergency Landing","summarize: We present a numeric method to compute the safe operating flight conditions for a helicopter such that we can ensure a safe landing in the event of a partial or total engine failure. The unsafe operating region is the complement of the backwards reachable tube, which can be found as the sub-zero level set of the viscosity solution of a Hamilton-Jacobi equation. Traditionally, numerical methods used to solve the HJ equation rely on a discrete grid of the solution space and exhibit exponential scaling with dimension, which is problematic for the high-fidelity dynamics models required for accurate helicopter modeling. We avoid the use of spatial grids by formulating a trajectory optimization problem, where the solution at each initial condition can be computed in a computationally efficient manner. The proposed method is shown to compute an autonomous landing trajectory from any operating condition, even in non-cruise flight conditions.",0.1578947368],["social media presence of peerJ articles is high. about 68.18% of the papers","Social Media Attention Increases Article Visits: An Investigation on Article-Level Referral Data of PeerJ","summarize: In order to better understand the effect of social media in the dissemination of scholarly articles, employing the daily updated referral data of 110 PeerJ articles collected over a period of 345 days, we analyze the relationship between social media attention and article visitors directed by social media. Our results show that social media presence of PeerJ articles is high. About 68.18% of the papers receive at least one tweet from Twitter accounts other than @PeerJ, the official account of the journal. Social media attention increases the dissemination of scholarly articles. Altmetrics could not only act as the complement of traditional citation measures but also play an important role in increasing the article downloads and promoting the impacts of scholarly articles. There also exists a significant correlation among the online attention from different social media platforms. Articles with more Facebook shares tend to get more tweets. The temporal trends show that social attention comes immediately following publication but does not last long, so do the social media directed article views.",0.1851922157],["point defect engineering is found to drive the phase transition from the as-grown mixed rh","Controllable Defect Driven Symmetry Change and Domain Structure Evolution in BiFeO3 with Enhanced Tetragonality","summarize: Defect engineering has been a powerful tool to enable the creation of exotic phases and the discovery of intriguing phenomena in ferroelectric oxides. However, accurate control the concentration of defects remains a big challenge. In this work, ion implantation, that can provide controllable point defects, allows us the ability to produce a controlled defect-driven true super-tetragonal phase with enhanced tetragonality in ferroelectric BiFeO3 thin films. This point defect engineering is found to drive the phase transition from the as-grown mixed rhombohedral-like and tetragonal-like phase to true tetragonal symmetry. By further increasing the injected dose of He ion, we demonstrate an enhanced tetragonality super-tetragonal phase with the largest c\/a ratio that has ever been experimentally achieved in BiFeO3. A combination of morphology change and domain evolution further confirm that the mixed R\/MC phase structure transforms to the single-domain-state true tetragonal phase. Moreover, the re-emergence of R phase and in-plane stripe nanodomains after heat treatment reveal the memory effect and reversible phase transition. Our findings demonstrate the control of R-Mc-T-super T symmetry changes and the creation of true T phase BiFeO3 with enhanced tetragonality through controllable defect engineering. This work also provides a pathway to generate large tetragonality that could be extended to other ferroelectric material systems which may lead to strong polarization enhancement.",0.0],["the proposed FT protocol can support an unbounded number of faulty nodes as long","Toward Fault-Tolerant Deadlock-Free Routing in HyperSurface-Embedded Controller Networks","summarize: HyperSurfaces consist of structurally reconfigurable metasurfaces whose electromagnetic properties can be changed via a software interface, using an embedded miniaturized network of controllers. With the HSF controllers, interconnected in an irregular, near-Manhattan geometry, we propose a robust, deterministic Fault-Tolerant , deadlock- and livelock-free routing protocol where faults are contained in a set of disjointed rectangular regions called faulty blocks. The proposed FT protocol can support an unbounded number of faulty nodes as long as nodes outside the faulty blocks are connected. Simulation results show the efficacy of the proposed FT protocol under various faulty node distribution scenarios.",0.0666666667],["the discretization for the tempered fractional substantial derivative is derived. the finite","Numerical schemes of the time tempered fractional Feynman-Kac equation","summarize: This paper focuses on providing the computation methods for the backward time tempered fractional Feynman-Kac equation, being one of the models recently proposed in . The discretization for the tempered fractional substantial derivative is derived, and the corresponding finite difference and finite element schemes are designed with well established stability and convergence. The performed numerical experiments show the effectiveness of the presented schemes.",0.2631578947],["multi-level summarizer supervised method to construct abstractive summaries.","Interpretable Multi-Headed Attention for Abstractive Summarization at Controllable Lengths","summarize: Abstractive summarization at controllable lengths is a challenging task in natural language processing. It is even more challenging for domains where limited training data is available or scenarios in which the length of the summary is not known beforehand. At the same time, when it comes to trusting machine-generated summaries, explaining how a summary was constructed in human-understandable terms may be critical. We propose Multi-level Summarizer , a supervised method to construct abstractive summaries of a text document at controllable lengths. The key enabler of our method is an interpretable multi-headed attention mechanism that computes attention distribution over an input document using an array of timestep independent semantic kernels. Each kernel optimizes a human-interpretable syntactic or semantic property. Exhaustive experiments on two low-resource datasets in the English language show that MLS outperforms strong baselines by up to 14.70% in the METEOR score. Human evaluation of the summaries also suggests that they capture the key concepts of the document at various length-budgets.",0.0],["we obtain the so-called Nourdin-Peccati bound. from this","Normal Convergence Using Malliavin Calculus With Applications and Examples","summarize: We prove the chain rule in the more general framework of the Wiener-Poisson space, allowing us to obtain the so-called Nourdin-Peccati bound. From this bound we obtain a second-order Poincare-type inequality that is useful in terms of computations. For completeness we survey these results on the Wiener space, the Poisson space, and the Wiener-Poisson space. We also give several applications to central limit theorems with relevant examples: linear functionals of Gaussian subordinated fields , Poisson functionals in the first Poisson chaos restricted to infinitely many \\small jumps and the product of two Ornstein-Uhlenbeck processes . We also obtain bounds for their rate of convergence to normality.",0.0],["circuit obfuscation is a frequently used approach to conceal logic functionalities","Exploiting Spin-Orbit Torque Devices as Reconfigurable Logic for Circuit Obfuscation","summarize: Circuit obfuscation is a frequently used approach to conceal logic functionalities in order to prevent reverse engineering attacks on fabricated chips. Efficient obfuscation implementations are expected with lower design complexity and overhead but higher attack difficulties. In this paper, an emerging obfuscation approach is proposed by leveraging spinorbit torque devices based look-up-tables as reconfigurable logic to replace the carefully selected gates. It is essentially impossible to identify the obfuscated gate with SOTs inside according to the physical geometry characteristics because the configured functionalities are represented by magnetization states. Such an obfuscation approach makes the circuit security further improved with high exponential attack complexities. Experiments on MCNC and ISCAS 85\/89 benchmark suits show that the proposed approach could reduce the area overheads due to obfuscation by 10% averagely.",0.1875],["traditional solutions to HDR imaging are designed for and applied to CMOS image sensors.","HDR Imaging with Quanta Image Sensors: Theoretical Limits and Optimal Reconstruction","summarize: High dynamic range imaging is one of the biggest achievements in modern photography. Traditional solutions to HDR imaging are designed for and applied to CMOS image sensors . However, the mainstream one-micron CIS cameras today generally have a high read noise and low frame-rate. These, in turn, limit the acquisition speed and quality, making the cameras slow in the HDR mode. In this paper, we propose a new computational photography technique for HDR imaging. Recognizing the limitations of CIS, we use the Quanta Image Sensor to trade the spatial-temporal resolution with bit-depth. QIS is a single-photon image sensor that has comparable pixel pitch to CIS but substantially lower dark current and read noise. We provide a complete theoretical characterization of the sensor in the context of HDR imaging, by proving the fundamental limits in the dynamic range that QIS can offer and the trade-offs with noise and speed. In addition, we derive an optimal reconstruction algorithm for single-bit and multi-bit QIS. Our algorithm is theoretically optimal for \\emph linear reconstruction schemes based on exposure bracketing. Experimental results confirm the validity of the theory and algorithm, based on synthetic and real QIS data.",0.125],["the viewer imports chunk video objects through the proxy server. the situation requires either the proxy","Content Delivery Through Hybrid Architecture in Video on Demand System","summarize: Peer-to-Peer network needs architectural modification for smooth and fast transportation of video content. The viewer imports chunk video objects through the proxy server. The enormous growth of user requests in a small session of time creates huge load on the VOD system. The situation requires either the proxy server streamed video-content fully or partly to the viewers. The missing chunk at the proxy server is imported from the connected peer nodes. Peers exchange chunks among themselves according to some chunk selection policy. Peer node randomly contacts another peer to download a missing chunk from the buffers during each time slot. In video streaming, when the relevant frame is required at the viewer ends that should be available at the respective proxy server. The video watcher also initiates various types of interactive operations like a move forward or skips some finite number of frames that create congestion inside the VOD system. To elevate the situation it needs an effective content delivery mechanism for smooth transportation of content. The proposed hybrid architecture is composed of P2P and mesh architecture that effectively enhances the search mechanism and content transportation in the VOD system.",0.0],["the proposed formulation achieves even greater domains of attraction. the proposed formulation achieves even","Harmonic based model predictive control for set-point tracking","summarize: This paper presents a novel model predictive control formulation for set-point tracking. Stabilizing predictive controllers based on terminal ingredients may exhibit stability and feasibility issues in the event of a reference change for small to moderate prediction horizons. In the MPC for tracking formulation, these issues are solved by the addition of an artificial equilibrium point as a new decision variable, providing a significantly enlarged domain of attraction and guaranteeing recursive feasibility for any reference change. However, it may suffer from performance issues if the prediction horizon is not large enough. This paper presents an extension of this formulation where a harmonic artificial reference is used in place of the equilibrium point. The proposed formulation achieves even greater domains of attraction and can significantly outperform other MPC formulations when the prediction horizon is small. We prove the asymptotic stability and recursive feasibility of the proposed controller, as well as provide guidelines for the design of its main ingredients. Finally, we highlight its advantages with a case study of a ball and plate system.",0.0],["this paper investigates distributed control and incentive mechanisms. it proposes a distributed stoch","Online Stochastic Optimization of Networked Distributed Energy Resources","summarize: This paper investigates distributed control and incentive mechanisms to coordinate distributed energy resources with both continuous and discrete decision variables as well as device dynamics in distribution grids. We formulate a multi-period social welfare maximization problem, and based on its convex relaxation propose a distributed stochastic dual gradient algorithm for managing DERs. We further extend it to an online realtime setting with time-varying operating conditions, asynchronous updates by devices, and feedback being leveraged to account for nonlinear power flows as well as reduce communication overhead. The resulting algorithm provides a general online stochastic optimization algorithm for coordinating networked DERs with discrete power setpoints and dynamics to meet operational and economic objectives and constraints. We characterize the convergence of the algorithm analytically and evaluate its performance numerically.",0.1052631579],["we try to search for the neuron configuration of a fixed network architecture. using it","NeuralScale: Efficient Scaling of Neurons for Resource-Constrained Deep Neural Networks","summarize: Deciding the amount of neurons during the design of a deep neural network to maximize performance is not intuitive. In this work, we attempt to search for the neuron configuration of a fixed network architecture that maximizes accuracy. Using iterative pruning methods as a proxy, we parameterize the change of the neuron number of each layer with respect to the change in parameters, allowing us to efficiently scale an architecture across arbitrary sizes. We also introduce architecture descent which iteratively refines the parameterized function used for model scaling. The combination of both proposed methods is coined as NeuralScale. To prove the efficiency of NeuralScale in terms of parameters, we show empirical simulations on VGG11, MobileNetV2 and ResNet18 using CIFAR10, CIFAR100 and TinyImageNet as benchmark datasets. Our results show an increase in accuracy of 3.04%, 8.56% and 3.41% for VGG11, MobileNetV2 and ResNet18 on CIFAR10, CIFAR100 and TinyImageNet respectively under a parameter-constrained setting of default configuration with scaling factor of 0.25).",0.3157894737],["magnetic Particle Imaging is an emerging medical imaging modality. most of the working MPI","Single-Sided Field-Free Line Generator Magnet for Multidimensional Magnetic Particle Imaging","summarize: Magnetic Particle Imaging is an emerging medical imaging modality that is not yet adopted by clinical practice. Most of the working MPI prototypes including commercial-grade research MPI scanners utilize cylindrical bores that limit the access to the scanner and the imaging volume. Recently a single-sided or an asymmetric device that is based on a field-free point coplanar coil topology has been introduced that shows promise in alleviating access constraint issues. In this paper, we present a simulation study of selection coils for a novel design of a single-sided MPI device that has an advantage of a more sensitive field-free line topology.",0.1428571429],["pseudoconvex Hartogs domains are bounded in smooth bounded","On compactness of Hankel and the ","summarize: We prove that on smooth bounded pseudoconvex Hartogs domains in ",0.0],["despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures","Extracting Headless MWEs from Dependency Parse Trees: Parsing, Tagging, and Joint Modeling Approaches","summarize: An interesting and frequent type of multi-word expression is the headless MWE, for which there are no true internal syntactic dominance relations; examples include many named entities and dates as well as certain productive constructions . Despite their special status and prevalence, current dependency-annotation schemes require treating such flat structures as if they had internal syntactic heads, and most current parsers handle them in the same fashion as headed constructions. Meanwhile, outside the context of parsing, taggers are typically used for identifying MWEs, but taggers might benefit from structural information. We empirically compare these two common strategies--parsing and tagging--for predicting flat MWEs. Additionally, we propose an efficient joint decoding algorithm that combines scores from both strategies. Experimental results on the MWE-Aware English Dependency Corpus and on six non-English dependency treebanks with frequent flat structures show that: tagging is more accurate than parsing for identifying flat-structure MWEs, our joint decoder reconciles the two different views and, for non-BERT features, leads to higher accuracies, and most of the gains result from feature sharing between the parsers and taggers.",0.0714285714],["in this paper, we investigate the moduli of continuity for viscosity solutions.","Moduli of Continuity for Viscosity Solutions","summarize: In this paper, we investigate the moduli of continuity for viscosity solutions of a wide class of nonsingular quasilinear evolution equations and also for the level set mean curvature flow, which is an example of singular degenerate equations. We prove that the modulus of continuity is a viscosity subsolution of some one dimensional equation. This work extends B. Andrews' recent result on moduli of continuity for smooth spatially periodic solutions.",0.2142857143],["agricultural data is a key foundation to establishing a crop intelligence platform. the agricultural","Designing and Implementing Data Warehouse for Agricultural Big Data","summarize: In recent years, precision agriculture that uses modern information and communication technologies is becoming very popular. Raw and semi-processed agricultural data are usually collected through various sources, such as: Internet of Thing , sensors, satellites, weather stations, robots, farm equipment, farmers and agribusinesses, etc. Besides, agricultural datasets are very large, complex, unstructured, heterogeneous, non-standardized, and inconsistent. Hence, the agricultural data mining is considered as Big Data application in terms of volume, variety, velocity and veracity. It is a key foundation to establishing a crop intelligence platform, which will enable resource efficient agronomy decision making and recommendations. In this paper, we designed and implemented a continental level agricultural data warehouse by combining Hive, MongoDB and Cassandra. Our data warehouse capabilities: flexible schema; data integration from real agricultural multi datasets; data science and business intelligent support; high performance; high storage; security; governance and monitoring; replication and recovery; consistency, availability and partition tolerant; distributed and cloud deployment. We also evaluate the performance of our data warehouse.",0.3783783784],["quantaloid is a small quantaloid.","Lax distributive laws for topology, II","summarize: For a small quantaloid ",0.2046826883],["we propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving","Softwarization of Internet of Things Infrastructure for Secure and Smart Healthcare","summarize: We propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving deployment of Internet of Things for smart healthcare applications and services. It integrates state-of-the-art networking and virtualization techniques across IoT, fog and cloud domains, employing Blockchain, Tor and message brokers to provide security and privacy for patients and healthcare providers. We propose a novel platform using Machine-to-Machine messaging and rule-based beacons for seamless data management and discuss the role of data and decision fusion in the cloud and the fog, respectively, for smart healthcare applications and services.",0.2],["the images can be captured in such scenarios using Near-Infrared and Thermal cameras","PCSGAN: Perceptual Cyclic-Synthesized Generative Adversarial Networks for Thermal and NIR to Visible Image Transformation","summarize: In many real world scenarios, it is difficult to capture the images in the visible light spectrum due to bad lighting conditions. However, the images can be captured in such scenarios using Near-Infrared and Thermal cameras. The NIR and THM images contain the limited details. Thus, there is a need to transform the images from THM\/NIR to VIS for better understanding. However, it is non-trivial task due to the large domain discrepancies and lack of abundant datasets. Nowadays, Generative Adversarial Network is able to transform the images from one domain to another domain. Most of the available GAN based methods use the combination of the adversarial and the pixel-wise losses as the objective function for training. The quality of transformed images in case of THM\/NIR to VIS transformation is still not up to the mark using such objective function. Thus, better objective functions are needed to improve the quality, fine details and realism of the transformed images. A new model for THM\/NIR to VIS image transformation called Perceptual Cyclic-Synthesized Generative Adversarial Network is introduced to address these issues. The PCSGAN uses the combination of the perceptual losses along with the pixel-wise and the adversarial losses. Both the quantitative and qualitative measures are used to judge the performance of the PCSGAN model over the WHU-IIP face and the RGB-NIR scene datasets. The proposed PCSGAN outperforms the state-of-the-art image transformation models, including Pix2pix, DualGAN, CycleGAN, PS2GAN, and PAN in terms of the SSIM, MSE, PSNR and LPIPS evaluation measures. The code is available at https:\/\/github.com\/KishanKancharagunta\/PCSGAN.",0.1984202311],["pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from","Pseudo-Outcrop Visualization of Borehole Images and Core Scans","summarize: A pseudo-outcrop visualization is demonstrated for borehole and full-diameter rock core images to augment the ubiquitous unwrapped cylinder view and thereby to assist non-specialist interpreters. The pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from borehole to earth frame of reference that creates a solid volume sliced longitudinally to reveal two or more faces in which the orientations of geological features indicate what is observed in the subsurface. A proxy for grain size is used to modulate the external dimensions of the plot to mimic profiles seen in real outcrops. The volume is created from a mixture of geological boundary elements and texture, the latter being the residue after the sum of boundary elements is subtracted from the original data. In the case of measurements from wireline microresistivity tools, whose circumferential coverage is substantially less than 100%, the missing circumferential data is first inpainted using multiscale directional transforms, which decompose the image into its elemental building structures, before reconstructing the full image. The pseudo-outcrop view enables direct observation of the angular relationships between features and aids visual comparison between borehole and core images, especially for the interested non-specialist.",0.3888888889],["data analytics and player modeling are beneficial as they allow for the study of player behavior at scale","Data-Driven Game Development: Ethical Considerations","summarize: In recent years, the games industry has made a major move towards data-driven development, using data analytics and player modeling to inform design decisions. Data-driven techniques are beneficial as they allow for the study of player behavior at scale, making them very applicable to modern digital game development. However, with this move towards data driven decision-making comes a number of ethical concerns. Previous work in player modeling as well as work in the fields of AI and machine learning have demonstrated several ways in which algorithmic decision-making can be flawed due to data or algorithmic bias or lack of data from specific groups. Further, black box algorithms create a trust problem due to lack of interpretability and transparency of the results or models developed based on the data, requiring blind faith in the results. In this position paper, we discuss several factors affecting the use of game data in the development cycle. In addition to issues raised by previous work, we also raise issues with algorithms marginalizing certain player groups and flaws in the resulting models due to their inability to reason about situational factors affecting players' decisions. Further, we outline some work that seeks to address these problems and identify some open problems concerning ethics and game data science.",0.0909090909],["a case that Microscopic Black Holes of mass.","Ramjet Acceleration of Microscopic Black Holes Within Stellar Material","summarize: In this work we present a case that Microscopic Black Holes of mass ",0.6618726769],["the IRC, equipped with a a planetary camera, obtained a low-","AKARI\/IRC Near-Infrared Spectral Atlas of Galactic Planetary Nebulae","summarize: Near-infrared low-resolution spectra of 72 Galactic planetary nebulae were obtained with the Infrared Camera in the post-helium phase. The IRC, equipped with a ",0.75],["the optimal harvesting strategy is determined for the generic configuration of a flexible cable fixed at both","Optimal Energy Harvesting from Vortex-Induced Vibrations of Cables","summarize: Vortex-induced vibrations of flexible cables are an example of flow-induced vibrations that can act as energy harvesting systems by converting energy associated with the spontaneous cable motion into electricity. This work investigates the optimal positioning of the harvesting devices along the cable, using numerical simulations with a wake oscillator model to describe the unsteady flow forcing. Using classical gradient-based optimization, the optimal harvesting strategy is determined for the generic configuration of a flexible cable fixed at both ends, including the effect of flow forces and gravity on the cable's geometry. The optimal strategy is found to consist systematically in a concentration of the harvesting devices at one of the cable's ends, relying on deformation waves along the cable to carry the energy toward this harvesting site. Furthermore, we show that the performance of systems based on VIV of flexible cables is significantly more robust to flow velocity variations, in comparison with a rigid cylinder device. This results from two passive control mechanisms inherent to the cable geometry : the adaptability to the flow velocity of the fundamental frequencies of cables through the flow-induced tension and the selection of successive vibration modes by the flow velocity for cables with gravity-induced tension.",0.2222222222],["proposed algorithm is designed for polarization division multiplexing systems. the frame and frequency","Robust Frame and Frequency Synchronization Based on Alamouti Coding for RGI-CO-OFDM","summarize: We propose an algorithm for carrying out joint frame and frequency synchronization in reduced-guard-interval coherent optical orthogonal frequency division multiplexing systems. The synchronization is achieved by using the same training symbols employed for training-aided channel estimation , thereby avoiding additional training overhead. The proposed algorithm is designed for polarization division multiplexing RGI-CO-OFDM systems that use the Alamouti-type polarization-time coding for TA-CE. Due to their optimal TA-CE performance, Golay complementary sequences have been used as the TS in the proposed algorithm. The frame synchronization is accomplished by exploiting the cross-correlation between the received TS from the two orthogonal polarizations. The arrangement of the TS is also used to estimate the carrier frequency offset. Simulation results of a PDM RGI-CO-OFDM system operating at 238.1 Gb\/s data rate , with a total overhead of 9.2% , show that the proposed scheme has accurate synchronization, and is robust to linear fiber impairments.",0.1428571429],["the computational cost of HMC lies in numerical integration. this is because the numerical integrations","Geometric integrators and the Hamiltonian Monte Carlo method","summarize: This paper surveys in detail the relations between numerical integration and the Hamiltonian Monte Carlo method . Since the computational cost of HMC mainly lies in the numerical integrations, these should be performed as efficiently as possible. However, HMC requires methods that have the geometric properties of being volume-preserving and reversible, and this limits the number of integrators that may be used. On the other hand, these geometric properties have important quantitative implications on the integration error, which in turn have an impact on the acceptance rate of the proposal. While at present the velocity Verlet algorithm is the method of choice for good reasons, we argue that Verlet can be improved upon. We also discuss in detail the behavior of HMC as the dimensionality of the target distribution increases.",0.1304347826],["pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from","Pseudo-Outcrop Visualization of Borehole Images and Core Scans","summarize: A pseudo-outcrop visualization is demonstrated for borehole and full-diameter rock core images to augment the ubiquitous unwrapped cylinder view and thereby to assist non-specialist interpreters. The pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from borehole to earth frame of reference that creates a solid volume sliced longitudinally to reveal two or more faces in which the orientations of geological features indicate what is observed in the subsurface. A proxy for grain size is used to modulate the external dimensions of the plot to mimic profiles seen in real outcrops. The volume is created from a mixture of geological boundary elements and texture, the latter being the residue after the sum of boundary elements is subtracted from the original data. In the case of measurements from wireline microresistivity tools, whose circumferential coverage is substantially less than 100%, the missing circumferential data is first inpainted using multiscale directional transforms, which decompose the image into its elemental building structures, before reconstructing the full image. The pseudo-outcrop view enables direct observation of the angular relationships between features and aids visual comparison between borehole and core images, especially for the interested non-specialist.",0.3888888889],["philosophy of physics and philosophy of physics has played a major role. symmetry","The Role of Symmetry in Mathematics","summarize: Over the past few decades the notion of symmetry has played a major role in physics and in the philosophy of physics. Philosophers have used symmetry to discuss the ontology and seeming objectivity of the laws of physics. We introduce several notions of symmetry in mathematics and explain how they can also be used in resolving different problems in the philosophy of mathematics. We use symmetry to discuss the objectivity of mathematics, the role of mathematical objects, the unreasonable effectiveness of mathematics and the relationship of mathematics to physics.",0.1739130435],["the charged anisotropic star on paraboloidal spacetime is reported by choosing particular form","Charged Anisotropic Star on Paraboloidal Spacetime","summarize: The charged anisotropic star on paraboloidal spacetime is reported by choosing particular form of radial pressure and electric field intensity. The non-singular solution of Einstein-Maxwell system of equation have been derived and it is shown that model satisfy all the physical plausibility conditions. It is observed that in the absence of electric field intensity, model reduces to particular case of uncharged Sharma \\& Ratanpal model. It is also observed that the parameter used in electric field intensity directly effects the mass of the star.",0.1428571429],["in 1964, he began working in the Physics Department of the Advanced Studies Center of the National","Brief Notes and History Computing in Mexico during 50 years","summarize: The history of computing in Mexico can not be thought without the name of Prof. Harold V. McIntosh . For almost 50 years, in Mexico he contributed to the development of computer science with wide international recognition. Approximately in 1964, McIntosh began working in the Physics Department of the Advanced Studies Center of the National Polytechnic Institute , now called CINVESTAV. In 1965, at the National Center of Calculus , he was a founding member of the Master in Computing, first in Latin America. With the support of Mario Baez Camargo and Enrique Melrose, McIntosh continues his research of Martin-Baltimore Computer Center and University of Florida at IBM 709.",0.1875],["pilot allocation in massive MIMO is a hard combinatorial problem. it depends on","Location-Aware Pilot Allocation in Multi-Cell Multi-User Massive MIMO Networks","summarize: We propose a location-aware pilot allocation algorithm for a massive multiple-input multiple-output network with high-mobility users, where the wireless channels are subject to Rician fading. Pilot allocation in massive MIMO is a hard combinatorial problem and depends on the locations of users. As such, it is highly complex to achieve the optimal pilot allocation in real-time for a network with high-mobility users. Against this background, we propose a low-complexity pilot allocation algorithm, which exploits the behavior of line-of-sight interference among the users and allocate the same pilot sequence to the users with small LOS interference. Our examination demonstrates that our proposed algorithm significantly outperforms the existing algorithms, even with localization errors. Specifically, for the system considered in this work, our proposed algorithm provides up to 37.26% improvement in sum spectral efficiency and improves the sum SE of the worst interference-affected users by up to 2.57 bits\/sec\/Hz, as compared to the existing algorithms.",0.380952381],["new interaction estimates for a system introduced by Baiti and Jenssen. they are","New interaction estimates for the Baiti-Jenssen system","summarize: We establish new interaction estimates for a system introduced by Baiti and Jenssen. These estimates are pivotal to the analysis of the wave front-tracking approximation. In a companion paper we use them to construct a counter-example which shows that Schaeffer's Regularity Theorem for scalar conservation laws does not extend to systems. The counter-example we construct shows, furthermore, that a wave-pattern containing infinitely many shocks can be robust with respect to perturbations of the initial data. The proof of the interaction estimates is based on the explicit computation of the wave fan curves and on a perturbation argument.",0.5],["Let us know what you think about it!","Closed range of ","summarize: Let ",0.0],["sparse control inputs arise naturally in networked systems. derived conditions can","Controllability of Linear Dynamical Systems Under Input Sparsity Constraints","summarize: In this work, we consider the controllability of a discrete-time linear dynamical system with sparse control inputs. Sparsity constraints on the input arises naturally in networked systems, where activating each input variable adds to the cost of control. We derive algebraic necessary and sufficient conditions for ensuring controllability of a system with an arbitrary transfer matrix. The derived conditions can be verified in polynomial time complexity, unlike the more traditional Kalman-type rank tests. Further, we characterize the minimum number of input vectors required to satisfy the derived conditions for controllability. Finally, we present a generalized Kalman decomposition-like procedure that separates the state-space into subspaces corresponding to sparse-controllable and sparse-uncontrollable parts. These results form a theoretical basis for designing networked linear control systems with sparse inputs.",0.1666666667],["the FAIR facility is an international accelerator centre for research with ion and antiprot","Twin GEM-TPC Prototype Beam Test at GSI and Jyv\\askyl\\a - a Development for the Super-FRS at FAIR","summarize: The FAIR facility is an international accelerator centre for research with ion and antiproton beams. It is being built at Darmstadt, Germany as an extension to the current GSI research institute. One major part of the facility will be the Super-FRS separator, which will be include in phase one of the project construction. The NUSTAR experiments will benefit from the Super-FRS, which will deliver an unprecedented range of radioactive ion beams . These experiments will use beams of different energies and characteristics in three different branches; the high-energy which utilizes the RIB at relativistic energies 300-1500 MeV\/u as created in the production process, the low-energy branch aims to use beams in the range of 0-150 MeV\/u whereas the ring branch will cool and store beams in the NESR ring. The main tasks for the Super-FRS beam diagnostics chambers will be for the set up and adjustment of the separator as well as to provide tracking and event-by-event particle identification. The Helsinki Institute of Physics, and the Detector Laboratory and Experimental Electronics at GSI are in a joint R&D of a GEM-TPC detector which could satisfy the requirements of such tracking detectors, in terms of tracking efficiency, space resolution, count rate capability and momenta resolution. The current prototype, which is the generation four of this type, is two GEM-TPCs in twin configuration inside the same vessel. This means that one of the GEM-TPC is flipped on the middle plane w.r.t. the other one. This chamber was tested at Jyv\\askyl\\a accelerator with protons projectiles and at GSI with Uranium, fragments and Carbon beams during this year 2016.",0.2373875726],["the discretisation is based on several adaptations of the Hybrid-High-Or","An arbitrary order scheme on generic meshes for miscible displacements in porous media","summarize: We design, analyse and implement an arbitrary order scheme applicable to generic meshes for a coupled elliptic-parabolic PDE system describing miscible displacement in porous media. The discretisation is based on several adaptations of the Hybrid-High-Order method due to Di Pietro et al. . The equation governing the pressure is discretised using an adaptation of the HHO method for variable diffusion, while the discrete concentration equation is based on the HHO method for advection-diffusion-reaction problems combined with numerically stable flux reconstructions for the advective velocity that we have derived using the results of Cockburn et al. . We perform some rigorous analysis of the method to demonstrate its ",0.1389034164],["counting functions are counted in the counting function. counting functions are counted in","The average order of the M\\bius function for Beurling primes","summarize: In this paper, we study the counting functions ",0.1142857143],["GW170104 was measured by the twin advanced detectors of the laser interferometer","GW170104: Observation of a 50-Solar-Mass Binary Black Hole Coalescence at Redshift 0.2","summarize: We describe the observation of GW170104, a gravitational-wave signal produced by the coalescence of a pair of stellar-mass black holes. The signal was measured on January 4, 2017 at 10:11:58.6 UTC by the twin advanced detectors of the Laser Interferometer Gravitational-Wave Observatory during their second observing run, with a network signal-to-noise ratio of 13 and a false alarm rate less than 1 in 70,000 years. The inferred component black hole masses are ",0.1428571429],["net of transition chain is the net of transition chain.","The genericity of Arnold diffusion in nearly integrable Hamiltonian systems","summarize: In this paper, we prove that the net of transition chain is ",0.1111111111],["a new interpretation of the WAY theorem is based on a relation","A relational perspective on the Wigner-Araki-Yanase theorem","summarize: We present a novel interpretation of the Wigner-Araki-Yanase theorem based on a relational view of quantum mechanics. Several models are analysed in detail, backed up by general considerations, which serve to illustrate that the moral of the WAY theorem may be that in the presence of symmetry, a measuring apparatus must fulfil the dual purpose of both reflecting the statistical behaviour of the system under investigation, and acting as a physical reference system serving to define those quantities which must be understood as relative.",0.6956521739],["post-detection SETI protocols are non-binding and too general.","Post-Detection SETI Protocols & METI: The Time Has Come To Regulate Them Both","summarize: Regulations governing METI are weak or non-existent. Post-detection SETI protocols are non-binding and too general. Vastly increased SETI capabilities, Chinese involvement in the field, and an intensified effort by METI-ists to initiate radio transmissions to the stars are among reasons cited for urgency in addressing the question of appropriate regulations. Recommendations include regulations at the agency level and laws at the national level as well as international treaties and oversight.",0.0669076786],["we analyse the choice of the regularisation parameter for linear ill-posed problems. we","Heuristic Parameter Choice Rules for Tikhonov Regularisation with Weakly Bounded Noise","summarize: We study the choice of the regularisation parameter for linear ill-posed problems in the presence of noise that is possibly unbounded but only finite in a weaker norm, and when the noise-level is unknown. For this task, we analyse several heuristic parameter choice rules, such as the quasi-optimality, heuristic discrepancy, and Hanke-Raus rules and adapt the latter two to the weakly bounded noise case. We prove convergence and convergence rates under certain noise conditions. Moreover, we analyse and provide conditions for the convergence of the parameter choice by the generalised cross-validation and predictive mean-square error rules.",0.0588235294],["the stochastic model admits a folded node singularity and exhibits a","Stochastic mixed-mode oscillations in a three-species predator-prey model","summarize: The effect of demographic stochasticity, in the form of Gaussian white noise, in a predator-prey model with one fast and two slow variables is studied. We derive the stochastic differential equations from a discrete model. For suitable parameter values, the deterministic drift part of the model admits a folded node singularity and exhibits a singular Hopf bifurcation. We focus on the parameter regime near the Hopf bifurcation, where small amplitude oscillations exist as stable dynamics in the absence of noise. In this regime, the stochastic model admits noise-driven mixed-mode oscillations , which capture the intermediate dynamics between two cycles of population outbreaks. We perform numerical simulations to calculate the distribution of the random number of small oscillations between successive spikes for varying noise intensities and distance to the Hopf bifurcation. We also study the effect of noise on a suitable Poincar\\'e map. Finally, we prove that the stochastic model can be transformed into a normal form near the folded node, which can be linked to recent results on the interplay between deterministic and stochastic small amplitude oscillations. The normal form can also be used to study the parameter influence on the noise level near folded singularities.",0.4285714286],["outlier and noise detection processes are highly useful in the quality assessment of any kind of database","A Proposal for Outlier and Noise Detection in Public Officials' Affidavits","summarize: Outlier and noise detection processes are highly useful in the quality assessment of any kind of database. Such processes may have novel civic and public applications in the detection of anomalies in public data. The purpose of this work is to explore the possibilities of experimentation with, validation and application of hybrid outlier and noise detection procedures in public officials' affidavit systems currently available in Argentina.",0.1],["the observed sample of such supernovae is still low. the supernovae are similar","A low-luminosity core-collapse supernova very similar to SN 2005cs","summarize: We present observations and analysis of PSN J17292918+7542390, a low-luminosity Type II-P supernova . The observed sample of such events is still low, and their nature is still under debate. Such supernovae are similar to SN 2005cs, a well-observed low-luminosity Type II-P event, having low expansion velocities, and small ejected ",0.1176470588],["quantum computation is an emerging technology that promises to be powerful tool in many areas. the development","Procedural generation using quantum computation","summarize: Quantum computation is an emerging technology that promises to be a powerful tool in many areas. Though some years likely still remain until significant quantum advantage is demonstrated, the development of the technology has led to a range of valuable resources. These include publicly available prototype quantum hardware, advanced simulators for small quantum programs and programming frameworks to test and develop quantum software. In this provocation paper we seek to demonstrate that these resources are sufficient to provide the first useful results in the field of procedural generation. This is done by introducing a proof-of-principle method: a quantum generalization of a blurring process, in which quantum interference is used to provide a unique effect. Through this we hope to show that further developments in the technology are not required before it becomes useful for procedural generation. Rather, fruitful experimentation with this new technology can begin now.",0.1818181818],["approximate nearest neighbour algorithm determines the minimax learning rates. neighbours are computed in","Minimax rates for cost-sensitive learning on manifolds with approximate nearest neighbours","summarize: We study the approximate nearest neighbour method for cost-sensitive classification on low-dimensional manifolds embedded within a high-dimensional feature space. We determine the minimax learning rates for distributions on a smooth manifold, in a cost-sensitive setting. This generalises a classic result of Audibert and Tsybakov. Building upon recent work of Chaudhuri and Dasgupta we prove that these minimax rates are attained by the approximate nearest neighbour algorithm, where neighbours are computed in a randomly projected low-dimensional space. In addition, we give a bound on the number of dimensions required for the projection which depends solely upon the reach and dimension of the manifold, combined with the regularity of the marginal.",0.5],["the analysis is based on the recent results on the characterization of the maximum coding rate","Performance of Non-orthogonal Multiple Access under Finite Blocklength","summarize: In this paper, we present a finite-block-length comparison between the orthogonal multiple access scheme and the non-orthogonal multiple access for the uplink channel. First, we consider the Gaussian channel, and derive the closed form expressions for the rate and outage probability. Then, we extend our results to the quasi-static Rayleigh fading channel. Our analysis is based on the recent results on the characterization of the maximum coding rate at finite block-length and finite block-error probability. The overall system throughput is evaluated as a function of the number of information bits, channel uses and power. We find what would be the respective values of these different parameters that would enable throughput maximization. Furthermore, we analyze the system performance in terms of reliability and throughput when applying the type-I ARQ protocol with limited number of retransmissions. The throughput and outage probability are evaluated for different blocklengths and number of information bits. Our analysis reveals that there is a trade-off between reliability and throughput in the ARQ. While increasing the number of retransmissions boosts reliability by minimizing the probability of reception error, it results in more delay which decreases the throughput. Nevertheless, the results show that NOMA always outperforms OMA in terms of throughput, reliability and latency regardless of the users priority or the number of retransmissions in both Gaussian and fading channels.",0.1515151515],["a single self-play architecture has learned three different games at super human level. the","Warm-Start AlphaZero Self-Play Search Enhancements","summarize: Recently, AlphaZero has achieved landmark results in deep reinforcement learning, by providing a single self-play architecture that learned three different games at super human level. AlphaZero is a large and complicated system with many parameters, and success requires much compute power and fine-tuning. Reproducing results in other games is a challenge, and many researchers are looking for ways to improve results while reducing computational demands. AlphaZero's design is purely based on self-play and makes no use of labeled expert data ordomain specific enhancements; it is designed to learn from scratch. We propose a novel approach to deal with this cold-start problem by employing simple search enhancements at the beginning phase of self-play training, namely Rollout, Rapid Action Value Estimate and dynamically weighted combinations of these with the neural network, and Rolling Horizon Evolutionary Algorithms . Our experiments indicate that most of these enhancements improve the performance of their baseline player in three different board games, with especially RAVE based variants playing strongly.",0.2857142857],["we establish sharp exponential deviation estimates of the information content. we also establish a sharp bound","Concentration of information content for convex measures","summarize: We establish sharp exponential deviation estimates of the information content as well as a sharp bound on the varentropy for the class of convex measures on Euclidean spaces. This generalizes a similar development for log-concave measures in the recent work of Fradelizi, Madiman and Wang . In particular, our results imply that convex measures in high dimensions are concentrated in an annulus between two convex sets despite their possibly having much heavier tails. Various tools and consequences are developed, including a sharp comparison result for R\\'enyi entropies, inequalities of Kahane-Khinchine type for convex measures that extend those of Koldobsky, Pajor and Yaskin for log-concave measures, and an extension of Berwald's inequality .",0.1935483871],["low-frequency, GMRT observations at 240, 610 and 1300 MHz","GMRT observations of IC 711 -- The longest head-tail radio galaxy known","summarize: We present low-frequency, GMRT observations at 240, 610 and 1300 MHz of IC~711, a narrow angle tail radio galaxy. The total angular extent of the radio emission, ",0.2149593932],["graph jobs are a complex structure represented by graphs. the nodes denote the","Power-Aware Allocation of Graph Jobs in Geo-Distributed Cloud Networks","summarize: In the era of big-data, the jobs submitted to the clouds exhibit complicated structures represented by graphs, where the nodes denote the sub-tasks each of which can be accommodated at a slot in a server, while the edges indicate the communication constraints among the sub-tasks. We develop a framework for efficient allocation of graph jobs in geo-distributed cloud networks , explicitly considering the power consumption of the datacenters . We address the following two challenges arising in graph job allocation: i) the allocation problem belongs to NP-hard nonlinear integer programming; ii) the allocation requires solving the NP-complete sub-graph isomorphism problem, which is particularly cumbersome in large-scale GDCNs. We develop a suite of efficient solutions for GDCNs of various scales. For small-scale GDCNs, we propose an analytical approach based on convex programming. For medium-scale GDCNs, we develop a distributed allocation algorithm exploiting the processing power of DCs in parallel. Afterward, we provide a novel low-complexity sub-graph extraction method, based on which we introduce cloud crawlers aiming to extract allocations of good potentials for large-scale GDCNs. Given these suggested strategies, we further investigate strategy selection under both fixed and adaptive DC pricing schemes, and propose an online learning algorithm for each.",0.2105263158],["the current distribution is a linear cylindrical antenna center-driven by a delta-function generator","Alleviating Oscillatory Approximate-Kernel Solutions for Cylindrical Antennas Embedded in a Conducting Medium: a Numerical and Asymptotic Study","summarize: We alleviate the unnatural oscillations occurring in the current distribution along a linear cylindrical antenna center-driven by a delta-function generator and embedded in a conducting medium. The intensely fluctuating current arises as a small-",0.4200808461],["sneutrino inflation employs the fermionic partners of the inflaton","Sneutrino Inflation with ","summarize: Sneutrino inflation employs the fermionic partners of the inflaton and stabilizer field as right-handed neutrinos to realize the seesaw mechanism for light neutrino masses. We show that one can improve the latest version of this scenario and its consistency with the Planck data by embedding it in the theory of cosmological ",0.0],["bibliographic analysis considers author's research areas, the citation network and the paper content","Bibliographic Analysis on Research Publications using Authors, Categorical Labels and the Citation Network","summarize: Bibliographic analysis considers the author's research areas, the citation network and the paper content among other things. In this paper, we combine these three in a topic model that produces a bibliographic model of authors, topics and documents, using a nonparametric extension of a combination of the Poisson mixed-topic link model and the author-topic model. This gives rise to the Citation Network Topic Model . We propose a novel and efficient inference algorithm for the CNTM to explore subsets of research publications from CiteSeerX. The publication datasets are organised into three corpora, totalling to about 168k publications with about 62k authors. The queried datasets are made available online. In three publicly available corpora in addition to the queried datasets, our proposed model demonstrates an improved performance in both model fitting and document clustering, compared to several baselines. Moreover, our model allows extraction of additional useful knowledge from the corpora, such as the visualisation of the author-topics network. Additionally, we propose a simple method to incorporate supervision into topic modelling to achieve further improvement on the clustering task.",0.2],["the method has been made available as the CABS-flex web server. it uses","CABS-flex 2.0: a web server for fast simulations of flexibility of protein structures","summarize: Classical simulations of protein flexibility remain computationally expensive, especially for large proteins. A few years ago, we developed a fast method for predicting protein structure fluctuations that uses a single protein model as the input. The method has been made available as the CABS-flex web server and applied in numerous studies of protein structure-function relationships. Here, we present a major update of the CABS-flex web server to version 2.0. The new features include: extension of the method to significantly larger and multimeric proteins, customizable distance restraints and simulation parameters, contact maps and a new, enhanced web server interface. CABS-flex 2.0 is freely available at http:\/\/biocomp.chem.uw.edu.pl\/CABSflex2",0.3125],["new approach leads to giant gain enhancement in a Fabry-Perot cavity","Giant Gain Enhancement in Photonic Crystals with a Degenerate Band Edge","summarize: We propose a new approach leading to giant gain enhancement. It is based on unconventional slow wave resonance associated to a degenerate band edge in the dispersion diagram for a special class of photonic crystals supporting two modes at each frequency. We show that the gain enhancement in a Fabry-Perot cavity when operating at the DBE is several orders of magnitude stronger when compared to a cavity of the same length made of a standard photonic crystal with a regular band edge . The giant gain condition is explained by a significant increase in the photon lifetime and in the local density of states. We have demonstrated the existence of DBE operated special cavities that provide for superior gain conditions for solid-state lasers, quantum cascade lasers, traveling wave tubes, and distributed solid state amplifiers. We also report the possibility to achieve low-threshold lasing in FPC with DBE compared to RBE-based lasers.",0.5],["ancient Chinese translation, poem generation, and couplet generation are natural language processing tasks. previous","AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding and Generation","summarize: Ancient Chinese is the essence of Chinese culture. There are several natural language processing tasks of ancient Chinese domain, such as ancient-modern Chinese translation, poem generation, and couplet generation. Previous studies usually use the supervised models which deeply rely on parallel data. However, it is difficult to obtain large-scale parallel data of ancient Chinese. In order to make full use of the more easily available monolingual ancient Chinese corpora, we release AnchiBERT, a pre-trained language model based on the architecture of BERT, which is trained on large-scale ancient Chinese corpora. We evaluate AnchiBERT on both language understanding and generation tasks, including poem classification, ancient-modern Chinese translation, poem generation, and couplet generation. The experimental results show that AnchiBERT outperforms BERT as well as the non-pretrained models and achieves state-of-the-art results in all cases.",0.1333333333],["we give a detailed description of the intensity measure. the properties of this measure on loop","Poisson ensembles of loops of one-dimensional diffusions","summarize: We study the analogue of Poisson ensembles of Markov loops in the setting of one-dimensional diffusions. We give a detailed description of the corresponding intensity measure. The properties of this measure on loops lead us to an extension of Vervaat's bridge-to-excursion transformation that relates the bridges conditioned by their minimum and the excursions of all the diffusion we consider and not just the Brownian motion. Further we describe the Poisson point process of loops, their occupation fields and explain how to sample these Poisson ensembles of loops using two-dimensional Markov processes. Finally we introduce a couple of interwoven determinantal point processes on the line which is a dual through Wilson's algorithm of Poisson ensembles of loops and study the properties of these determinantal point processes.",0.3076923077],["a keyword based systematic mapping study was conducted to help in identifying, def","The Key Concepts of Ethics of Artificial Intelligence - A Keyword based Systematic Mapping Study","summarize: The growing influence and decision-making capacities of Autonomous systems and Artificial Intelligence in our lives force us to consider the values embedded in these systems. But how ethics should be implemented into these systems? In this study, the solution is seen on philosophical conceptualization as a framework to form practical implementation model for ethics of AI. To take the first steps on conceptualization main concepts used on the field needs to be identified. A keyword based Systematic Mapping Study on the keywords used in AI and ethics was conducted to help in identifying, defying and comparing main concepts used in current AI ethics discourse. Out of 1062 papers retrieved SMS discovered 37 re-occurring keywords in 83 academic papers. We suggest that the focus on finding keywords is the first step in guiding and providing direction for future research in the AI ethics field.",0.2707591324],["parallelization program, called MPI_XSTAR, has been developed and implemented in the C","MPI_XSTAR: MPI-based Parallelization of the XSTAR Photoionization Program","summarize: We describe a program for the parallel implementation of multiple runs of XSTAR, a photoionization code that is used to predict the physical properties of an ionized gas from its emission and\/or absorption lines. The parallelization program, called MPI_XSTAR, has been developed and implemented in the C++ language by using the Message Passing Interface protocol, a conventional standard of parallel computing. We have benchmarked parallel multiprocessing executions of XSTAR, using MPI_XSTAR, against a serial execution of XSTAR, in terms of the parallelization speedup and the computing resource efficiency. Our experience indicates that the parallel execution runs significantly faster than the serial execution, however, the efficiency in terms of the computing resource usage decreases with increasing the number of processors used in the parallel computing.",0.0833333333],["new approaches have demonstrated the potential in real world settings. but they still fail to produce reconstruction","PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization","summarize: Recent advances in image-based 3D human shape estimation have been driven by the significant improvement in representation power afforded by deep neural networks. Although current approaches have demonstrated the potential in real world settings, they still fail to produce reconstructions with the level of detail often present in the input images. We argue that this limitation stems primarily form two conflicting requirements; accurate predictions require large context, but precise predictions require high resolution. Due to memory limitations in current hardware, previous approaches tend to take low resolution images as input to cover large spatial context, and produce less precise 3D estimates as a result. We address this limitation by formulating a multi-level architecture that is end-to-end trainable. A coarse level observes the whole image at lower resolution and focuses on holistic reasoning. This provides context to an fine level which estimates highly detailed geometry by observing higher-resolution images. We demonstrate that our approach significantly outperforms existing state-of-the-art techniques on single image human shape reconstruction by fully leveraging 1k-resolution input images.",0.0],["we show that any of the above are not a'stupid'","Approximating matrices and convex bodies through Kadison-Singer","summarize: We show that any ",0.0],["we consider a chain of chains of ad hoc.","Hyperbolic scaling limit of non-equilibrium fluctuations for a weakly anharmonic chain","summarize: We consider a chain of ",0.4270599482],["a model for analyzing spatial data is shown over a coastline. four models were","Coastline Kriging: A Bayesian Approach","summarize: Statistical interpolation of chemical concentrations at new locations is an important step in assessing a worker's exposure level. When measurements are available from coastlines, as is the case in coastal clean-up operations in oil spills, one may need a mechanism to carry out spatial interpolation at new locations along the coast. In this paper we present a simple model for analyzing spatial data that is observed over a coastline. We demonstrate four different models using two different representations of the coast using curves. The four models were demonstrated on simulated data and one of them was also demonstrated on a dataset from the GuLF STUDY. Our contribution here is to offer practicing hygienists and exposure assessors with a simple and easy method to implement Bayesian hierarchical models for analyzing and interpolating coastal chemical concentrations.",0.2580645161],["thin bismuth films on Bi. could be considered for realization of topological supercon","Origin of Suppression of Proximity Induced Superconductivity in Bi\/Bi","summarize: Mixing of topological states with superconductivity could result in topological superconductivity with the elusive Majorana fermions potentially applicable in fault-tolerant quantum computing. One possible candidate considered for realization of topological superconductivity is thin bismuth films on Bi",0.25],["Let us know what you think about it!","Determination of Baum-Bott residues of higher codimensional foliations","summarize: Let ",0.0],["boundary control input is subject to constant delay while the open loop system might exhibit a finite","Feedback Stabilization of a Class of Diagonal Infinite-Dimensional Systems with Delay Boundary Control","summarize: This paper studies the boundary feedback stabilization of a class of diagonal infinite-dimensional boundary control systems. In the studied setting, the boundary control input is subject to a constant delay while the open loop system might exhibit a finite number of unstable modes. The proposed control design strategy consists in two main steps. First, a finite-dimensional subsystem is obtained by truncation of the original Infinite-Dimensional System via modal decomposition. It includes the unstable components of the infinite-dimensional system and allows the design of a finite-dimensional delay controller by means of the Artstein transformation and the pole-shifting theorem. Second, it is shown via the selection of an adequate Lyapunov function that 1) the finite-dimensional delay controller successfully stabilizes the original infinite-dimensional system; 2) the closed-loop system is exponentially Input-to-State Stable with respect to distributed disturbances. Finally, the obtained ISS property is used to derive a small gain condition ensuring the stability of an IDS-ODE interconnection.",0.25],["Dirac spinor fields in five and four dimensions share many features. in five dimensions we","Dirac spinors and their application to Bianchi-I space-times in 5 dimensions","summarize: We consider a five-dimensional Einstein--Cartan spacetime upon which Dirac spinor fields can be defined. Dirac spinor fields in five and four dimensions share many features, like the fact that both are described by four-component spinor fields, but they are also characterized by strong differences, like the fact that in five dimensions we do not have the possibility to project on left-handed and right-handed chiral parts unlike what happens in the four-dimensional instance: we conduct a polar decomposition of the spinorial fields, so to highlight all similarities and discrepancies. As an application of spinor fields in five dimensions, we study Bianchi-I spacetimes, verifying whether the Dirac fields in five dimensions can give rise to inflation or dark-energy dominated cosmological eras or not.",0.3913043478],["fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change","Delayed Impact of Fair Machine Learning","summarize: Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect. We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably. Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",0.1428571429],["a mixture model with online knowledge distillation can achieve better evaluation performance. the proposed distill","MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization","summarize: In this paper, we present and discuss a deep mixture model with online knowledge distillation for large-scale video temporal concept localization, which is ranked 3rd in the 3rd YouTube-8M Video Understanding Challenge. Specifically, we find that by enabling knowledge sharing with online distillation, fintuning a mixture model on a smaller dataset can achieve better evaluation performance. Based on this observation, in our final solution, we trained and fintuned 12 NeXtVLAD models in parallel with a 2-layer online distillation structure. The experimental results show that the proposed distillation structure can effectively avoid overfitting and shows superior generalization performance. The code is publicly available at: https:\/\/github.com\/linrongc\/solution_youtube8m_v3",0.2976613134],["a long baseline interferometer. where two distantly located radio telescopes receive simultaneous","Radio VLBI and the quantum interference paradox","summarize: We address here the question of interference of radio signals from astronomical sources like distant quasars, in a very long baseline interferometer , where two distantly located radio telescopes , receive simultaneous signal from the sky. In an equivalent optical two-slit experiment, it is generally argued that for the photons involved in the interference pattern on the screen, it is not possible, even in principle, to know which of the two slits a particular photon went through and that any procedure to ascertain this destroys the interference pattern. But in the case of the modern radio VLBI, it is a routine matter to record the phase and amplitude of the voltage outputs from the two radio antennas on a recording media separately and then do the correlation between the two recorded signals later in an offline manner. Does this not violate the quantum interference principle? We provide a resolution of this problem here.",0.2941176471],["a new study is underway to weave ethics into advancing ML research. a","Theories of Parenting and their Application to Artificial Intelligence","summarize: As machine learning systems have advanced, they have acquired more power over humans' lives, and questions about what values are embedded in them have become more complex and fraught. It is conceivable that in the coming decades, humans may succeed in creating artificial general intelligence that thinks and acts with an open-endedness and autonomy comparable to that of humans. The implications would be profound for our species; they are now widely debated not just in science fiction and speculative research agendas but increasingly in serious technical and policy conversations. Much work is underway to try to weave ethics into advancing ML research. We think it useful to add the lens of parenting to these efforts, and specifically radical, queer theories of parenting that consciously set out to nurture agents whose experiences, objectives and understanding of the world will necessarily be very different from their parents'. We propose a spectrum of principles which might underpin such an effort; some are relevant to current ML research, while others will become more important if AGI becomes more likely. These principles may encourage new thinking about the development, design, training, and release into the world of increasingly autonomous agents.",0.3461538462],["graphene helicoid has been a new feature in thermal properties.","Graphene Helicoid: The Distinct Properties Promote Application of Graphene Related Materials in Thermal Management","summarize: The extremely high thermal conductivity of graphene has received great attention both in experiments and calculations. Obviously, new feature in thermal properties is of primary importance for application of graphene-based materials in thermal management in nanoscale. Here, we studied the thermal conductivity of graphene helicoid, a newly reported graphene-related nanostructure, using molecular dynamics simulation. Interestingly, in contrast to the converged cross-plane thermal conductivity in multi-layer graphene, axial thermal conductivity of graphene helicoid keeps increasing with thickness with a power law scaling relationship, which is a consequence of the divergent in-plane thermal conductivity of two-dimensional graphene. Moreover, the large overlap between adjacent layers in graphene helicoid also promotes higher thermal conductivity than multi-layer graphene. Furthermore, in the small strain regime , compressive strain can effectively increase the thermal conductivity of graphene helicoid, while in the ultra large strain regime , tensile strain does not decrease the heat current, unlike that in generic solid-state materials. Our results reveal that the divergence in thermal conductivity, associated with the anomalous strain dependence and the unique structural flexibility, make graphene helicoid a new platform for studying fascinating phenomena of key relevance to the scientific understanding and technological applications of graphene-related materials.",0.2872800197],["NISP will be tested at Laboratoire d'Astrophysique de","3D metrology with a laser tracker inside a vacuum chamber for NISP test campaign","summarize: In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille in ERIOS chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the EUCLID object plane, a telescope simulator with a very well know focal distance will be installed in front of NISP into ERIOS chamber. We need to measure at cold and vacuum the position of reflectors installed on NISP instrument and the telescope simulator. From these measurements, we will provide at operational temperature the measurement of references frames set on the telescope simulator and NISP, the knowledge of the coordinates of the object point source provided by the telescope simulator and the measurement of the angle between the telescope simulator optical axis and NISP optical axis. In this context, we have developed a metrology method based on the use of a laser tracker to measure the position of the reflectors inside ERIOS. The laser tracker is installed outside the vacuum chamber and measure through a curved window reflectors put inside the chamber either at ambient pressure or vacuum pressure. Several tests campaigns have been done at LAM to demonstrate the measurement performance with this configuration. Using a well know reflectors configuration, we show that it is possible to correct the laser tracker measurement from the window disturbances and from the vacuum impact. A corrective term is applied to the data and allows retrieving the real coordinates of the reflectors with a bias lower than 30",0.1574555176],["the relevance of RE research to practitioners is vital for a long-term dissemination of research results","How do Practitioners Perceive the Relevance of Requirements Engineering Research?","summarize: The relevance of Requirements Engineering research to practitioners is vital for a long-term dissemination of research results to everyday practice. Some authors have speculated about a mismatch between research and practice in the RE discipline. However, there is not much evidence to support or refute this perception. This paper presents the results of a study aimed at gathering evidence from practitioners about their perception of the relevance of RE research and at understanding the factors that influence that perception. We conducted a questionnaire-based survey of industry practitioners with expertise in RE. The participants rated the perceived relevance of 435 scientific papers presented at five top RE-related conferences. The 153 participants provided a total of 2,164 ratings. The practitioners rated RE research as essential or worthwhile in a majority of cases. However, the percentage of non-positive ratings is still higher than we would like. Among the factors that affect the perception of relevance are the research's links to industry, the research method used, and respondents' roles. The reasons for positive perceptions were primarily related to the relevance of the problem and the soundness of the solution, while the causes for negative perceptions were more varied. The respondents also provided suggestions for future research, including topics researchers have studied for decades, like elicitation or requirement quality criteria.",0.2222222222],["the complete part of the earthquake frequency-magnitude distribution is well described by the Gutenberg","Generalized Earthquake Frequency-Magnitude Distribution Described by Asymmetric Laplace Mixture Modelling","summarize: The complete part of the earthquake frequency-magnitude distribution , above completeness magnitude mc, is well described by the Gutenberg-Richter law. The parameter mc however varies in space due to the seismic network configuration, yielding a convoluted FMD shape below max. This paper investigates the shape of the generalized FMD , which may be described as a mixture of elemental FMDs defined as asymmetric Laplace distributions of mode mc . An asymmetric Laplace mixture model is thus proposed with its parameters estimated using a semi-supervised hard expectation maximization approach including BIC penalties for model complexity. The performance of the proposed method is analysed, with encouraging results obtained: kappa, beta, and the mc distribution range are retrieved for different GFMD shapes in simulations, as well as in regional catalogues , in a global catalogue, and in an aftershock sequence . We find max to be conservative compared to other methods, kappa = k\/log = 3 in most catalogues = 1), but also that biases in kappa and beta may occur when rounding errors are present below completeness. The GFMD-ALMM, by modelling different FMD shapes in an autonomous manner, opens the door to new statistical analyses in the realm of incomplete seismicity data, which could in theory improve earthquake forecasting by considering c. ten times more events.",0.0952380952],["a gap exists in our ability to systematically generate networks that adhere to theoretical guarantees for the","Designing Networks: A Mixed-Integer Linear Optimization Approach","summarize: Designing networks with specified collective properties is useful in a variety of application areas, enabling the study of how given properties affect the behavior of network models, the downscaling of empirical networks to workable sizes, and the analysis of network evolution. Despite the importance of the task, there currently exists a gap in our ability to systematically generate networks that adhere to theoretical guarantees for the given property specifications. In this paper, we propose the use of Mixed-Integer Linear Optimization modeling and solution methodologies to address this Network Generation Problem. We present a number of useful modeling techniques and apply them to mathematically express and constrain network properties in the context of an optimization formulation. We then develop complete formulations for the generation of networks that attain specified levels of connectivity, spread, assortativity and robustness, and we illustrate these via a number of computational case studies.",0.1333333333],["author linked the integer to essential codimension of projections. he says the resulting","Restricted diagonalization of finite spectrum normal operators and a theorem of Arveson","summarize: Kadison characterized the diagonals of projections and observed the presence of an integer. Arveson later recognized this integer as a Fredholm index obstruction applicable to any normal operator with finite spectrum coincident with its essential spectrum whose elements are the vertices of a convex polygon. Recently, in joint work with Kaftal, the author linked the Kadison integer to essential codimension of projections. This paper provides an analogous link between Arveson's obstruction and essential codimension as well as a new approach to Arveson's theorem which also allows for generalization to any finite spectrum normal operator. In fact, we prove that Arveson's theorem is a corollary of a trace invariance property of arbitrary normal operators. An essential ingredient is a formulation of Arveson's theorem in terms of diagonalization by a unitary which is a Hilbert-Schmidt perturbation of the identity.",0.2941176471],["clinical dermoscopic features may indicate melanoma. a neural network architecture","Fully Convolutional Neural Networks to Detect Clinical Dermoscopic Features","summarize: The presence of certain clinical dermoscopic features within a skin lesion may indicate melanoma, and automatically detecting these features may lead to more quantitative and reproducible diagnoses. We reformulate the task of classifying clinical dermoscopic features within superpixels as a segmentation problem, and propose a fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images. Our neural network architecture uses interpolated feature maps from several intermediate network layers, and addresses imbalanced labels by minimizing a negative multi-label Dice-F",0.2222222222],["tests designed on distribution\/density and regression models play a central role in the mathematical","Smoothing-based tests with directional random variables","summarize: Testing procedures for assessing specific parametric model forms, or for checking the plausibility of simplifying assumptions, play a central role in the mathematical treatment of the uncertain. No certain answers are obtained by testing methods, but at least the uncertainty of these answers is properly quantified. This is the case for tests designed on the two most general data generating mechanisms in practice: distribution\/density and regression models. Testing proposals are usually formulated on the Euclidean space, but important challenges arise in non-Euclidean settings, such as when directional variables are involved. This work reviews some of the smoothing-based testing procedures for density and regression models that comprise directional variables. The asymptotic distributions of the revised proposals are presented, jointly with some numerical illustrations justifying the need of employing resampling mechanisms for effective test calibration.",0.3913043478],["the feasible region can be employed for the selection of feasible footholds and CoM tra","Feasible Region: an Actuation-Aware Extension of the Support Region","summarize: In legged locomotion the projection of the robot Center of Mass being inside the convex hull of the contact points is a commonly accepted sufficient condition to achieve static balancing. However, some of these configurations cannot be realized because the joint torques required to sustain them would be above their limits . In this manuscript we rule out such configurations and define the Feasible Region, a revisited support region that guarantees both global static stability in the sense of tipover and slippage avoidance and of existence of a set of joint-torques that are able to sustain the robot body weight. We show that the feasible region can be employed for the selection of feasible footholds and CoM trajectories to achieve static locomotion on rough terrains, also in presence of load intensive tasks. Key results of our approach include the efficiency in the computation of the feasible region thanks to an Iterative Projection algorithm. This allowed us to carry out successful experiments on the HyQ robot, that was able to negotiate obstacles of moderate dimensions while carrying an extra 10 kg payload.",0.1578947368],["hazard function of empirical duration data is dominated by a bathtub curve.","Modelling the Dropout Patterns of MOOC Learners","summarize: We adopted survival analysis for the viewing durations of massive open online courses. The hazard function of empirical duration data is dominated by a bathtub curve and has the Lindy effect in its tail. To understand the evolutionary mechanisms underlying these features, we categorized learners into two classes due to their different distributions of viewing durations, namely lognormal distribution and power law with exponential cutoff. Two random differential equations are provided to describe the growth patterns of viewing durations for the two classes respectively. The expected duration change rate of the learners featured by lognormal distribution is supposed to be dependent on their past duration, and that of the rest learners is supposed to be inversely proportional to time. Solutions to the equations predict the features of viewing duration distributions, and those of the hazard function. The equations also reveal the feature of memory and that of memorylessness for the viewing behaviors of the two classes respectively.",0.15],["the hybrid policy gradient estimator is shown to be biased, but has variance reduced property.","A Hybrid Stochastic Policy Gradient Algorithm for Reinforcement Learning","summarize: We propose a novel hybrid stochastic policy gradient estimator by combining an unbiased policy gradient estimator, the REINFORCE estimator, with another biased one, an adapted SARAH estimator for policy optimization. The hybrid policy gradient estimator is shown to be biased, but has variance reduced property. Using this estimator, we develop a new Proximal Hybrid Stochastic Policy Gradient Algorithm to solve a composite policy optimization problem that allows us to handle constraints or regularizers on the policy parameters. We first propose a single-looped algorithm then introduce a more practical restarting variant. We prove that both algorithms can achieve the best-known trajectory complexity ",0.125],["research paper outlines information warehousing and online synthetic dispensation OLAP","A Peta-Scale Data Movement and Analysis in Data Warehouse ","summarize: In this research paper so as to handle Information warehousing as well as online synthetic dispensation OLAP are necessary aspects of conclusion support which takes more and more turn into a focal point of the data source business.This paper offers an outline of information warehousing also OLAP systems with a highlighting on their latest necessities.All of us explain backside end tackle for extract clean-up and load information into an Data warehouse multi dimensional data model usual of OLAP frontend user tools for query and facts evaluation server extension for useful query dispensation and apparatus for metadata managing and for supervision the stockroom. Insights centered on complete data on customer actions manufactured goods act and souk performance are powerful advance and opposition in the internet gap .In this research conclude the company inspiration and the program and efficiency of servers working in a data warehouse through use of some new techniques and get better and efficient results. Data in petabyte scale. This test shows the data dropping rate in data warehouse. The locomotive is in creation at Yahoo! since 2007 and presently manages more than half a dozen peta bytes of data.",0.1],["hybrid fiber link combines effective optical frequency transfer and evaluation of performances with a self-synchron","Studying fundamental limit of optical fiber links to ","summarize: We present an hybrid fiber link combining effective optical frequency transfer and evaluation of performances with a self-synchronized two-way comparison. It enables us to detect the round-trip fiber noise and each of the forward and backward one-way fiber noises simultaneously. The various signals acquired with this setup allow us to study quantitatively several properties of optical fiber links. We check the reciprocity of the accumulated noise forth and back over a bi-directional fiber to the level of ",0.3333333333],["two vehicular networking technologies are envisioned to jointly support the full range of ITS services","An SDN hybrid architecture for vehicular networks: Application to Intelligent Transport System","summarize: Vehicular networks are one of the cornerstone of an Intelligent Transportation System . They are expected to provide ubiquitous network connectivity to moving vehicles while supporting various ITS services, some with very stringent requirements in terms of latency and reliability. Two vehicular networking technologies are envisioned to jointly support the full range of ITS services : DSRC for direct vehicle to vehicle\/Road Side Units communications and cellular technologies. To the best of our knowledge, approaches from the literature usually divide ITS services on each of these networks according to their requirements and one single network is in charge of supporting the each service. Those that consider both network technologies to offer multi-path routing, load balancing or path splitting for a better quality of experience of ITS services assume obviously separately controlled networks. Under the umbrella of SDN , we propose in this paper a hybrid network architecture that enables the joint control of the networks providing connectivity to multi-homed vehicles and, also, explore the opportunities brought by such an architecture. We show through some use cases, that in addition to the flexibility and fine-grained programmability brought by SDN, it opens the way towards the development of effective network control algorithms that are the key towards the successful support of ITS services and especially those with stringent QoS. We also show how these algorithms could also benefit from information related to the environment or context in which vehicles evolve , which could be easily collected by data providers and made available via the cloud.",0.1875],["the newman-watts model is given by taking a cycle graph of","First passage percolation on the Newman-Watts small world model","summarize: The Newman-Watts model is given by taking a cycle graph of n vertices and then adding each possible edge ",0.4666666667],["xCELLigence real time cell analysis is now allowing to monitor the cell migration","A Macroscopic Mathematical Model For Cell Migration Assays Using A Real-Time Cell Analysis","summarize: Experiments of cell migration and chemotaxis assays have been classically performed in the so-called Boyden Chambers. A recent technology, xCELLigence Real Time Cell Analysis, is now allowing to monitor the cell migration in real time. This technology measures impedance changes caused by the gradual increase of electrode surface occupation by cells during the course of time and provide a Cell Index which is proportional to cellular morphology, spreading, ruffling and adhesion quality as well as cell number. In this paper we propose a macroscopic mathematical model, based on \\emph partial differential equations, describing the cell migration assay using the real-time technology. We carried out numerical simulations to compare simulated model dynamics with data of observed biological experiments on three different cell lines and in two experimental settings: absence of chemotactic signals and presence of a chemoattractant. Overall we conclude that our minimal mathematical model is able to describe the phenomenon in the real time scale and numerical results show a good agreement with the experimental evidences.",0.1176470588],["a corpus based on twitter has been used to analyse language variation. we use","Dialectometric analysis of language variation in Twitter","summarize: In the last few years, microblogging platforms such as Twitter have given rise to a deluge of textual data that can be used for the analysis of informal communication between millions of individuals. In this work, we propose an information-theoretic approach to geographic language variation using a corpus based on Twitter. We test our models with tens of concepts and their associated keywords detected in Spanish tweets geolocated in Spain. We employ dialectometric measures to quantify the linguistic distance on the lexical level between cells created in a uniform grid over the map. This can be done for a single concept or in the general case taking into account an average of the considered variants. The latter permits an analysis of the dialects that naturally emerge from the data. Interestingly, our results reveal the existence of two dialect macrovarieties. The first group includes a region-specific speech spoken in small towns and rural areas whereas the second cluster encompasses cities that tend to use a more uniform variety. Since the results obtained with the two different metrics qualitatively agree, our work suggests that social media corpora can be efficiently used for dialectometric analyses.",0.4583333333],["quantaloid is a small quantaloid.","Lax distributive laws for topology, II","summarize: For a small quantaloid ",0.2046826883],["superconductor electronics was proposed as a sub-terahertz clock frequency alternative","Superconductor Digital Electronics: Scalability and Energy Efficiency Issues","summarize: Superconductor digital electronics using Josephson junctions as ultrafast switches and magnetic-flux encoding of information was proposed over 30 years ago as a sub-terahertz clock frequency alternative to semiconductor electronics based on complementary metal-oxide-semiconductor transistors. Recently, interest in developing superconductor electronics has been renewed due to a search for energy saving solutions in applications related to high-performance computing. The current state of superconductor electronics and fabrication processes are reviewed in order to evaluate whether this electronics is scalable to a very large scale integration required to achieve computation complexities comparable to CMOS processors. A fully planarized process at MIT Lincoln Laboratory, perhaps the most advanced process developed so far for superconductor electronics, is used as an example. The process has nine superconducting layers: eight Nb wiring layers with the minimum feature size of 350 nm, and a thin superconducting layer for making compact high-kinetic-inductance bias inductors. All circuit layers are fully planarized using chemical mechanical planarization of SiO2 interlayer dielectric. The physical limitations imposed on the circuit density by Josephson junctions, circuit inductors, shunt and bias resistors, etc., are discussed. Energy dissipation in superconducting circuits is also reviewed in order to estimate whether this technology, which requires cryogenic refrigeration, can be energy efficient. Fabrication process development required for increasing the density of superconductor digital circuits by a factor of ten and achieving densities above 10^7 Josephson junctions per cm^2 is described.",0.25],["the conductivity tensor is calculated based on a linear response theory.","Optical responses induced by spin gauge field at the second order","summarize: Optical responses of ferromagnetic materials with spin gauge field that drives intrinsic spin curren is theoretically studied. The conductivity tensor is calculated based on a linear response theory to the applied electric field taking account of the non-linear effects of the spin gauge field to the second order. We consider the case where the spin gauge field is uniform, realized for spiral magnetization structure or uniform spin-orbit interaction. The spin gauge field, or an intrinsic spin current, turns out to give rise to anisotropic optical responses, which is expected to be useful to experimental detection of magnetization structures.",0.3888888889],["incorporating ImageNet did not help much. a similar approach was proposed to combine scene","Scene recognition with CNNs: objects, scales and dataset bias","summarize: Since scenes are composed in part of objects, accurate recognition of scenes requires knowledge about both scenes and objects. In this paper we address two related problems: 1) scale induced dataset bias in multi-scale convolutional neural network architectures, and 2) how to combine effectively scene-centric and object-centric knowledge in CNNs. An earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help much. Here we propose an alternative method taking the scale into account, resulting in significant recognition gains. By analyzing the response of ImageNet-CNNs and Places-CNNs at different scales we find that both operate in different scale ranges, so using the same network for all the scales induces dataset bias resulting in limited performance. Thus, adapting the feature extractor to each particular scale is crucial to improve recognition, since the objects in the scenes have their specific range of scales. Experimental results show that the recognition accuracy highly depends on the scale, and that simple yet carefully chosen multi-scale combinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art recognition accuracy in SUN397 up to 66.26% .",0.25],["research studies on the blockchain have taken place in recent years. the aim of this research is","A Critical Review of Concepts, Benefits, and Pitfalls of Blockchain Technology Using Concept Map","summarize: Blockchain is relatively a new area of research. However, a surge of research studies on the blockchain has taken place in recent years. These research studies have mostly focused on designing and developing conceptual frameworks to build more reliable, transparent and efficient digital systems. While blockchain brings a wide variety of benefits, it also imposes certain challenges. Therefore, the objective of this research is to understand the properties of blockchain, its current uses, observed benefits and pitfalls to provide a balanced understanding of blockchain. A systematic literature review approach was adopted in this paper in order to attain the objective. A total of 51 articles were selected and reviewed. As outcomes, this research provides a summary of the state-of-the-art research studies conducted in the area of blockchain. Furthermore, we develop a set of concept maps aiming to provide in-depth knowledge on blockchain technology for its efficient and effective usage in the development of future technological solutions.",0.1739130435],["the duration of the sounds used in the diagnosis affects the speed of the diagnosis. the","Effect of Window Size for Detection of Abnormalities in Respiratory Sounds","summarize: The recording of respiratory sounds was of significant benefit in the diagnosis of abnormalities in respiratory sounds. The duration of the sounds used in the diagnosis affects the speed of the diagnosis. In this study, the effect of window size on diagnosis of abnormalities in respiratory sounds and the most efficient recording time for diagnosis were analyzed. First, window size was applied to each sound in the data set consisting of normal and abnormal breathing sounds, 0.5 second and from 1 to 20 seconds Increased by 1 second. Then, the data applied to window size was inferred feature extraction with Mel Frequency Cepstral Coefficient and the performance of each window was calculated using the leave one-out classifier and the k-nearest neighbor algorithm. As a result, it was determined that the data was significant with an average performance of 92.06% in the records between 2 and 10 seconds.",0.1],["a pedestrian dynamics modeling approach was proposed by the authors. it modeled quantitatively the","High-statistics modeling of complex pedestrian avoidance scenarios","summarize: Quantitatively modeling the trajectories and behavior of pedestrians walking in crowds is an outstanding fundamental challenge deeply connected with the physics of flowing active matter, from a scientific point of view, and having societal applications entailing individual safety and comfort, from an application perspective. In this contribution, we review a pedestrian dynamics modeling approach, previously proposed by the authors, aimed at reproducing some of the statistical features of pedestrian motion. Comparing with high-statistics pedestrian dynamics measurements collected in real-life conditions , we modeled quantitatively the statistical features of the undisturbed motion as well as the avoidance dynamics triggered by a pedestrian incoming in the opposite direction. This was accomplished through Langevin equations with potentials including multiple preferred velocity states and preferred paths. In this chapter we review this model, discussing some of its limitations, in view of its extension toward a more complex case: the avoidance dynamics of a single pedestrian walking through a crowd that is moving in the opposite direction. We analyze some of the challenges connected to this case and present extensions to the model capable of reproducing some features of the motion.",0.28],["the associated Legendre and Ferrers functions of the first and second kind are based on multi","Multi-integral representations for associated Legendre and Ferrers functions","summarize: For the associated Legendre and Ferrers functions of the first and second kind, we obtain new multi-derivative and multi-integral representation formulas. The multi-integral representation formulas that we derive for these functions generalize some classical multi-integration formulas. As a result of the determination of these formulae, we compute some interesting special values and integral representations for certain particular combinations of the degree and order including the case where there is symmetry and antisymmetry for the degree and order parameters. As a consequence of our analysis, we obtain some new results for the associated Legendre function of the second kind including parameter values for which this function is identically zero.",0.3636363636],["proposed scheme can operate in three different modes. we propose a new offloading scheme","On the Performance and Optimization for MEC Networks Using Uplink NOMA","summarize: In this paper, we investigate a non-orthogonal multiple access based mobile edge computing network, in which two users may partially offload their respective tasks to a single MEC server through uplink NOMA. We propose a new offloading scheme that can operate in three different modes, namely the partial computation offloading, the complete local computation, and the complete offloading. We further derive a closed-form expression of the successful computation probability for the proposed scheme. As part of the proposed offloading scheme, we formulate a problem to maximize the successful computation probability by jointly optimizing the time for offloading, the power allocation of the two users and the offloading ratios which decide how many tasks should be offloaded to the MEC server. We obtain the optimal solutions in the closed forms. Simulation results show that our proposed scheme can achieve the highest successful computation probability than the existing schemes.",0.2380952381],["the three-dimensional Pauli equation for a spin-1\/2 particle is derived from the","On the Three-Dimensional Pauli Equation in Noncommutative Phase-Space","summarize: In this paper, we obtained the three-dimensional Pauli equation for a spin-1\/2 particle in the presence of an electromagnetic field in noncommutative phase-space, as well the corresponding deformed continuity equation, where the cases of a constant and non-constant magnetic field are considered. Due to the absence of the current magnetization term in the deformed continuity equation as expected, we had to extract it from the noncommutative Pauli equation itself without modifying the continuity equation. It is shown that the non-constant magnetic field lifts the order of the noncommutativity parameter in both the Pauli equation and the corresponding continuity equation. However, we successfully examined the effect of the noncommutativity on the current density and the magnetization current. By using a classical treatment, we derived the semi-classical noncommutative partition function of the three-dimensional Pauli system of the one-particle and N-particle systems. Then, we employed it for calculating the corresponding Helmholtz free energy followed by the magnetization and the magnetic susceptibility of electrons in both commutative and noncommutative phase-spaces. Knowing that with both the three-dimensional Bopp-Shift transformation and the Moyal-Weyl product, we introduced the phase-space noncommutativity in the problems in question.",0.4444444444],["social media presence of peerJ articles is high. about 68.18% of the papers","Social Media Attention Increases Article Visits: An Investigation on Article-Level Referral Data of PeerJ","summarize: In order to better understand the effect of social media in the dissemination of scholarly articles, employing the daily updated referral data of 110 PeerJ articles collected over a period of 345 days, we analyze the relationship between social media attention and article visitors directed by social media. Our results show that social media presence of PeerJ articles is high. About 68.18% of the papers receive at least one tweet from Twitter accounts other than @PeerJ, the official account of the journal. Social media attention increases the dissemination of scholarly articles. Altmetrics could not only act as the complement of traditional citation measures but also play an important role in increasing the article downloads and promoting the impacts of scholarly articles. There also exists a significant correlation among the online attention from different social media platforms. Articles with more Facebook shares tend to get more tweets. The temporal trends show that social attention comes immediately following publication but does not last long, so do the social media directed article views.",0.1851922157],["the original minimal path model computes the globally minimal geodesic by solving an Eikonal","Global Minimum for a Finsler Elastica Minimal Path Approach","summarize: In this paper, we propose a novel curvature-penalized minimal path model via an orientation-lifted Finsler metric and the Euler elastica curve. The original minimal path model computes the globally minimal geodesic by solving an Eikonal partial differential equation . Essentially, this first-order model is unable to penalize curvature which is related to the path rigidity property in the classical active contour models. To solve this problem, we present an Eikonal PDE-based Finsler elastica minimal path approach to address the curvature-penalized geodesic energy minimization problem. We were successful at adding the curvature penalization to the classical geodesic energy. The basic idea of this work is to interpret the Euler elastica bending energy via a novel Finsler elastica metric that embeds a curvature penalty. This metric is non-Riemannian, anisotropic and asymmetric, and is defined over an orientation-lifted space by adding to the image domain the orientation as an extra space dimension. Based on this orientation lifting, the proposed minimal path model can benefit from both the curvature and orientation of the paths. Thanks to the fast marching method, the global minimum of the curvature-penalized geodesic energy can be computed efficiently. We introduce two anisotropic image data-driven speed functions that are computed by steerable filters. Based on these orientation-dependent speed functions, we can apply the proposed Finsler elastica minimal path model to the applications of closed contour detection, perceptual grouping and tubular structure extraction. Numerical experiments on both synthetic and real images show that these applications of the proposed model indeed obtain promising results.",0.0],["we obtain a condition under which these two forms of sensitivity are equivalent. we also","Exploring F-Sensitivity for Non-Autonomous Systems","summarize: We study some stronger forms of sensitivity, namely, F-sensitivity and weakly F-sensitivity for non-autonomous discrete dynamical systems. We obtain a condition under which these two forms of sensitivity are equivalent. We also justify the difference between F-sensitivity and some other stronger forms of sensitivity through examples. We explore the relation between the F-sensitivity of the non-autonomous system and autonomous system , where fn is a sequence of continuous functions converging uniformly to f. We also study the F-sensitivity of a non-autonomous system , generated by a finite family of maps F = f1, f2, : : : , fk and give an example showing that such non-autonomous systems can be F-sensitive, even when none of the maps in the family F is F-sensitive.",0.0],["the affinity matrix is constructed by shallow linear embedding methods. we pretrain a","Generative approach to unsupervised deep local learning","summarize: Most existing feature learning methods optimize inflexible handcrafted features and the affinity matrix is constructed by shallow linear embedding methods. Different from these conventional methods, we pretrain a generative neural network by stacking convolutional autoencoders to learn the latent data representation and then construct an affinity graph with them as a prior. Based on the pretrained model and the constructed graph, we add a self-expressive layer to complete the generative model and then fine-tune it with a new loss function, including the reconstruction loss and a deliberately defined locality-preserving loss. The locality-preserving loss designed by the constructed affinity graph serves as prior to preserve the local structure during the fine-tuning stage, which in turn improves the quality of feature representation effectively. Furthermore, the self-expressive layer between the encoder and decoder is based on the assumption that each latent feature is a linear combination of other latent features, so the weighted combination coefficients of the self-expressive layer are used to construct a new refined affinity graph for representing the data structure. We conduct experiments on four datasets to demonstrate the superiority of the representation ability of our proposed model over the state-of-the-art methods.",0.3333333333],["transmission line theory is used to shed new light on the topology of a wired network","On the Exploitation of Admittance Measurements for Wired Network Topology Derivation","summarize: The knowledge of the topology of a wired network is often of fundamental importance. For instance, in the context of Power Line Communications networks it is helpful to implement data routing strategies, while in power distribution networks and Smart Micro Grids it is required for grid monitoring and for power flow management. In this paper, we use the transmission line theory to shed new light and to show how the topological properties of a wired network can be found exploiting admittance measurements at the nodes. An analytic proof is reported to show that the derivation of the topology can be done in complex networks under certain assumptions. We also analyze the effect of the network background noise on admittance measurements. In this respect, we propose a topology derivation algorithm that works in the presence of noise. We finally analyze the performance of the algorithm using values that are typical of power line distribution networks.",0.2857142857],["survey reviews major aspects related to consistency issues in cloud data storage systems. a common problem","A Brief Survey on Replica Consistency in Cloud Environments","summarize: Cloud computing is a general term that involves delivering hosted services over the Internet. With the accelerated growth of the volume of data used by applications, many organizations have moved their data into cloud servers to provide scalable, reliable and highly available services. A particularly challenging issue that arises in the context of cloud storage systems with geographically-distributed data replication is how to reach a consistent state for all replicas. This survey reviews major aspects related to consistency issues in cloud data storage systems, categorizing recently proposed methods into three categories: fixed consistency methods, configurable consistency methods and consistency monitoring methods.",0.0869565217],["data warehousing and on-line analytical processing are vital fundamentals of resolution hold","Reduce The Wastage of Data During Movement in Data Warehouse","summarize: In this research paper so as to handle Data in warehousing as well as reduce the wastage of data and provide a better results which takes more and more turn into a focal point of the data source business. Data warehousing and on-line analytical processing are vital fundamentals of resolution hold, which has more and more become a focal point of the database manufacturing. Lots of marketable yield and services be at the present accessible, and the entire primary database management organization vendor nowadays have contributions in the area assessment hold up spaces some quite dissimilar necessities on record technology compare to conventional on-line transaction giving out application. This article gives a general idea of data warehousing and OLAP technologies, with the highlighting on top of their latest necessities. So tools which is used for extract, clean-up and load information into back end of a information warehouse; multidimensional data model usual of OLAP; front end client tools for querying and data analysis; server extension for proficient query processing; and tools for data managing and for administration the warehouse. In adding to survey the circumstances of the art, this article also identify a number of capable research issue, a few which are interrelated to data wastage troubles. In this paper use some new techniques to reduce the wastage of data, provide better results. In this paper take some values, put in anova table and give results through graphs which shows performance.",0.1538461538],["automatic CS recognition can help communication between the deaf people and others. re","Re-synchronization using the Hand Preceding Model for Multi-modal Fusion in Automatic Continuous Cued Speech Recognition","summarize: Cued Speech is an augmented lip reading complemented by hand coding, and it is very helpful to the deaf people. Automatic CS recognition can help communications between the deaf people and others. Due to the asynchronous nature of lips and hand movements, fusion of them in automatic CS recognition is a challenging problem. In this work, we propose a novel re-synchronization procedure for multi-modal fusion, which aligns the hand features with lips feature. It is realized by delaying hand position and hand shape with their optimal hand preceding time which is derived by investigating the temporal organizations of hand position and hand shape movements in CS. This re-synchronization procedure is incorporated into a practical continuous CS recognition system that combines convolutional neural network with multi-stream hidden markov model . A significant improvement of about 4.6\\% has been achieved retaining 76.6\\% CS phoneme recognition correctness compared with the state-of-the-art architecture , which did not take into account the asynchrony of multi-modal fusion in CS. To our knowledge, this is the first work to tackle the asynchronous multi-modal fusion in the automatic continuous CS recognition.",0.1714807838],["a graph is based on a graph.","Long Cycles and Spanning Subgraphs of Locally Maximal 1-planar Graphs","summarize: A graph is ",0.4981592793],["affine connections on manifolds are determined by the 1-connection form and the fundamental","Flat Affine Manifolds And Their Transformations","summarize: We give a characterization of flat affine connections on manifolds by means of a natural affine representation of the universal covering of the Lie group of diffeomorphisms preserving the connection. From the infinitesimal point of view, this representation is determined by the 1-connection form and the fundamental form of the bundle of linear frames of the manifold. We show that the group of affine transformations of a real flat affine ",0.1052631579],["theorists are often told to express things in the observational plane. observ","Measuring Space-Time Geometry over the Ages","summarize: Theorists are often told to express things in the observational plane. One can do this for space-time geometry, considering visual observations of matter in our universe by a single observer over time, with no assumptions about isometries, initial conditions, nor any particular relation between matter and geometry, such as Einstein's equations. Using observables as coordinates naturally leads to a parametrization of space-time geometry in terms of other observables, which in turn prescribes an observational program to measure the geometry. Under the assumption of vorticity-free matter flow we describe this observational program, which includes measurements of gravitational lensing, proper motion, and redshift drift. Only 15% of the curvature information can be extracted without long time baseline observations, and this increases to 35% with observations that will take decades. The rest would likely require centuries of observations. The formalism developed is exact, non-perturbative, and more general than the usual cosmological analysis.",0.125],["deep reinforcement learning has been successfully applied to a variety of tasks. deep RL has","Cross-Modal Contrastive Learning of Representations for Navigation using Lightweight, Low-Cost Millimeter Wave Radar for Adverse Environmental Conditions","summarize: Deep reinforcement learning , where the agent learns from mistakes, has been successfully applied to a variety of tasks. With the aim of learning collision-free policies for unmanned vehicles, deep RL has been used for training with various types of data, such as colored images, depth images, and LiDAR point clouds, without the use of classic map--localize--plan approaches. However, existing methods are limited by their reliance on cameras and LiDAR devices, which have degraded sensing under adverse environmental conditions . In response, we propose the use of single-chip millimeter-wave radar, which is lightweight and inexpensive, for learning-based autonomous navigation. However, because mmWave radar signals are often noisy and sparse, we propose a cross-modal contrastive learning for representation method that maximizes the agreement between mmWave radar data and LiDAR data in the training stage. We evaluated our method in real-world robot compared with 1) a method with two separate networks using cross-modal generative reconstruction and an RL policy and 2) a baseline RL policy without cross-modal representation. Our proposed end-to-end deep RL policy with contrastive learning successfully navigated the robot through smoke-filled maze environments and achieved better performance compared with generative reconstruction methods, in which noisy artifact walls or obstacles were produced. All pretrained models and hardware settings are open access for reproducing this study and can be obtained at https:\/\/arg-nctu.github.io\/projects\/deeprl-mmWave.html",0.2800554621],["magnetic fluctuations can often have strong electromagnetic fluctuations. correlation scale is smaller than electron Larmor","Diffusion and Radiation in Magnetized Collisionless Plasmas with High-Frequency Small-Scale Turbulence","summarize: Magnetized high-energy-density plasmas can often have strong electromagnetic fluctuations whose correlation scale is smaller than the electron Larmor radius. Radiation from the electrons in such plasmas, which markedly differs from both synchrotron and cyclotron radiation, and their energy and pitch-angle diffusion are tightly related. In this paper, we present a comprehensive theoretical and numerical study of the particles' transport in both cold, small-scale Langmuir and Whistler-mode turbulence and its relation to the spectra of radiation simultaneously produced by these particles. We emphasize that this relation is a superb diagnostic tool of laboratory, astrophysical, interplanetary, and solar plasmas with a mean magnetic field and strong small-scale turbulence.",0.0555555556],["the gamma-ray emission from the region HESS J1837-069","Diffuse gamma-ray emission from the vicinity of young massive star cluster RSGC 1","summarize: We report the Fermi Large Area Telescope detection of the gamma-ray emission towards the young massive star cluster RSGC 1. Using the latest source catalog and diffuse background models, we found that the diffuse gamma-ray emission in this region can be resolved into three different components. The GeV gamma-ray emission from the region HESS J1837-069 has a photon index of 1.83 ",0.2676307143],["we give an introduction to the Mathematica packages masterPVA and masterPVAmulti","MasterPVA and WAlg: Mathematica packages for Poisson vertex algebras and classical affine ","summarize: We give an introduction to the Mathematica packages MasterPVA and MasterPVAmulti used to compute lambda-brackets in Poisson vertex algebras, which play an important role in the theory of infinite-dimensional Hamiltonian systems. As an application, we give an introduction to the Mathematica package WAlg aimed to compute the lambda-brackets among the generators of classical affine W-algebras. The use of these packages is shown by providing some explicit examples.",0.3573226792],["the gemini planet imager is a next-generation high-contrast imager","Large collaboration in observational astronomy: the Gemini Planet Imager exoplanet survey case","summarize: The Gemini Planet Imager is a next-generation high-contrast imager built for the Gemini Observatory. The GPI exoplanet survey consortium is made up of 102 researchers from 28 institutions in North and South America and Europe. In November 2014, we launched a search for young Jovian planets and debris disks. In this paper, we discuss how we have coordinated the work done by this large team to improve the technical and scientific productivity of the campaign, and describe lessons we have learned that could be useful for future instrumentation-based astronomical surveys. The success of GPIES lies mostly on its decentralized structure, clear definition of policies that are signed by each member, and the heavy use of modern tools for communicating, exchanging information, and processing data.",0.3582656553],["we propose a natural greedy algorithm for response-dependent costs. we bound the app","Submodular Learning and Covering with Response-Dependent Costs","summarize: We consider interactive learning and covering problems, in a setting where actions may incur different costs, depending on the response to the action. We propose a natural greedy algorithm for response-dependent costs. We bound the approximation factor of this greedy algorithm in active learning settings as well as in the general setting. We show that a different property of the cost function controls the approximation factor in each of these scenarios. We further show that in both settings, the approximation factor of this greedy algorithm is near-optimal among all greedy algorithms. Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting.",0.1578947368],["a previously undiscovered extension of the radio continuum emission was found. the population of","Probing the magnetic field of the nearby galaxy pair Arp 269","summarize: We present a multiwavelength radio study of the nearby galaxy pair Arp 269 . High sensitivity to extended structures gained by using the merged interferometric and single- dish maps allowed us to reveal a previously undiscovered extension of the radio continuum emission. Its direction is significantly different from that of the neutral gas tail, suggesting that different physical processes might be involved in their creation. The population of radio- emitting electrons is generally young, signifying an ongoing, vigorous star formation -- this claim is supported by strong magnetic fields , similar to the ones found in much larger spiral galaxies. From the study of the spectral energy distribution, we conclude that the electron population in the intergalactic bridge between member galaxies originates from the disc areas, and therefore its age reflects the time-scale of the interaction. We have also discovered an angularly near compact steep source -- which is a member of a different galaxy pair -- at a redshift of approximately 0.125.",0.4761904762],["we factor the observed video into a matrix product between the unknown hidden scene video and an unknown","Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization","summarize: We recover a video of the motion taking place in a hidden scene by observing changes in indirect illumination in a nearby uncalibrated visible region. We solve this problem by factoring the observed video into a matrix product between the unknown hidden scene video and an unknown light transport matrix. This task is extremely ill-posed, as any non-negative factorization will satisfy the data. Inspired by recent work on the Deep Image Prior, we parameterize the factor matrices using randomly initialized convolutional neural networks trained in a one-off manner, and show that this results in decompositions that reflect the true motion in the hidden scene.",0.2],["a corpus based on twitter has been used to analyse language variation. we use","Dialectometric analysis of language variation in Twitter","summarize: In the last few years, microblogging platforms such as Twitter have given rise to a deluge of textual data that can be used for the analysis of informal communication between millions of individuals. In this work, we propose an information-theoretic approach to geographic language variation using a corpus based on Twitter. We test our models with tens of concepts and their associated keywords detected in Spanish tweets geolocated in Spain. We employ dialectometric measures to quantify the linguistic distance on the lexical level between cells created in a uniform grid over the map. This can be done for a single concept or in the general case taking into account an average of the considered variants. The latter permits an analysis of the dialects that naturally emerge from the data. Interestingly, our results reveal the existence of two dialect macrovarieties. The first group includes a region-specific speech spoken in small towns and rural areas whereas the second cluster encompasses cities that tend to use a more uniform variety. Since the results obtained with the two different metrics qualitatively agree, our work suggests that social media corpora can be efficiently used for dialectometric analyses.",0.4583333333],["the paper defends three propositions that arise in the context of AI alignment. it","Artificial Intelligence, Values and Alignment","summarize: This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify 'true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.",0.0],["in this paper we introduce a transformation technique. the method can prove existence and uniqueness","A Numerical Method for SDEs with Discontinuous Drift","summarize: In this paper we introduce a transformation technique, which can on the one hand be used to prove existence and uniqueness for a class of SDEs with discontinuous drift coefficient. One the other hand we present a numerical method based on transforming the Euler-Maruyama scheme for such a class of SDEs. We prove convergence of order ",0.0952380952],["we develop a version of a version of the e-book.","","summarize: We develop a version of ",0.0],["the current UAV navigation schemes are unable to capture the UAV motion and select the best","Deep Reinforcement Learning for UAV Navigation Through Massive MIMO Technique","summarize: Unmanned aerial vehicles technique has been recognized as a promising solution in future wireless connectivity from the sky, and UAV navigation is one of the most significant open research problems, which has attracted wide interest in the research community. However, the current UAV navigation schemes are unable to capture the UAV motion and select the best UAV-ground links in real time, and these weaknesses overwhelm the UAV navigation performance. To tackle these fundamental limitations, in this paper, we merge the state-of-theart deep reinforcement learning with the UAV navigation through massive multiple-input-multiple-output technique. To be specific, we carefully design a deep Q-network for optimizing the UAV navigation by selecting the optimal policy, and then we propose a learning mechanism for processing the DQN. The DQN is trained so that the agent is capable of making decisions based on the received signal strengths for navigating theUAVs with the aid of the powerful Q-learning. Simulation results are provided to corroborate the superiority of the proposed schemes in terms of the coverage and convergence compared with those of the other schemes.",0.0833333333],["a symmetric Kohn-Sham potential can be used in density functional theory.","Degenerate Density Perturbation Theory","summarize: Fractional occupation numbers can be used in density functional theory to create a symmetric Kohn-Sham potential, resulting in orbitals with degenerate eigenvalues. We develop the corresponding perturbation theory and apply it to a system of ",0.1333333333],["ROBEL introduces two robots, each aimed to accelerate reinforcement learning research in different","ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots","summarize: ROBEL is an open-source platform of cost-effective robots designed for reinforcement learning in the real world. ROBEL introduces two robots, each aimed to accelerate reinforcement learning research in different task domains: D'Claw is a three-fingered hand robot that facilitates learning dexterous manipulation tasks, and D'Kitty is a four-legged robot that facilitates learning agile legged locomotion tasks. These low-cost, modular robots are easy to maintain and are robust enough to sustain on-hardware reinforcement learning from scratch with over 14000 training hours registered on them to date. To leverage this platform, we propose an extensible set of continuous control benchmark tasks for each robot. These tasks feature dense and sparse task objectives, and additionally introduce score metrics as hardware-safety. We provide benchmark scores on an initial set of tasks using a variety of learning-based methods. Furthermore, we show that these results can be replicated across copies of the robots located in different institutions. Code, documentation, design files, detailed assembly instructions, final policies, baseline details, task videos, and all supplementary materials required to reproduce the results are available at www.roboticsbenchmarks.org.",0.125],["a multigrid approach is used to propagate and invert the wave. the","Multigrid-based inversion for volumetric radar imaging with asteroid interior reconstruction as a potential application","summarize: This study concentrates on advancing mathematical and computational methodology for radar tomography imaging in which the unknown volumetric velocity distribution of a wave within a bounded domain is to be reconstructed. Our goal is to enable effective simulation and inversion of a large amount of full-wave data within a realistic 2D or 3D geometry. For propagating and inverting the wave, we present a rigorous multigrid-based forward approach which utilizes the finite-difference time-domain method and a nested finite element grid structure. Based on the multigrid approach, we introduce and validate a multiresolution algorithm which allows regularization of the unknown distribution through a coarse-to-fine inversion scheme. In this approach, sparse signals can be effectively inverted, as the coarse fluctuations are reconstructed before the finer ones. Furthermore, the number of nonzero entries in the system matrix can be compressed and thus the inversion procedure can be speeded up. As a test scenario we investigate satellite-based asteroid interior reconstruction. We use both full-wave and projected wave data and estimate the accuracy of the inversion under different error sources: noise and positioning inaccuracies. The results suggest that the present full-wave inversion approach allows recovering the interior with a single satellite recording backscattering data. It seems that robust results can be achieved, when the peak-to-peak signal-to-noise ratio is above 10 dB. Furthermore, it seems that reconstructing the deep interior can be enhanced if two satellites can be utilized in the measurements.",0.2962686037],["we consider application of a standard generic divisor doubling for construction of new auto transformations","Backlund transformations and divisor doubling","summarize: In classical mechanics well-known cryptographic algorithms and protocols can be very useful for construction canonical transformations preserving form of Hamiltonians. We consider application of a standard generic divisor doubling for construction of new auto B\\cklund transformations for the Lagrange top and H\\'non-Heiles system separable in parabolic coordinates.",0.32],["Object tracking systems play important roles in tracking objects. problems arise from the difficulties in creating","Tracking Systems as Thinging Machine: A Case Study of a Service Company","summarize: Object tracking systems play important roles in tracking moving objects and overcoming problems such as safety, security and other location-related applications. Problems arise from the difficulties in creating a well-defined and understandable description of tracking systems. Nowadays, describing such processes results in fragmental representation that most of the time leads to difficulties creating documentation. Additionally, once learned by assigned personnel, repeated tasks result in them continuing on autopilot in a way that often degrades their effectiveness. This paper proposes the modeling of tracking systems in terms of a new diagrammatic methodology to produce engineering-like schemata. The resultant diagrams can be used in documentation, explanation, communication, education and control.",0.3076923077],["gyrofluid model for kinetic Alfv'en waves in","Inverse cascade and magnetic vortices in kinetic Alfv\\'en-wave turbulence","summarize: A Hamiltonian two-field gyrofluid model for kinetic Alfv\\'en waves in a magnetized electron-proton plasma, retaining ion finite-Larmor-radius corrections and parallel magnetic field fluctuations, is used to study the inverse cascades that develop when turbulence is randomly driven at sub-ion scales. In the directions perpendicular to the ambient field, the dynamics of the cascade turns out to be nonlocal and the ratio ",0.2818039849],["the main problem is that self-awareness cannot be observed from an outside perspective.","Will we ever have Conscious Machines?","summarize: The question of whether artificial beings or machines could become self-aware or consciousness has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of whether something is really self-aware or merely a clever program that pretends to do so cannot be answered without access to accurate knowledge about the mechanism's inner workings. We review the current state-of-the-art regarding these developments and investigate common machine learning approaches with respect to their potential ability to become self-aware. We realise that many important algorithmic steps towards machines with a core consciousness have already been devised. For human-level intelligence, however, many additional techniques have to be discovered.",0.0],["effective applications of vehicular ad hoc networks in traffic signal control require new methods for","Detection of malicious data in vehicular ad-hoc networks for traffic signal control applications","summarize: Effective applications of vehicular ad hoc networks in traffic signal control require new methods for detection of malicious data. Injection of malicious data can result in significantly decreased performance of such applications, increased vehicle delays, fuel consumption, congestion, or even safety threats. This paper introduces a method, which combines a model of expected driver behaviour with position verification in order to detect the malicious data injected by vehicle nodes that perform Sybil attacks. Effectiveness of this approach was demonstrated in simulation experiments for a decentralized self-organizing system that controls the traffic signals at multiple intersections in an urban road network. Experimental results show that the proposed method is useful for mitigating the negative impact of malicious data on the performance of traffic signal control.",0.7333333333],["these lecture notes are based on the hand-written notes which I prepared for the cosm","Lecture Notes in Cosmology","summarize: These lecture notes are based on the hand-written notes which I prepared for the cosmology course taught to graduate students of PPGFis and PPGCosmo at the Federal University of Esp\\'irito Santo , starting from 2014. This course covers topics ranging from the evidence of the expanding universe to Cosmic Microwave Background anisotropies. They can be found also on my personal webpage http:\/\/ofp.cosmo-ufes.org\/ and shall be published by Springer in 2018.",0.0],["turbulence is a paradigm for far-from-equilibrium systems without","Potential landscape and flux field theory for turbulence and nonequilibrium fluid systems","summarize: Turbulence is a paradigm for far-from-equilibrium systems without time reversal symmetry. To capture the nonequilibrium irreversible nature of turbulence and investigate its implications, we develop a potential landscape and flux field theory for turbulent flow and more general nonequilibrium fluid systems governed by stochastic Navier-Stokes equations. We find that equilibrium fluid systems with time reversibility are characterized by a detailed balance constraint that quantifies the detailed balance condition. In nonequilibrium fluid systems with nonequilibrium steady states, detailed balance breaking leads directly to a pair of interconnected consequences, namely, the non-Gaussian potential landscape and the irreversible probability flux, forming a 'nonequilibrium trinity'. The nonequilibrium trinity characterizes the nonequilibrium irreversible essence of fluid systems with intrinsic time irreversibility and is manifested in various aspects of these systems. The nonequilibrium stochastic dynamics of fluid systems including turbulence with detailed balance breaking is shown to be driven by both the non-Gaussian potential landscape gradient and the irreversible probability flux, together with the reversible convective force and the stochastic stirring force. We reveal an underlying connection of the energy flux essential for turbulence energy cascade to the irreversible probability flux and the non-Gaussian potential landscape generated by detailed balance breaking. Using the energy flux as a center of connection, we demonstrate that the four-fifths law in fully developed turbulence is a consequence and reflection of the nonequilibrium trinity. We also show how the nonequilibrium trinity can affect the scaling laws in turbulence.",0.3859740562],["the original lemma states that concentration of the Laplace spectrum implies combinatorial expansion in","Mixing in high-dimensional expanders","summarize: We prove a generalization of the Expander Mixing Lemma for arbitrary simplicial complexes. The original lemma states that concentration of the Laplace spectrum of a graph implies combinatorial expansion . Recently, an analogue of this Lemma was proved for simplicial complexes of arbitrary dimension, provided that the skeleton of the complex is complete. More precisely, it was shown that a concentrated spectrum of the simplicial Hodge Laplacian implies a similar type of expansion as in graphs. In this paper we remove the assumption of a complete skeleton, showing that concentration of the Laplace spectra in all dimensions implies combinatorial expansion in any complex. As applications we show that spectral concentration implies Gromov's geometric overlap property, and can be used to bound the chromatic number of a complex.",0.1111111111],["in the last few years, telecom and computer networks have witnessed new concepts and technologies through Network Function","Enhancing Middleware-based IoT Applications through Run-Time Pluggable QoS Management Mechanisms. Application to a oneM2M compliant IoT Middleware","summarize: In the recent years, telecom and computer networks have witnessed new concepts and technologies through Network Function Virtualization and Software-Defined Networking . SDN, which allows applications to have a control over the network, and NFV, which allows deploying network functions in virtualized environments, are two paradigms that are increasingly used for the Internet of Things . This Internet brings the promise to interconnect billions of devices in the next few years rises several scientific challenges in particular those of the satisfaction of the quality of service required by the IoT applications. In order to address this problem, we have identified two bottlenecks with respect to the QoS: the traversed networks and the intermediate entities that allows the application to interact with the IoT devices. In this paper, we first present an innovative vision of a network function with respect to their deployment and runtime environment. Then, we describe our general approach of a solution that consists in the dynamic, autonomous, and seamless deployment of QoS management mechanisms. We also describe the requirements for the implementation of such approach. Finally, we present a redirection mechanism, implemented as a network function, allowing the seamless control of the data path of a given middleware traffic. This mechanism is assessed through a use case related to vehicular transportation.",0.1],["decomposition of a non-empty simple graph.","Coloring decompositions of complete geometric graphs","summarize: A decomposition of a non-empty simple graph ",0.5714285714],["a polyhedral active set algorithm is developed for solving a nonlinear optimization","An Active Set Algorithm for Nonlinear Optimization with Polyhedral Constraints","summarize: A polyhedral active set algorithm PASA is developed for solving a nonlinear optimization problem whose feasible set is a polyhedron. Phase one of the algorithm is the gradient projection method, while phase two is any algorithm for solving a linearly constrained optimization problem. Rules are provided for branching between the two phases. Global convergence to a stationary point is established, while asymptotically PASA performs only phase two when either a nondegeneracy assumption holds, or the active constraints are linearly independent and a strong second-order sufficient optimality condition holds.",0.375],["chapter presents foundations of computing paradigms for realizing emerging IoT applications.","Internet of Things and New Computing Paradigms","summarize: The chapter presents foundations of computing paradigms for realizing emerging IoT applications, especially fog and edge computing, their background, characteristics, architectures and open challenges.",0.0909090909],["static analysis aims to find spreadsheet formula errors. it uses information-theoretic approach","ExceLint: Automatically Finding Spreadsheet Formula Errors","summarize: Spreadsheets are one of the most widely used programming environments, and are widely deployed in domains like finance where errors can have catastrophic consequences. We present a static analysis specifically designed to find spreadsheet formula errors. Our analysis directly leverages the rectangular character of spreadsheets. It uses an information-theoretic approach to identify formulas that are especially surprising disruptions to nearby rectangular regions. We present ExceLint, an implementation of our static analysis for Microsoft Excel. We demonstrate that ExceLint is fast and effective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5 seconds per spreadsheet, and it significantly outperforms the state of the art analysis.",0.0833333333],["chiral CFTs are generated by quantum fields depending on one light-ray coordinate","Operator algebras and vertex operator algebras","summarize: In two-dimensional conformal field theory the building blocks are given by chiral CFTs, i.e.~CFTs on the unit circle . They are generated by quantum fields depending on one light-ray coordinate only. There are two mathematical formulations of chiral CFT, the one based on vertex operator algebras and the one based on conformal nets. We describe some recent results which, for first time, gives a general construction of conformal nets from VOAs.",0.0],["a polyhedral active set algorithm is developed for solving a nonlinear optimization","An Active Set Algorithm for Nonlinear Optimization with Polyhedral Constraints","summarize: A polyhedral active set algorithm PASA is developed for solving a nonlinear optimization problem whose feasible set is a polyhedron. Phase one of the algorithm is the gradient projection method, while phase two is any algorithm for solving a linearly constrained optimization problem. Rules are provided for branching between the two phases. Global convergence to a stationary point is established, while asymptotically PASA performs only phase two when either a nondegeneracy assumption holds, or the active constraints are linearly independent and a strong second-order sufficient optimality condition holds.",0.375],["uncertainty relations for non-commutative space are computed. we obtain a better","Probing Uncertainty Relations in Non-Commutative Space","summarize: In this paper, we compute uncertainty relations for non-commutative space and obtain a better lower bound than the standard one obtained from Heisenberg's uncertainty relation. We also derive the reverse uncertainty relation for product and sum of uncertainties of two incompatible variables for one linear and another non-linear model of the harmonic oscillator. The non-linear model in non-commutating space yields two different expressions for Schr\\odinger and Heisenberg uncertainty relation. This distinction does not arise in commutative space, and even in the linear model of non-commutative space.",0.2352941176],["black hole solution is a new charge for black hole solution. the topological charge is","The Last Lost Charge And Phase Transition In Schwarzschild AdS Minimally Coupled to a Cloud of Strings","summarize: In this paper we study the Schwarzschild AdS black hole with a cloud of string background in an extended phase space and investigate a new phase transition related to the topological charge. By treating the topological charge as a new charge for black hole solution we study its thermodynamics in this new extended phase space. We treat by two approaches to study the phase transition behavior via both ",0.1750346638],["the present paper is a comment on the robustness of the chirality in presence of","Robustness of chiral symmetry in atomic nuclei with reflection-asymmetric shapes","summarize: The present paper is a comment regarding the robustness of the chirality in presence of the space-reflection asymmetry, which leads to pairs of interleaved positive- and negative-parity bands. The recent results reported in Ref. \\cite which introduced the ",0.28],["a novel progressive learning paradigm of NODEs for long-term time series forecasting","Progressive Growing of Neural ODEs","summarize: Neural Ordinary Differential Equations have proven to be a powerful modeling tool for approximating and forecasting irregularly sampled time series data. However, their performance degrades substantially when applied to real-world data, especially long-term data with complex behaviors . To address the modeling of such complex data with different behaviors at different frequencies , we propose a novel progressive learning paradigm of NODEs for long-term time series forecasting. Specifically, following the principle of curriculum learning, we gradually increase the complexity of data and network capacity as training progresses. Our experiments with both synthetic data and real traffic data show that our training methodology consistently improves the performance of vanilla NODEs by over 64%.",0.1176470588],["magnetic materials with strong perpendicular anisotropy offer stability and high recording density","New highly-anisotropic Rh-based Heusler compound for magnetic recording","summarize: The development of high-density magnetic recording media is limited by the superparamagnetism in very small ferromagnetic crystals. Hard magnetic materials with strong perpendicular anisotropy offer stability and high recording density. To overcome the difficulty of writing media with a large coercivity, heat assisted magnetic recording has been developed, rapidly heating the media to the Curie temperature Tc before writing, followed by rapid cooling. Requirements are a suitable Tc, coupled with anisotropic thermal conductivity and hard magnetic properties. Here we introduce Rh2CoSb as a new hard magnet with potential for thin film magnetic recording. A magnetocrystalline anisotropy of 3.6 MJm-3 is combined with a saturation magnetization of 0Ms = 0.52 T at 2 K . The magnetic hardness parameter of 3.7 at room temperature is the highest observed for any rare-earth free hard magnet. The anisotropy is related to an unquenched orbital moment of 0.42 B on Co, which is hybridized with neighbouring Rh atoms with a large spin-orbit interaction. Moreover, the pronounced temperature-dependence of the anisotropy that follows from its Tc of 450 K, together with a high thermal conductivity of 20 Wm-1K-1, makes Rh2CoSb a candidate for development for heat assisted writing with a recording density in excess of 10 Tb\/in2.",0.25],["acyclic graphs are a class of graphs. a t","Identifying causal effects in maximally oriented partially directed acyclic graphs","summarize: We develop a necessary and sufficient causal identification criterion for maximally oriented partially directed acyclic graphs . MPDAGs as a class of graphs include directed acyclic graphs , completed partially directed acyclic graphs , and CPDAGs with added background knowledge. As such, they represent the type of graph that can be learned from observational data and background knowledge under the assumption of no latent variables. Our identification criterion can be seen as a generalization of the g-formula of Robins . We further obtain a generalization of the truncated factorization formula and compare our criterion to the generalized adjustment criterion of Perkovi\\'c et al. which is sufficient, but not necessary for causal identification.",0.6614029733],["the interlayer vdW coupling determines the properties of 2D multi-layer","Polytypism and Unexpected Strong Interlayer Coupling of two-Dimensional Layered ReS2","summarize: The anisotropic two-dimensional van der Waals layered materials, with both scientific interest and potential application, have one more dimension to tune the properties than the isotropic 2D materials. The interlayer vdW coupling determines the properties of 2D multi-layer materials by varying stacking orders. As an important representative anisotropic 2D materials, multilayer rhenium disulfide was expected to be random stacking and lack of interlayer coupling. Here, we demonstrate two stable stacking orders of N layer ReS2 from ultralow-frequency and high-frequency Raman spectroscopy, photoluminescence spectroscopy and first-principles density functional theory calculation. Two interlayer shear modes are observed in aa-stacked NL-ReS2 while only one interlayer shear mode appears in a-b-stacked NL-ReS2, suggesting anisotropic-like and isotropic-like stacking orders in aa- and a-b-stacked NL-ReS2, respectively. The frequency of the interlayer shear and breathing modes reveals unexpected strong interlayer coupling in aa- and a-b-NL-ReS2, the force constants of which are 55-90% to those of multilayer MoS2. The observation of strong interlayer coupling and polytypism in multi-layer ReS2 stimulate future studies on the structure, electronic and optical properties of other 2D anisotropic materials.",0.0833333333],["the gemini planet imager is a next-generation high-contrast imager","Large collaboration in observational astronomy: the Gemini Planet Imager exoplanet survey case","summarize: The Gemini Planet Imager is a next-generation high-contrast imager built for the Gemini Observatory. The GPI exoplanet survey consortium is made up of 102 researchers from 28 institutions in North and South America and Europe. In November 2014, we launched a search for young Jovian planets and debris disks. In this paper, we discuss how we have coordinated the work done by this large team to improve the technical and scientific productivity of the campaign, and describe lessons we have learned that could be useful for future instrumentation-based astronomical surveys. The success of GPIES lies mostly on its decentralized structure, clear definition of policies that are signed by each member, and the heavy use of modern tools for communicating, exchanging information, and processing data.",0.3582656553],["a critical reason for such bad recommendations lies in the intrinsic assumption that recommended users and items are","Non-IID Recommender Systems: A Review and Framework of Recommendation Paradigm Shifting","summarize: While recommendation plays an increasingly critical role in our living, study, work, and entertainment, the recommendations we receive are often for irrelevant, duplicate, or uninteresting products and services. A critical reason for such bad recommendations lies in the intrinsic assumption that recommended users and items are independent and identically distributed in existing theories and systems. Another phenomenon is that, while tremendous efforts have been made to model specific aspects of users or items, the overall user and item characteristics and their non-IIDness have been overlooked. In this paper, the non-IID nature and characteristics of recommendation are discussed, followed by the non-IID theoretical framework in order to build a deep and comprehensive understanding of the intrinsic nature of recommendation problems, from the perspective of both couplings and heterogeneity. This non-IID recommendation research triggers the paradigm shift from IID to non-IID recommendation research and can hopefully deliver informed, relevant, personalized, and actionable recommendations. It creates exciting new directions and fundamental solutions to address various complexities including cold-start, sparse data-based, cross-domain, group-based, and shilling attack-related issues.",0.25],["we study the notion of algebraic tangent cones at singularities of reflexive she","Algebraic tangent cones of reflexive sheaves","summarize: We study the notion of algebraic tangent cones at singularities of reflexive sheaves. These correspond to extensions of reflexive sheaves across a negative divisor. We show the existence of optimal extensions in a constructive manner, and we prove the uniqueness in a suitable sense. The results here are an algebro-geometric counterpart of our previous study on singularities of Hermitian-Yang-Mills connections.",0.4],["cross-site scripting flaws are a class of security flaws that permit","DjangoChecker: Applying Extended Taint Tracking and Server Side Parsing for Detection of Context-Sensitive XSS Flaws","summarize: Cross-site scripting flaws are a class of security flaws that permit the injection of malicious code into a web application. In simple situations, these flaws can be caused by missing input sanitizations. Sometimes, however, all application inputs are sanitized, but the sanitizations are not appropriate for the browser contexts of the sanitized values. Using an incorrect sanitizer can make the application look protected, when it is in fact vulnerable as if no sanitization was used, creating a context-sensitive XSS flaw. To discover context-sensitive XSS flaws, we introduce DjangoChecker. DjangoChecker combines extended dynamic taint tracking with a model browser for context analysis. We demonstrate the practical application of DjangoChecker on eight mature web applications based on Django, discovering previously unknown flaws in seven of the eight applications, including highly severe flaws that allow arbitrary JavaScript execution in the seven flawed applications.",0.2703337499],["we develop a framework for the average-case analysis of random quadratic problems.","Average-case Acceleration Through Spectral Density Estimation","summarize: We develop a framework for the average-case analysis of random quadratic problems and derive algorithms that are optimal under this analysis. This yields a new class of methods that achieve acceleration given a model of the Hessian's eigenvalue distribution. We develop explicit algorithms for the uniform, Marchenko-Pastur, and exponential distributions. These methods are momentum-based algorithms, whose hyper-parameters can be estimated without knowledge of the Hessian's smallest singular value, in contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum. Through empirical benchmarks on quadratic and logistic regression problems, we identify regimes in which the the proposed methods improve over classical accelerated methods.",0.2380952381],["a movie description of a singular link cobordism in 4-space is a","Movie moves for singular link cobordisms in 4-dimensional space","summarize: Two singular links are cobordant if one can be obtained from the other by singular link isotopy together with a combination of births or deaths of simple unknotted curves, and saddle point transformations. A movie description of a singular link cobordism in 4-space is a sequence of singular link diagrams obtained from a projection of the cobordism into 3-space by taking 2-dimensional cross sections perpendicular to a fixed direction. We present a set of movie moves that are sufficient to connect any two movies of isotopic singular link cobordisms.",0.5925925926],["the existence of a counterexample to the infinite-dimensional Carleson embedd","Two more counterexamples to the infinite dimensional Carleson embedding theorem","summarize: The existence of a counterexample to the infinite-dimensional Carleson embedding theorem has been established by Nazarov, Pisier, Treil, and Volberg. We provide an explicit construction of such an example. We also obtain a non-constructive example of particularly simple form; the density function of the measure is the tensor-square of a Hilbert space-valued analytic function. This special structure of the measure has implications for Hankel-like operators appearing in control theory.",0.7333333333],["quantum gravity is a reasonable form of quantum gravity. cosmologies with an average","The rotation problem","summarize: Any reasonable form of quantum gravity can explain why on a large scale, inertial frames seem not to rotate relative to the average matter distribution in the universe without the need for absolute space, finely tuned initial conditions, or without giving up independent degrees of freedom for the gravitational field. A simple saddlepoint approximation to a path-integral calculation for a perfect fluid cosmology shows that only cosmologies with an average present relative rotation rate smaller than about ",0.0357142857],["the method has been made available as the CABS-flex web server. it uses","CABS-flex 2.0: a web server for fast simulations of flexibility of protein structures","summarize: Classical simulations of protein flexibility remain computationally expensive, especially for large proteins. A few years ago, we developed a fast method for predicting protein structure fluctuations that uses a single protein model as the input. The method has been made available as the CABS-flex web server and applied in numerous studies of protein structure-function relationships. Here, we present a major update of the CABS-flex web server to version 2.0. The new features include: extension of the method to significantly larger and multimeric proteins, customizable distance restraints and simulation parameters, contact maps and a new, enhanced web server interface. CABS-flex 2.0 is freely available at http:\/\/biocomp.chem.uw.edu.pl\/CABSflex2",0.3125],["initialization of weights in backpropagation neural net is heavily affected by initialization of","A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition","summarize: Convergence rate of training algorithms for neural networks is heavily affected by initialization of weights. In this paper, an original algorithm for initialization of weights in backpropagation neural net is presented with application to character recognition. The initialization method is mainly based on a customization of the Kalman filter, translating it into Bayesian statistics terms. A metrological approach is used in this context considering weights as measurements modeled by mutually dependent normal random variables. The algorithm performance is demonstrated by reporting and discussing results of simulation trials. Results are compared with random weights initialization and other methods. The proposed method shows an improved convergence rate for the backpropagation training algorithm.",0.4178540304],["the simulation software ParFlow has been demonstrated to meet this requirement. the code requires further enhancement","Enhancing speed and scalability of the ParFlow simulation code","summarize: Regional hydrology studies are often supported by high resolution simulations of subsurface flow that require expensive and extensive computations. Efficient usage of the latest high performance parallel computing systems becomes a necessity. The simulation software ParFlow has been demonstrated to meet this requirement and shown to have excellent solver scalability for up to 16,384 processes. In the present work we show that the code requires further enhancements in order to fully take advantage of current petascale machines. We identify ParFlow's way of parallelization of the computational mesh as a central bottleneck. We propose to reorganize this subsystem using fast mesh partition algorithms provided by the parallel adaptive mesh refinement library p4est. We realize this in a minimally invasive manner by modifying selected parts of the code to reinterpret the existing mesh data structures. We evaluate the scaling performance of the modified version of ParFlow, demonstrating good weak and strong scaling up to 458k cores of the Juqueen supercomputer, and test an example application at large scale.",0.25],["a non convex least-squares fit criterion and a penalty","Rational Optimization for Nonlinear Reconstruction with Approximate ","summarize: Recovering nonlinearly degraded signal in the presence of noise is a challenging problem. In this work, this problem is tackled by minimizing the sum of a non convex least-squares fit criterion and a penalty term. We assume that the nonlinearity of the model can be accounted for by a rational function. In addition, we suppose that the signal to be sought is sparse and a rational approximation of the ",0.5263157895],[".","","summarize: In this work we prove optimal ",0.0],["the derivative of the Dirichlet to Neumann map and the Neumann to Dirichlet","Differentiability of the Dirichlet to Neumann map under movements of polygonal inclusions with an application to shape optimization","summarize: In this paper we derive rigorously the derivative of the Dirichlet to Neumann map and of the Neumann to Dirichlet map of the conductivity equation with respect to movements of vertices of triangular conductivity inclusions. We apply this result to formulate an optimization problem based on a shape derivative approach.",0.3539704471],["multiple mobile agents need to collaborate to move the message. each agent has a limited energy","Collaborative Delivery with Energy-Constrained Mobile Robots","summarize: We consider the problem of collectively delivering some message from a specified source to a designated target location in a graph, using multiple mobile agents. Each agent has a limited energy which constrains the distance it can move. Hence multiple agents need to collaborate to move the message, each agent handing over the message to the next agent to carry it forward. Given the positions of the agents in the graph and their respective budgets, the problem of finding a feasible movement schedule for the agents can be challenging. We consider two variants of the problem: in non-returning delivery, the agents can stop anywhere; whereas in returning delivery, each agent needs to return to its starting location, a variant which has not been studied before. We first provide a polynomial-time algorithm for returning delivery on trees, which is in contrast to the known NP-hardness of the non-returning version. In addition, we give resource-augmented algorithms for returning delivery in general graphs. Finally, we give tight lower bounds on the required resource augmentation for both variants of the problem. In this sense, our results close the gap left by previous research.",0.1153846154],["compact ordered spaces are not dually equivalent to any SP-class of finitary algebras","On the axiomatisability of the dual of compact ordered spaces","summarize: We provide a direct and elementary proof of the fact that the category of Nachbin's compact ordered spaces is dually equivalent to an Aleph_1-ary variety of algebras. Further, we show that Aleph_1 is a sharp bound: compact ordered spaces are not dually equivalent to any SP-class of finitary algebras.",0.3076923077],["the analysis is based on the recent results on the characterization of the maximum coding rate","Performance of Non-orthogonal Multiple Access under Finite Blocklength","summarize: In this paper, we present a finite-block-length comparison between the orthogonal multiple access scheme and the non-orthogonal multiple access for the uplink channel. First, we consider the Gaussian channel, and derive the closed form expressions for the rate and outage probability. Then, we extend our results to the quasi-static Rayleigh fading channel. Our analysis is based on the recent results on the characterization of the maximum coding rate at finite block-length and finite block-error probability. The overall system throughput is evaluated as a function of the number of information bits, channel uses and power. We find what would be the respective values of these different parameters that would enable throughput maximization. Furthermore, we analyze the system performance in terms of reliability and throughput when applying the type-I ARQ protocol with limited number of retransmissions. The throughput and outage probability are evaluated for different blocklengths and number of information bits. Our analysis reveals that there is a trade-off between reliability and throughput in the ARQ. While increasing the number of retransmissions boosts reliability by minimizing the probability of reception error, it results in more delay which decreases the throughput. Nevertheless, the results show that NOMA always outperforms OMA in terms of throughput, reliability and latency regardless of the users priority or the number of retransmissions in both Gaussian and fading channels.",0.1515151515],["we investigate the non-linear response and energy absorption in bulk silicon irradi","Ultrafast energy absorption and photoexcitation of bulk plasmon in crystalline silicon subjected to intense near-infrared ultrashort laser pulses","summarize: We investigate the non-linear response and energy absorption in bulk silicon irradiated by intense 12-fs near-infrared laser pulses. Depending on the laser intensity, we distinguish two regimes of non-linear absorption of the laser energy: for low intensities, energy deposition and photoionization involve perturbative three-photon transition through the direct bandgap of silicon. For laser intensities near and above 10",0.3465889484],["SIS logistic epidemic model is used to model spread of an infectious disease. a stoc","Extinction times in the subcritical stochastic SIS logistic epidemic","summarize: Many real epidemics of an infectious disease are not straightforwardly super- or sub-critical, and the understanding of epidemic models that exhibit such complexity has been identified as a priority for theoretical work. We provide insights into the near-critical regime by considering the stochastic SIS logistic epidemic, a well-known birth-and-death chain used to model the spread of an epidemic within a population of a given size ",0.347826087],["multi layer Capacitors MLCs are considered as the most promising refrigerant elements","Large heat flux in electrocaloric multilayer capacitors","summarize: Multi Layer Capacitors MLCs are considered as the most promising refrigerant elements to design and develop electrocaloric cooling devices. Recently, the heat transfer of these MLCs has been considered. However, the heat exchange with the surrounding environment has been poorly, if not, addressed. In this work, we measure by infrared thermography the temperature change versus time in four different heat exchange configurations. Depending on the configurations, Newtonian and non-Newtonian regimes with their corresponding Biot number are determined allowing to provide useful thermal characteristics. Indeed, in case of large area thermal pad contacts, heat transfer coefficients up to 3400 W m-2 K-1 are obtained showing that the standard MLCs already reach the needs for designing efficient prototypes. We also determine the ideal Brayton cooling power in case of thick wires contact which varies between 3.4 mW and 9.8 mW for operating frequencies varying from 0.25 Hz to 1 Hz. While only heat conduction is considered here, our work provides some design rules for improving heat exchanges in future devices.",0.1666666667],["we define a.","Self-dual Grassmannian, Wronski map, and representations of ","summarize: We define a ",0.0629585343],["the complete part of the earthquake frequency-magnitude distribution is well described by the Gutenberg","Generalized Earthquake Frequency-Magnitude Distribution Described by Asymmetric Laplace Mixture Modelling","summarize: The complete part of the earthquake frequency-magnitude distribution , above completeness magnitude mc, is well described by the Gutenberg-Richter law. The parameter mc however varies in space due to the seismic network configuration, yielding a convoluted FMD shape below max. This paper investigates the shape of the generalized FMD , which may be described as a mixture of elemental FMDs defined as asymmetric Laplace distributions of mode mc . An asymmetric Laplace mixture model is thus proposed with its parameters estimated using a semi-supervised hard expectation maximization approach including BIC penalties for model complexity. The performance of the proposed method is analysed, with encouraging results obtained: kappa, beta, and the mc distribution range are retrieved for different GFMD shapes in simulations, as well as in regional catalogues , in a global catalogue, and in an aftershock sequence . We find max to be conservative compared to other methods, kappa = k\/log = 3 in most catalogues = 1), but also that biases in kappa and beta may occur when rounding errors are present below completeness. The GFMD-ALMM, by modelling different FMD shapes in an autonomous manner, opens the door to new statistical analyses in the realm of incomplete seismicity data, which could in theory improve earthquake forecasting by considering c. ten times more events.",0.0952380952],["maximum commutative subalgebras containing only Toeplitz matric","A Family of Maximal Algebras of Block Toeplitz matrices","summarize: The maximal commutative subalgebras containing only Toeplitz matrices have been identified as generalized circulants. A similar simple description cannot be obtained for block Toeplitz matrices. We introduce and investigate certain families of maximal commutative algebras of block Toeplitz matrices.",0.214707798],["the instrument onboard the InSight mission to Mars is the critical instrument. it is","Evaluating the wind-induced mechanical noise on the InSight seismometers","summarize: The SEIS instrument onboard the InSight mission to Mars is the critical instrument for determining the interior structure of Mars, the current level of tectonic activity and the meteorite flux. Meeting the performance requirements of the SEIS instrument is vital to successfully achieve these mission objectives. Here we analyse in-situ wind measurements from previous Mars space missions to understand the wind environment that we are likely to encounter on Mars, and then we use an elastic ground deformation model to evaluate the mechanical noise contributions on the SEIS instrument due to the interaction between the Martian winds and the InSight lander. Lander mechanical noise maps that will be used to select the best deployment site for SEIS once the InSight lander arrives on Mars are also presented. We find the lander mechanical noise may be a detectable signal on the InSight seismometers. However, for the baseline SEIS deployment position, the noise is expected to be below the total noise requirement >97% of the time and is, therefore, not expected to endanger the InSight mission objectives.",0.4074074074],["the Schrodinger equation violates local causality. it causes instantan","Nonlocality and local causality in the Schr\\odinger Equation with time-dependent boundary conditions","summarize: We investigate the nonlocal dynamics of a single particle placed in an infinite well with moving walls. It is shown that in this situation, the Schr\\odinger equation violates local causality by causing instantaneous changes in the probability current everywhere inside the well. This violation is formalized by designing a gedanken faster-than-light communication device which uses an ensemble of long narrow cavities and weak measurements to resolve the weak value of the momentum far away from the movable wall. Our system is free from the usual features causing nonphysical violations of local causality when using the SE, such as instantaneous changes in potentials or states involving arbitrarily high energies or velocities. We explore in detail several possible artifacts that could account for the failure of the SE to respect local causality for systems involving time-dependent boundary conditions.",0.3582656553],["nilpotent Lie algebras run in time polynomial in the order","Testing isomorphism of graded algebras","summarize: We present a new algorithm to decide isomorphism between finite graded algebras. For a broad class of nilpotent Lie algebras, we demonstrate that it runs in time polynomial in the order of the input algebras. We introduce heuristics that often dramatically improve the performance of the algorithm and report on an implementation in Magma.",0.25],["new HDP based online review rating regression model. combines topics, word sentiment","Unifying Topic, Sentiment & Preference in an HDP-Based Rating Regression Model for Online Reviews","summarize: This paper proposes a new HDP based online review rating regression model named Topic-Sentiment-Preference Regression Analysis . TSPRA combines topics , word sentiment and user preference as regression factors, and is able to perform topic clustering, review rating prediction, sentiment analysis and what we invent as critical aspect analysis altogether in one framework. TSPRA extends sentiment approaches by integrating the key concept user preference in collaborative filtering models into consideration, while it is distinct from current CF models by decoupling user preference and sentiment as independent factors. Our experiments conducted on 22 Amazon datasets show overwhelming better performance in rating predication against a state-of-art model FLAME in terms of error, Pearson's Correlation and number of inverted pairs. For sentiment analysis, we compare the derived word sentiments against a public sentiment resource SenticNet3 and our sentiment estimations clearly make more sense in the context of online reviews. Last, as a result of the de-correlation of user preference from sentiment, TSPRA is able to evaluate a new concept critical aspects, defined as the product aspects seriously concerned by users but negatively commented in reviews. Improvement to such critical aspects could be most effective to enhance user experience.",0.0705401437],["graphic design encompasses a wide range of activities from the design of traditional print media to","Using Computer Vision Techniques for Moving Poster Design","summarize: Graphic Design encompasses a wide range of activities from the design of traditional print media to site-specific and electronic media . Its practice always explores the new possibilities of information and communication technologies. Therefore, interactivity and participation have become key features in the design process. Even in traditional print media, graphic designers are trying to enhance user experience and exploring new interaction models. Moving posters are an example of this. This type of posters combine the specific features of motion and print worlds in order to produce attractive forms of communication that explore and exploit the potential of digital screens. In our opinion, the next step towards the integration of moving posters with the surroundings, where they operate, is incorporating data from the environment, which also enables the seamless participation of the audience. As such, the adoption of computer vision techniques for moving poster design becomes a natural approach. Following this line of thought, we present a system wherein computer vision techniques are used to shape a moving poster. Although it is still a work in progress, the system is already able to sense the surrounding physical environment and translate the collected data into graphical information. The data is gathered from the environment in two ways: directly using motion tracking; and indirectly via contextual ambient data. In this sense, each user interaction with the system results in a different experience and in a unique poster design.",0.0],["MERLiN algorithm recovers a causal variable from an observed linear mixture.","Recovery of non-linear cause-effect relationships from linearly mixed neuroimaging data","summarize: Causal inference concerns the identification of cause-effect relationships between variables. However, often only linear combinations of variables constitute meaningful causal variables. For example, recovering the signal of a cortical source from electroencephalography requires a well-tuned combination of signals recorded at multiple electrodes. We recently introduced the MERLiN algorithm that is able to recover, from an observed linear mixture, a causal variable that is a linear effect of another given variable. Here we relax the assumption of this cause-effect relationship being linear and present an extended algorithm that can pick up non-linear cause-effect relationships. Thus, the main contribution is an algorithm that has broader applicability and allows for a richer model class. Furthermore, a comparative analysis indicates that the assumption of linear cause-effect relationships is not restrictive in analysing electroencephalographic data.",0.5],["two indicators are classically used to evaluate the quality of rule-based classification systems. predictive","On Evaluating the Quality of Rule-Based Classification Systems","summarize: Two indicators are classically used to evaluate the quality of rule-based classification systems: predictive accuracy, i.e. the system's ability to successfully reproduce learning data and coverage, i.e. the proportion of possible cases for which the logical rules constituting the system apply. In this work, we claim that these two indicators may be insufficient, and additional measures of quality may need to be developed. We theoretically show that classification systems presenting good predictive accuracy and coverage can, nonetheless, be trivially improved and illustrate this proposition with examples.",0.1333333333],["RL methods learn optimal decisions in the presence of a stationary environment. but the stationary","Reinforcement Learning in Non-Stationary Environments","summarize: Reinforcement learning methods learn optimal decisions in the presence of a stationary environment. However, the stationary assumption on the environment is very restrictive. In many real world problems like traffic signal control, robotic applications, one often encounters situations with non-stationary environments and in these scenarios, RL methods yield sub-optimal decisions. In this paper, we thus consider the problem of developing RL methods that obtain optimal decisions in a non-stationary environment. The goal of this problem is to maximize the long-term discounted reward achieved when the underlying model of the environment changes over time. To achieve this, we first adapt a change point algorithm to detect change in the statistics of the environment and then develop an RL algorithm that maximizes the long-run reward accrued. We illustrate that our change point method detects change in the model of the environment effectively and thus facilitates the RL algorithm in maximizing the long-run reward. We further validate the effectiveness of the proposed solution on non-stationary random Markov decision processes, a sensor energy management problem and a traffic signal control problem.",0.16],["arbitrary channels are constructed by imposing an arbitrary quasigroup structure. the block error","Polar Codes for Arbitrary DMCs and Arbitrary MACs","summarize: Polar codes are constructed for arbitrary channels by imposing an arbitrary quasigroup structure on the input alphabet. Just as with usual polar codes, the block error probability under successive cancellation decoding is ",0.0625],["soft robotic system employs the inherent qualities of soft fluidic actuators. it establishe","Design and integration of a parallel, soft robotic end-effector for extracorporeal ultrasound","summarize: Objective: In this work we address limitations in state-of-the-art ultrasound robots by designing and integrating a novel soft robotic system for ultrasound imaging. It employs the inherent qualities of soft fluidic actuators to establish safe, adaptable interaction between ultrasound probe and patient. Methods: We acquire clinical data to determine the movement ranges and force levels required in prenatal foetal ultrasound imaging and design the soft robotic end-effector accordingly. We verify its mechanical characteristics, derive and validate a kinetostatic model and demonstrate controllability and imaging capabilities on an ultrasound phantom. Results: The soft robot exhibits the desired stiffness characteristics and is able to reach 100% of the required workspace when no external force is present, and 95% of the workspace when considering its compliance. The model can accurately predict the end-effector pose with a mean error of 1.18+\/-0.29mm in position and 0.92+\/-0.47deg in orientation. The derived controller is, with an average position error of 0.39mm, able to track a target pose efficiently without and with externally applied loads. Ultrasound images acquired with the system are of equally good quality compared to a manual sonographer scan. Conclusion: The system is able to withstand loads commonly applied during foetal ultrasound scans and remains controllable with a motion range similar to manual scanning. Significance: The proposed soft robot presents a safe, cost-effective solution to offloading sonographers in day-to-day scanning routines. The design and modelling paradigms are greatly generalizable and particularly suitable for designing soft robots for physical interaction tasks.",0.2777777778],["the rigid existing Internet of things architecture blocks current traffic management technology to provide a real differentiated","Dynamic Load-Balancing Vertical Control for Large-Scale Software-Defined Internet of Things","summarize: As the global Internet of things increasingly is popular with consumers and business environment, network flow management has become an important topic to optimize the performance on Internet of Things. The rigid existing Internet of things architecture blocks current traffic management technology to provide a real differentiated service for large-scale IoT. Software-defined Internet of Things is a new computing paradigm that separates control plane and data plane, and enables centralized logic control. In this paper, we first present a general framework for SD-IoT, which consists of two main components: SD-IoT controllers and SD-IoT switches. The controllers of SD-IoT uses resource pooling technology, and the pool is responsible for the centralized control of the entire network. The switches of SD-IoT integrate with the gateway functions, which is responsible for data access and forwarding. The SD-IoT controller pool is designed as a vertical control architecture, which includes the main control layer and the base control layer. The controller of the main control layer interacts upward with the application layer, interacts with the base control layer downwards, and the controller of the basic control layer interacts with the data forwarding layer. We propose a dynamic balancing algorithm of the main controller based on election mechanism and a dynamic load balancing algorithm of the basic controller based on the balanced delay, respectively. The experimental results show that the dynamic balancing algorithm based on the election mechanism can ensure the consistency of the messages between the main controllers, and the dynamic load balancing algorithm based on the balanced delay can balance between these different workloads in the basic controllers.",0.3913043478],["continuous bag of Words is a powerful text embedding method. it is not","CBOW Is Not All You Need: Combining CBOW with the Compositional Matrix Space Model","summarize: Continuous Bag of Words is a powerful text embedding method. Due to its strong capabilities to encode word content, CBOW embeddings perform well on a wide range of downstream tasks while being efficient to compute. However, CBOW is not capable of capturing the word order. The reason is that the computation of CBOW's word embeddings is commutative, i.e., embeddings of XYZ and ZYX are the same. In order to address this shortcoming, we propose a learning algorithm for the Continuous Matrix Space Model, which we call Continual Multiplication of Words . Our algorithm is an adaptation of word2vec, so that it can be trained on large quantities of unlabeled text. We empirically show that CMOW better captures linguistic properties, but it is inferior to CBOW in memorizing word content. Motivated by these findings, we propose a hybrid model that combines the strengths of CBOW and CMOW. Our results show that the hybrid CBOW-CMOW-model retains CBOW's strong ability to memorize word content while at the same time substantially improving its ability to encode other linguistic information by 8%. As a result, the hybrid also performs better on 8 out of 11 supervised downstream tasks with an average improvement of 1.2%.",0.1736177022],["topological photonics provides a new paradigm in studying cavity quantum electrodynamics with robust","Cavity Quantum Electrodynamics with Second-Order Topological Corner State","summarize: Topological photonics provides a new paradigm in studying cavity quantum electrodynamics with robustness to disorder. In this work, we demonstrate the coupling between single quantum dots and the second-order topological corner state. Based on the second-order topological corner state, a topological photonic crystal cavity is designed and fabricated into GaAs slabs with quantum dots embedded. The coexistence of corner state and edge state with high quality factor close to 2000 is observed. The enhancement of photoluminescence intensity and emission rate are both observed when the quantum dot is on resonance with the corner state. This result enables the application of topology into cavity quantum electrodynamics, offering an approach to topological devices for quantum information processing.",0.3],["the DEA model is modelled with a probability distribution. the model is based","Robust DEA efficiency scores: A probabilistic\/combinatorial approach","summarize: In this paper we propose robust efficiency scores for the scenario in which the specification of the inputs\/outputs to be included in the DEA model is modelled with a probability distribution. This proba- bilistic approach allows us to obtain three different robust efficiency scores: the Conditional Expected Score, the Unconditional Expected Score and the Expected score under the assumption of Maximum Entropy principle. The calculation of the three efficiency scores involves the resolution of an exponential number of linear problems. The algorithm presented in this paper allows to solve over 200 millions of linear problems in an affordable time when considering up 20 inputs\/outputs and 200 DMUs. The approach proposed is illustrated with an application to the assessment of professional tennis players.",0.24],["effective applications of vehicular ad hoc networks in traffic signal control require new methods for","Detection of malicious data in vehicular ad-hoc networks for traffic signal control applications","summarize: Effective applications of vehicular ad hoc networks in traffic signal control require new methods for detection of malicious data. Injection of malicious data can result in significantly decreased performance of such applications, increased vehicle delays, fuel consumption, congestion, or even safety threats. This paper introduces a method, which combines a model of expected driver behaviour with position verification in order to detect the malicious data injected by vehicle nodes that perform Sybil attacks. Effectiveness of this approach was demonstrated in simulation experiments for a decentralized self-organizing system that controls the traffic signals at multiple intersections in an urban road network. Experimental results show that the proposed method is useful for mitigating the negative impact of malicious data on the performance of traffic signal control.",0.7333333333],["a ring is always nil, which extends a well known result from","Nilpotent, algebraic and quasi-regular elements in rings and algebras","summarize: We prove that an integral Jacobson radical ring is always nil, which extends a well known result from algebras over fields to rings. As a consequence we show that if every element x of a ring R is a zero of some polynomial p_x with integer coefficients, such that p_x=1, then R is a nil ring. With these results we are able to give new characterizations of the upper nilradical of a ring and a new class of rings that satisfy the K\\othe conjecture, namely the integral rings.",0.5],["twitter has a huge number of Arabic users who mostly post and write their tweets using the","Arabic Language Sentiment Analysis on Health Services","summarize: The social media network phenomenon leads to a massive amount of valuable data that is available online and easy to access. Many users share images, videos, comments, reviews, news and opinions on different social networks sites, with Twitter being one of the most popular ones. Data collected from Twitter is highly unstructured, and extracting useful information from tweets is a challenging task. Twitter has a huge number of Arabic users who mostly post and write their tweets using the Arabic language. While there has been a lot of research on sentiment analysis in English, the amount of researches and datasets in Arabic language is limited. This paper introduces an Arabic language dataset which is about opinions on health services and has been collected from Twitter. The paper will first detail the process of collecting the data from Twitter and also the process of filtering, pre-processing and annotating the Arabic text in order to build a big sentiment analysis dataset in Arabic. Several Machine Learning algorithms alongside Deep and Convolutional Neural Networks were utilized in our experiments of sentiment analysis on our health dataset.",0.2380952381],["the modeling approach has been widely employed to investigate and analyze the energy consumption of Cloud applications.","A Survey on Modeling Energy Consumption of Cloud Applications: Deconstruction, State of the Art, and Trade-off Debates","summarize: Given the complexity and heterogeneity in Cloud computing scenarios, the modeling approach has widely been employed to investigate and analyze the energy consumption of Cloud applications, by abstracting real-world objects and processes that are difficult to observe or understand directly. It is clear that the abstraction sacrifices, and usually does not need, the complete reflection of the reality to be modeled. Consequently, current energy consumption models vary in terms of purposes, assumptions, application characteristics and environmental conditions, with possible overlaps between different research works. Therefore, it would be necessary and valuable to reveal the state-of-the-art of the existing modeling efforts, so as to weave different models together to facilitate comprehending and further investigating application energy consumption in the Cloud domain. By systematically selecting, assessing and synthesizing 76 relevant studies, we rationalized and organized over 30 energy consumption models with unified notations. To help investigate the existing models and facilitate future modeling work, we deconstructed the runtime execution and deployment environment of Cloud applications, and identified 18 environmental factors and 12 workload factors that would be influential on the energy consumption. In particular, there are complicated trade-offs and even debates when dealing with the combinational impacts of multiple factors.",0.2631578947],["a multilayer network approach combines different network layers to create a single mathematical object","A framework to evaluate whether to pool or separate behaviors in a multilayer network","summarize: A multilayer network approach combines different network layers, which are connected by interlayer edges, to create a single mathematical object. These networks can contain a variety of information types and represent different aspects of a system. However, the process for selecting which information to include is not always straightforward. Using data on two agonistic behaviors in a captive population of monk parakeets , we developed a framework for investigating how pooling or splitting behaviors at the scale of dyadic relationships affects individual- and group-level social properties. We designed two reference models to test whether randomizing the number of interactions across behavior types results in similar structural patterns as the observed data. Although the behaviors were correlated, the first reference model suggests that the two behaviors convey different information about some social properties and should therefore not be pooled. However, once we controlled for data sparsity, we found that the observed measures corresponded with those from the second reference model. Hence, our initial result may have been due to the unequal frequencies of each behavior. Overall, our findings support pooling the two behaviors. Awareness of how selected measurements can be affected by data properties is warranted, but nonetheless our framework disentangles these efforts and as a result can be used for myriad types of behaviors and questions. This framework will help researchers make informed and data-driven decisions about which behaviors to pool or separate, prior to using the data in subsequent multilayer network analyses.",0.5882352941],["gradient-flow algorithm finds a good global minimum despite spurious local minima.","Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model","summarize: Gradient-based algorithms are effective for many machine learning tasks, but despite ample recent effort and some progress, it often remains unclear why they work in practice in optimising high-dimensional non-convex functions and why they find good minima instead of being trapped in spurious ones. Here we present a quantitative theory explaining this behaviour in a spiked matrix-tensor model. Our framework is based on the Kac-Rice analysis of stationary points and a closed-form analysis of gradient-flow originating from statistical physics. We show that there is a well defined region of parameters where the gradient-flow algorithm finds a good global minimum despite the presence of exponentially many spurious local minima. We show that this is achieved by surfing on saddles that have strong negative direction towards the global minima, a phenomenon that is connected to a BBP-type threshold in the Hessian describing the critical points of the landscapes.",0.2606789731],["di-hadron correlations are used to compare low momentum jets. the distance-","Disappearance of the Mach Cone in heavy ion collisions","summarize: We present an analysis of di-hadron correlations using recently developed methods for background subtraction which allow for higher precision measurements with fewer assumptions about the background. These studies indicate that low momentum jets interacting with the medium do not equilibrate with the medium, but rather that interactions with the medium lead to more subtle increases in their widths and fragmentation functions, consistent with observations from studies of higher momentum fully reconstructed jets. The away-side shape is not consistent with a Mach cone.",0.0833333333],["wireless steganography hides covert information inside primary information. primary information will still","Hiding Data in Plain Sight: Undetectable Wireless Communications Through Pseudo-Noise Asymmetric Shift Keying","summarize: Undetectable wireless transmissions are fundamental to avoid eavesdroppers. To address this issue, wireless steganography hides covert information inside primary information by slightly modifying the transmitted waveform such that primary information will still be decodable, while covert information will be seen as noise by agnostic receivers. Since the addition of covert information inevitably decreases the SNR of the primary transmission, key challenges in wireless steganography are: i) to assess the impact of the covert channel on the primary channel as a function of different channel conditions; and ii) to make sure that the covert channel is undetectable. Existing approaches are protocol-specific, also we notice that existing wireless technologies rely on phase-keying modulations that in most cases do not use the channel up to its Shannon capacity. Therefore, the residual capacity can be leveraged to implement a wireless system based on a pseudo-noise asymmetric shift keying modulation, where covert symbols are mapped by shifting the amplitude of primary symbols. This way, covert information will be undetectable, since a receiver expecting phase-modulated symbols will see their shift in amplitude as an effect of channel\/path loss degradation. We first investigate the SER of PN-ASK as a function of the channel; then, we find the optimal PN-ASK parameters that optimize primary and covert throughput under different channel condition. We evaluate the throughput performance and undetectability of PN-ASK through extensive simulations and on an experimental testbed based on USRP N210 software-defined radios. We show that PN-ASK improves the throughput by more than 8x with respect to prior art. Finally, we demonstrate through experiments that PN-ASK is able to transmit covert data on top of IEEE 802.11g frames, which are correctly decoded by an off-the-shelf laptop WiFi.",0.0],["the first order Sobolev spaces are contained in the generally nonlinear class of general","Sobolev functions on varifolds","summarize: This paper introduces first order Sobolev spaces on certain rectifiable varifolds. These complete locally convex spaces are contained in the generally nonlinear class of generalised weakly differentiable functions and share key functional analytic properties with their Euclidean counterparts. Assuming the varifold to satisfy a uniform lower density bound and a dimensionally critical summability condition on its mean curvature, the following statements hold. Firstly, continuous and compact embeddings of Sobolev spaces into Lebesgue spaces and spaces of continuous functions are available. Secondly, the geodesic distance associated to the varifold is a continuous, not necessarily H\\older continuous Sobolev function with bounded derivative. Thirdly, if the varifold additionally has bounded mean curvature and finite measure, the present Sobolev spaces are isomorphic to those previously available for finite Radon measures yielding many new results for those classes as well. Suitable versions of the embedding results obtained for Sobolev functions hold in the larger class of generalised weakly differentiable functions.",0.0526315789],["in this paper, we develop an upper bound for the SPARSEVA estimation error.","An analysis of the SPARSEVA estimate for the finite sample data case","summarize: In this paper, we develop an upper bound for the SPARSEVA estimation error in a general scheme, i.e., when the cost function is strongly convex and the regularized norm is decomposable for a pair of subspaces. We show how this general bound can be applied to a sparse regression problem to obtain an upper bound for the traditional SPARSEVA problem. Numerical results are used to illustrate the effectiveness of the suggested bound.",0.3846153846],["gravity field maps of the satellite gravimetry missions GRACE and GRACE follow-On are","Revisiting the Light Time Correction in Gravimetric Missions Like GRACE and GRACE Follow-On","summarize: The gravity field maps of the satellite gravimetry missions GRACE and GRACE Follow-On are derived by means of precise orbit determination. The key observation is the biased inter-satellite range, which is measured primarily by a K-Band Ranging system in GRACE and GRACE Follow-On. The GRACE Follow-On satellites are additionally equipped with a Laser Ranging Interferometer , which provides measurements with lower noise compared to the KBR. The biased range of KBR and LRI needs to be converted for gravity field recovery into an instantaneous range, i.e. the biased Euclidean distance between the satellites' center-of-mass at the same time. One contributor to the difference between measured and instantaneous range arises due to the non-zero travel time of electro-magnetic waves between the spacecraft. We revisit the calculation of the light time correction from first principles considering general relativistic effects and state-of-the-art models of Earth's potential field. The novel analytical expressions for the LTC of KBR and LRI can circumvent numerical limitations of the classical approach. The dependency of the LTC on geopotential models and on the parameterization is studied, and afterwards the results are compared against the LTC provided in the official datasets of GRACE and GRACE Follow-On. It is shown that the new approach has a significantly lower noise, well below the instrument noise of current instruments, especially relevant for the LRI, and even if used with kinematic orbit products. This allows calculating the LTC accurate enough even for the next generation of gravimetric missions.",0.4],["domain adaptation techniques solve this problem by training transferable models from label-rich source domain to label","Conditional Coupled Generative Adversarial Networks for Zero-Shot Domain Adaptation","summarize: Machine learning models trained in one domain perform poorly in the other domains due to the existence of domain shift. Domain adaptation techniques solve this problem by training transferable models from the label-rich source domain to the label-scarce target domain. Unfortunately, a majority of the existing domain adaptation techniques rely on the availability of target-domain data, and thus limit their applications to a small community across few computer vision problems. In this paper, we tackle the challenging zero-shot domain adaptation problem, where target-domain data is non-available in the training stage. For this purpose, we propose conditional coupled generative adversarial networks by extending the coupled generative adversarial networks into a conditioning model. Compared with the existing state of the arts, our proposed CoCoGAN is able to capture the joint distribution of dual-domain samples in two different tasks, i.e. the relevant task and an irrelevant task . We train CoCoGAN with both source-domain samples in RT and dual-domain samples in IRT to complete the domain adaptation. While the former provide high-level concepts of the non-available target-domain data, the latter carry the sharing correlation between the two domains in RT and IRT. To train CoCoGAN in the absence of target-domain data for RT, we propose a new supervisory signal, i.e. the alignment between representations across tasks. Extensive experiments carried out demonstrate that our proposed CoCoGAN outperforms existing state of the arts in image classifications.",0.0],["turbulence is unlikely to occur in isothermal constant density quasi-Keple","Boundary-layer turbulence in experiments of quasi-Keplerian flows","summarize: Most flows in nature and engineering are turbulent because of their large velocities and spatial scales. Laboratory experiments of rotating quasi-Keplerian flows, for which the angular velocity decreases radially but the angular momentum increases, are however laminar at Reynolds numbers exceeding one million. This is in apparent contradiction to direct numerical simulations showing that in these experiments turbulence transition is triggered by the axial boundaries. We here show numerically that as the Reynolds number increases turbulence becomes progressively confined to the boundary layers and the flow in the bulk fully relaminarizes. Our findings support that turbulence is unlikely to occur in isothermal constant density quasi-Keplerian flows.",0.2727272727],["these lecture notes are based on the hand-written notes which I prepared for the cosm","Lecture Notes in Cosmology","summarize: These lecture notes are based on the hand-written notes which I prepared for the cosmology course taught to graduate students of PPGFis and PPGCosmo at the Federal University of Esp\\'irito Santo , starting from 2014. This course covers topics ranging from the evidence of the expanding universe to Cosmic Microwave Background anisotropies. They can be found also on my personal webpage http:\/\/ofp.cosmo-ufes.org\/ and shall be published by Springer in 2018.",0.0],["the research aims to examine areas of the Compton cross section of annihilation","Probing entanglement in Compton interactions","summarize: This theoretical research aims to examine areas of the Compton cross section of entangled annihilation photons for the purpose of testing for possible break down of theory, which could have consequences for predicted optimal capabilities of Compton PET systems.We provide maps of the cross section for entangled annihilation photons for experimental verification.We introduce a strategy to derive cross sections in a relatively straight forward manner for the Compton scattering of a hypothetical separable, mixed and entangled states. To understand the effect that entanglement has on the cross section for annihilation photons, we derive the cross section so that it is expressed in terms of the cross section of a hypothetical separable state and of a hypothetical forbidden maximally entangled state.We find lobe-like structures in the cross section which are regions where entanglement has the greatest effect.We also find that mixed states do not reproduce the cross section for annihilation photons, contrary to a recent investigation which reported otherwise.We review the motivation and method of the most precise Compton scattering experiment for annihilation photons, in order to resolve conflicting reports regarding the extent to which the cross section itself has been experimentally verified.",0.1111111111],["resetting is a potential particle diffusing in a potential.","Diffusion with resetting in a logarithmic potential","summarize: We study the effect of resetting on diffusion in a logarithmic potential. In this model, a particle diffusing in a potential ",0.4761904762],["spatial collection efficiency portrays the driving forces and loss mechanisms in photovoltaic and photoe","The spatial collection efficiency of photogenerated charge carriers in photovoltaic and photoelectrochemical devices","summarize: The spatial collection efficiency portrays the driving forces and loss mechanisms in photovoltaic and photoelectrochemical devices. It is defined as the fraction of photogenerated charge carriers created at a specific point within the device that contribute to the photocurrent. In stratified planar structures, the spatial collection efficiency can be extracted out of photocurrent action spectra measurements empirically, with few a priori assumptions. Although this method was applied to photovoltaic cells made of well-understood materials, it has never been used to study unconventional materials such as metal-oxide semiconductors that are often employed in photoelectrochemical cells. This perspective shows the opportunities that this method has to offer for investigating new materials and devices with unknown properties. The relative simplicity of the method, and its applicability to operando performance characterization, makes it an important tool for analysis and design of new photovoltaic and photoelectrochemical materials and devices.",0.4705882353],["a pig phantom maintained at 20 degree was a pig phan","Quantitative MRI molecular imaging in the evaluation of early post mortem changes in muscles. A feasibility study on a pig phantom","summarize: Estimating early postmortem interval EPI is a difficult task in daily forensic activity due to limitations of accurate and reliable methods. The aim of the present work is to describe a novel approach in the estimation of EPI based on quantitative magnetic resonance molecular imaging qMRMI using a pig phantom since post mortem degradation of pig meat is similar to that of human muscles. On a pig phantom maintained at 20 degree, using a 1.5 T MRI scanner we performed 10 scans, every 4 hours, monitoring apparent diffusion coefficient ADC, fractional anisotropy FA, magnetization transfer ration MTR, tractography and susceptibility weighted changes in muscles until 36 hours after death. Cooling of the phantom during the experiment was recorded. Histology was also obtained. Pearson's Test was carried out for statistical correlation. We found a significative statistical inverse correlation between ADC, FA, MT and PMI. Our preliminary data shows that post mortem qMRMI is a potential powerful tool in accurately determining EPI and is worth of further investigation.",0.3021677411],["proposed framework includes a network-based model that connected sentences based on their semantic similarity","Semantic flow in language networks","summarize: In this study we propose a framework to characterize documents based on their semantic flow. The proposed framework encompasses a network-based model that connected sentences based on their semantic similarity. Semantic fields are detected using standard community detection methods. as the story unfolds, transitions between semantic fields are represent in Markov networks, which in turned are characterized via network motifs . Here we show that the proposed framework can be used to classify books according to their style and publication dates. Remarkably, even without a systematic optimization of parameters, philosophy and investigative books were discriminated with an accuracy rate of 92.5%. Because this model captures semantic features of texts, it could be used as an additional feature in traditional network-based models of texts that capture only syntactical\/stylistic information, as it is the case of word adjacency networks.",0.1363636364],["weak* solutions can be extended to include solutions containing vacuums. we present a","Weak* Solutions II: The Vacuum in Lagrangian Gas Dynamics","summarize: We develop a framework in which to make sense of solutions containing the vacuum in Lagrangian gas dynamics. At and near vacuum, the specific volume becomes infinite and enclosed vacuums are represented by Dirac masses, so they cannot be treated in the usual weak sense. However, the weak* solutions recently introduced by the authors can be extended to include solutions containing vacuums. We present a definition of these natural vacuum solutions and provide explicit examples which demonstrate some of their features. Our examples are isentropic for clarity, and we briefly discuss the extension to the full ",0.25],["we study the behaviour of the Laplacian flow evolving closed G.","Closed warped G","summarize: We study the behaviour of the Laplacian flow evolving closed G",0.0],["study aims to shed light on the impact semantic context cues have on the user acceptance","The Impact of Semantic Context Cues on the User Acceptance of Tag Recommendations: An Online Study","summarize: In this paper, we present the results of an online study with the aim to shed light on the impact that semantic context cues have on the user acceptance of tag recommendations. Therefore, we conducted a work-integrated social bookmarking scenario with 17 university employees in order to compare the user acceptance of a context-aware tag recommendation algorithm called 3Layers with the user acceptance of a simple popularity-based baseline. In this scenario, we validated and verified the hypothesis that semantic context cues have a higher impact on the user acceptance of tag recommendations in a collaborative tagging setting than in an individual tagging setting. With this paper, we contribute to the sparse line of research presenting online recommendation studies.",0.3636363636],["the paper on the nonlocal Cahn-Hilliard-Brinkman and Cah","Erratum: On the nonlocal Cahn-Hilliard-Brinkman and Cahn-Hilliard-Hele-Shaw systems ","summarize: In this note, we want to highlight and correct an error in the paper On the nonlocal Cahn-Hilliard-Brinkman and Cahn-Hilliard-Hele-Shaw systems written by the authors.",0.5883312684],["compound matrices are analogues to properties of conjugate matrices","Tropical compound matrix identities","summarize: We prove identities on compound matrices in extended tropical semirings. Such identities include analogues to properties of conjugate matrices, powers of matrices and~",0.0909090909],["the resulting extractions are a valuable resource for downstream tasks such as knowledge base construction,","OPIEC: An Open Information Extraction Corpus","summarize: Open information extraction systems extract relations and their arguments from natural language text in an unsupervised manner. The resulting extractions are a valuable resource for downstream tasks such as knowledge base construction, open question answering, or event schema induction. In this paper, we release, describe, and analyze an OIE corpus called OPIEC, which was extracted from the text of English Wikipedia. OPIEC complements the available OIE resources: It is the largest OIE corpus publicly available to date and contains valuable metadata such as provenance information, confidence scores, linguistic annotations, and semantic annotations including spatial and temporal information. We analyze the OPIEC corpus by comparing its content with knowledge bases such as DBpedia or YAGO, which are also based on Wikipedia. We found that most of the facts between entities present in OPIEC cannot be found in DBpedia and\/or YAGO, that OIE facts often differ in the level of specificity compared to knowledge base facts, and that OIE open relations are generally highly polysemous. We believe that the OPIEC corpus is a valuable resource for future research on automated knowledge base construction.",0.12],["a distribution over kernels formed by modelling a spectral mixture density with a","Scalable L\\'evy Process Priors for Spectral Kernel Learning","summarize: Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a L\\'evy process. The resulting distribution has support for all stationary covariances--including the popular RBF, periodic, and Mat\\'ern kernels--combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the L\\'evy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for ",0.5454545455],["a movie description of a singular link cobordism in 4-space is a","Movie moves for singular link cobordisms in 4-dimensional space","summarize: Two singular links are cobordant if one can be obtained from the other by singular link isotopy together with a combination of births or deaths of simple unknotted curves, and saddle point transformations. A movie description of a singular link cobordism in 4-space is a sequence of singular link diagrams obtained from a projection of the cobordism into 3-space by taking 2-dimensional cross sections perpendicular to a fixed direction. We present a set of movie moves that are sufficient to connect any two movies of isotopic singular link cobordisms.",0.5925925926],["specificity prediction systems predict very coarse labels. the aim of this work is to generalize","Domain Agnostic Real-Valued Specificity Prediction","summarize: Sentence specificity quantifies the level of detail in a sentence, characterizing the organization of information in discourse. While this information is useful for many downstream applications, specificity prediction systems predict very coarse labels and are trained on and tailored toward specific domains . The goal of this work is to generalize specificity prediction to domains where no labeled data is available and output more nuanced real-valued specificity ratings. We present an unsupervised domain adaptation system for sentence specificity prediction, specifically designed to output real-valued estimates from binary training labels. To calibrate the values of these predictions appropriately, we regularize the posterior distribution of the labels towards a reference distribution. We show that our framework generalizes well to three different domains with 50%~68% mean absolute error reduction than the current state-of-the-art system trained for news sentence specificity. We also demonstrate the potential of our work in improving the quality and informativeness of dialogue generation systems.",0.0],["a graph representation will accelerate breadth-first search based on sparse-mat","SlimSell: A Vectorizable Graph Representation for Breadth-First Search","summarize: Vectorization and GPUs will profoundly change graph processing. Traditional graph algorithms tuned for 32- or 64-bit based memory accesses will be inefficient on architectures with 512-bit wide instruction units that are already present in the Intel Knights Landing manycore CPU. Anticipating this shift, we propose SlimSell: a vectorizable graph representation to accelerate Breadth-First Search based on sparse-matrix dense-vector products. SlimSell extends and combines the state-of-the-art SIMD-friendly Sell-C-sigma matrix storage format with tropical, real, boolean, and sel-max semiring operations. The resulting design reduces the necessary storage and thus pressure on the memory subsystem. We augment SlimSell with the SlimWork and SlimChunk schemes that reduce the amount of work and improve load balance, further accelerating BFS. We evaluate all the schemes on Intel Haswell multicore CPUs, the state-of-the-art Intel Xeon Phi KNL manycore CPUs, and NVIDIA Tesla GPUs. Our experiments indicate which semiring offers highest speedups for BFS and illustrate that SlimSell accelerates a tuned Graph500 BFS code by up to 33%. This work shows that vectorization can secure high-performance in BFS based on SpMV products; the proposed principles and designs can be extended to other graph algorithms.",0.3],["tin-selenide and tin-sulfide classes undergo","Electronic, vibrational, and electron-phonon coupling properties in SnSe","summarize: The tin-selenide and tin-sulfide classes of materials undergo multiple structural transitions under high pressure leading to periodic lattice distortions, superconductivity, and topologically non-trivial phases, yet a number of controversies exist regarding the structural transformations in these systems. We perform first-principles calculations within the framework of density functional theory and a careful comparison of our results with available experiments on SnSe",0.1097623272],["byte-addressable non-volatile memory features high density, D","System Evaluation of the Intel Optane Byte-addressable NVM","summarize: Byte-addressable non-volatile memory features high density, DRAM comparable performance, and persistence. These characteristics position NVM as a promising new tier in the memory hierarchy. Nevertheless, NVM has asymmetric read and write performance, and considerably higher write energy than DRAM. Our work provides an in-depth evaluation of the first commercially available byte-addressable NVM -- the Intel Optane DC persistent memory. The first part of our study quantifies the latency, bandwidth, power efficiency, and energy consumption under eight memory configurations. We also evaluate the real impact on in-memory graph processing workloads. Our results show that augmenting NVM with DRAM is essential, and the combination can effectively bridge the performance gap and provide reasonable performance with higher capacity. We also identify NUMA-related performance characteristics for accesses to memory on a remote socket. In the second part, we employ two fine-grained allocation policies to control traffic distribution between DRAM and NVM. Our results show that bandwidth spilling between DRAM and NVM could provide 2.0x bandwidth and enable ",0.0],["smallest eigenvalue of Wishart-Laguerre ensemble is expressible in terms","Recursion for the smallest eigenvalue density of ","summarize: The statistics of the smallest eigenvalue of Wishart-Laguerre ensemble is important from several perspectives. The smallest eigenvalue density is typically expressible in terms of determinants or Pfaffians. These results are of utmost significance in understanding the spectral behavior of Wishart-Laguerre ensembles and, among other things, unveil the underlying universality aspects in the asymptotic limits. However, obtaining exact and explicit expressions by expanding determinants or Pfaffians becomes impractical if large dimension matrices are involved. For the real matrices Edelman has provided an efficient recurrence scheme to work out exact and explicit results for the smallest eigenvalue density which does not involve determinants or matrices. Very recently, an analogous recurrence scheme has been obtained for the complex matrices . In the present work we extend this to ",0.3],["SIS logistic epidemic model is used to model spread of an infectious disease. a stoc","Extinction times in the subcritical stochastic SIS logistic epidemic","summarize: Many real epidemics of an infectious disease are not straightforwardly super- or sub-critical, and the understanding of epidemic models that exhibit such complexity has been identified as a priority for theoretical work. We provide insights into the near-critical regime by considering the stochastic SIS logistic epidemic, a well-known birth-and-death chain used to model the spread of an epidemic within a population of a given size ",0.347826087],["a graph of the graph shows the sex of the sex of the","Perfect State Transfer in Laplacian Quantum Walk","summarize: For a graph ",0.0810810811],["we propose two conditions, isomeric condition and relative well-conditionedness. the proposed","Matrix Completion with Deterministic Sampling: Theories and Methods","summarize: In some significant applications such as data forecasting, the locations of missing entries cannot obey any non-degenerate distributions, questioning the validity of the prevalent assumption that the missing data is randomly chosen according to some probabilistic model. To break through the limits of random sampling, we explore in this paper the problem of real-valued matrix completion under the setup of deterministic sampling. We propose two conditions, isomeric condition and relative well-conditionedness, for guaranteeing an arbitrary matrix to be recoverable from a sampling of the matrix entries. It is provable that the proposed conditions are weaker than the assumption of uniform sampling and, most importantly, it is also provable that the isomeric condition is necessary for the completions of any partial matrices to be identifiable. Equipped with these new tools, we prove a collection of theorems for missing data recovery as well as convex\/nonconvex matrix completion. Among other things, we study in detail a Schatten quasi-norm induced method termed isomeric dictionary pursuit , and we show that IsoDP exhibits some distinct behaviors absent in the traditional bilinear programs.",0.0666666667],["CS methods always recover the scene images in pixel level. this causes the smoothness","Perceptual Compressive Sensing","summarize: Compressive sensing works to acquire measurements at sub-Nyquist rate and recover the scene images. Existing CS methods always recover the scene images in pixel level. This causes the smoothness of recovered images and lack of structure information, especially at a low measurement rate. To overcome this drawback, in this paper, we propose perceptual CS to obtain high-level structured recovery. Our task no longer focuses on pixel level. Instead, we work to make a better visual effect. In detail, we employ perceptual loss, defined on feature level, to enhance the structure information of the recovered images. Experiments show that our method achieves better visual results with stronger structure information than existing CS methods at the same measurement rate.",0.0625],["econometric model shifts day-ahead supply curves to calculate intraday","The Impact of Renewable Energy Forecasts on Intraday Electricity Prices","summarize: In this paper we study the impact of errors in wind and solar power forecasts on intraday electricity prices. We develop a novel econometric model which is based on day-ahead wholesale auction curves data and errors in wind and solar power forecasts. The model shifts day-ahead supply curves to calculate intraday prices. We apply our model to the German EPEX SPOT SE data. Our model outperforms both linear and non-linear benchmarks. Our study allows us to conclude that errors in renewable energy forecasts exert a non-linear impact on intraday prices. We demonstrate that additional wind and solar power capacities induce non-linear changes in the intraday price volatility. Finally, we comment on economical and policy implications of our findings.",0.0],["the two approaches each have unique advantages but are hard to reconcile. applying a similar","Interpreting Frame Transformations as Diagonalization of Harmonic Transfer Functions","summarize: Analysis of ac electrical systems can be performed via frame transformations in the time-domain or via harmonic transfer functions in the frequency-domain. The two approaches each have unique advantages but are hard to reconcile because the coupling effect in the frequency-domain leads to infinite dimensional HTF matrices that need to be truncated. This paper explores the relation between the two representations and shows that applying a similarity transformation to an HTF matrix creates a direct equivalence to a frame transformation on the input-output signals. Under certain conditions, such similarity transformations have a diagonalizing effect which, essentially, reduces the HTF matrix order from infinity to two or one, making the matrix tractable mathematically without truncation or approximation. This theory is applied to a droop-controlled voltage source inverter as an illustrative example. A stability criterion is derived in the frequency-domain which agrees with the conventional state-space model but offers greater insights into the mechanism of instability in terms of the negative damping under droop control. The paper not only establishes a unified view in theory but also offers an effective practical tool for stability assessment.",0.3461538462],["CYGNO will use a gaseous TPC with optical readout to detect","CYGNO: a gaseous TPC with optical readout for dark matter directional search","summarize: The CYGNO project has the goal to use a gaseous TPC with optical readout to detect dark matter and solar neutrinos with low energy threshold and directionality. The CYGNO demonstrator will consist of 1 m 3 volume filled with He:CF 4 gas mixture at atmospheric pressure. Optical readout with high granularity CMOS sensors, combined with fast light detectors, will provide a detailed reconstruction of the event topology. This will allow to discriminate the nuclear recoil signal from the background, mainly represented by low energy electron recoils induced by radioactivity. Thanks to the high reconstruction efficiency, CYGNO will be sensitive to low mass dark matter, and will have the potential to overcome the neutrino floor, that ultimately limits non-directional dark matter searches.",0.6522147973],["we have implemented a platform that integrates DLTs with a monitoring system based","Trusted Wireless Monitoring based on Blockchain over NB-IoT Connectivity","summarize: The data collected from Internet of Things devices on various emissions or pollution, can have a significant economic value for the stakeholders. This makes it prone to abuse or tampering and brings forward the need to integrate IoT with a Distributed Ledger Technology to collect, store, and protect the IoT data. However, DLT brings an additional overhead to the frugal IoT connectivity and symmetrizes the IoT traffic, thus changing the usual assumption that IoT is uplink-oriented. We have implemented a platform that integrates DLTs with a monitoring system based on narrowband IoT . We evaluate the performance and discuss the tradeoffs in two use cases: data authorization and real-time monitoring.",0.2],["Let us know what you think about it!","On Monotonous Separately Continuous Functions","summarize: Let ",0.125],["sphere packing bound with a prefactor that is polynomial in the block length","The Sphere Packing Bound via Augustin's Method","summarize: A sphere packing bound with a prefactor that is polynomial in the block length ",0.2222222222],["turbulence is a paradigm for far-from-equilibrium systems without","Potential landscape and flux field theory for turbulence and nonequilibrium fluid systems","summarize: Turbulence is a paradigm for far-from-equilibrium systems without time reversal symmetry. To capture the nonequilibrium irreversible nature of turbulence and investigate its implications, we develop a potential landscape and flux field theory for turbulent flow and more general nonequilibrium fluid systems governed by stochastic Navier-Stokes equations. We find that equilibrium fluid systems with time reversibility are characterized by a detailed balance constraint that quantifies the detailed balance condition. In nonequilibrium fluid systems with nonequilibrium steady states, detailed balance breaking leads directly to a pair of interconnected consequences, namely, the non-Gaussian potential landscape and the irreversible probability flux, forming a 'nonequilibrium trinity'. The nonequilibrium trinity characterizes the nonequilibrium irreversible essence of fluid systems with intrinsic time irreversibility and is manifested in various aspects of these systems. The nonequilibrium stochastic dynamics of fluid systems including turbulence with detailed balance breaking is shown to be driven by both the non-Gaussian potential landscape gradient and the irreversible probability flux, together with the reversible convective force and the stochastic stirring force. We reveal an underlying connection of the energy flux essential for turbulence energy cascade to the irreversible probability flux and the non-Gaussian potential landscape generated by detailed balance breaking. Using the energy flux as a center of connection, we demonstrate that the four-fifths law in fully developed turbulence is a consequence and reflection of the nonequilibrium trinity. We also show how the nonequilibrium trinity can affect the scaling laws in turbulence.",0.3859740562],["network traffic forensic plays an important role in cybercrime investigation. we used a","Network Traffic Forensics on Firefox Mobile OS: Facebook, Twitter and Telegram as Case Studies","summarize: Development of mobile web-centric OS such as Firefox OS has created new challenges, and opportunities for digital investigators. Network traffic forensic plays an important role in cybercrime investigation to detect subject and object of the crime. In this chapter, we detect and analyze residual network traffic artefacts of Firefox OS in relation to two popular social networking applications and one instant messaging application . We utilized a Firefox OS simulator to generate relevant traffic while all communication data were captured using network monitoring tools. Captured network packets were examined and remnants with forensic value were reported. This paper as the first focused study on mobile Firefox OS network traffic analysis should pave the way for the future research in this direction.",0.3240863775],["fluorescein-a fluorescent marker used as a proxy for drug molecules.","Quantitative Kinetic Models from Intravital Microcopy: A Case Study Using Hepatic Transport","summarize: The liver performs critical physiological functions, including metabolizing and removing substances, such as toxins and drugs, from the bloodstream. Hepatotoxicity itself is intimately linked to abnormal hepatic transport and hepatotoxicity remains the primary reason drugs in development fail and approved drugs are withdrawn from the market. For this reason, we propose to analyze, across liver compartments, the transport kinetics of fluorescein-a fluorescent marker used as a proxy for drug molecules-using intravital microscopy data. To resolve the transport kinetics quantitatively from fluorescence data, we account for the effect that different liver compartments have on fluorescein's emission rate. To do so, we develop ordinary differential equation transport models from the data where the kinetics are related to the observable fluorescence levels by measurement parameters that vary across different liver compartments. On account of the steep non-linearities in the kinetics and stochasticity inherent to the model, we infer kinetic and measurement parameters by generalizing the method of parameter cascades. For this application, the method of parameter cascades ensures fast and precise parameter estimates from noisy time traces.",0.3148964435],["a macroscopic theory of interfacial interactions targeting terrestrial applications has been developed","Exploratory numerical experiments with a macroscopic theory of interfacial interactions","summarize: Phenomenological theories of interfacial interactions have targeted terrestrial applications since long time and their exploitation has inspired our research programme to build up a macroscopic theory of gas-surface interactions targeting the complex phenomenology of hypersonic reentry flows as alternative to standard methods based on accommodation coefficients. The objective of this paper is the description of methods employed and results achieved in an exploratory study, that is, the unsteady heat transfer between two solids in contact with and without interface. It is a simple numerical-demonstrator test case designed to facilitate quick numerical calculations and to bring forth already sufficiently meaningful aspects relevant to thermal protection due to the formation of the interface. The paper begins with a brief introduction on the subject matter and a review of relevant literature. Then the case is considered in which the interface is absent. The importance of tension continuity as boundary condition on the same footing of heat-flux continuity is recognised and the role of the former in governing the establishment of the temperature-difference distribution over the separation surface is explicitly shown. Evidence is given that the standard temperature-continuity boundary condition is just a particular case. Subsequently the case in which the interface is formed between the solids is analysed. The coupling among the heat-transfer equations applicable in the solids and the balance equation for the surface thermodynamic energy formulated in terms of the surface temperature is discussed. Results are illustrated for planar and cylindrical configuration; they show unequivocally that the thermal-protection action of the interface turns out to be driven exclusively by thermophysical properties of the solids and of the interface; accommodation coefficients are not needed.",0.5714285714],["the Bethe-Salpeter eigenvalue problem is a dense structured","Structure Preserving Parallel Algorithms for Solving the Bethe-Salpeter Eigenvalue Problem","summarize: The Bethe-Salpeter eigenvalue problem is a dense structured eigenvalue problem arising from discretized Bethe-Salpeter equation in the context of computing exciton energies and states. A computational challenge is that at least half of the eigenvalues and the associated eigenvectors are desired in practice. We establish the equivalence between Bethe-Salpeter eigenvalue problems and real Hamiltonian eigenvalue problems. Based on theoretical analysis, structure preserving algorithms for a class of Bethe-Salpeter eigenvalue problems are proposed. We also show that for this class of problems all eigenvalues obtained from the Tamm-Dancoff approximation are overestimated. In order to solve large scale problems of practical interest, we discuss parallel implementations of our algorithms targeting distributed memory systems. Several numerical examples are presented to demonstrate the efficiency and accuracy of our algorithms.",0.4248004271],["end-to-end acoustic speech recognition has gained widespread popularity. but","Fusing information streams in end-to-end audio-visual speech recognition","summarize: End-to-end acoustic speech recognition has quickly gained widespread popularity and shows promising results in many studies. Specifically the joint transformer\/CTC model provides very good performance in many tasks. However, under noisy and distorted conditions, the performance still degrades notably. While audio-visual speech recognition can significantly improve the recognition rate of end-to-end models in such poor conditions, it is not obvious how to best utilize any available information on acoustic and visual signal quality and reliability in these models. We thus consider the question of how to optimally inform the transformer\/CTC model of any time-variant reliability of the acoustic and visual information streams. We propose a new fusion strategy, incorporating reliability information in a decision fusion net that considers the temporal effects of the attention mechanism. This approach yields significant improvements compared to a state-of-the-art baseline model on the Lip Reading Sentences 2 and 3 corpus. On average, the new system achieves a relative word error rate reduction of 43% compared to the audio-only setup and 31% compared to the audiovisual end-to-end baseline.",0.3333333333],["the lid moves parallel to the cube sidewall or parallel to the diagonal plane. the","Linear instability of the lid-driven flow in a cubic cavity","summarize: Primary instability of the lid-driven flow in a cube is studied by a comprehensive linear stability approach. Two cases, in which the lid moves parallel to the cube sidewall or parallel to the diagonal plane, are considered. The SIMPLE procedure is applied for evaluation of the Krylov vectors needed for application of the Newton and Arnoldi iteration methods. The finite volume grid is gradually refined from 1003 to 2563 nodes. The computations result in the grid convergent values of the critical Reynolds number and oscillation frequency. Patterns of the most unstable perturbation are reported. Finally, some new arguments supporting the assumption that the centrifugal mechanism triggers instability in both cases are given.",0.1612903226],["model is a matching model for response selection in retrieval-based dialogue systems. the","Learning a Matching Model with Co-teaching for Multi-turn Response Selection in Retrieval-based Dialogue Systems","summarize: We study learning of a matching model for response selection in retrieval-based dialogue systems. The problem is equally important with designing the architecture of a model, but is less explored in existing literature. To learn a robust matching model from noisy training data, we propose a general co-teaching framework with three specific teaching strategies that cover both teaching with loss functions and teaching with data curriculum. Under the framework, we simultaneously learn two matching models with independent training sets. In each iteration, one model transfers the knowledge learned from its training set to the other model, and at the same time receives the guide from the other model on how to overcome noise in training. Through being both a teacher and a student, the two models learn from each other and get improved together. Evaluation results on two public data sets indicate that the proposed learning approach can generally and significantly improve the performance of existing matching models.",0.3703844315],["computer vision and pattern recognition techniques have gained traction for such inspection and defect detection tasks.","FaultNet: Faulty Rail-Valves Detection using Deep Learning and Computer Vision","summarize: Regular inspection of rail valves and engines is an important task to ensure the safety and efficiency of railway networks around the globe. Over the past decade, computer vision and pattern recognition based techniques have gained traction for such inspection and defect detection tasks. An automated end-to-end trained system can potentially provide a low-cost, high throughput, and cheap alternative to manual visual inspection of these components. However, such systems require a huge amount of defective images for networks to understand complex defects. In this paper, a multi-phase deep learning based technique is proposed to perform accurate fault detection of rail-valves. Our approach uses a two-step method to perform high precision image segmentation of rail-valves resulting in pixel-wise accurate segmentation. Thereafter, a computer vision technique is used to identify faulty valves. We demonstrate that the proposed approach results in improved detection performance when compared to current state-of-theart techniques used in fault detection.",0.1111111111],["quantitative graph theory can be used for novel applications in network science. the main goal of quantitative","Quantitative Graph Theory: A new branch of graph theory and in network science","summarize: In this paper, we describe and argue it is a new graph-theoretical branch in network science, however, with significant different features compared to classical graph theory. The main goal of quantitative graph theory is the structural quantification of information contained in complex networks by employing a based on numerical invariants and comparisons. Furthermore, the methods as well as the networks do not need to be deterministic but can be statistic. As such this complements the field of classical graph theory, which is descriptive and deterministic in nature. We provide examples of how quantitative graph theory can be used for novel applications in the context of the overarching concept network science.",0.2857142857],["system identification is a key enabling component for the implementation of quantum technologies. we consider","System identification for passive linear quantum systems","summarize: System identification is a key enabling component for the implementation of quantum technologies, including quantum control. In this paper, we consider the class of passive linear input-output systems, and investigate several basic questions: which parameters can be identified? Given sufficient input-output data, how do we reconstruct the system parameters? How can we optimize the estimation precision by preparing appropriate input states and performing measurements on the output? We show that minimal systems can be identified up to a unitary transformation on the modes, and systems satisfying a Hamiltonian connectivity condition called infecting are completely identifiable. We propose a frequency domain design based on a Fisher information criterion, for optimizing the estimation precision for coherent input state. As a consequence of the unitarity of the transfer function, we show that the Heisenberg limit with respect to the input energy can be achieved using non-classical input states.",0.4210526316],["the balance function measures the correlation between opposite sign charge pairs. the study of the balance function","Reaction Plane and Beam Energy Dependence Of The Balance Function at RHIC","summarize: The balance function, which measures the correlation between opposite sign charge pairs, is sensitive to the mechanisms of charge formation and the subsequent relative diffusion of the balancing charges. The study of the balance function can provide information about charge creation time as well as the subsequent collective behavior of particles. In this paper, we present a reaction-plane-dependent balance function study for Au+Au collisions at ",0.0],["energy-constrained sensor nodes can adaptively optimize their energy consumption. eco is tailored","Eco: A Hardware-Software Co-Design for In Situ Power Measurement on Low-end IoT Systems","summarize: Energy-constrained sensor nodes can adaptively optimize their energy consumption if a continuous measurement exists. This is of particular importance in scenarios of high dynamics such as energy harvesting or adaptive task scheduling. However, self-measuring of power consumption at reasonable cost and complexity is unavailable as a generic system service. In this paper, we present Eco, a hardware-software co-design enabling generic energy management on IoT nodes. Eco is tailored to devices with limited resources and thus targets most of the upcoming IoT scenarios. The proposed measurement module combines commodity components with a common system interfaces to achieve easy, flexible integration with various hardware platforms and the RIOT IoT operating system. We thoroughly evaluate and compare accuracy and overhead. Our findings indicate that our commodity design competes well with highly optimized solutions, while being significantly more versatile. We employ Eco for energy management on RIOT and validate its readiness for deployment in a five-week field trial integrated with energy harvesting.",0.0],["quantum Internet requires knowledge about link qualities used for optimal route selection. ruleSet-based protocol","Simulation of a Dynamic, RuleSet-based Quantum Network","summarize: Similar to the classical Internet, the quantum Internet will require knowledge regarding link qualities used for purposes such as optimal route selection. This is commonly accomplished by performing link-level tomography with or without purification -- a.k.a. quantum link bootstrapping. Meanwhile, the gate selection and the resource selection for a task must be coordinated beforehand. This thesis introduces the RuleSet-based communication protocol aimed for supporting the autonomous coordination of quantum operations among distant nodes, with minimal classical packet transmission. This thesis also discusses the RuleSet-based quantum link bootstrapping protocol, which consists of recurrent purifications and link-level tomography, evaluated over a Markov-Chain Monte-Carlo simulation with noisy systems modeled on real world quality hardware. Given a 10km MeetInTheMiddle based two-node system, each with 100 memory qubits ideally connected to the optical fiber, the Recurrent Single selection - Single error purification protocol is capable of improving the fidelity from an average input ",0.0],["we study generalised linear regression and classification for a synthetically generated dataset. we consider","Generalisation error in learning with random features and the hidden manifold model","summarize: We study generalised linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning with random features, neural networks in the lazy training regime, and the hidden manifold model. We consider the high-dimensional regime and using the replica method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalised linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks.",0.32],["byte-addressable non-volatile memory features high density, D","System Evaluation of the Intel Optane Byte-addressable NVM","summarize: Byte-addressable non-volatile memory features high density, DRAM comparable performance, and persistence. These characteristics position NVM as a promising new tier in the memory hierarchy. Nevertheless, NVM has asymmetric read and write performance, and considerably higher write energy than DRAM. Our work provides an in-depth evaluation of the first commercially available byte-addressable NVM -- the Intel Optane DC persistent memory. The first part of our study quantifies the latency, bandwidth, power efficiency, and energy consumption under eight memory configurations. We also evaluate the real impact on in-memory graph processing workloads. Our results show that augmenting NVM with DRAM is essential, and the combination can effectively bridge the performance gap and provide reasonable performance with higher capacity. We also identify NUMA-related performance characteristics for accesses to memory on a remote socket. In the second part, we employ two fine-grained allocation policies to control traffic distribution between DRAM and NVM. Our results show that bandwidth spilling between DRAM and NVM could provide 2.0x bandwidth and enable ",0.0],["a graph of geometric shapes is used to compute geometrical geometric shapes. we compute geometric","Wick rotations, Eichler integrals, and multi-loop Feynman diagrams","summarize: Using contour deformations and integrations over modular forms, we compute certain Bessel moments arising from diagrammatic expansions in two-dimensional quantum field theory. We evaluate these Feynman integrals as either explicit constants or critical values of modular ",0.15625],["applications and Internet service providers are increasingly adopting micro-services application architecture. despite","Exploring Micro-Services for Enhancing Internet QoS","summarize: With the enhancements in the field of software-defined networking and virtualization technologies, novel networking paradigms such as network function virtualization and the Internet of things are rapidly gaining ground. Development of IoT as well as 5G networks and explosion in online services has resulted in an exponential growth of devices connected to the network. As a result, application service providers and Internet service providers are being confronted with the unprecedented challenge of accommodating increasing service and traffic demands from the geographically distributed users. To tackle this problem, many ASPs and ISPs, such as Netflix, Facebook, AT&T and others are increasingly adopting micro-services application architecture. Despite the success of MS in the industry, there is no specific standard or research work for service providers as guidelines, especially from the perspective of basic micro-service operations. In this work, we aim to bridge this gap between industry and academia and discuss different micro-service deployment, discovery and communication options for service providers as a means to forming complete service chains. In addition, we address the problem of scheduling micro-services across multiple clouds, including micro-clouds. We consider different user-level SLAs, such as latency and cost, while scheduling such services. We aim to reduce overall turnaround time as well as costs for the deployment of complete end-to-end service. In this work, we present a novel affinity-based fair weighted scheduling heuristic to solve this problem. We also compare the results of proposed solution with standard greedy scheduling algorithms presented in the literature and observe significant improvements.",0.0714285714],["electroencephalogram based brain-computer interface speller allows a user to input text","Tiny noise, big mistakes: Adversarial perturbations induce errors in Brain-Computer Interface spellers","summarize: An electroencephalogram based brain-computer interface speller allows a user to input text to a computer by thought. It is particularly useful to severely disabled individuals, e.g., amyotrophic lateral sclerosis patients, who have no other effective means of communication with another person or a computer. Most studies so far focused on making EEG-based BCI spellers faster and more reliable; however, few have considered their security. This study, for the first time, shows that P300 and steady-state visual evoked potential BCI spellers are very vulnerable, i.e., they can be severely attacked by adversarial perturbations, which are too tiny to be noticed when added to EEG signals, but can mislead the spellers to spell anything the attacker wants. The consequence could range from merely user frustration to severe misdiagnosis in clinical applications. We hope our research can attract more attention to the security of EEG-based BCI spellers, and more broadly, EEG-based BCIs, which has received little attention before.",0.3759826479],["handwritten annotations can appear in form of underlines and text. the best model achieve","Recognizing Challenging Handwritten Annotations with Fully Convolutional Networks","summarize: This paper introduces a very challenging dataset of historic German documents and evaluates Fully Convolutional Neural Network based methods to locate handwritten annotations of any kind in these documents. The handwritten annotations can appear in form of underlines and text by using various writing instruments, e.g., the use of pencils makes the data more challenging. We train and evaluate various end-to-end semantic segmentation approaches and report the results. The task is to classify the pixels of documents into two classes: background and handwritten annotation. The best model achieves a mean Intersection over Union score of 95.6% on the test documents of the presented dataset. We also present a comparison of different strategies used for data augmentation and training on our presented dataset. For evaluation, we use the Layout Analysis Evaluator for the ICDAR 2017 Competition on Layout Analysis for Challenging Medieval Manuscripts.",0.1875],["study examines the effect of different feedback modalities in a table setting robot assistant for","Feedback modalities for a table setting robot assistant for elder care","summarize: The interaction of Older adults with robots requires effective feedback to keep them aware of the state of the interaction for optimum interaction quality. This study examines the effect of different feedback modalities in a table setting robot assistant for elder care. Two different feedback modalities and their combination were evaluated for three complexity levels. The visual feedback included the use of LEDs and a GUI screen. The auditory feedback included alerts and verbal commands. The results revealed that the quality of interaction was influenced mainly by the feedback modality, and complexity had less influence. The verbal feedback was significantly preferable and increased the involvement of the participants during the experiment. The combination of LED lights and verbal commands increased participants' understanding contributing to the quality of interaction.",0.6086956522],["phase retrieval problems occur in a wide range of applications in physics and engineering such as","Non-negativity constraints in the one-dimensional discrete-time phase retrieval problem","summarize: Phase retrieval problems occur in a width range of applications in physics and engineering such as crystallography, astronomy, and laser optics. Common to all of them is the recovery of an unknown signal from the intensity of its Fourier transform. Because of the well-known ambiguousness of these problems, the determination of the original signal is generally challenging. Although there are many approaches in the literature to incorporate the assumption of non-negativity of the solution into numerical algorithms, theoretical considerations about the solvability with this constraint occur rarely. In this paper, we consider the one-dimensional discrete-time setting and investigate whether the usually applied a priori non-negativity can overcame the ambiguousness of the phase retrieval problem or not. We show that the assumed non-negativity of the solution is usually not a sufficient a priori condition to ensure uniqueness in one-dimensional phase retrieval. More precisely, using an appropriate characterization of the occurring ambiguities, we show that neither the uniqueness nor the ambiguousness are rare exceptions.",0.4],["the mechanical properties of Mg-4wt.% Zn alloy single crystals along the","Precipitate strengthening of pyramidal slip in Mg-Zn alloys","summarize: The mechanical properties of Mg-4wt.% Zn alloy single crystals along the orientation were measured through micropillar compression at 23C and 100C. Basal slip was dominant in the solution treated alloy, while pyramidal slip occurred in the precipitation hardened alloy. Pyramidal dislocations pass the precipitates by forming Orowan loops, leading to homogeneous deformation and to a strong hardening. The predictions of the yield stress based on the Orowan model were in reasonable agreement with the experimental data. The presence of rod-shape precipitates perpendicular to the basal plane leads to a strong reduction in the plastic anisotropy of Mg.",0.3846153846],["proposed formulation is consistent with reservoir-type models of the MFD literature. proposed multi-","A Continuum Model for Cities Based on the Macroscopic Fundamental Diagram: a Semi-Lagrangian Solution Method","summarize: This paper presents a formulation of the reactive dynamic user equilibrium problem in continuum form using a network-level Macroscopic Fundamental Diagram . Compared to existing continuum models for cities -- all based in Hughes' pedestrian model in 2002 -- the proposed formulation is consistent with reservoir-type models of the MFD literature, shedding some light into the connection between these two modeling approaches, can have destinations continuously distributed on the region, and can incorporate multi-commodity flows without additional numerical error. The proposed multi-reservoir numerical solution method treats the multi-commodity component of the model in Lagrangian coordinates, which is the natural representation to propagate origin-destination information through the traffic stream. Fluxes between reservoir boundaries are computed in the Eulerian representation, and are used to calculate the speed of vehicles crossing the boundary. Simple examples are included that show the convergence of the model and its agreements with the available analytical solutions. We find that when origins and destinations are uniformly distributed in a region, the distribution of the travel times can be approximated analytically, the magnitude of the detours from the optimal free-flow route due to congestion increase linearly with the inflow and decreases with the square of the speed, and the total delay of vehicles in the network converges to the analytical approximation when the size of reservoirs tends to zero.",0.0535877449],["we study the behaviour of the Laplacian flow evolving closed G.","Closed warped G","summarize: We study the behaviour of the Laplacian flow evolving closed G",0.0],["recent detections of C","Are fulleranes responsible for the 21 micron feature?","summarize: Recent detections of C",0.0],["Lie groupoids are a regular groupoid. the vertical","The Lie group of vertical bisections of a regular Lie groupoid","summarize: In this note we construct an infinite-dimensional Lie group structure on the group of vertical bisections of a regular Lie groupoid. We then identify the Lie algebra of this group and discuss regularity properties for these Lie groups. If the groupoid is locally trivial, i.e. a gauge groupoid, the vertical bisections coincide with the gauge group of the underlying bundle. Hence the construction recovers the well known Lie group structure of the gauge groups. To establish the Lie theoretic properties of the vertical bisections of a Lie groupoid over a non-compact base, we need to generalise the Lie theoretic treatment of Lie groups of bisections for Lie groupoids over non-compact bases.",0.3436446394],["a network of two nodes separated by a noisy channel with two-sided state information","Strong coordination of signals and actions over noisy channels with two-sided state information","summarize: We consider a network of two nodes separated by a noisy channel with two-sided state information, in which the input and output signals have to be coordinated with the source and its reconstruction. In the case of non-causal encoding and decoding, we propose a joint source-channel coding scheme and develop inner and outer bounds for the strong coordination region. While the inner and outer bounds do not match in general, we provide a complete characterization of the strong coordination region in three particular cases: i) when the channel is perfect; ii) when the decoder is lossless; and iii) when the random variables of the channel are independent from the random variables of the source. Through the study of these special cases, we prove that the separation principle does not hold for joint source-channel strong coordination. Finally, in the absence of state information, we show that polar codes achieve the best known inner bound for the strong coordination region, which therefore offers a constructive alternative to random binning and coding proofs.",0.7857142857],["a mixed binary binary of a neutron star and a black hole can account for","Distinguishing binary neutron star from neutron star-black hole mergers with gravitational waves","summarize: The gravitational-wave signal from the merger of two neutron stars cannot be easily differentiated from the signal produced by a comparable-mass mixed binary of a neutron star and a black hole. Indeed, both binary types can account for the gravitational-wave signal GW170817 even if its electromagnetic counterpart emission is taken into account. We propose a method that to identify mixed binaries of neutron stars merging with low-mass black holes using gravitational-waves alone. This method is based on the fact that certain neutron star properties that can be measured with gravitational-waves are common or similar for all neutron stars. For example all neutron stars share the same equation of state and if the latter is hadronic, neutron stars have similar radii. If a mixed binary is misidentified as a neutron star binary, the inferred neutron star properties will be misestimated and appear as outliers in a population of low-mass binaries. We show that as few as ",0.6818181818],["the classic Alternating Direction Method of Multipliers is a popular framework to solve linear","Nonconvex Generalization of Alternating Direction Method of Multipliers for Nonlinear Equality Constrained Problems","summarize: The classic Alternating Direction Method of Multipliers is a popular framework to solve linear-equality constrained problems. In this paper, we extend the ADMM naturally to nonlinear equality-constrained problems, called neADMM. The difficulty of neADMM is to solve nonconvex subproblems. We provide globally optimal solutions to them in two important applications. Experiments on synthetic and real-world datasets demonstrate excellent performance and scalability of our proposed neADMM over existing state-of-the-start methods.",0.6315789474],["the methods are: echo state network, deep feed-forward artificial neural network, and","Data-driven prediction of a multi-scale Lorenz 96 chaotic system using deep learning methods: Reservoir computing, ANN, and RNN-LSTM","summarize: In this paper, the performance of three deep learning methods for predicting short-term evolution and for reproducing the long-term statistics of a multi-scale spatio-temporal Lorenz 96 system is examined. The methods are: echo state network , deep feed-forward artificial neural network , and recurrent neural network with long short-term memory . This Lorenz 96 system has three tiers of nonlinearly interacting variables representing slow\/large-scale , intermediate , and fast\/small-scale processes. For training or testing, only ",0.1299708557],["the sex of the u.s.","A comparison of the innate flexibilities of six chains in F","summarize: The ",0.1290832337],["new model-based reinforcement learning algorithm uses supervision to constrain exploration and learn efficiently while handling complex constraints","Safety Augmented Value Estimation from Demonstrations : Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","summarize: Reinforcement learning for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations , which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations. Code and supplementary material is available at https:\/\/tinyurl.com\/saved-rl.",0.0],["the FAIR facility is an international accelerator centre for research with ion and antiprot","Twin GEM-TPC Prototype Beam Test at GSI and Jyv\\askyl\\a - a Development for the Super-FRS at FAIR","summarize: The FAIR facility is an international accelerator centre for research with ion and antiproton beams. It is being built at Darmstadt, Germany as an extension to the current GSI research institute. One major part of the facility will be the Super-FRS separator, which will be include in phase one of the project construction. The NUSTAR experiments will benefit from the Super-FRS, which will deliver an unprecedented range of radioactive ion beams . These experiments will use beams of different energies and characteristics in three different branches; the high-energy which utilizes the RIB at relativistic energies 300-1500 MeV\/u as created in the production process, the low-energy branch aims to use beams in the range of 0-150 MeV\/u whereas the ring branch will cool and store beams in the NESR ring. The main tasks for the Super-FRS beam diagnostics chambers will be for the set up and adjustment of the separator as well as to provide tracking and event-by-event particle identification. The Helsinki Institute of Physics, and the Detector Laboratory and Experimental Electronics at GSI are in a joint R&D of a GEM-TPC detector which could satisfy the requirements of such tracking detectors, in terms of tracking efficiency, space resolution, count rate capability and momenta resolution. The current prototype, which is the generation four of this type, is two GEM-TPCs in twin configuration inside the same vessel. This means that one of the GEM-TPC is flipped on the middle plane w.r.t. the other one. This chamber was tested at Jyv\\askyl\\a accelerator with protons projectiles and at GSI with Uranium, fragments and Carbon beams during this year 2016.",0.2373875726],["hazard function of empirical duration data is dominated by a bathtub curve.","Modelling the Dropout Patterns of MOOC Learners","summarize: We adopted survival analysis for the viewing durations of massive open online courses. The hazard function of empirical duration data is dominated by a bathtub curve and has the Lindy effect in its tail. To understand the evolutionary mechanisms underlying these features, we categorized learners into two classes due to their different distributions of viewing durations, namely lognormal distribution and power law with exponential cutoff. Two random differential equations are provided to describe the growth patterns of viewing durations for the two classes respectively. The expected duration change rate of the learners featured by lognormal distribution is supposed to be dependent on their past duration, and that of the rest learners is supposed to be inversely proportional to time. Solutions to the equations predict the features of viewing duration distributions, and those of the hazard function. The equations also reveal the feature of memory and that of memorylessness for the viewing behaviors of the two classes respectively.",0.15],["a number of works have observed that Convolutional Neural Nets are invertible","Towards Understanding the Invertibility of Convolutional Neural Networks","summarize: Several recent works have empirically observed that Convolutional Neural Nets are invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable re- construction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.",0.4705882353],["learned models can achieve significant performance gains over traditional methods. this special issue covers the state of","Editorial: Introduction to the Issue on Deep Learning for Image\/Video Restoration and Compression","summarize: Recent works have shown that learned models can achieve significant performance gains, especially in terms of perceptual quality measures, over traditional methods. Hence, the state of the art in image restoration and compression is getting redefined. This special issue covers the state of the art in learned image\/video restoration and compression to promote further progress in innovative architectures and training methods for effective and efficient networks for image\/video restoration and compression.",0.0526315789],["experimental observation and analyses of supercritical bifurcation of combustion instabilities triggered by","Inlet temperature driven supercritical bifurcation of combustion instabilities in a lean premixed prevaporized combustor","summarize: The present article reports experimental observation and analyses of a supercritical bifurcation of combustion instabilities triggered by the air inlet temperature . The studies are performed with a pressurised kerosene fuelled Lean Premixed Prevaporized combustor operated under elevated temperature. Unlike some previous studies, starting from an unstable condition of the system, the amplitude of combustion instabilities suddenly decrease when Ta exceeds a critical value of Ta =570 K. When the temperature is lowered back the system returns to being unstable without featuring any hysteresis behaviour, as expected in case of a supercritical bifurcation. The unstable flames feature a periodic axial motion of lift-off and re-ignition, characterized as Helmholtz mode. The phase difference between chemiluminescence and pressure signals is found to increase with Ta, exceeding 90 degrees for temperatures higher than 570 K. A low order network framework is conducted, illustrating that when Ta is increased a stability shift of this mode is predicted at Ta near 570 K, in good agreement with the experimental observations. The impact of Ta on the spray characteristics is also examined, finding that higher Ta promotes fuel evaporation and reduces equivalence ratio fluctuation at the exit of the swirler.",0.3627778821],["the cosmological constant of the Kerr-de Sitter family of black holes is","The global non-linear stability of the Kerr-de Sitter family of black holes","summarize: We establish the full global non-linear stability of the Kerr-de Sitter family of black holes, as solutions of the initial value problem for the Einstein vacuum equations with positive cosmological constant, for small angular momenta, and without any symmetry assumptions on the initial data. We achieve this by extending the linear and non-linear analysis on black hole spacetimes described in a sequence of earlier papers by the authors: We develop a general framework which enables us to deal systematically with the diffeomorphism invariance of Einstein's equations. In particular, the iteration scheme used to solve Einstein's equations automatically finds the parameters of the Kerr-de Sitter black hole that the solution is asymptotic to, the exponentially decaying tail of the solution, and the gauge in which we are able to find the solution; the gauge here is a wave map\/DeTurck type gauge, modified by source terms which are treated as unknowns, lying in a suitable finite-dimensional space.",0.6875],["we study the existence\/nonexistence of positive solution of positive solution.","Caffarelli-Kohn-Nirenberg type equations of fourth order with the critical exponent and Rellich potential","summarize: We study the existence\/nonexistence of positive solution of ",0.1389034164],["authors review several refinements of Young's integral inequality. authors review several refinements of","Refinements of Young's integral inequality via fundamental inequalities and mean value theorems for derivatives","summarize: In the paper, the authors review several refinements of Young's integral inequality via several mean value theorems, such as Lagrange's and Taylor's mean value theorems of Lagrange's and Cauchy's type remainders, and via several fundamental inequalities, such as \\veby\\vev's integral inequality, Hermite--Hadamard's type integral inequalities, H\\older's integral inequality, and Jensen's discrete and integral inequalities, in terms of higher order derivatives and their norms, survey several applications of several refinements of Young's integral inequality, and further refine Young's integral inequality via P\\'olya's type integral inequalities.",0.2012958867],["traditional plane-based clustering methods measure cost of within-cluster and between-cluster","Ramp-based Twin Support Vector Clustering","summarize: Traditional plane-based clustering methods measure the cost of within-cluster and between-cluster by quadratic, linear or some other unbounded functions, which may amplify the impact of cost. This letter introduces a ramp cost function into the plane-based clustering to propose a new clustering method, called ramp-based twin support vector clustering . RampTWSVC is more robust because of its boundness, and thus it is more easier to find the intrinsic clusters than other plane-based clustering methods. The non-convex programming problem in RampTWSVC is solved efficiently through an alternating iteration algorithm, and its local solution can be obtained in a finite number of iterations theoretically. In addition, the nonlinear manifold-based formation of RampTWSVC is also proposed by kernel trick. Experimental results on several benchmark datasets show the better performance of our RampTWSVC compared with other plane-based clustering methods.",0.0],["we consider three problems in which we assume different levels of knowledge about the diffusion rates, observation times","Network Inference from Consensus Dynamics with Unknown Parameters","summarize: We explore the problem of inferring the graph Laplacian of a weighted, undirected network from snapshots of a single or multiple discrete-time consensus dynamics, subject to parameter uncertainty, taking place on the network. Specifically, we consider three problems in which we assume different levels of knowledge about the diffusion rates, observation times, and the input signal power of the dynamics. To solve these underdetermined problems, we propose a set of algorithms that leverage the spectral properties of the observed data and tools from convex optimization. Furthermore, we provide theoretical performance guarantees associated with these algorithms. We complement our theoretical work with numerical experiments, that demonstrate how our proposed methods outperform current state-of-the-art algorithms and showcase their effectiveness in recovering both synthetic and real-world networks.",0.0],["the p=1 theory is based on the Deligne sheaf intersection homology","Singular chain intersection homology for traditional and super-perversities","summarize: We introduce a singular chain intersection homology theory which generalizes that of King and which agrees with the Deligne sheaf intersection homology of Goresky and MacPherson on any topological stratified pseudomanifold, compact or not, with constant or local coefficients, and with traditional perversities or superperversities >0). For the case p=1, these latter perversitie were introduced by Cappell and Shaneson and play a key role in their superduality theorem for embeddings. We further describe the sheafification of this singular chain complex and its adaptability to broader classes of stratified spaces.",0.25],["we study the generalization properties of minimum-norm solutions for three over-parametri","The Generalization Error of the Minimum-norm Solutions for Over-parameterized Neural Networks","summarize: We study the generalization properties of minimum-norm solutions for three over-parametrized machine learning models including the random feature model, the two-layer neural network model and the residual network model. We proved that for all three models, the generalization error for the minimum-norm solution is comparable to the Monte Carlo rate, up to some logarithmic terms, as long as the models are sufficiently over-parametrized.",0.2727272727],["a graph has an integer weight on each edge, so the directed sum of the weights","Colorings, determinants and Alexander polynomials for spatial graphs","summarize: A spatial graph has an integer weight on each edge, so that the directed sum of the weights at each vertex is zero. We describe the Alexander module and polynomial for balanced spatial graphs , and examine their behavior under some common operations on the graph. We use the Alexander module to define the determinant and ",0.3043478261],["Kuwert and Schatzle showed in 2001 that the Willmore flow converges","Asymptotic estimates for the Willmore flow with small energy","summarize: Kuwert and Sch\\atzle showed in 2001 that the Willmore flow converges to a standard round sphere, if the initial energy is small. In this situation, we prove stability estimates for the barycenter and the quadratic moment of the surface. Moreover, in codimension one we obtain stability bounds for the enclosed volume and averaged mean curvature. As direct applications, we recover a quasi-rigidity estimate due to De Lellis and M\\uller and an estimate for the isoperimetric deficit by R\\oger and Sch\\atzle , whose original proofs used different methods.",0.2727272727],["68 solar mass black hole detected in binary system LB-1. black hole is twice","No signature of the orbital motion of a putative 70 solar mass black hole in LB-1","summarize: Liu et al. recently reported the detection of a 68 solar mass black hole paired with an 8.2 Msun B-type sub-giant star in the 78.9-day spectroscopic binary system LB-1. Such a black hole is over twice as massive as any other known stellar-mass black hole with non-compact companions2 and its mass approaches those that result from BH-BH coalescences that are detected by gravitational wave interferometers. Its presence in a solar-like metallicity environment challenges conventional theories of massive binary evolution, stellar winds and core-collapse supernovae, so that more exotic scenarios seem to be needed to explain the existence and properties of LB-1. Here, we show that the observational diagnostics used to derive the BH mass results from the orbital motion of the B-type star, not that of the BH. As a consequence, no evidence for a massive BH remains in the data, therefore solving the existing tension with formation models of such a massive BH at solar metallicity and with theories of massive star evolution in general.",0.3193760683],["Using the machinery of weak fibration categories due to Schlank and the first author, we","Model structure on projective systems of ","summarize: Using the machinery of weak fibration categories due to Schlank and the first author, we construct a convenient model structure on the pro-category of separable ",0.0555555556],["accretion rates are similar to the 'quasar mode' in previous work","Simulating galaxy formation with black hole driven thermal and kinetic feedback","summarize: The inefficiency of star formation in massive elliptical galaxies is widely believed to be caused by the interactions of an active galactic nucleus with the surrounding gas. Achieving a sufficiently rapid reddening of moderately massive galaxies without expelling too many baryons has however proven difficult for hydrodynamical simulations of galaxy formation, prompting us to explore a new model for the accretion and feedback effects of supermassive black holes. For high accretion rates relative to the Eddington limit, we assume that a fraction of the accreted rest mass energy heats the surrounding gas thermally, similar to the `quasar mode' in previous work. For low accretion rates, we invoke a new, pure kinetic feedback model which imparts momentum into the surrounding gas in a stochastic manner. These two modes of feedback are motivated both by theoretical conjectures for the existence of different types of accretion flows as well as recent observational evidence for the importance of kinetic AGN winds in quenching galaxies. We find that a large fraction of the injected kinetic energy in this mode thermalises via shocks in the surrounding gas, thereby providing a distributed heating channel. In cosmological simulations, the resulting model produces red, non star-forming massive elliptical galaxies, and achieves realistic gas fractions, black hole growth histories and thermodynamic profiles in large haloes.",0.1818181818],["galaxies grow through internal and external processes. in about 10% of nearby red gal","The growth of the central region by acquisition of counter-rotating gas in star-forming galaxies","summarize: Galaxies grow through both internal and external processes. In about 10% of nearby red galaxies with little star formation, gas and stars are counter-rotating, demonstrating the importance of external gas acquisition in these galaxies. However, systematic studies of such phenomena in blue, star-forming galaxies are rare, leaving uncertain the role of external gas acquisition in driving evolution of blue galaxies. Based on new measurements with integral field spectroscopy of a large representative galaxy sample, we find an appreciable fraction of counter-rotators among blue galaxies . The central regions of blue counter-rotators show younger stellar populations and more intense, ongoing star formation than their outer parts, indicating ongoing growth of the central regions. The result offers observational evidence that the acquisition of external gas in blue galaxies is possible; the interaction with pre-existing gas funnels the gas into nuclear regions to form new stars.",0.375],["paper presents augmented Markovian system model for non-Markovian quantum systems.","Modelling and Filtering for Non-Markovian Quantum Systems","summarize: This paper presents an augmented Markovian system model for non-Markovian quantum systems. In this augmented system model, ancillary systems are introduced to play the role of internal modes of the non-Markovian environment converting white noise to colored noise. Consequently, non-Markovian dynamics are represented as resulting from direct interaction of the principal system with the ancillary system. To demonstrate the utility of the proposed augmented system model, it is applied to design whitening quantum filters for non-Markovian quantum systems. Examples are presented to illustrate how whitening quantum filters can be utilized for estimating non-Markovian linear quantum systems and qubit systems. In particular, we showed that the augmented Markovian formulation can be used to theoretically model the environment for an observed non-Markovian behavior in a recent experiment on quantum dots ",0.1666666667],["thesis is entitled Dynamic programming systems for modeling and control of traffic in transportation networks. two","Dynamic programming systems for modeling and control of the traffic in transportation networks","summarize: This thesis is entitled Dynamic programming systems for modeling and control of the traffic in transportation networks. Two parts are distinguished in this dissertation: 1) methods and approaches based on min-plus or max-plus algebra, where the dynamics are deterministic dynamic programming systems; 2) methods and approaches whose dynamic systems are non-linear but are interpreted as stochastic dynamic programming systems. Each of the two parts includes a chapter of necessary reviews, two main chapters and a chapter summarizing other works related to the concerned part. Part 1 includes a first chapter containing an introduction and some necessary reviews; two main chapters, one on the max-plus algebra model for the train dynamics on a metro line, the other one on the network calculus approach for modeling and calculating performance bounds on road networks; and a final chapter summarizing my other contributions on the topic of this part. Part 2 includes a first chapter containing an introduction and some necessary reviews; two main chapters, one on the microscopic modeling of traffic taking into account anticipation in driving, the other one on the modeling of the train dynamics on a metro line taking into account the passenger travel demand; and a final chapter summarizing my other contributions on the topic of this part.",0.7],["we apply Debbasch proposal to obtain mean metric of coarse graining of quantum perturbe","Reissner-Nordstr\\m black holes statistical ensembles and first order thermodynamic phase transition","summarize: We apply Debbasch proposal to obtain mean metric of coarse graining of quantum perturbed Reissner-Nordst\\m black hole ",0.0],["overlapping temporal windows are represented by a Fisher vector. features from each window are","Joint Recognition and Segmentation of Actions via Probabilistic Integration of Spatio-Temporal Fisher Vectors","summarize: We propose a hierarchical approach to multi-action recognition that performs joint classification and segmentation. A given video is processed via a sequence of overlapping temporal windows. Each frame in a temporal window is represented through selective low-level spatio-temporal features which efficiently capture relevant local dynamics. Features from each window are represented as a Fisher vector, which captures first and second order statistics. Instead of directly classifying each Fisher vector, it is converted into a vector of class probabilities. The final classification decision for each frame is then obtained by integrating the class probabilities at the frame level, which exploits the overlapping of the temporal windows. Experiments were performed on two datasets: s-KTH , and the challenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an accuracy of 85.0%, significantly outperforming two recent approaches based on GMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, the proposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMM approaches which obtained 33.7% and 38.4%, respectively. Furthermore, the proposed system is on average 40 times faster than the GMM based approach.",0.347826087],["Thomassen conjectures triangle-free planar graphs have an exponential number of exponential","Do triangle-free planar graphs have exponentially many 3-colorings?","summarize: Thomassen conjectured that triangle-free planar graphs have an exponential number of ",0.6],["global design pressures can be difficult to trace in subsystem design. systems physics provides","Statistical Physics of Design","summarize: A key challenge in complex design problems that permeate science and engineering is the need to balance design objectives for specific design elements or subsystems with global system objectives. Global objectives give rise to competing design pressures, whose effects can be difficult to trace in subsystem design. Here, using examples from arrangement problems, we show that the systems-level application of statistical physics principles, which we term systems physics, provides a detailed characterization of subsystem design in terms of the concepts of stress and strain from materials physics. We analyze instances of routing problems in naval architectures, and show that systems physics provides a direct means of classifying architecture types, and quantifying trade-offs between subsystem- and overall performance. Our approach generalizes straightforwardly to design problems in a wide range of other disciplines that require concrete understanding of how the pressure to meet overall design objectives drives the outcomes for component subsystems.",0.0],["Cremona group in 2 dimension is not simple, over any field. some elements satisfy","Non simplicit\\'e du groupe de Cremona sur tout corps","summarize: Using a theorem of Dahmani, Guirardel and Osin we prove that the Cremona group in 2 dimension is not simple, over any field. More precisely, we show that some elements of this group satisfy a weakened WPD property which is equivalent in our particular context to the Bestvina and Fujiwara's one.",0.1333333333],["state space models often used in machine learning to learn from data streams exhibiting temporal dependencies","Dynamical Systems as Temporal Feature Spaces","summarize: Parameterized state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks . In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; Simple cycle high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.",0.0],["can we recover a complex signal from its Fourier magnitudes?.","A Geometric Analysis of Phase Retrieval","summarize: Can we recover a complex signal from its Fourier magnitudes? More generally, given a set of ",0.2307692308],["in this paper, we propose to interpolate between these two extremes. we propose","Sparse matrix factorizations for fast linear solvers with application to Laplacian systems","summarize: In solving a linear system with iterative methods, one is usually confronted with the dilemma of having to choose between cheap, inefficient iterates over sparse search directions , or expensive iterates in well-chosen search directions . In this paper, we propose to interpolate between these two extremes, and show how to perform cheap iterations along non-sparse search directions, provided that these directions can be extracted from a new kind of sparse factorization. For example, if the search directions are the columns of a hierarchical matrix, then the cost of each iteration is typically logarithmic in the number of variables. Using some graph-theoretical results on low-stretch spanning trees, we deduce as a special case a nearly-linear time algorithm to approximate the minimal norm solution of a linear system ",0.1],["new Python software for the parametric design of stabilizing feedback laws with time delays. the","Partial Pole Placement via Delay Action: A Python Software for Delayed Feedback Stabilizing Design","summarize: This paper presents a new Python software for the parametric design of stabilizing feedback laws with time delays, called Partial Pole Placement via Delay Action . After an introduction recalling recent theoretical results on the multiplicity-induced-dominancy and coexisting real roots-induced-dominancy properties and their use for the feedback stabilization of control systems operating under time delays, the paper presents the current version of P3",0.1666666667],["the proposed FT protocol can support an unbounded number of faulty nodes as long","Toward Fault-Tolerant Deadlock-Free Routing in HyperSurface-Embedded Controller Networks","summarize: HyperSurfaces consist of structurally reconfigurable metasurfaces whose electromagnetic properties can be changed via a software interface, using an embedded miniaturized network of controllers. With the HSF controllers, interconnected in an irregular, near-Manhattan geometry, we propose a robust, deterministic Fault-Tolerant , deadlock- and livelock-free routing protocol where faults are contained in a set of disjointed rectangular regions called faulty blocks. The proposed FT protocol can support an unbounded number of faulty nodes as long as nodes outside the faulty blocks are connected. Simulation results show the efficacy of the proposed FT protocol under various faulty node distribution scenarios.",0.0666666667],["the tempered power-law is the more reasonable choice for the CTRW model.","Variational formulation and efficient implementation for solving the tempered fractional problems","summarize: Because of the finiteness of the life span and boundedness of the physical space, the more reasonable or physical choice is the tempered power-law instead of pure power-law for the CTRW model in characterizing the waiting time and jump length of the motion of particles. This paper focuses on providing the variational formulation and efficient implementation for solving the corresponding deterministic\/macroscopic models, including the space tempered fractional equation and time tempered fractional equation. The convergence, numerical stability, and a series of variational equalities are theoretically proved. And the theoretical results are confirmed by numerical experiments.",0.2777777778],["we consider the critical behaviour of the continuous-time weakly self-avoiding walk with contact self","Four-dimensional weakly self-avoiding walk with contact self-attraction","summarize: We consider the critical behaviour of the continuous-time weakly self-avoiding walk with contact self-attraction on ",0.4444444444],["a two-dimensional model is used for solid liquid crystalline plates. the model equation","A plate theory for nematic liquid crystalline solids","summarize: We derive a F\\ppl-von K\\'rm\\'n-type constitutive model for solid liquid crystalline plates where the nematic director may or may not rotate freely relative to the elastic network. To obtain the reduced two-dimensional model, we rely on the deformation decomposition of a nematic solid into an elastic deformation and a natural shape change. The full solution to the resulting equilibrium equations consists of both the deformation displacement and stress fields. The model equations are applicable to a wide range of thin nematic bodies subject to optothermal stimuli and mechanical loads. For illustration, we consider certain reversible natural shape changes in simple systems which are stress free, and their counterparts, where the natural deformations are blocked and internal stresses appear. More general problems can be addressed within the same framework.",0.4210526316],["morphology of smooth deposits covering parts of the surface suggests liquid extrusions.","Cryomagma ascent on Europa","summarize: Europa's surface exhibits morphological features which, associated with a low crater density, might be interpreted to have formed as a result of recent cryovolcanic activity. In particular, the morphology of smooth deposits covering parts of the surface, and their relationship to the surrounding terrains, suggest that they result from liquid extrusions. Furthermore, recent literature suggests that the emplacement of liquid-related features, such as double ridges, lenticulae and chaos could result from the presence of liquid reservoirs beneath the surface. We model the ascent of liquid water through a fracture or a pipe-like conduit from a subsurface reservoir to Europa\\textquoteright s surface and calculate the eruption time-scale and the total volume extruded during the eruption, as a function of the reservoir volume and depth. We also estimate the freezing time of a subsurface reservoir necessary to trigger an eruption. Our model is derived for pure liquid water and for a briny mixture outlined by Kargel : 81 wt% H",0.0],["we establish sharp exponential deviation estimates of the information content. we also establish a sharp bound","Concentration of information content for convex measures","summarize: We establish sharp exponential deviation estimates of the information content as well as a sharp bound on the varentropy for the class of convex measures on Euclidean spaces. This generalizes a similar development for log-concave measures in the recent work of Fradelizi, Madiman and Wang . In particular, our results imply that convex measures in high dimensions are concentrated in an annulus between two convex sets despite their possibly having much heavier tails. Various tools and consequences are developed, including a sharp comparison result for R\\'enyi entropies, inequalities of Kahane-Khinchine type for convex measures that extend those of Koldobsky, Pajor and Yaskin for log-concave measures, and an extension of Berwald's inequality .",0.1935483871],["we apply Debbasch proposal to obtain mean metric of coarse graining of quantum perturbe","Reissner-Nordstr\\m black holes statistical ensembles and first order thermodynamic phase transition","summarize: We apply Debbasch proposal to obtain mean metric of coarse graining of quantum perturbed Reissner-Nordst\\m black hole ",0.0],["the present method is able to handle close-located discontinuities. the method is","An incremental-stencil WENO reconstruction for simulation of compressible two-phase flows","summarize: An incremental-stencil WENO reconstruction method, which uses low-order candidate stencils with incrementally increasing width, is proposed for finite-volume simulation of compressible two-phase flow with the quasi-conservative interface model. While recovering the original 5th-order WENO reconstruction in smooth region of the solution, due to the presence of 2-point candidate stencils, the present method is able to handle closely located discontinuities, which is a typical scenario of shock-interface interaction. Furthermore, a MOOD-type positivity preserving approach is applied to ensure physical meaningful reconstruction. Compared with the hybrid method which switches between with the 5th-order WENO and 2nd-order reconstructions, the present method is free of problem-dependent tunable parameters. A number of numerical examples show that the present method achieves small numerical dissipation and good robustness for simulating two-phase flow problems with strong shock-interface interaction and large density ratio.",0.0],["a pig phantom maintained at 20 degree was a pig phan","Quantitative MRI molecular imaging in the evaluation of early post mortem changes in muscles. A feasibility study on a pig phantom","summarize: Estimating early postmortem interval EPI is a difficult task in daily forensic activity due to limitations of accurate and reliable methods. The aim of the present work is to describe a novel approach in the estimation of EPI based on quantitative magnetic resonance molecular imaging qMRMI using a pig phantom since post mortem degradation of pig meat is similar to that of human muscles. On a pig phantom maintained at 20 degree, using a 1.5 T MRI scanner we performed 10 scans, every 4 hours, monitoring apparent diffusion coefficient ADC, fractional anisotropy FA, magnetization transfer ration MTR, tractography and susceptibility weighted changes in muscles until 36 hours after death. Cooling of the phantom during the experiment was recorded. Histology was also obtained. Pearson's Test was carried out for statistical correlation. We found a significative statistical inverse correlation between ADC, FA, MT and PMI. Our preliminary data shows that post mortem qMRMI is a potential powerful tool in accurately determining EPI and is worth of further investigation.",0.3021677411],["robots in unstructured retail environments have to master complex perception, knowledgeprocessing, and","Knowledge-Enabled Robotic Agents for Shelf Replenishment in Cluttered Retail Environments","summarize: Autonomous robots in unstructured and dynamically changing retail environments have to master complex perception, knowledgeprocessing, and manipulation tasks. To enable them to act competently, we propose a framework based on three core components: a knowledge-enabled perception system, capable of combining diverse information sources to cope with occlusions and stacked objects with a variety of textures and shapes, knowledge processing methods produce strategies for tidying up supermarket racks, and the necessary manipulation skills in confined spaces to arrange objects in semi-accessible rack shelves. We demonstrate our framework in an simulated environment as well as on a real shopping rack using a PR2 robot. Typical supermarket products are detected and rearranged in the retail rack, tidying up what was found to be misplaced items.",0.0769230769],["elastic properties of a material with spherical voids of equal volume are","Beam model for the elastic properties of material with spherical voids","summarize: The elastic properties of a material with spherical voids of equal volume are analysed using a new model, with particular attention paid to the hexagonal close-packed and the face-centred cubic arrangement of voids. Void fractions well above 74 \\% are considered, yielding overlapping voids as in an open-cell foam and hence a connected pore structure. The material is represented by a network of beams. The elastic behaviour of each beam is derived analytically from the material structure. By computing the linear elastic properties of the beam network, the Young's moduli and Poisson ratios for different directions are evaluated. In the limit of rigidity loss a power law is obtained, describing the relation between Young's modulus and void fraction with an exponent of 5\/2 for bending-dominated and 3\/2 for stretching-dominated directions. The corresponding Poisson ratios vary between 0 and 1. With decreasing void fraction, these exponents become 2.3 and 1.3, respectively. The data obtained analytically and from the new beam model are compared to Finite Element simulations carried out in a companion study, and good agreement is found. The hexagonal close-packed void arrangement features very anisotropic behaviour, comparable to that of fibre-reinforced materials, This might allow for new applications of open-cell materials.",0.65],["branch-and-bound is a decision space search method. the branching is performed","Branch-and-bound for biobjective mixed-integer linear programming","summarize: We present a generic branch-and-bound algorithm for finding all the Pareto solutions of a biobjective mixed-integer linear program. The main contributions are new algorithms for obtaining dual bounds at a node, for checking node fathoming, presolve and duality gap measurement. Our branch-and-bound is predominantly a decision space search method since the branching is performed on the decision variables, akin to single objective problems, although we also sometimes split gaps and branch in the objective space. The various algorithms are implemented using a data structure for storing Pareto sets. Computational experiments are carried out on literature instances and also on a new set of instances that we generate using the MIPLIB benchmark library for single objective problems. We also perform comparisons against the triangle splitting method from literature, which is an objective space search algorithm.",0.2],["we investigated the typical environment and physical properties of red discs and blue bulges.","NoSOCS in SDSS. V. Red Disc and Blue Bulge Galaxies Across Different Environments","summarize: We investigated the typical environment and physical properties of red discs and blue bulges, comparing those to the normal objects in the blue cloud and red sequence. Our sample is composed of cluster members and field galaxies at ",0.125],["markerless image registration method was used for Augmented reality-based knee replacement surgery to guide and","A Novel Visualization System of Using Augmented Reality in Knee Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm","summarize: Background and aim: Image registration and alignment are the main limitations of augmented reality-based knee replacement surgery. This research aims to decrease the registration error, eliminate outcomes that are trapped in local minima to improve the alignment problems, handle the occlusion, and maximize the overlapping parts. Methodology: markerless image registration method was used for Augmented reality-based knee replacement surgery to guide and visualize the surgical operation. While weight least square algorithm was used to enhance stereo camera-based tracking by filling border occlusion in right to left direction and non-border occlusion from left to right direction. Results: This study has improved video precision to 0.57 mm~0.61 mm alignment error. Furthermore, with the use of bidirectional points, for example, forwards and backwards directional cloud point, the iteration on image registration was decreased. This has led to improve the processing time as well. The processing time of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear that this proposed system has focused on overcoming the misalignment difficulty caused by movement of patient and enhancing the AR visualization during knee replacement surgery. The proposed system was reliable and favorable which helps in eliminating alignment error by ascertaining the optimal rigid transformation between two cloud points and removing the outliers and non-Gaussian noise. The proposed augmented reality system helps in accurate visualization and navigation of anatomy of knee such as femur, tibia, cartilage, blood vessels, etc.",0.0583448879],["four-dimensional large-scale large-scales are considered asymptotically-","Spectral sum rules for confining large-N theories","summarize: We consider asymptotically-free four-dimensional large-",0.0],["metasurface absorbers are designed using two types of unit cells. each of them absorbs","Metasurfaces for Efficient Digital Noise Absorption","summarize: We numerically demonstrate two types of metasurface absorbers to efficiently absorb digital signals. First, we show that the digital waveforms used in this study contain not only a fundamental wave but also nonnegligible harmonic waves, which limits the absorption performance of a conventional metasurface absorber operating in only a single, finite frequency band. The first type of the proposed absorbers is designed using two kinds of unit cells, each of which absorbs either a fundamental frequency or third harmonic of an incident digital waveform. This dual-band metasurface absorber exhibits absorption performance exceeding that of the conventional metasurface absorber and more strongly dissipates the energy of a digital waveform. In addition, the second type of absorber exploits the concept of nonlinear analogous circuits to convert an incoming wave to a different waveform, specifically, a triangular waveform that has a larger magnitude at a fundamental frequency. Therefore, the incoming waveform is more effectively absorbed by this waveform-conversion metasurface absorber as well. Although still there remain some issues to put these digital signal absorbers into practice, including experimental validation, our results contribute to mitigating electromagnetic interference issues caused by digital noise and realising physically smaller, lighter digital signal processing products for the next generation.",0.0],["neutrinos are formed in core-collapse supernova explosions. the cooling time","A New Approach to Mass and Radius of Neutron Stars with Supernova Neutrinos","summarize: Neutron stars are formed in core-collapse supernova explosions, where a large number of neutrinos are emitted. In this paper, supernova neutrino light curves are computed for the cooling phase of protoneutron stars, which lasts a few minutes. In the numerical simulations, 90 models of the phenomenological equation of state with different incompressibilities, symmetry energies, and nucleon effective masses are employed for a comprehensive study. It is found that the cooling timescale is longer for a model with a larger neutron star mass and a smaller neutron star radius. Furthermore, a theoretical expression of the cooling timescale is presented as a function of the mass and radius and it is found to describe the numerical results faithfully. These findings suggest that diagnosing the mass and radius of a newly formed neutron star using its neutrino signal is possible.",0.0617348517],["variance-reduced incremental methods are based on SAGA and SVRG.","Proximal Splitting Meets Variance Reduction","summarize: Despite the rise to fame of incremental variance-reduced methods in recent years, their use in nonsmooth optimization is still limited to few simple cases. This is due to the fact that existing methods require to evaluate the proximity operator for the nonsmooth terms, which can be a costly operation for complex penalties. In this work we introduce two variance-reduced incremental methods based on SAGA and SVRG that can efficiently take into account complex penalties which can be expressed as a sum of proximal terms. This includes penalties such as total variation, group lasso with overlap and trend filtering, to name a few. Furthermore, we also develop sparse variants of the proposed algorithms which can take advantage of sparsity in the input data. Like other incremental methods, it only requires to evaluate the gradient of a single sample per iteration, and so is ideally suited for large scale applications. We provide a convergence rate analysis for the proposed methods and show that they converge with a fixed step-size, achieving in some cases the same asymptotic rate as their full gradient variants. Empirical benchmarks on 3 different datasets illustrate the practical advantages of the proposed methods.",0.1111111111],["the proposed FT protocol can support an unbounded number of faulty nodes as long","Toward Fault-Tolerant Deadlock-Free Routing in HyperSurface-Embedded Controller Networks","summarize: HyperSurfaces consist of structurally reconfigurable metasurfaces whose electromagnetic properties can be changed via a software interface, using an embedded miniaturized network of controllers. With the HSF controllers, interconnected in an irregular, near-Manhattan geometry, we propose a robust, deterministic Fault-Tolerant , deadlock- and livelock-free routing protocol where faults are contained in a set of disjointed rectangular regions called faulty blocks. The proposed FT protocol can support an unbounded number of faulty nodes as long as nodes outside the faulty blocks are connected. Simulation results show the efficacy of the proposed FT protocol under various faulty node distribution scenarios.",0.0666666667],["the main problem is that self-awareness cannot be observed from an outside perspective.","Will we ever have Conscious Machines?","summarize: The question of whether artificial beings or machines could become self-aware or consciousness has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of whether something is really self-aware or merely a clever program that pretends to do so cannot be answered without access to accurate knowledge about the mechanism's inner workings. We review the current state-of-the-art regarding these developments and investigate common machine learning approaches with respect to their potential ability to become self-aware. We realise that many important algorithmic steps towards machines with a core consciousness have already been devised. For human-level intelligence, however, many additional techniques have to be discovered.",0.0],["the EnLSTM is compared with commonly-used models on a published dataset.","Ensemble long short-term memory network","summarize: In this study, we propose an ensemble long short-term memory network, which can be trained on a small dataset and process sequential data. The EnLSTM is built by combining the ensemble neural network and the cascaded long short-term memory network to leverage their complementary strengths. In order to resolve the issues of over-convergence and disturbance compensation associated with training failure owing to the nature of small-data problems, model parameter perturbation and high-fidelity observation perturbation methods are introduced. The EnLSTM is compared with commonly-used models on a published dataset, and proven to be the state-of-the-art model in generating well logs with a mean-square-error reduction of 34%. In the case study, 12 well logs that cannot be measured while drilling are generated based on logging-while-drilling data. The EnLSTM is capable to reduce cost and save time in practice.",0.0625],["study of algorithmic complexity of election control is known as election control. it is proposed to","Election Control by Manipulating Issue Significance","summarize: Integrity of elections is vital to democratic systems, but it is frequently threatened by malicious actors. The study of algorithmic complexity of the problem of manipulating election outcomes by changing its structural features is known as election control. One means of election control that has been proposed is to select a subset of issues that determine voter preferences over candidates. We study a variation of this model in which voters have judgments about relative importance of issues, and a malicious actor can manipulate these judgments. We show that computing effective manipulations in this model is NP-hard even with two candidates or binary issues. However, we demonstrate that the problem is tractable with a constant number of voters or issues. Additionally, while it remains intractable when voters can vote stochastically, we exhibit an important special case in which stochastic voting enables tractable manipulation.",0.0],["effective applications of vehicular ad hoc networks in traffic signal control require new methods for","Detection of malicious data in vehicular ad-hoc networks for traffic signal control applications","summarize: Effective applications of vehicular ad hoc networks in traffic signal control require new methods for detection of malicious data. Injection of malicious data can result in significantly decreased performance of such applications, increased vehicle delays, fuel consumption, congestion, or even safety threats. This paper introduces a method, which combines a model of expected driver behaviour with position verification in order to detect the malicious data injected by vehicle nodes that perform Sybil attacks. Effectiveness of this approach was demonstrated in simulation experiments for a decentralized self-organizing system that controls the traffic signals at multiple intersections in an urban road network. Experimental results show that the proposed method is useful for mitigating the negative impact of malicious data on the performance of traffic signal control.",0.7333333333],["crowd models can be used to assess occupant exposure in confined spaces. the proposed model","EXPOSED: An occupant exposure model for confined spaces to retrofit crowd models during a pandemic","summarize: Crowd models can be used for the simulation of people movement in the built environment. Crowd model outputs have been used for evaluating safety and comfort of pedestrians, inform crowd management and perform forensic investigations. Microscopic crowd models allow the representation of each person and the obtainment of information concerning their location over time and interactions with the physical space\/other people. Pandemics such as COVID-19 have posed several questions on safe building usage, given the risk of disease transmission among building occupants. Here we show how crowd modelling can be used to assess occupant exposure in confined spaces. The policies adopted concerning building usage and social distancing during a pandemic can vary greatly, and they are mostly based on the macroscopic analysis of the spread of disease rather than a safety assessment performed at a building level. The proposed model allows the investigation of occupant exposure in buildings based on the analysis of microscopic people movement. Risk assessment is performed by retrofitting crowd models with a universal model for exposure assessment which can account for different types of disease transmissions. This work allows policy makers to perform informed decisions concerning building usage during a pandemic.",0.6470588235],["billiard table made of two circular cavities connected by a straight channel. determin","Deterministic reversible model of non-equilibrium phase transitions and stochastic counterpart","summarize: N point particles move within a billiard table made of two circular cavities connected by a straight channel. The usual billiard dynamics is modified so that it remains deterministic, phase space volumes preserving and time reversal invariant. Particles move in straight lines and are elastically reflected at the boundary of the table, as usual, but those in a channel that are moving away from a cavity invert their motion , if their number exceeds a given threshold T. When the geometrical parameters of the billiard table are fixed, this mechanism gives rise to non--equilibrium phase transitions in the large N limit: letting T\/N decrease, the homogeneous particle distribution abruptly turns into a stationary inhomogeneous one. The equivalence with a modified Ehrenfest two urn model, motivated by the ergodicity of the billiard with no rebound, allows us to obtain analytical results that accurately describe the numerical billiard simulation results. Thus, a stochastic exactly solvable model that exhibits non-equilibrium phase transitions is also introduced.",0.3],["the complete part of the earthquake frequency-magnitude distribution is well described by the Gutenberg","Generalized Earthquake Frequency-Magnitude Distribution Described by Asymmetric Laplace Mixture Modelling","summarize: The complete part of the earthquake frequency-magnitude distribution , above completeness magnitude mc, is well described by the Gutenberg-Richter law. The parameter mc however varies in space due to the seismic network configuration, yielding a convoluted FMD shape below max. This paper investigates the shape of the generalized FMD , which may be described as a mixture of elemental FMDs defined as asymmetric Laplace distributions of mode mc . An asymmetric Laplace mixture model is thus proposed with its parameters estimated using a semi-supervised hard expectation maximization approach including BIC penalties for model complexity. The performance of the proposed method is analysed, with encouraging results obtained: kappa, beta, and the mc distribution range are retrieved for different GFMD shapes in simulations, as well as in regional catalogues , in a global catalogue, and in an aftershock sequence . We find max to be conservative compared to other methods, kappa = k\/log = 3 in most catalogues = 1), but also that biases in kappa and beta may occur when rounding errors are present below completeness. The GFMD-ALMM, by modelling different FMD shapes in an autonomous manner, opens the door to new statistical analyses in the realm of incomplete seismicity data, which could in theory improve earthquake forecasting by considering c. ten times more events.",0.0952380952],["we consider the problem of pointwise stabilization of a one-dimensional wave equation.","Rapid exponential stabilization of a 1-D transmission wave equation with in-domain anti-damping","summarize: We consider the problem of pointwise stabilization of a one-dimensional wave equation with an internal spatially varying anti-damping term. We design a feedback law based on the backstepping method and prove exponential stability of the closed-loop system with a desired decay rate.",0.5789473684],["uncertainty relations for non-commutative space are computed. we obtain a better","Probing Uncertainty Relations in Non-Commutative Space","summarize: In this paper, we compute uncertainty relations for non-commutative space and obtain a better lower bound than the standard one obtained from Heisenberg's uncertainty relation. We also derive the reverse uncertainty relation for product and sum of uncertainties of two incompatible variables for one linear and another non-linear model of the harmonic oscillator. The non-linear model in non-commutating space yields two different expressions for Schr\\odinger and Heisenberg uncertainty relation. This distinction does not arise in commutative space, and even in the linear model of non-commutative space.",0.2352941176],["Jordan algebras were first introduced in an effort to restructure quantum mechanics purely in","The geometry of physical observables","summarize: Jordan algebras were first introduced in an effort to restructure quantum mechanics purely in terms of physical observables. In this paper we explain why, if one attempts to reformulate the internal structure of the standard model of particle physics geometrically, one arrives naturally at a discrete internal geometry that is coordinatized by a Jordan algebra.",0.0],["GMRES solver solves a system of constant state, singularly perturbed","Multilevel Schwarz preconditioners for singularly perturbed symmetric reaction-diffusion systems","summarize: We present robust and highly parallel multilevel non-overlapping Schwarz preconditioners, to solve an interior penalty discontinuous Galerkin finite element discretization of a system of steady state, singularly perturbed reaction-diffusion equations with a singular reaction operator, using a GMRES solver. We provide proofs of convergence for the two-level setting and the multigrid V-cycle as well as numerical results for a wide range of regimes.",0.4615384615],["Indian Centre for Space Physics has taken a new strategy to study low energy cosmic rays","Study of high energy phenomena from near space using low-cost meteorological balloons","summarize: Indian Centre for Space Physics has taken a novel strategy to study low energy cosmic rays and astrophysical X-ray sources which involve very light weight payloads up to about five kilograms on board a single or multiple balloons which are usually used for meteorological purposes. The mission duration could be anywhere from 3-12 hours. Our strategy provides extreme flexibility in mission preparation and its operation using a very economical budget. There are several limitations but our innovative approach has been able to extract significant amount of scientific data out of these missions. So far, over one hundred missions have been completed by us to near space and a wealth of data has been collected. The payloads are recovered and are used again. Scientific data is stored on board computer and the atmospheric data or payload location is sent to ground in real time. Since each mission is different, we present here the general strategy for a typical payload and provide some results we obtained in some of these missions.",0.3181818182],["compression is used to reduce communication costs in distributed data-parallel training of deep neural networks","On the Discrepancy between the Theoretical Analysis and Practical Implementations of Compressed Communication for Distributed Deep Learning","summarize: Compressed communication, in the form of sparsification or quantization of stochastic gradients, is employed to reduce communication costs in distributed data-parallel training of deep neural networks. However, there exists a discrepancy between theory and practice: while theoretical analysis of most existing compression methods assumes compression is applied to the gradients of the entire model, many practical implementations operate individually on the gradients of each layer of the model. In this paper, we prove that layer-wise compression is, in theory, better, because the convergence rate is upper bounded by that of entire-model compression for a wide range of biased and unbiased compression methods. However, despite the theoretical bound, our experimental study of six well-known methods shows that convergence, in practice, may or may not be better, depending on the actual trained model and compression ratio. Our findings suggest that it would be advantageous for deep learning frameworks to include support for both layer-wise and entire-model compression.",0.1944829598],["work presents robust and efficient sharp interface immersed boundary framework. the work deploys an in","A Novel Sharp Interface Immersed Boundary Framework for Viscous Flow Simulations at Arbitrary Mach Number Involving Complex and Moving Boundaries","summarize: This work presents a robust and efficient sharp interface immersed boundary framework, which is applicable for all-speed flow regimes and is capable of handling arbitrarily complex bodies . The work deploys an in-house, parallel, multi-block structured finite volume flow solver, which employs a 3D unsteady Favre averaged Navier Stokes equations in a generalized curvilinear coordinate system; while we employ a combination of HCIB method and GC for solution reconstruction near immersed boundary interface. A significant difficulty for these sharp interface approaches is of handling sharp features\/edges of complex geometries. In this study, we observe that apart from the need for robust node classification strategy and higher order boundary formulations, the direction in which the reconstruction procedures are performed plays an important role in handling sharp edges. Taking this into account we present a versatile interface tracking procedure based on ray tracing algorithm and a novel three step solution reconstruction procedure that computes pseudo-normals in the regions where the normal is not well-defined and reconstructs the flow field along those directions. We demonstrate that this procedure enables solver to efficiently handle and accurately represent sharp-edged regions. A fifth-order weighted essentially non-oscillatory scheme is used for capturing shock-induced discontinuities and complex fluid-solid interactions with high resolution. The developed IBM framework is applied to a wide range of flow phenomena encompassing all-speed regimes . A total of seven benchmark cases are presented involving various geometries and the predictions are found to be in excellent agreement with the published results.",0.2047232316],["this article develops the algebraic structure that results from the algebraic structure.","Coherent States and Generalized Hermite Polynomials for fractional statistics -- interpolating from fermions to bosons","summarize: This article develops the algebraic structure that results from the ",0.0432667102],["we discuss the origin, an improved definition and the key reciprocity property of the trilinear","Redei reciprocity, governing fields, and negative Pell","summarize: We discuss the origin, an improved definition and the key reciprocity property of the trilinear symbol introduced by R\\'edei in the study of 8-ranks of narrow class groups of quadratic number fields. It can be used to show that such 8-ranks are governed by Frobenius conditions on the primes dividing the discriminant, a fact used the recent work of A. Smith. In addition, we explain its impact in the progress towards proving my conjectural density for solvability of the negative Pell equation.",0.1363636364],["we consider the problem of designing a stabilizing and optimal static controller. this problem is","On Separable Quadratic Lyapunov Functions for Convex Design of Distributed Controllers","summarize: We consider the problem of designing a stabilizing and optimal static controller with a pre-specified sparsity pattern. Since this problem is NP-hard in general, it is necessary to resort to approximation approaches. In this paper, we characterize a class of convex restrictions of this problem that are based on designing a separable quadratic Lyapunov function for the closed-loop system. This approach generalizes previous results based on optimizing over diagonal Lyapunov functions, thus allowing for improved feasibility and performance. Moreover, we suggest a simple procedure to compute favourable structures for the Lyapunov function yielding high-performance distributed controllers. Numerical examples validate our results.",0.3181818182],["algorithm was developed in an open-source python library. it aims","Generating Periodic Grain Boundary Structures: Algorithm and Open-Source Python Library","summarize: An algorithm implemented in an open-source python library was developed for building periodic coincidence site lattice grain boundary models in a universal fashion. The software framework aims to generate tilt and twist grain boundaries from cubic and tetragonal crystals for ab-initio and classical atomistic simulation. This framework has two useful features: i) it can calculate all the CSL matrices for generating CSL from a given Sigma value and rotation axis, allowing the users to build the specific CSL and grain boundary models; ii) it provides a convenient command line tool to enable high-throughput generation of tilt and twist grain boundaries by assigning an input crystal structure, value, rotation axis, and grain boundary plane. The developed algorithm in the open-source python library is expected to facilitate studies of grain boundary in materials science. The software framework is available on the website: aimsgb.org.",0.2727272727],["the random forest can be trained with the small number of training data. the algorithm can be","Experiments on Learning Based Industrial Bin-picking with Iterative Visual Recognition","summarize: This paper shows experimental results on learning based randomized bin-picking combined with iterative visual recognition. We use the random forest to predict whether or not a robot will successfully pick an object for given depth images of the pile taking the collision between a finger and a neighboring object into account. For the discriminator to be accurate, we consider estimating objects' poses by merging multiple depth images of the pile captured from different points of view by using a depth sensor attached at the wrist. We show that, even if a robot is predicted to fail in picking an object with a single depth image due to its large occluded area, it is finally predicted as success after merging multiple depth images. In addition, we show that the random forest can be trained with the small number of training data.",0.0344827586],["we prove the statement in the title using the connectedness of the interval in real line.","A Short Proof that Lebesgue Outer Measure of an Interval is its Length","summarize: We prove the statement in the title using the connectedness of the interval in real line.",0.0277777778],["metallic back reflectors are not suitable for tandem cell configurations. but metallic back reflect","Implementing strong interference in ultrathin film top absorbers for tandem solar cells","summarize: Strong interference in ultrathin film semiconductor absorbers on metallic back reflectors has been shown to enhance the light harvesting efficiency of solar cell materials. However, metallic back reflectors are not suitable for tandem cell configurations because photons cannot be transmitted through the device. Here, we introduce a method to implement strong interference in ultrathin film top absorbers in a tandem cell configuration through use of distributed Bragg reflectors . We showcase this by designing and fabricating a photoelectrochemical-photovoltaic stacked tandem cell in a V-shaped configuration where short wavelength photons are reflected back to the photoanode material , whereas long wavelength photons are transmitted to the bottom silicon PV cell. We employ optical simulations to determine the optimal thicknesses of the DBR layers and the V-shape angle to maximize light absorption in the ultrathin hematite film. The DBR spectral response can be tailored to allow for a more than threefold enhancement in absorbed photons compared to a layer of the same thickness on transparent current collectors. Using a DBR to couple a bottom silicon PV cell with an ultrathin hematite top PEC cell, we demonstrate unassisted solar water splitting and show that DBRs can be designed to enhance strong interference in ultrathin films while enabling stacked tandem cell configuration.",0.1578947368],["metasurfaces are based on plasmonic Au nanostructures. they can be","Planar Aperiodic Arrays as Metasurfaces for Optical Near-Field Patterning","summarize: Plasmonic metasurfaces have spawned the field of flat optics using nanostructured planar metallic or dielectric surfaces that can replace bulky optical elements and enhance the capabilities of traditional far-field optics. Furthermore, the potential of flat optics can go far beyond far-field modulation, and can be exploited for functionality in the near-field itself. Here, we design metasurfaces based on aperiodic arrays of plasmonic Au nanostructures for tailoring the optical near-field in the visible and near-infrared spectral range. The basic element of the arrays is a rhomboid that is modulated in size, orientation and position to achieve the desired functionality of the micron-size metasurface structure. Using two-photon-photoluminescence as a tool to probethe near-field profiles in the plane of the metasurfaces, we demonstrate the molding of light into different near-field intensity patterns and active pattern control via the far-field illumination. Finite element method simulations reveal that the near-field modulation occurs via a combination of the plasmonic resonances of the rhomboids and field enhancement in the nanoscale gaps in between the elements. This approach enables optical elements that can switch the near-field distribution across the metasurface via wavelength and polarization of the incident far-field light, and provides pathways for light matter interaction in integrated devices.",0.0],["the rigid existing Internet of things architecture blocks current traffic management technology to provide a real differentiated","Dynamic Load-Balancing Vertical Control for Large-Scale Software-Defined Internet of Things","summarize: As the global Internet of things increasingly is popular with consumers and business environment, network flow management has become an important topic to optimize the performance on Internet of Things. The rigid existing Internet of things architecture blocks current traffic management technology to provide a real differentiated service for large-scale IoT. Software-defined Internet of Things is a new computing paradigm that separates control plane and data plane, and enables centralized logic control. In this paper, we first present a general framework for SD-IoT, which consists of two main components: SD-IoT controllers and SD-IoT switches. The controllers of SD-IoT uses resource pooling technology, and the pool is responsible for the centralized control of the entire network. The switches of SD-IoT integrate with the gateway functions, which is responsible for data access and forwarding. The SD-IoT controller pool is designed as a vertical control architecture, which includes the main control layer and the base control layer. The controller of the main control layer interacts upward with the application layer, interacts with the base control layer downwards, and the controller of the basic control layer interacts with the data forwarding layer. We propose a dynamic balancing algorithm of the main controller based on election mechanism and a dynamic load balancing algorithm of the basic controller based on the balanced delay, respectively. The experimental results show that the dynamic balancing algorithm based on the election mechanism can ensure the consistency of the messages between the main controllers, and the dynamic load balancing algorithm based on the balanced delay can balance between these different workloads in the basic controllers.",0.3913043478],["CS methods always recover the scene images in pixel level. this causes the smoothness","Perceptual Compressive Sensing","summarize: Compressive sensing works to acquire measurements at sub-Nyquist rate and recover the scene images. Existing CS methods always recover the scene images in pixel level. This causes the smoothness of recovered images and lack of structure information, especially at a low measurement rate. To overcome this drawback, in this paper, we propose perceptual CS to obtain high-level structured recovery. Our task no longer focuses on pixel level. Instead, we work to make a better visual effect. In detail, we employ perceptual loss, defined on feature level, to enhance the structure information of the recovered images. Experiments show that our method achieves better visual results with stronger structure information than existing CS methods at the same measurement rate.",0.0625],["numerical optimization has played a central role in addressing wireless resource management problems. but optimization","Learning to Optimize: Training Deep Neural Networks for Wireless Resource Management","summarize: For the past couple of decades, numerical optimization has played a central role in addressing wireless resource management problems such as power control and beamformer design. However, optimization algorithms often entail considerable complexity, which creates a serious gap between theoretical design\/analysis and real-time processing. To address this challenge, we propose a new learning-based approach. The key idea is to treat the input and output of a resource allocation algorithm as an unknown non-linear mapping and use a deep neural network to approximate it. If the non-linear mapping can be learned accurately by a DNN of moderate size, then resource allocation can be done in almost real time -- since passing the input through a DNN only requires a small number of simple operations. In this work, we address both the thereotical and practical aspects of DNN-based algorithm approximation with applications to wireless resource management. We first pin down a class of optimization algorithms that are `learnable' in theory by a fully connected DNN. Then, we focus on DNN-based approximation to a popular power allocation algorithm named WMMSE . We show that using a DNN to approximate WMMSE can be fairly accurate -- the approximation error ",0.2592592593],["a distribution over kernels formed by modelling a spectral mixture density with a","Scalable L\\'evy Process Priors for Spectral Kernel Learning","summarize: Gaussian processes are rich distributions over functions, with generalization properties determined by a kernel function. When used for long-range extrapolation, predictions are particularly sensitive to the choice of kernel parameters. It is therefore critical to account for kernel uncertainty in our predictive distributions. We propose a distribution over kernels formed by modelling a spectral mixture density with a L\\'evy process. The resulting distribution has support for all stationary covariances--including the popular RBF, periodic, and Mat\\'ern kernels--combined with inductive biases which enable automatic and data efficient learning, long-range extrapolation, and state of the art predictive performance. The proposed model also presents an approach to spectral regularization, as the L\\'evy process introduces a sparsity-inducing prior over mixture components, allowing automatic selection over model order and pruning of extraneous components. We exploit the algebraic structure of the proposed process for ",0.5454545455],["magnetic field of a homogeneously magnetized cylindrical tile geometry is found. the","The magnetic field from a homogeneously magnetized cylindrical tile","summarize: The magnetic field of a homogeneously magnetized cylindrical tile geometry, i.e. an angular section of a finite hollow cylinder, is found. The field is expressed as the product between a tensor field describing the geometrical part of the problem and a column vector holding the magnetization of the tile. Outside the tile, the tensor is identical to the demagnetization tensor. We find that four components of the tensor, ",0.6666666667],["new model-based reinforcement learning algorithm uses supervision to constrain exploration and learn efficiently while handling complex constraints","Safety Augmented Value Estimation from Demonstrations : Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","summarize: Reinforcement learning for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations , which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations. Code and supplementary material is available at https:\/\/tinyurl.com\/saved-rl.",0.0],["the deployment of GPS sensors aboard taxis and buses has led to an abundance of data on human","Can multimodal sensing detect and localize transient events?","summarize: With the increased focus on making cities smarter, we see an upsurge in investment in sensing technologies embedded in the urban infrastructure. The deployment of GPS sensors aboard taxis and buses, smartcards replacing paper tickets, and other similar initiatives have led to an abundance of data on human mobility, generated at scale and available real-time. Further still, users of social media platforms such as Twitter and LBSNs continue to voluntarily share multimedia content revealing in-situ information on their respective localities. The availability of such longitudinal multimodal data not only allows for both the characterization of the dynamics of the city, but also, in detecting anomalies, resulting from events that disrupt such dynamics, transiently. In this work, we investigate the capabilities of such urban sensor modalities, both physical and social, in detecting a variety of local events of varying intensities using statistical outlier detection techniques. We look at loading levels on arriving bus stops, telecommunication records, and taxi trips, accrued via the public APIs made available through the local transport authorities from Singapore and New York City, and Twitter\/Foursquare check-ins collected during the same period, and evaluate against a set of events assimilated from multiple event websites. In particular, we report on our early findings on the spatial impact evident via each modality , and the utility in combining decisions from the collection of sensors using rudimentary fusion techniques.",0.1739130435],["co-part segmentation methods are mostly supervised learning. the network learns to predict","Motion-supervised Co-Part Segmentation","summarize: Recent co-part segmentation methods mostly operate in a supervised learning setting, which requires a large amount of annotated data for training. To overcome this limitation, we propose a self-supervised deep learning method for co-part segmentation. Differently from previous works, our approach develops the idea that motion information inferred from videos can be leveraged to discover meaningful object parts. To this end, our method relies on pairs of frames sampled from the same video. The network learns to predict part segments together with a representation of the motion between two frames, which permits reconstruction of the target image. Through extensive experimental evaluation on publicly available video sequences we demonstrate that our approach can produce improved segmentation maps with respect to previous self-supervised co-part segmentation approaches.",0.0833333333],["in this paper we introduce a transformation technique. the method can prove existence and uniqueness","A Numerical Method for SDEs with Discontinuous Drift","summarize: In this paper we introduce a transformation technique, which can on the one hand be used to prove existence and uniqueness for a class of SDEs with discontinuous drift coefficient. One the other hand we present a numerical method based on transforming the Euler-Maruyama scheme for such a class of SDEs. We prove convergence of order ",0.0952380952],["network traffic forensic plays an important role in cybercrime investigation. we used a","Network Traffic Forensics on Firefox Mobile OS: Facebook, Twitter and Telegram as Case Studies","summarize: Development of mobile web-centric OS such as Firefox OS has created new challenges, and opportunities for digital investigators. Network traffic forensic plays an important role in cybercrime investigation to detect subject and object of the crime. In this chapter, we detect and analyze residual network traffic artefacts of Firefox OS in relation to two popular social networking applications and one instant messaging application . We utilized a Firefox OS simulator to generate relevant traffic while all communication data were captured using network monitoring tools. Captured network packets were examined and remnants with forensic value were reported. This paper as the first focused study on mobile Firefox OS network traffic analysis should pave the way for the future research in this direction.",0.3240863775],["s4cmb is a Python package designed to simulate raw data streams in time","Simulating instrumental systematics of Cosmic Microwave Background experiments with s4cmb","summarize: The observation of cosmic microwave background anisotropies is one of the key probes of physical cosmology. The weak nature of this signal has driven the construction of increasingly complex and sensitive experiments observing the sky at multiple frequencies with thousands of polarization sensitive detectors. Given the high sensitivity of such experiments, instrumental systematic effects can become the limiting factor towards the full scientific exploitation of their data. In this paper we present s4cmb , a Python package designed to simulate raw data streams in time domain of modern CMB experiments based on bolometric technology, and to inject in these realistic instrumental systematics effects. The aim of the package is to help assessing the contamination due to instrumental systematic effects on real data, to guide the design of future instruments, as well as to increase the realism of simulated data sets required in the development of accurate data analysis methods.",0.35],["authors review several refinements of Young's integral inequality. authors review several refinements of","Refinements of Young's integral inequality via fundamental inequalities and mean value theorems for derivatives","summarize: In the paper, the authors review several refinements of Young's integral inequality via several mean value theorems, such as Lagrange's and Taylor's mean value theorems of Lagrange's and Cauchy's type remainders, and via several fundamental inequalities, such as \\veby\\vev's integral inequality, Hermite--Hadamard's type integral inequalities, H\\older's integral inequality, and Jensen's discrete and integral inequalities, in terms of higher order derivatives and their norms, survey several applications of several refinements of Young's integral inequality, and further refine Young's integral inequality via P\\'olya's type integral inequalities.",0.2012958867],["skew monoidal categories are a perfect correspondence between skew monoidal categories and","Skew monoidal categories and skew multicategories","summarize: We describe a perfect correspondence between skew monoidal categories and certain generalised multicategories, called skew multicategories, that arise in nature.",0.5416666667],["the primary goal of a Smart City is to counteract these problems and mitigate their effects by","MONICA in Hamburg: Towards Large-Scale IoT Deployments in a Smart City","summarize: Modern cities and metropolitan areas all over the world face new management challenges in the 21st century primarily due to increasing demands on living standards by the urban population. These challenges range from climate change, pollution, transportation, and citizen engagement, to urban planning, and security threats. The primary goal of a Smart City is to counteract these problems and mitigate their effects by means of modern ICT to improve urban administration and infrastructure. Key ideas are to utilise network communication to inter-connect public authorities; but also to deploy and integrate numerous sensors and actuators throughout the city infrastructure - which is also widely known as the Internet of Things . Thus, IoT technologies will be an integral part and key enabler to achieve many objectives of the Smart City vision. The contributions of this paper are as follows. We first examine a number of IoT platforms, technologies and network standards that can help to foster a Smart City environment. Second, we introduce the EU project MONICA which aims for demonstration of large-scale IoT deployments at public, inner-city events and give an overview on its IoT platform architecture. And third, we provide a case-study report on SmartCity activities by the City of Hamburg and provide insights on recent field tests of a vertically integrated, end-to-end IoT sensor application.",0.32],["Gavruta introduces the new gavruta.","Weaving K-frames in Hilbert Spaces","summarize: Gavruta introduced ",0.0],[".","","summarize: In this work we prove optimal ",0.0],["a number of Monte Carlo samples are usually required to obtain a smoother.","Approximate Smoothing and Parameter Estimation in High-Dimensional State-Space Models","summarize: We present approximate algorithms for performing smoothing in a class of high-dimensional state-space models via sequential Monte Carlo methods . In high dimensions, a prohibitively large number of Monte Carlo samples -- growing exponentially in the dimension of the state space -- is usually required to obtain a useful smoother. Using blocking strategies as in Rebeschini and Van Handel , we exploit the spatial ergodicity properties of the model to circumvent this curse of dimensionality. We thus obtain approximate smoothers that can be computed recursively in time and in parallel in space. First, we show that the bias of our blocked smoother is bounded uniformly in the time horizon and in the model dimension. We then approximate the blocked smoother with particles and derive the asymptotic variance of idealised versions of our blocked particle smoother to show that variance is no longer adversely effected by the dimension of the model. Finally, we employ our method to successfully perform maximum-likelihood estimation via stochastic gradient-ascent and stochastic expectation--maximisation algorithms in a 100-dimensional state-space model.",0.56],["the ALICE collaboration measured the recent results for D-meson production.","Mid-rapidity D-meson production in pp, Pb-Pb and p-Pb collisions at the LHC","summarize: We present the recent results for D-meson production measured by the ALICE collaboration in pp collisions at ",0.272910251],["chroma intra-prediction is a complex and computationally intensive coding scheme","Attention-Based Neural Networks for Chroma Intra Prediction in Video Coding","summarize: Neural networks can be successfully used to improve several modules of advanced video coding schemes. In particular, compression of colour components was shown to greatly benefit from usage of machine learning models, thanks to the design of appropriate attention-based architectures that allow the prediction to exploit specific samples in the reference region. However, such architectures tend to be complex and computationally intense, and may be difficult to deploy in a practical video coding pipeline. This work focuses on reducing the complexity of such methodologies, to design a set of simplified and cost-effective attention-based architectures for chroma intra-prediction. A novel size-agnostic multi-model approach is proposed to reduce the complexity of the inference process. The resulting simplified architecture is still capable of outperforming state-of-the-art methods. Moreover, a collection of simplifications is presented in this paper, to further reduce the complexity overhead of the proposed prediction architecture. Thanks to these simplifications, a reduction in the number of parameters of around 90% is achieved with respect to the original attention-based methodologies. Simplifications include a framework for reducing the overhead of the convolutional operations, a simplified cross-component processing model integrated into the original architecture, and a methodology to perform integer-precision approximations with the aim to obtain fast and hardware-aware implementations. The proposed schemes are integrated into the Versatile Video Coding prediction pipeline, retaining compression efficiency of state-of-the-art chroma intra-prediction methods based on neural networks, while offering different directions for significantly reducing coding complexity.",0.2666666667],["cellOS: Hides low-level network details through a general virtual network abstraction.","CellOS: Zero-touch Softwarized Open Cellular Networks","summarize: Current cellular networks rely on closed and inflexible infrastructure tightly controlled by a handful of vendors. Their configuration requires vendor support and lengthy manual operations, which prevent Telco Operators from unlocking the full network potential and from performing fine grained performance optimization, especially on a per-user basis. To address these key issues, this paper introduces CellOS, a fully automated optimization and management framework for cellular networks that requires negligible intervention . CellOS leverages softwarization and automatic optimization principles to bridge Software-Defined Networking and cross-layer optimization. Unlike state-of-the-art SDN-inspired solutions for cellular networking, CellOS: Hides low-level network details through a general virtual network abstraction; allows TOs to define high-level control objectives to dictate the desired network behavior without requiring knowledge of optimization techniques, and automatically generates and executes distributed control programs for simultaneous optimization of heterogeneous control objectives on multiple network slices. CellOS has been implemented and evaluated on an indoor testbed with two different LTE-compliant implementations: OpenAirInterface and srsLTE. We further demonstrated CellOS capabilities on the long-range outdoor POWDER-RENEW PAWR 5G platform. Results from scenarios with multiple base stations and users show that CellOS is platform-independent and self-adapts to diverse network deployments. Our investigation shows that CellOS outperforms existing solutions on key metrics, including throughput , energy efficiency and fairness .",0.1111111111],["a signed graph","Homomorphism bounds of signed bipartite ","summarize: A signed graph ",0.1839397206],["wholeness is defined as a mathematical structure of physical space in our surroundings. but there","A Recursive Definition of Goodness of Space for Bridging the Concepts of Space and Place for Sustainability","summarize: Conceived and developed by Christopher Alexander through his life's work: The Nature of Order, wholeness is defined as a mathematical structure of physical space in our surroundings. Yet, there was no mathematics, as Alexander admitted then, that was powerful enough to capture his notion of wholeness. Recently, a mathematical model of wholeness, together with its topological representation, has been developed that is capable of addressing not only why a space is good, but also how much goodness the space has. This paper develops a structural perspective on goodness of space - both large- and small-scale - in order to bridge two basic concepts of space and place through the very concept of wholeness. The wholeness provides a de facto recursive definition of goodness of space from a holistic and organic point of view. A space is good, genuinely and objectively, if its adjacent spaces are good, the larger space to which it belongs is good, and what is contained in the space is also good. Eventually, goodness of space - sustainability of space - is considered a matter of fact rather than of opinion under the new view of space: space is neither lifeless nor neutral, but a living structure capable of being more living or less living, or more sustainable or less sustainable. Under the new view of space, geography or architecture will become part of complexity science, not only for understanding complexity, but also for making and remaking complex or living structures. Keywords: Scaling law, head\/tail breaks, living structure, beauty, streets, cities",0.3805101387],["flow instabilities and fluctuating shear stresses are held responsible for a variety of cardiovascular","Nonlinear hydrodynamic instability and turbulence in pulsatile flow","summarize: Pulsating flows through tubular geometries are laminar provided that velocities are moderate. This in particular is also believed to apply to cardiovascular flows where inertial forces are typically too low to sustain turbulence. On the other hand flow instabilities and fluctuating shear stresses are held responsible for a variety of cardiovascular diseases. Here we report a nonlinear instability mechanism for pulsating pipe flow that gives rise to bursts of turbulence at low flow rates. Geometrical distortions of small, yet finite amplitude are found to excite a state consisting of helical vortices during flow deceleration. The resulting flow pattern grows rapidly in magnitude, breaks down into turbulence, and eventually returns to laminar when the flow accelerates. This scenario causes shear stress fluctuations and flow reversal during each pulsation cycle. Such unsteady conditions can adversely affect blood vessels and have been shown to promote inflammation and dysfunction of the shear stress sensitive endothelial cell layer.",0.3043478261],["rBPF is a register-based VM based on extended Berkeley Packe","Minimal Virtual Machines on IoT Microcontrollers: The Case of Berkeley Packet Filters with rBPF","summarize: Virtual machines are widely used to host and isolate software modules. However, extremely small memory and low-energy budgets have so far prevented wide use of VMs on typical microcontroller-based IoT devices. In this paper, we explore the potential of two minimal VM approaches on such low-power hardware. We design rBPF, a register-based VM based on extended Berkeley Packet Filters . We compare it with a stack-based VM based on WebAssembly adapted for embedded systems. We implement prototypes of each VM, hosted in the IoT operating system RIOT. We perform measurements on commercial off-the-shelf IoT hardware. Unsurprisingly, we observe that both Wasm and rBPF virtual machines yield execution time and memory overhead, compared to not using a VM. We show however that this execution time overhead is tolerable for low-throughput, low-energy IoT devices. We further show that, while using a VM based on Wasm entails doubling the memory budget for a simple networked IoT application using a 6LoWPAN\/CoAP stack, using a VM based on rBPF requires only negligible memory overhead . rBPF is thus a promising approach to host small software modules, isolated from OS software, and updatable on-demand, over low-power networks.",0.3575040246],["the instrument onboard the InSight mission to Mars is the critical instrument. it is","Evaluating the wind-induced mechanical noise on the InSight seismometers","summarize: The SEIS instrument onboard the InSight mission to Mars is the critical instrument for determining the interior structure of Mars, the current level of tectonic activity and the meteorite flux. Meeting the performance requirements of the SEIS instrument is vital to successfully achieve these mission objectives. Here we analyse in-situ wind measurements from previous Mars space missions to understand the wind environment that we are likely to encounter on Mars, and then we use an elastic ground deformation model to evaluate the mechanical noise contributions on the SEIS instrument due to the interaction between the Martian winds and the InSight lander. Lander mechanical noise maps that will be used to select the best deployment site for SEIS once the InSight lander arrives on Mars are also presented. We find the lander mechanical noise may be a detectable signal on the InSight seismometers. However, for the baseline SEIS deployment position, the noise is expected to be below the total noise requirement >97% of the time and is, therefore, not expected to endanger the InSight mission objectives.",0.4074074074],["factorizations are a factoring factor.","Set-Direct Factorizations of Groups","summarize: We consider factorizations ",0.1666666667],["we have developed an online radiative-transfer suite applicable to a broad range of","Planetary Spectrum Generator: an accurate online radiative transfer suite for atmospheres, comets, small bodies and exoplanets","summarize: We have developed an online radiative-transfer suite applicable to a broad range of planetary objects . The Planetary Spectrum Generator can synthesize planetary spectra for a broad range of wavelengths from any observatory , any orbiter , or any lander . This is achieved by combining several state-of-the-art radiative transfer models, spectroscopic databases and planetary databases . PSG has a 3D orbital calculator for most bodies in the solar system, and all confirmed exoplanets, while the radiative-transfer models can ingest billions of spectral signatures for hundreds of species from several spectroscopic repositories. It integrates the latest radiative-transfer and scattering methods in order to compute high resolution spectra via line-by-line calculations, and utilizes the efficient correlated-k method at moderate resolutions, while for computing cometary spectra, PSG handles non-LTE and LTE excitation processes. PSG includes a realistic noise calculator that integrates several telescope \/ instrument configurations and detector technologies . Such an integration of advanced spectroscopic methods into an online tool can greatly serve the planetary community, ultimately enabling the retrieval of planetary parameters from remote sensing data, efficient mission planning strategies, interpretation of current and future planetary data, calibration of spectroscopic data, and development of new instrument\/spacecraft concepts.",0.5292817719],["cite is one of the most promising agent-based models for market impact. it","Market Impact in a Latent Order Book","summarize: The latent order book of \\cite is one of the most promising agent-based models for market impact. This work extends the minimal model by allowing agents to exhibit mean-reversion, a commonly observed pattern in real markets. This modification leads to new order book dynamics, which we explicitly study and analyze. Underlying our analysis is a mean-field assumption that views the order book through its \\textit density. We show how price impact develops in this new model, providing a flexible family of solutions that can potentially improve calibration to real data. While no closed-form solution is provided, we complement our theoretical investigation with extensive numerical results, including a simulation scheme for the entire order book.",0.0],["a panel of 14 observers graded 160 biopsies with and without AI assistance","Artificial Intelligence Assistance Significantly Improves Gleason Grading of Prostate Biopsies by Pathologists","summarize: While the Gleason score is the most important prognostic marker for prostate cancer patients, it suffers from significant observer variability. Artificial Intelligence systems, based on deep learning, have proven to achieve pathologist-level performance at Gleason grading. However, the performance of such systems can degrade in the presence of artifacts, foreign tissue, or other anomalies. Pathologists integrating their expertise with feedback from an AI system could result in a synergy that outperforms both the individual pathologist and the system. Despite the hype around AI assistance, existing literature on this topic within the pathology domain is limited. We investigated the value of AI assistance for grading prostate biopsies. A panel of fourteen observers graded 160 biopsies with and without AI assistance. Using AI, the agreement of the panel with an expert reference standard significantly increased . Our results show the added value of AI systems for Gleason grading, but more importantly, show the benefits of pathologist-AI synergy.",0.3684210526],["we develop a framework for the average-case analysis of random quadratic problems.","Average-case Acceleration Through Spectral Density Estimation","summarize: We develop a framework for the average-case analysis of random quadratic problems and derive algorithms that are optimal under this analysis. This yields a new class of methods that achieve acceleration given a model of the Hessian's eigenvalue distribution. We develop explicit algorithms for the uniform, Marchenko-Pastur, and exponential distributions. These methods are momentum-based algorithms, whose hyper-parameters can be estimated without knowledge of the Hessian's smallest singular value, in contrast with classical accelerated methods like Nesterov acceleration and Polyak momentum. Through empirical benchmarks on quadratic and logistic regression problems, we identify regimes in which the the proposed methods improve over classical accelerated methods.",0.2380952381],["the dynamic dispatch of battery energy storage systems in microgrids integrated with volatile energy resources is","Stochastic Dispatch of Energy Storage in Microgrids: An Augmented Reinforcement Learning Approach","summarize: The dynamic dispatch of battery energy storage systems in microgrids integrated with volatile energy resources is essentially a multiperiod stochastic optimization problem . Because the life span of a BESS is significantly affected by its charging and discharging behaviors, its lifecycle degradation costs should be incorporated into the DD model of BESSs, which makes it non-convex. In general, this MSOP is intractable. To solve this problem, we propose a reinforcement learning solution augmented with Monte-Carlo tree search and domain knowledge expressed as dispatching rules. In this solution, the Q-learning with function approximation is employed as the basic learning architecture that allows multistep bootstrapping and continuous policy learning. To improve the computation efficiency of randomized multistep simulations, we employed the MCTS to estimate the expected maximum action values. Moreover, we embedded a few dispatching rules in RL as probabilistic logics to reduce infeasible action explorations, which can improve the quality of the data-driven solution. Numerical test results show the proposed algorithm outperforms other baseline RL algorithms in all cases tested.",0.2],["management decisions are crucial to achieving a given management goal. a spatially explicit","Exploring the effect of the spatial scale of fishery management","summarize: For any spatially explicit management, determining the appropriate spatial scale of management decisions is critical to success at achieving a given management goal. Specifically, managers must decide how much to subdivide a given managed region: from implementing a uniform approach across the region to considering a unique approach in each of one hundred patches and everything in between. Spatially explicit approaches, such as the implementation of marine spatial planning and marine reserves, are increasingly used in fishery management. Using a spatially explicit bioeconomic model, we quantify how the management scale affects optimal fishery profit, biomass, fishery effort, and the fraction of habitat in marine reserves. We find that, if habitats are randomly distributed, the fishery profit increases almost linearly with the number of segments. However, if habitats are positively autocorrelated, then the fishery profit increases with diminishing returns. Therefore, the true optimum in management scale given cost to subdivision depends on the habitat distribution pattern.",0.3243243243],["we use hundreds of annotated real-world gaits to classify perceived human emotion from","STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits","summarize: We present a novel classifier network called STEP, to classify perceived human emotion from gaits, based on a Spatial Temporal Graph Convolutional Network architecture. Given an RGB video of an individual walking, our formulation implicitly exploits the gait features to classify the emotional state of the human into one of four emotions: happy, sad, angry, or neutral. We use hundreds of annotated real-world gait videos and augment them with thousands of annotated synthetic gaits generated using a novel generative network called STEP-Gen, built on an ST-GCN based Conditional Variational Autoencoder . We incorporate a novel push-pull regularization loss in the CVAE formulation of STEP-Gen to generate realistic gaits and improve the classification accuracy of STEP. We also release a novel dataset , which consists of ",0.0769230769],["limb orientations are modeled jointly with 2D keypoint detections. the 3","OriNet: A Fully Convolutional Network for 3D Human Pose Estimation","summarize: In this paper, we propose a fully convolutional network for 3D human pose estimation from monocular images. We use limb orientations as a new way to represent 3D poses and bind the orientation together with the bounding box of each limb region to better associate images and predictions. The 3D orientations are modeled jointly with 2D keypoint detections. Without additional constraints, this simple method can achieve good results on several large-scale benchmarks. Further experiments show that our method can generalize well to novel scenes and is robust to inaccurate bounding boxes.",0.0909090909],["the kinematic jerk index is estimated on the basis of gyr","A Smartphone-Based Acquisition System for Hips Rotation Fluency Assessment","summarize: The present contribution is motivated by recent studies on the assessment of the fluency of body movements during complex motor tasks. In particular, we focus on the estimation of the Cartesian kinematic jerk of the hips' orientation during a full three-dimensional movement. The kinematic jerk index is estimated on the basis of gyroscopic signals acquired through a smartphone. A specific free mobile application available for the Android mobile operating system, HyperIMU, is used to acquire the gyroscopic signals and to transmit them to a personal computer via a User Datagram Protocol through a wireless network. The personal computer elaborates the acquired data through a MATLAB script, either in real time or offline, and returns the kinematic jerk index associated to a motor task.",0.1428571429],["phishing attacks have become increasingly necessary. we propose a deep learning based data","HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep Learning Techniques on HTML Analysis","summarize: Recently, the development and implementation of phishing attacks require little technical skills and costs. This uprising has led to an ever-growing number of phishing attacks on the World Wide Web. Consequently, proactive techniques to fight phishing attacks have become extremely necessary. In this paper, we propose HTMLPhish, a deep learning based data-driven end-to-end automatic phishing web page classification approach. Specifically, HTMLPhish receives the content of the HTML document of a web page and employs Convolutional Neural Networks to learn the semantic dependencies in the textual contents of the HTML. The CNNs learn appropriate feature representations from the HTML document embeddings without extensive manual feature engineering. Furthermore, our proposed approach of the concatenation of the word and character embeddings allows our model to manage new features and ensure easy extrapolation to test data. We conduct comprehensive experiments on a dataset of more than 50,000 HTML documents that provides a distribution of phishing to benign web pages obtainable in the real-world that yields over 93 percent Accuracy and True Positive Rate. Also, HTMLPhish is a completely language-independent and client-side strategy which can, therefore, conduct web page phishing detection regardless of the textual language.",0.1683565598],["a seq2seq model is widely adopted in Neural Machine Translation.","Guiding attention in Sequence-to-sequence models for Dialogue Act prediction","summarize: The task of predicting dialog acts based on conversational dialog is a key component in the development of conversational agents. Accurately predicting DAs requires a precise modeling of both the conversation and the global tag dependencies. We leverage seq2seq approaches widely adopted in Neural Machine Translation to improve the modelling of tag sequentiality. Seq2seq models are known to learn complex global dependencies while currently proposed approaches using linear conditional random fields only model local tag dependencies. In this work, we introduce a seq2seq model tailored for DA classification using: a hierarchical encoder, a novel guided attention mechanism and beam search applied to both training and inference. Compared to the state of the art our model does not require handcrafted features and is trained end-to-end. Furthermore, the proposed approach achieves an unmatched accuracy score of 85% on SwDA, and state-of-the-art accuracy score of 91.6% on MRDA.",0.3125],["limit roots of a root system for W are the accumulation points of directions of roots of","On the Limit Set of Root Systems of Coxeter Groups acting on Lorentzian spaces","summarize: The notion of limit roots of a Coxeter group W was recently introduced : they are the accumulation points of directions of roots of a root system for W. In the case where the root system lives in a Lorentzian space W admits a faithful representation as a discrete reflection group of isometries on a hyperbolic space; the accumulation set of any of its orbits is then classically called the limit set of W. In this article we show that the limit roots of a Coxeter group W acting on a Lorentzian space is equal to the limit set of W seen as a discrete reflection group of hyperbolic isometries. We aim for this article to be as self-contained as possible in order to be accessible to the community familiar with reflection groups and root systems and to the community familiar with discrete subgroups of isometries in hyperbolic geometry.",0.3333333333],["this paper we continue the investigation of the regularity of the so-called weak.","On regularity theory for n\/p-harmonic maps into manifolds","summarize: In this paper we continue the investigation of the regularity of the so-called weak ",0.1818181818],["van der Waals heterostructures are a new approach to produce artificial systems. we","Synthetic Semimetals with van der Waals Interfaces","summarize: The assembly of suitably designed van der Waals heterostructures represents a new approach to produce artificial systems with engineered electronic properties. Here, we apply this strategy to realize synthetic semimetals based on vdW interfaces formed by two different semiconductors. Guided by existing ab-initio calculations, we select WSe",0.380952381],["XACMET defines a typed graph that models the XACML policy evaluation","An automated model-based test oracle for access control systems","summarize: In the context of XACML-based access control systems, an intensive testing activity is among the most adopted means to assure that sensible information or resources are correctly accessed. Unfortunately, it requires a huge effort for manual inspection of results: thus automated verdict derivation is a key aspect for improving the cost-effectiveness of testing. To this purpose, we introduce XACMET, a novel approach for automated model-based oracle definition. XACMET defines a typed graph, called the XAC-Graph, that models the XACML policy evaluation. The expected verdict of a specific request execution can thus be automatically derived by executing the corresponding path in such graph. Our validation of the XACMET prototype implementation confirms the effectiveness of the proposed approach.",0.3333333333],["a time-variable photochemical model is used to study the distribution of stratos","Seasonal stratospheric photochemistry on Uranus and Neptune","summarize: A time-variable 1D photochemical model is used to study the distribution of stratospheric hydrocarbons as a function of altitude, latitude, and season on Uranus and Neptune. The results for Neptune indicate that in the absence of stratospheric circulation or other meridional transport processes, the hydrocarbon abundances exhibit strong seasonal and meridional variations in the upper stratosphere, but that these variations become increasingly damped with depth due to increasing dynamical and chemical time scales. At high altitudes, hydrocarbon mixing ratios are typically largest where the solar insolation is the greatest, leading to strong hemispheric dichotomies between the summer-to-fall hemisphere and winter-to-spring hemisphere. At mbar pressures and deeper, slower chemistry and diffusion lead to latitude variations that become more symmetric about the equator. On Uranus, the stagnant, poorly mixed stratosphere confines methane and its photochemical products to higher pressures, where chemistry and diffusion time scales remain large. Seasonal variations in hydrocarbons are therefore predicted to be more muted on Uranus, despite the planet's very large obliquity. Radiative-transfer simulations demonstrate that latitude variations in hydrocarbons on both planets are potentially observable with future JWST mid-infrared spectral imaging. Our seasonal model predictions for Neptune compare well with retrieved C2H2 and C2H6 abundances from spatially resolved ground-based observations , suggesting that stratospheric circulation -- which was not included in these models -- may have little influence on the large-scale meridional hydrocarbon distributions on Neptune, unlike the situation on Jupiter and Saturn.",0.4736842105],["financial intelligence is the core technology of the AI 2.0 era. it has elicit","FinBrain: When Finance Meets AI 2.0","summarize: Artificial intelligence is the core technology of technological revolution and industrial transformation. As one of the new intelligent needs in the AI 2.0 era, financial intelligence has elicited much attention from the academia and industry. In our current dynamic capital market, financial intelligence demonstrates a fast and accurate machine learning capability to handle complex data and has gradually acquired the potential to become a financial brain. In this work, we survey existing studies on financial intelligence. First, we describe the concept of financial intelligence and elaborate on its position in the financial technology field. Second, we introduce the development of financial intelligence and review state-of-the-art techniques in wealth management, risk management, financial security, financial consulting, and blockchain. Finally, we propose a research framework called FinBrain and summarize four open issues, namely, explainable financial agents and causality, perception and prediction under uncertainty, risk-sensitive and robust decision making, and multi-agent game and mechanism design. We believe that these research directions can lay the foundation for the development of AI 2.0 in the finance field.",0.1176470588],["the study of pointwise power decay rates is completed. the exact decay rate is determined by","Decay rates for the quadratic and super-quadratic tilt-excess of integral varifolds","summarize: This paper concerns integral varifolds of arbitrary dimension in an open subset of Euclidean space satisfying integrability conditions on their first variation. Firstly, the study of pointwise power decay rates almost everywhere of the quadratic tilt-excess is completed by establishing the precise decay rate for two-dimensional integral varifolds of locally bounded first variation. In order to obtain the exact decay rate, a coercive estimate involving a height-excess quantity measured in Orlicz spaces is established. Moreover, counter-examples to pointwise power decay rates almost everywhere of the super-quadratic tilt-excess are obtained. These examples are optimal in terms of the dimension of the varifold and the exponent of the integrability condition in most cases, for example if the varifold is not two-dimensional. These examples also demonstrate that within the scale of Lebesgue spaces no local higher integrability of the second fundamental form, of an at least two-dimensional curvature varifold, may be deduced from boundedness of its generalised mean curvature vector. Amongst the tools are Cartesian products of curvature varifolds.",0.2],["neural networks have led to state-of-the-art results in many medical imaging tasks including","Layer-Wise Relevance Propagation for Explaining Deep Neural Network Decisions in MRI-Based Alzheimer's Disease Classification","summarize: Deep neural networks have led to state-of-the-art results in many medical imaging tasks including Alzheimer's disease detection based on structural magnetic resonance imaging data. However, the network decisions are often perceived as being highly non-transparent, making it difficult to apply these algorithms in clinical routine. In this study, we propose using layer-wise relevance propagation to visualize convolutional neural network decisions for AD based on MRI data. Similarly to other visualization methods, LRP produces a heatmap in the input space indicating the importance\/relevance of each voxel contributing to the final classification outcome. In contrast to susceptibility maps produced by guided backpropagation , the LRP method is able to directly highlight positive contributions to the network classification in the input space. In particular, we show that the LRP method is very specific for individuals with high inter-patient variability, there is very little relevance for AD in healthy controls and areas that exhibit a lot of relevance correlate well with what is known from literature. To quantify the latter, we compute size-corrected metrics of the summed relevance per brain area, e.g., relevance density or relevance gain. Although these metrics produce very individual fingerprints of relevance patterns for AD patients, a lot of importance is put on areas in the temporal lobe including the hippocampus. After discussing several limitations such as sensitivity toward the underlying model and computation parameters, we conclude that LRP might have a high potential to assist clinicians in explaining neural network decisions for diagnosing AD based on structural MRI data.",0.1736177022],["CPCL is a distributed MIMO radar service. it can be offered by mobile","Cooperative Passive Coherent Location: A Promising 5G Service to Support Road Safety","summarize: 5G promises many new vertical service areas beyond simple communication and data transfer. We propose CPCL , a distributed MIMO radar service, which can be offered by mobile radio network operators as a service for public user groups. CPCL comes as an inherent part of the radio network and takes advantage of the most important key features proposed for 5G. It extends the well-known idea of passive radar by introducing cooperative principles. These range from cooperative, synchronous radio signaling, and MAC up to radar data fusion on sensor and scenario levels. By using software-defined radio and network paradigms, as well as real-time mobile edge computing facilities intended for 5G, CPCL promises to become a ubiquitous radar service which may be adaptive, reconfigurable, and perhaps cognitive. As CPCL makes double use of radio resources , it can be considered a green technology. Although we introduce the CPCL idea from the viewpoint of vehicle-to-vehicle\/infrastructure communication, it can definitely also be applied to many other applications in industry, transport, logistics, and for safety and security applications.",0.2941176471],["spectral method and semigroup theory are based on verified computations. the method","Rigorous numerical computations for 1D advection equations with variable coefficients","summarize: This paper provides a methodology of verified computing for solutions to 1-dimensional advection equations with variable coefficients. The advection equation is typical partial differential equations of hyperbolic type. There are few results of verified numerical computations to initial-boundary value problems of hyperbolic PDEs. Our methodology is based on the spectral method and semigroup theory. The provided method in this paper is regarded as an efficient application of semigroup theory in a sequence space associated with the Fourier series of unknown functions. This is a foundational approach of verified numerical computations for hyperbolic PDEs. Numerical examples show that the rigorous error estimate showing the well-posedness of the exact solution is given with high accuracy and high speed.",0.1875],["affluent spatial information can inhibit the subsequent SOD performance. the depth quality","Multi-level Cross-modal Interaction Network for RGB-D Salient Object Detection","summarize: Depth cues with affluent spatial information have been proven beneficial in boosting salient object detection , while the depth quality directly affects the subsequent SOD performance. However, it is inevitable to obtain some low-quality depth cues due to limitations of its acquisition devices, which can inhibit the SOD performance. Besides, existing methods tend to combine RGB images and depth cues in a direct fusion or a simple fusion module, which makes they can not effectively exploit the complex correlations between the two sources. Moreover, few methods design an appropriate module to fully fuse multi-level features, resulting in cross-level feature interaction insufficient. To address these issues, we propose a novel Multi-level Cross-modal Interaction Network for RGB-D based SOD. Our MCI-Net includes two key components: 1) a cross-modal feature learning network, which is used to learn the high-level features for the RGB images and depth cues, effectively enabling the correlations between the two sources to be exploited; and 2) a multi-level interactive integration network, which integrates multi-level cross-modal features to boost the SOD performance. Extensive experiments on six benchmark datasets demonstrate the superiority of our MCI-Net over 14 state-of-the-art methods, and validate the effectiveness of different components in our MCI-Net. More important, our MCI-Net significantly improves the SOD performance as well as has a higher FPS.",0.0],["generators of the Poincar'e group are usually expressed in the momentum space","Remarks on QFT in the Coordinate Space","summarize: Generators of the Poincar\\'e group, for a free massive scalar field, are usually expressed in the momentum space. In this work we perform a transformation of these generators into the coordinate space. This -position space is spanned by eigenvectors of the Newton-Wigner-Pryce operator. The motivation is a deeper understanding of the commutative spatial coordinate space in QFT, in order to investigate the non-commutative version thereof.",0.2666666667],["tin-selenide and tin-sulfide classes undergo","Electronic, vibrational, and electron-phonon coupling properties in SnSe","summarize: The tin-selenide and tin-sulfide classes of materials undergo multiple structural transitions under high pressure leading to periodic lattice distortions, superconductivity, and topologically non-trivial phases, yet a number of controversies exist regarding the structural transformations in these systems. We perform first-principles calculations within the framework of density functional theory and a careful comparison of our results with available experiments on SnSe",0.1097623272],["can we recover a complex signal from its Fourier magnitudes?.","A Geometric Analysis of Phase Retrieval","summarize: Can we recover a complex signal from its Fourier magnitudes? More generally, given a set of ",0.2307692308],["patching is a common activity in software development. it is generally performed on a","FixMiner: Mining Relevant Fix Patterns for Automated Program Repair","summarize: Patching is a common activity in software development. It is generally performed on a source code base to address bugs or add new functionalities. In this context, given the recurrence of bugs across projects, the associated similar patches can be leveraged to extract generic fix actions. While the literature includes various approaches leveraging similarity among patches to guide program repair, these approaches often do not yield fix patterns that are tractable and reusable as actionable input to APR systems. In this paper, we propose a systematic and automated approach to mining relevant and actionable fix patterns based on an iterative clustering strategy applied to atomic changes within patches. The goal of FixMiner is thus to infer separate and reusable fix patterns that can be leveraged in other patch generation systems. Our technique, FixMiner, leverages Rich Edit Script which is a specialized tree structure of the edit scripts that captures the AST-level context of the code changes. FixMiner uses different tree representations of Rich Edit Scripts for each round of clustering to identify similar changes. These are abstract syntax trees, edit actions trees, and code context trees. We have evaluated FixMiner on thousands of software patches collected from open source projects. Preliminary results show that we are able to mine accurate patterns, efficiently exploiting change information in Rich Edit Scripts. We further integrated the mined patterns to an automated program repair prototype, PARFixMiner, with which we are able to correctly fix 26 bugs of the Defects4J benchmark. Beyond this quantitative performance, we show that the mined fix patterns are sufficiently relevant to produce patches with a high probability of correctness: 81% of PARFixMiner's generated plausible patches are correct.",0.4137931034],["a variety of applications are envisioned to enable real-time control of metamaterials","Toward Localization in Terahertz-Operating Energy Harvesting Software-Defined Metamaterials: Context Analysis","summarize: Software-defined metamaterials represent a novel paradigm for real-time control of metamaterials. SDMs are envisioned to enable a variety of exciting applications in the domains such as smart textiles and sensing in challenging conditions. Many of these applications envisage deformations of the SDM structure . This affects the relative position of the metamaterial elements and requires their localization relative to each other. The question of how to perform such localization is, however, yet to spark in the community. We consider that the metamaterial elements are controlled wirelessly through a Terahertz -operating nanonetwork. Moreover, we consider the elements to be energy constrained, with their sole powering option being to harvest environmental energy. For such a setup, we demonstrate sub-millimeter accuracy of the two-way Time of Flight -based localization, as well as high availability of the service , which is a result of the low energy consumed in localization. Finally, we provide the localization context for a number of relevant system parameters such as operational frequency, bandwidth, and harvesting rate.",0.5652173913],["traditional solutions to HDR imaging are designed for and applied to CMOS image sensors.","HDR Imaging with Quanta Image Sensors: Theoretical Limits and Optimal Reconstruction","summarize: High dynamic range imaging is one of the biggest achievements in modern photography. Traditional solutions to HDR imaging are designed for and applied to CMOS image sensors . However, the mainstream one-micron CIS cameras today generally have a high read noise and low frame-rate. These, in turn, limit the acquisition speed and quality, making the cameras slow in the HDR mode. In this paper, we propose a new computational photography technique for HDR imaging. Recognizing the limitations of CIS, we use the Quanta Image Sensor to trade the spatial-temporal resolution with bit-depth. QIS is a single-photon image sensor that has comparable pixel pitch to CIS but substantially lower dark current and read noise. We provide a complete theoretical characterization of the sensor in the context of HDR imaging, by proving the fundamental limits in the dynamic range that QIS can offer and the trade-offs with noise and speed. In addition, we derive an optimal reconstruction algorithm for single-bit and multi-bit QIS. Our algorithm is theoretically optimal for \\emph linear reconstruction schemes based on exposure bracketing. Experimental results confirm the validity of the theory and algorithm, based on synthetic and real QIS data.",0.125],["correlation-OTDR is a multi-processor system on chip and a","Fiber as a temperature sensor with portable Correlation-OTDR as interrogator","summarize: In this paper, we report on the integration of a Correlation-OTDR into a portable unit , based on a multi-processor system on chip and a small formfactor pluggable 2.5G transceiver. Going beyond telecommunication applications, this system is demonstrated for temperature measurements, based on the change of propagation delay with temperature in a short section of optical fiber. The temperature measurement accuracy is investigated as a function of the fiber length from 4 to 25 meters.",0.5033471157],["pattern avoidance in permutations on partially ordered sets extends concept. number of per","Pattern Avoidance in Poset Permutations","summarize: We extend the concept of pattern avoidance in permutations on a totally ordered set to pattern avoidance in permutations on partially ordered sets. The number of permutations on ",0.125],["hazard function of empirical duration data is dominated by a bathtub curve.","Modelling the Dropout Patterns of MOOC Learners","summarize: We adopted survival analysis for the viewing durations of massive open online courses. The hazard function of empirical duration data is dominated by a bathtub curve and has the Lindy effect in its tail. To understand the evolutionary mechanisms underlying these features, we categorized learners into two classes due to their different distributions of viewing durations, namely lognormal distribution and power law with exponential cutoff. Two random differential equations are provided to describe the growth patterns of viewing durations for the two classes respectively. The expected duration change rate of the learners featured by lognormal distribution is supposed to be dependent on their past duration, and that of the rest learners is supposed to be inversely proportional to time. Solutions to the equations predict the features of viewing duration distributions, and those of the hazard function. The equations also reveal the feature of memory and that of memorylessness for the viewing behaviors of the two classes respectively.",0.15],["sparse control inputs arise naturally in networked systems. derived conditions can","Controllability of Linear Dynamical Systems Under Input Sparsity Constraints","summarize: In this work, we consider the controllability of a discrete-time linear dynamical system with sparse control inputs. Sparsity constraints on the input arises naturally in networked systems, where activating each input variable adds to the cost of control. We derive algebraic necessary and sufficient conditions for ensuring controllability of a system with an arbitrary transfer matrix. The derived conditions can be verified in polynomial time complexity, unlike the more traditional Kalman-type rank tests. Further, we characterize the minimum number of input vectors required to satisfy the derived conditions for controllability. Finally, we present a generalized Kalman decomposition-like procedure that separates the state-space into subspaces corresponding to sparse-controllable and sparse-uncontrollable parts. These results form a theoretical basis for designing networked linear control systems with sparse inputs.",0.1666666667],["Let us know what you think about it!","Serre's problem on the density of isotropic fibres in conic bundles","summarize: Let ",0.0],["the image under this parametrization is the interior of a polytope. the","The Hilbert metric on Teichm\\uller space and Earthquake","summarize: Hamenst\\adt gave a parametrization of the Teichm\\uller space of punctured surfaces such that the image under this parametrization is the interior of a polytope. In this paper, we study the Hilbert metric on the Teichm\\uller space of punctured surfaces based on this parametrization. We prove that every earthquake ray is an almost geodesic under the Hilbert metric.",0.1739130435],["field equations are derived in an extended theory of gravity. the model behaves as","Magnetized cosmological model with variable deceleration parameter","summarize: In this paper, we have derived the field equations in an extended theory of gravity in an anisotropic space time background and in the presence of magnetic field. The physical and geometrical parameters of the models are determined with respect to the Hubble parameter using some algebraic approaches. A time varying scale factor has been introduced to analyze the behavior of the model. From some diagnostic approach, we found that the model behaves as ",0.0666666667],["we propose a natural greedy algorithm for response-dependent costs. we bound the app","Submodular Learning and Covering with Response-Dependent Costs","summarize: We consider interactive learning and covering problems, in a setting where actions may incur different costs, depending on the response to the action. We propose a natural greedy algorithm for response-dependent costs. We bound the approximation factor of this greedy algorithm in active learning settings as well as in the general setting. We show that a different property of the cost function controls the approximation factor in each of these scenarios. We further show that in both settings, the approximation factor of this greedy algorithm is near-optimal among all greedy algorithms. Experiments demonstrate the advantages of the proposed algorithm in the response-dependent cost setting.",0.1578947368],["black hole solution is a new charge for black hole solution. the topological charge is","The Last Lost Charge And Phase Transition In Schwarzschild AdS Minimally Coupled to a Cloud of Strings","summarize: In this paper we study the Schwarzschild AdS black hole with a cloud of string background in an extended phase space and investigate a new phase transition related to the topological charge. By treating the topological charge as a new charge for black hole solution we study its thermodynamics in this new extended phase space. We treat by two approaches to study the phase transition behavior via both ",0.1750346638],["Given","Exponential sum approximations for ","summarize: Given ",0.0],["affluent spatial information can inhibit the subsequent SOD performance. the depth quality","Multi-level Cross-modal Interaction Network for RGB-D Salient Object Detection","summarize: Depth cues with affluent spatial information have been proven beneficial in boosting salient object detection , while the depth quality directly affects the subsequent SOD performance. However, it is inevitable to obtain some low-quality depth cues due to limitations of its acquisition devices, which can inhibit the SOD performance. Besides, existing methods tend to combine RGB images and depth cues in a direct fusion or a simple fusion module, which makes they can not effectively exploit the complex correlations between the two sources. Moreover, few methods design an appropriate module to fully fuse multi-level features, resulting in cross-level feature interaction insufficient. To address these issues, we propose a novel Multi-level Cross-modal Interaction Network for RGB-D based SOD. Our MCI-Net includes two key components: 1) a cross-modal feature learning network, which is used to learn the high-level features for the RGB images and depth cues, effectively enabling the correlations between the two sources to be exploited; and 2) a multi-level interactive integration network, which integrates multi-level cross-modal features to boost the SOD performance. Extensive experiments on six benchmark datasets demonstrate the superiority of our MCI-Net over 14 state-of-the-art methods, and validate the effectiveness of different components in our MCI-Net. More important, our MCI-Net significantly improves the SOD performance as well as has a higher FPS.",0.0],["artificial intelligence has been applied to control players' decisions in board games for over half a century","Collaborative Agent Gameplay in the Pandemic Board Game","summarize: While artificial intelligence has been applied to control players' decisions in board games for over half a century, little attention is given to games with no player competition. Pandemic is an exemplar collaborative board game where all players coordinate to overcome challenges posed by events occurring during the game's progression. This paper proposes an artificial agent which controls all players' actions and balances chances of winning versus risk of losing in this highly stochastic environment. The agent applies a Rolling Horizon Evolutionary Algorithm on an abstraction of the game-state that lowers the branching factor and simulates the game's stochasticity. Results show that the proposed algorithm can find winning strategies more consistently in different games of varying difficulty. The impact of a number of state evaluation metrics is explored, balancing between optimistic strategies that favor winning and pessimistic strategies that guard against losing.",0.3076923077],["the proposed formulation generalises, decouples and incorporates features to the method providing more","Particle Swarms Reformulated towards a Unified and Flexible Framework","summarize: The Particle Swarm Optimisation algorithm has undergone countless modifications and adaptations since its original formulation in 1995. Some of these have become mainstream whereas many others have not been adopted and faded away. Thus, a myriad of alternative formulations have been proposed to the extent that the question arises as to what the basic features of an algorithm must be to belong in the PSO family. The aim of this paper is to establish what defines a PSO algorithm and to attempt to formulate it in such a way that it encompasses many existing variants. Therefore, different versions of the method may be posed as settings within the proposed unified framework. In addition, the proposed formulation generalises, decouples and incorporates features to the method providing more flexibility to the behaviour of each particle. The closed forms of the trajectory difference equation are obtained, different types of behaviour are identified, stochasticity is decoupled, and traditionally global features such as sociometries and constraint-handling are re-defined as particle's attributes.",0.1333333333],["relics in galaxy clusters are believed to trace merger shock fronts.","Can cluster merger shocks reproduce the luminosity and shape distribution of radio relics?","summarize: Radio relics in galaxy clusters are believed to trace merger shock fronts. If cosmological structure formation determines the luminosity, size and shape distributions of radio relics then merger shocks need to be lighted up in a homogeneous way. We investigate if a mock relic sample, obtained from zoomed galaxy cluster simulations, is able to match the properties of relics measured in the NRAO VLA Sky Survey . We compile a list of all radio relics known to date and homogeneously measure their parameters in all NVSS images and apply the same procedure to relics in our simulations. Number counts in the mock relic sample increase more steeply towards lower relic flux densities, suggesting an incompleteness of NVSS in this regime. Overall, we find that NVSS and mock samples show similar properties. However, large simulated relics tend to be somewhat smaller and closer to the cluster centre than observed ones. Besides this, the mock sample reproduces very well-known correlations for radio relics, in particular those relating the radio luminosity with the largest linear size and the X-ray luminosity. We show that these correlations are largely governed by the sensitivity of the NVSS observations. Mock relics show a similar orientation with respect to the direction to the cluster centre as the NVSS sample. Moreover, we find that their maximum radio luminosity roughly correlates with cluster mass, although displaying a large scatter. The overall good agreement between NVSS and the mock sample suggests that properties of radio relics are indeed governed by merger shock fronts, emitting in a homogeneous fashion. Our study demonstrates that the combination of mock observations and data from upcoming radio surveys will allow to shed light on both the origin of radio relics and the nature of the intracluster medium.",0.3031828793],["blockChain, a disruptive technology that has found many applications from cryptocurrencies to","BlockChain: A distributed solution to automotive security and privacy","summarize: Interconnected smart vehicles offer a range of sophisticated services that benefit the vehicle owners, transport authorities, car manufacturers and other service providers. This potentially exposes smart vehicles to a range of security and privacy threats such as location tracking or remote hijacking of the vehicle. In this article, we argue that BlockChain , a disruptive technology that has found many applications from cryptocurrencies to smart contracts, is a potential solution to these challenges. We propose a BC-based architecture to protect the privacy of the users and to increase the security of the vehicular ecosystem. Wireless remote software updates and other emerging services such as dynamic vehicle insurance fees, are used to illustrate the efficacy of the proposed security architecture. We also qualitatively argue the resilience of the architecture against common security attacks.",0.3157894737],["a combinatorial machinery is presented in this paper. the graph tower is a","Graph towers, laminations and their invariant measures","summarize: In this paper we present a combinatorial machinery, consisting of a graph tower ",0.5806451613],["Let us know what you think about it!","","summarize: Let ",0.0],["the validity of the measurement has been double-checked in the well-mixed","Moran-evolution of cooperation: From well-mixed to heterogeneous complex networks","summarize: Configurational arrangement of network architecture and interaction character of individuals are two most influential factors on the mechanisms underlying the evolutionary outcome of cooperation, which is explained by the well-established framework of evolutionary game theory. In the current study, not only qualitatively but also quantitatively, we measure Moran-evolution of cooperation to support an analytical agreement based on the consequences of the replicator equation in a finite population. The validity of the measurement has been double-checked in the well-mixed network by the Langevin stochastic differential equation and the Gillespie-algorithmic version of Moran-evolution, while in a structured network, the measurement of accuracy is verified by the standard numerical simulation. Considering the Birth-Death and Death-Birth updating rules through diffusion of individuals, the investigation is carried out in the wide range of game environments those relate to the various social dilemmas where we are able to draw a new rigorous mathematical track to tackle the heterogeneity of complex networks. The set of modified criteria reveals the exact fact about the emergence and maintenance of cooperation in the structured population. We find that in general, nature promotes the environment of coexistent traits.",0.1176470588],["positive maps for two positive maps.","Merging of positive maps: a construction of various classes of positive maps on matrix algebras","summarize: For two positive maps ",0.1785041281],["anisotropy in the dark energy pressure is found to evolve with cosmic expansion at least","Bianchi-V String Cosmological model with Dark Energy Anisotropy","summarize: The role of anisotropic components on the dark energy and the dynamics of the universe is investigated. An anisotropic dark energy fluid with different pressures along different spatial directions is assumed to incorporate the effect of anisotropy. One dimensional cosmic strings aligned along x-direction supplement some kind of anisotropy. Anisotropy in the dark energy pressure is found to evolve with cosmic expansion at least at late times. At an early phase, the anisotropic effect due to the cosmic strings substantially affect the dynamics of the accelerating universe.",0.1875],["the project is to focus on applications of muography. the telescope may have to be operated","A portable muon telescope based on small and gas-tight Resistive Plate Chambers","summarize: We report on the first steps in the development of a small-size muon telescope based on glass Resistive Plate Chambers with small active area . The long-term goal of this project is to focus on applications of muography where the telescope may have to be operated underground and\/or inside small rooms, and in challenging logistic situations. Driving principles in our design are therefore compact size, light weight, gas tightness, and robustness. The first data-taking experiences have been encouraging, and we elaborate on the lessons learnt and future directions for development.",0.2380952381],["the Andersen-Gill model has become increasingly popular in the analysis of recurrent","Sample size calculation for the Andersen-Gill model comparing rates of recurrent events","summarize: Recurrent events arise frequently in biomedical research, where the subject may experience the same type of events more than once. The Andersen-Gill model has become increasingly popular in the analysis of recurrent events particularly when the event rate is not constant over time. We propose a procedure for calculating the power and sample size for the robust Wald test from the AG model in superiority, noninferiority, and equivalence clinical trials. Its performance is demonstrated by numerical examples. Sample SAS code is provided in the Supplementary Material.",0.4375],["unsupervised anomaly detection approaches have been made in the medical domain. previously, deep spatial","Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images","summarize: Reliably modeling normality and differentiating abnormal appearances from normal cases is a very appealing approach for detecting pathologies in medical images. A plethora of such unsupervised anomaly detection approaches has been made in the medical domain, based on statistical methods, content-based retrieval, clustering and recently also deep learning. Previous approaches towards deep unsupervised anomaly detection model patches of normal anatomy with variants of Autoencoders or GANs, and detect anomalies either as outliers in the learned feature space or from large reconstruction errors. In contrast to these patch-based approaches, we show that deep spatial autoencoding models can be efficiently used to capture normal anatomical variability of entire 2D brain MR images. A variety of experiments on real MR data containing MS lesions corroborates our hypothesis that we can detect and even delineate anomalies in brain MR images by simply comparing input images to their reconstruction. Results show that constraints on the latent space and adversarial training can further improve the segmentation performance over standard deep representation learning.",0.1333333333],["we correct the double spend race analysis given in Nakamoto's foundational Bitcoin article","Double spend races","summarize: We correct the double spend race analysis given in Nakamoto's foundational Bitcoin article and give a closed-form formula for the probability of success of a double spend attack using the Regularized Incomplete Beta Function. We give a proof of the exponential decay on the number of confirmations, often cited in the literature, and find an asymptotic formula. Larger number of confirmations are necessary compared to those given by Nakamoto. We also compute the probability conditional to the known validation time of the blocks. This provides a finer risk analysis than the classical one.",0.1428571429],["kinetic models are commonly used to describe the dynamics of nonnegative systems. characterization is","Uniqueness of feasible equilibria for mass action law kinetic systems","summarize: This paper studies the relations among system parameters, uniqueness, and stability of equilibria, for kinetic systems given in the form of polynomial ODEs. Such models are commonly used to describe the dynamics of nonnegative systems, with a wide range of application fields such as chemistry, systems biology, process modeling or even transportation systems. Using a flux-based description of kinetic models, a canonical representation of the set of all possible feasible equilibria is developed. The characterization is made in terms of strictly stable compartmental matrices to define the so-called family of solutions. Feasibility is imposed by a set of constraints, which are linear on a log-transformed space of complexes, and relate to the kernel of a matrix, the columns of which span the stoichiometric subspace. One particularly interesting representation of these constraints can be expressed in terms of a class of monotonous decreasing functions. This allows connections to be established with classical results in CRNT that relate to the existence and uniqueness of equilibria along positive stoichiometric compatibility classes. In particular, monotonicity can be employed to identify regions in the set of possible reaction rate coefficients leading to complex balancing, and to conclude uniqueness of equilibria for a class of positive deficiency networks. The latter result might support constructing an alternative proof of the well-known deficiency one theorem. The developed notions and results are illustrated through examples.",0.1428571429],["a dual-criteria time-stepping method is proposed to improve computational efficiency of particle hydro","Dual-criteria time stepping for weakly compressible smoothed particle hydrodynamics","summarize: Implementing particle-interaction configuration and time integration are performance intensive essentials of particle-based methods. In this paper, a dual-criteria time-stepping method is proposed to improve the computational efficiency of the weakly-compressible smoothed particle hydrodynamic method for modeling incompressible flows. The key idea is to introduce an advection time criterion, which is based on fluid velocity field, for recreating the particle-interaction configuration. Within this time criterion, several steps of pressure relaxation determined by the acoustic time criterion, based on the artificial speed of sound, can be carried out without updating the particle interaction configuration and much larger time-step sizes compared with the conventional counterpart. The method has shown optimized computational performance through CPU cost analysis. Good accuracy and performance is obtained for the presented benchmarks implying promising potential of the proposed method for incompressible flow and fluid-structure interaction simulations.",0.3888888889],["the ATLAS Inner Tracker will be an all-silicon detector. sensors for the","Mapping the depleted area of silicon diodes using a micro-focused X-ray beam","summarize: For the Phase-II Upgrade of the ATLAS detector at CERN, the current ATLAS Inner Detector will be replaced with the ATLAS Inner Tracker. The ATLAS Inner Tracker will be an all-silicon detector, consisting of a pixel tracker and a strip tracker. Sensors for the ITk strip tracker are required to have a low leakage current up to bias voltages of -700 V to maintain a low noise and power dissipation. In order to minimise sensor leakage currents, particularly in the high-radiation environment inside the ATLAS detector, sensors are foreseen to be operated at low temperatures and to be manufactured from wafers with a high bulk resistivity of several k cm. Simulations showed the electric field inside sensors with high bulk resistivity to extend towards the sensor edge, which could lead to increased surface currents for narrow dicing edges. In order to map the electric field inside biased silicon sensors with high bulk resistivity, three diodes from ATLAS silicon strip sensor prototype wafers were studied with a monochromatic, micro-focused X-ray beam at the Diamond Light Source. For all devices under investigation, the electric field inside the diode was mapped and its dependence on the applied bias voltage was studied. The findings showed that the electric field in each diode under investigation extended beyond its bias ring and reached the dicing edge.",0.2142857143],["the Polarimetric and Helioseismic Imager is the first deep-space","Performance analysis of the SO\/PHI software framework for on-board data reduction","summarize: The Polarimetric and Helioseismic Imager is the first deep-space solar spectropolarimeter, on-board the Solar Orbiter space mission. It faces: stringent requirements on science data accuracy, a dynamic environment, and severe limitations on telemetry volume. SO\/PHI overcomes these restrictions through on-board instrument calibration and science data reduction, using dedicated firmware in FPGAs. This contribution analyses the accuracy of a data processing pipeline by comparing the results obtained with SO\/PHI hardware to a reference from a ground computer. The results show that for the analysed pipeline the error introduced by the firmware implementation is well below the requirements of SO\/PHI.",0.2001843507],["constraints on scaling relations of galaxy cluster X-ray luminosity, temperature and gas mass","Weighing the Giants V: Galaxy Cluster Scaling Relations","summarize: We present constraints on the scaling relations of galaxy cluster X-ray luminosity, temperature and gas mass with mass and redshift, employing masses from robust weak gravitational lensing measurements. These are the first such results obtained from an analysis that simultaneously accounts for selection effects and the underlying mass function, and directly incorporates lensing data to constrain total masses. Our constraints on the scaling relations and their intrinsic scatters are in good agreement with previous studies, and reinforce a picture in which departures from self-similar scaling laws are primarily limited to cluster cores. However, the data are beginning to reveal new features that have implications for cluster astrophysics and provide new tests for hydrodynamical simulations. We find a positive correlation in the intrinsic scatters of luminosity and temperature at fixed mass, which is related to the dynamical state of the clusters. While the evolution of the nominal scaling relations over the redshift range ",0.0666666667],["conformal immersions are a must-see for conformal immersions.","Dirac Tori","summarize: We consider conformal immersions ",0.0588235294],["the model is studied systematically on a one-dimensional lattice. the","Off-site trimer superfluid on a one-dimensional optical lattice","summarize: The Bose-Hubbard model with an effective off-site three-body tunneling, characterized by jumps towards one another, between one atom on a site and a pair atoms on the neighborhood site, is studied systematically on a one-dimensional lattice, by using the density matrix renormalization group method. The off-site trimer superfluid, condensing at momentum ",0.4444444444],["Steinhaus theorem is a result that deals with a property of the difference","An Alternative Proof of Steinhaus Theorem","summarize: In measure theory, Steinhaus theorem is a result that deals with a property of the difference between two sets of positive measure. We give a simple elementary proof of the result.",0.2727272727],["abstract syntax tree mapping algorithms are widely used to analyze changes in source code. AST mapping","A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms","summarize: Abstract syntax tree mapping algorithms are widely used to analyze changes in source code. Despite the foundational role of AST mapping algorithms, little effort has been made to evaluate the accuracy of AST mapping algorithms, i.e., the extent to which an algorihtm captures the evolution of code. We observe that a program element often has only one best-mapped program element. Based on this observation, we propose a hierarchical approach to automatically compare the similarity of mapped statements and tokens by different algorithms. By performing the comparison, we determine if each of the compared algorithms generates inaccurate mappings for a statement or its tokens. We invite 12 external experts to determine if three commonly used AST mapping algorithms generate accurate mappings for a statement and its tokens for 200 statements. Based on the experts' feedback,we observe that our approach achieves a precision of 0.98--1.00 and a recall of 0.65--0.75. Furthermore, we conduct a large-scale study with a dataset of ten Java projects, containing a total of 263,165 file revisions. Our approach determines that GumTree, MTDiff and IJM generate inaccurate mappings for 20%--29%, 25%--36% and 21%--30% of the file revisions, respectively. Our experimental results show that state-of-art AST mapping agorithms still need improvements.",0.15],["Graph-based methods are known to be successful in many machine learning tasks. a","Stochastic Graphlet Embedding","summarize: Graph-based methods are known to be successful in many machine learning and pattern classification tasks. These methods consider semi-structured data as graphs where nodes correspond to primitives and edges characterize the relationships between these primitives. However, these non-vectorial graph data cannot be straightforwardly plugged into off-the-shelf machine learning algorithms without a preliminary step of -- explicit\/implicit -- graph vectorization and embedding. This embedding process should be resilient to intra-class graph variations while being highly discriminant. In this paper, we propose a novel high-order stochastic graphlet embedding that maps graphs into vector spaces. Our main contribution includes a new stochastic search procedure that efficiently parses a given graph and extracts\/samples unlimitedly high-order graphlets. We consider these graphlets, with increasing orders, to model local primitives as well as their increasingly complex interactions. In order to build our graph representation, we measure the distribution of these graphlets into a given graph, using particular hash functions that efficiently assign sampled graphlets into isomorphic sets with a very low probability of collision. When combined with maximum margin classifiers, these graphlet-based representations have positive impact on the performance of pattern comparison and recognition as corroborated through extensive experiments using standard benchmark databases.",0.2272727273],["the gemini multiconjugate adaptive optics System was in regular science operation with the Ge","Unveiling the nature of the Gemini multiconjugate adaptive optics system distortions","summarize: Astrometry was not a science case of the Gemini Multiconjugate adaptive optics System at its design stage. However, since GeMS has been in regular science operation with the Gemini South Adaptive Optics Imager , their astrometric performances have been deeply analysed. The non-linear component of the distortion map model shows a characteristic pattern which is similarly repeated in each detector of GSAOI. The nature of this pattern was unknown and subjected to different hypotheses. This paper describes the origin of the GeMS distortion pattern as well as its multi-epoch variation. At the end, it is showed a comparison with the current design of the Multiconjugate Adaptive Optics RelaY of the Extremely Large Telescope .",0.5882352941],["a robust and stable relationship between a country's productive structure and its economic growth has","The Impact of Services on Economic Complexity: Service Sophistication as Route for Economic Growth","summarize: Economic complexity reflects the amount of knowledge that is embedded in the productive structure of an economy. By combining tools from network science and econometrics, a robust and stable relationship between a country's productive structure and its economic growth has been established. Here we report that not only goods but also services are important for predicting the rate at which countries will grow. By adopting a terminology which classifies manufactured goods and delivered services as products, we investigate the influence of services on the country's productive structure. In particular, we provide evidence that complexity indices for services are in general higher than those for goods, which is reflected in a general tendency to rank countries with developed service sector higher than countries with economy centred on manufacturing of goods. By focusing on country dynamics based on experimental data, we investigate the impact of services on the economic complexity of countries measured in the product space . Importantly, we show that diversification of service exports and its sophistication can provide an additional route for economic growth in both developing and developed countries.",0.2068965517],["an annual well-being index constructed from thirteen socioeconomic factors is proposed. the index is","A Socioeconomic Well-Being Index","summarize: An annual well-being index constructed from thirteen socioeconomic factors is proposed in order to dynamically measure the mood of the US citizenry. Econometric models are fitted to the log-returns of the index in order to quantify its tail risk and perform option pricing and risk budgeting. By providing a statistically sound assessment of socioeconomic content, the index is consistent with rational finance theory, enabling the construction and valuation of insurance-type financial instruments to serve as contracts written against it. Endogenously, the VXO volatility measure of the stock market appears to be the greatest contributor to tail risk. Exogenously, stress-testing the index against the politically important factors of trade imbalance and legal immigration, quantify the systemic risk. For probability levels in the range of 5% to 10%, values of trade below these thresholds are associated with larger downward movements of the index than for immigration at the same level. The main intent of the index is to provide early-warning for negative changes in the mood of citizens, thus alerting policy makers and private agents to potential future market downturns.",0.0],["the study of the Diederich--Fornaess index of complex domain","On a global estimate of the Diederich--Fornaess index of Levi-flat real hypersurfaces","summarize: In this expository paper, we review a recent progress of the study of the Diederich--Fornaess index of complex domains with emphasis on the case of domains with Levi-flat boundary. It is exhibited that for any compact Levi-flat real hypersurface, the norm of its infinitesimal holonomy must exceed the curvature of its normal bundle at a point.",0.440942345],["pseudoconvex Hartogs domains are bounded in smooth bounded","On compactness of Hankel and the ","summarize: We prove that on smooth bounded pseudoconvex Hartogs domains in ",0.0],["Let us know what you think about it!","Localized Donaldson-Thomas theory of surfaces","summarize: Let ",0.0],["we introduce a new model, Region Convolutional 3D Network. it encodes","R-C3D: Region Convolutional 3D Network for Temporal Activity Detection","summarize: We address the problem of activity detection in continuous, untrimmed video streams. This is a difficult task that requires extracting meaningful spatio-temporal features to capture activities, accurately localizing the start and end times of each activity. We introduce a new model, Region Convolutional 3D Network , which encodes the video streams using a three-dimensional fully convolutional network, then generates candidate temporal regions containing activities, and finally classifies selected regions into specific activities. Computation is saved due to the sharing of convolutional features between the proposal and the classification pipelines. The entire model is trained end-to-end with jointly optimized localization and classification losses. R-C3D is faster than existing methods and achieves state-of-the-art results on THUMOS'14. We further demonstrate that our model is a general activity detection framework that does not rely on assumptions about particular dataset properties by evaluating our approach on ActivityNet and Charades. Our code is available at http:\/\/ai.bu.edu\/r-c3d\/.",0.5833333333],["binary or the interaction with a circumbinary disk may efficiently drive the system to sub-","Detecting eccentric supermassive black hole binaries with pulsar timing arrays: Resolvable source strategies","summarize: The couplings between supermassive black-hole binaries and their environments within galactic nuclei have been well studied as part of the search for solutions to the final parsec problem. The scattering of stars by the binary or the interaction with a circumbinary disk may efficiently drive the system to sub-parsec separations, allowing the binary to enter a regime where the emission of gravitational waves can drive it to merger within a Hubble time. However, these interactions can also affect the orbital parameters of the binary. In particular, they may drive an increase in binary eccentricity which survives until the system's gravitational-wave signal enters the pulsar-timing array band. Therefore, if we can measure the eccentricity from observed signals, we can potentially deduce some of the properties of the binary environment. To this end, we build on previous techniques to present a general Bayesian pipeline with which we can detect and estimate the parameters of an eccentric supermassive black-hole binary system with pulsar-timing arrays. Additionally, we generalize the pulsar-timing array ",0.2727272727],["one often encounters numerical difficulties in solving linear matrix inequality problems obtained from linear matrix inequality problems","Application of Facial Reduction to ","summarize: One often encounters numerical difficulties in solving linear matrix inequality problems obtained from ",0.0],["inequalities in a real Hilbert space are governed by a strongly monoton","Outer Approximation Methods for Solving Variational Inequalities in Hilbert Space","summarize: In this paper we study variational inequalities in a real Hilbert space, which are governed by a strongly monotone and Lipschitz continuous operator ",0.652173913],["the meta distribution is the distribution of the conditional success probability. the meta distribution is","Meta Distribution of SIR in Dual-Hop Internet-of-Things Networks","summarize: This paper characterizes the meta distribution of the downlink signal-to-interference ratio attained at a typical Internet-of-Things device in a dual-hop IoT network. The IoT device associates with either a serving macro base station for direct transmissions or associates with a decode and forward relay for dual-hop transmissions, depending on the biased received signal power criterion. In contrast to the conventional success probability, the meta distribution is the distribution of the conditional success probability , which is conditioned on the locations of the wireless transmitters. The meta distribution is a fine-grained performance metric that captures important network performance metrics such as the coverage probability and the mean local delay as its special cases. Specifically, we derive the moments of the CSP in order to calculate analytic expressions for the meta distribution. Further, we derive mathematical expressions for special cases such as the mean local delay, variance of the CSP, and success probability of a typical IoT device and typical relay with different offloading biases. We take in consideration in our analysis the association probabilities of IoT devices. Finally, we investigate the impact of increasing the relay density on the mean local delay using numerical results.",0.0697674419],["Buzzard irregularity does not have p-adic slope.","A remark on non-integral p-adic slopes for modular forms","summarize: We give a sufficient condition, namely Buzzard irregularity, for there to exist a cuspidal eigenform which does not have integral p-adic slope.",0.214707798],["the group field theory model is based on a group field theory model. the theory","Emergent Friedmann dynamics with a quantum bounce from quantum gravity condensates","summarize: We study the effective cosmological dynamics, emerging as the hydrodynamics of simple condensate states, of a group field theory model for quantum gravity coupled to a massless scalar field and reduced to its isotropic sector. The quantum equations of motion for these group field theory condensate states are given in relational terms with respect to the scalar field, from which effective dynamics for spatially flat, homogeneous and isotropic space-times can be extracted. The result is a generalization of the Friedmann equations, including quantum gravity modifications, in a specific regime of the theory corresponding to a Gross-Pitaevskii approximation where interactions are subdominant. The classical Friedmann equations of general relativity are recovered in a suitable semi-classical limit for some range of parameters of the microscopic dynamics. An important result is that the quantum geometries associated with these GFT condensate states are non-singular: a bounce generically occurs in the Planck regime. For some choices of condensate states, these modified Friedmann equations are very similar to those of loop quantum cosmology.",0.0833333333],["a sequence of the general form is a sequence of the general form.","On the monoid generated by a Lucas sequence","summarize: A Lucas sequence is a sequence of the general form ",0.3448275862],["large-aperture space-based telescopes will require segmented primaries.","Pair-based Analytical model for Segmented Telescopes Imaging from Space for sensitivity analysis","summarize: The imaging and spectroscopy of habitable worlds will require large-aperture space-based telescopes, to increase the collecting area and the angular resolution. These large telescopes will necessarily use segmented primaries to fit in a rocket. However, these massively segmented mirrors make high-contrast performance very difficult to achieve and stabilize, compared to more common monolithic primaries. Despite space telescopes operating in a friendlier environment than ground-based telescopes, remaining vibrations and resonant modes on the segments can still deteriorate the performance. In this context, we present the Pair-based Analytical model for Segmented Telescopes Imaging from Space that enables the establishment of a comprehensive error budget, both in term of segment alignment and stability. Using this model, one may evaluate the influence of the segment cophasing and surface quality evolution on the final images and contrasts, and set up requirements for any given mission. One can also identify the dominant modes of a given geometry for a given coronagraphic instrument and design the feedback control systems accordingly. In this paper, we first develop and validate this analytical model by comparing its outputs to the images and contrasts predicted by an end-to-end simulation. We show that the contrasts predicted using PASTIS are accurate enough compared to the end-to-end propagation results, at the exo-Earth detection level. Second, we develop a method for a fast and efficient error budget in term of segment manufacturing and alignment that takes into account the disparities of the segment effects on the final performance. This technique is then applied on a specific aperture to provide static and quasi-static requirements on each segment for local aberrations. Finally we discuss potential application of this new technique to future missions.",0.0],["symmetry breaks from a group G to a group H. the deep reason for","The geometric role of symmetry breaking in gravity","summarize: In gravity, breaking symmetry from a group G to a group H plays the role of describing geometry in relation to the geometry the homogeneous space G\/H. The deep reason for this is Cartan's method of equivalence, giving, in particular, an exact correspondence between metrics and Cartan connections. I argue that broken symmetry is thus implicit in any gravity theory, for purely geometric reasons. As an application, I explain how this kind of thinking gives a new approach to Hamiltonian gravity in which an observer field spontaneously breaks Lorentz symmetry and gives a Cartan connection on space.",0.2272727273],["review of current state of the art in remote sensing based monitoring of forest disturbances and","Methods for Mapping Forest Disturbance and Degradation from Optical Earth Observation Data: a Review","summarize: Purpose of review: This paper presents a review of the current state of the art in remote sensing based monitoring of forest disturbances and forest degradation from optical Earth Observation data. Part one comprises an overview of currently available optical remote sensing sensors, which can be used for forest disturbance and degradation mapping. Part two reviews the two main categories of existing approaches: classical image-to-image change detection and time series analysis. Recent findings: With the launch of the Sentinel-2a satellite and available Landsat imagery, time series analysis has become the most promising but also most demanding category of degradation mapping approaches. Four time series classification methods are distinguished. The methods are explained and their benefits and drawbacks are discussed. A separate chapter presents a number of recent forest degradation mapping studies for two different ecosystems: temperate forests with a geographical focus on Europe and tropical forests with a geographical focus on Africa. Summary: The review revealed that a wide variety of methods for the detection of forest degradation is already available. Today, the main challenge is to transfer these approaches to high resolution time series data from multiple sensors. Future research should also focus on the classification of disturbance types and the development of robust up-scalable methods to enable near real time disturbance mapping in support of operational reactive measures.",0.125],["graph theory defines a notion of combinatorial expansion for simplicial complexes of general","Isoperimetric Inequalities in Simplicial Complexes","summarize: In graph theory there are intimate connections between the expansion properties of a graph and the spectrum of its Laplacian. In this paper we define a notion of combinatorial expansion for simplicial complexes of general dimension, and prove that similar connections exist between the combinatorial expansion of a complex, and the spectrum of the high dimensional Laplacian defined by Eckmann. In particular, we present a Cheeger-type inequality, and a high-dimensional Expander Mixing Lemma. As a corollary, using the work of Pach, we obtain a connection between spectral properties of complexes and Gromov's notion of geometric overlap. Using the work of Gunder and Wagner, we give an estimate for the combinatorial expansion and geometric overlap of random Linial-Meshulam complexes.",0.0952380952],["generative adversarial networks have been successfully applied to zero-shot learning. generative","Alleviating Feature Confusion for Generative Zero-shot Learning","summarize: Lately, generative adversarial networks have been successfully applied to zero-shot learning and achieved state-of-the-art performance. By synthesizing virtual unseen visual features, GAN-based methods convert the challenging ZSL task into a supervised learning problem. However, GAN-based ZSL methods have to train the generator on the seen categories and further apply it to unseen instances. An inevitable issue of such a paradigm is that the synthesized unseen features are prone to seen references and incapable to reflect the novelty and diversity of real unseen instances. In a nutshell, the synthesized features are confusing. One cannot tell unseen categories from seen ones using the synthesized features. As a result, the synthesized features are too subtle to be classified in generalized zero-shot learning which involves both seen and unseen categories at the test stage. In this paper, we first introduce the feature confusion issue. Then, we propose a new feature generating network, named alleviating feature confusion GAN , to challenge the issue. Specifically, we present a boundary loss which maximizes the decision boundary of seen categories and unseen ones. Furthermore, a novel metric named feature confusion score is proposed to quantify the feature confusion. Extensive experiments on five widely used datasets verify that our method is able to outperform previous state-of-the-arts under both ZSL and GZSL protocols.",0.0],["energy efficient power control for uplink two-tier networks is investigated. the algorithm is implemented","Energy Efficient Power Control for the Two-tier Networks with Small Cells and Massive MIMO","summarize: In this paper, energy efficient power control for the uplink two-tier networks where a macrocell tier with a massive multiple-input multiple-output base station is overlaid with a small cell tier is investigated. We propose a distributed energy efficient power control algorithm which allows each user in the two-tier network taking individual decisions to optimize its own energy efficiency for the multi-user and multi-cell scenario. The distributed power control algorithm is implemented by decoupling the EE optimization problem into two steps. In the first step, we propose to assign the users on the same resource into the same group and each group can optimize its own EE, respectively. In the second step, multiple power control games based on evolutionary game theory are formulated for each group, which allows each user optimizing its own EE. In the EGT-based power control games, each player selects a strategy giving a higher payoff than the average payoff, which can improve the fairness among the users. The proposed algorithm has a linear complexity with respect to the number of subcarriers and the number of cells in comparison with the brute force approach which has an exponential complexity. Simulation results show the remarkable improvements in terms of fairness by using the proposed algorithm.",0.125],["the subgraph induced in Young's graph by the set of partitions with an odd","Odd partitions in Young's lattice","summarize: We show that the subgraph induced in Young's graph by the set of partitions with an odd number of standard Young tableaux is a binary tree. This tree exhibits self-similarities at all scales, and has a simple recursive description.",0.1666666667],["explainable AI approaches aim at mitigating lack of transparency in deep networks. evidence of","A Study on Multimodal and Interactive Explanations for Visual Question Answering","summarize: Explainability and interpretability of AI models is an essential factor affecting the safety of AI. While various explainable AI approaches aim at mitigating the lack of transparency in deep networks, the evidence of the effectiveness of these approaches in improving usability, trust, and understanding of AI systems are still missing. We evaluate multimodal explanations in the setting of a Visual Question Answering task, by asking users to predict the response accuracy of a VQA agent with and without explanations. We use between-subjects and within-subjects experiments to probe explanation effectiveness in terms of improving user prediction accuracy, confidence, and reliance, among other factors. The results indicate that the explanations help improve human prediction accuracy, especially in trials when the VQA system's answer is inaccurate. Furthermore, we introduce active attention, a novel method for evaluating causal attentional effects through intervention by editing attention maps. User explanation ratings are strongly correlated with human prediction accuracy and suggest the efficacy of these explanations in human-machine AI collaboration tasks.",0.1052631579],["Synapse is a proxy application for real workloads. it can emulate workload execution","Synapse: Synthetic Application Profiler and Emulator","summarize: We introduce Synapse motivated by the needs to estimate and emulate workload execution characteristics on high-performance and distributed heterogeneous resources. Synapse has a platform independent application profiler, and the ability to emulate profiled workloads on a variety of heterogeneous resources. Synapse is used as a proxy application for real workloads, with the added advantage that it can be tuned at arbitrary levels of granularity in ways that are simply not possible using real applications. Experiments show that automated profiling using Synapse represents application characteristics with high fidelity. Emulation using Synapse can reproduce the application behavior in the original runtime environment, as well as reproducing properties when used in a different run-time environments.",0.2272727273],["we represent the group of agents via their distribution and derive a method to estimate the dynamics","Modeling collective behaviors: A moment-based approach","summarize: In this work we introduce an approach for modeling and analyzing collective behavior of a group of agents using moments. We represent the group of agents via their distribution and derive a method to estimate the dynamics of the moments. We use this to predict the evolution of the distribution of agents by first computing the moment trajectories and then use this to reconstruct the distribution of the agents. In the latter an inverse problem is solved in order to reconstruct a nominal distribution and to recover the macro-scale properties of the group of agents. The proposed method is applicable for several types of multi-agent systems, e.g., leader-follower systems. We derive error bounds for the moment trajectories and describe how to take these error bounds into account for computing the moment dynamics. The convergence of the moment dynamics is also analyzed for cases with monomial moments. To illustrate the theory, two numerical examples are given. In the first we consider a multi-agent system with interactions and compare the proposed methods for several types of moments. In the second example we apply the framework to a leader-follower problem for modeling pedestrian crowd dynamics.",0.1538461538],["a graph of geometric shapes is used to compute geometrical geometric shapes. we compute geometric","Wick rotations, Eichler integrals, and multi-loop Feynman diagrams","summarize: Using contour deformations and integrations over modular forms, we compute certain Bessel moments arising from diagrammatic expansions in two-dimensional quantum field theory. We evaluate these Feynman integrals as either explicit constants or critical values of modular ",0.15625],["49 new times of minimum were obtained between 2005 and 2020. this allows the derivation","CK Aqr time keeping. Evidence for a third body","summarize: Photometric measurements of the contact binary system CK Aqr are presented. 49 new times of minimum were obtained between 2005 and 2020. Along with already published observations, this allows the derivation of an improved ephemeris. The resulting O-C diagram shows oscillations which are interpreted as the light time travel effect due to a third component, with a period of 8.2 years.",0.0],["the miniaturization of gears towards the nanoscale is a formidable task. we","Mechanical transmission of rotation for molecule gears and solid-state gears","summarize: The miniaturization of gears towards the nanoscale is a formidable task posing a variety of challenges to current fabrication technologies. In context, the understanding, via computer simulations, of the mechanisms mediating the transfer of rotational motion between nanoscale gears can be of great help to guide the experimental designs. Based on atomistic molecular dynamics simulations in combination with a nearly rigid-body approximation, we study the transmission of rotational motion between molecule gears and solid-state gears, respectively. For the molecule gears under continuous driving, we identify different regimes of rotational motion depending on the magnitude of the external torque. In contrast, the solid-state gears behave like ideal gears with nearly perfect transmission. Furthermore, we simulate the manipulation of the gears by a scanning-probe tip and we find that the mechanical transmission strongly depends on the center of mass distance between gears. A new regime of transmission is found for the solid-state gears.",0.5],["polaritons can propagate along anisotropic metasurfaces with either hyperbol","Collective near-field coupling in infrared-phononic metasurfaces for nano-light canalization","summarize: Polaritons, coupled excitations of photons and dipolar matter excitations, can propagate along anisotropic metasurfaces with either hyperbolic or elliptical dispersion. At the transition from hyperbolic to elliptical dispersion , various intriguing phenomena are found, such as an enhancement of the photonic density of states, polariton canalization and hyperlensing. Here we investigate theoretically and experimentally the topological transition and the polaritonic coupling of deeply subwavelength elements in a uniaxial infrared-phononic metasurface, a grating of hexagonal boron nitride nanoribbons. By hyperspectral infrared nanoimaging, we observe, for the first time, a synthetic transverse optical phonon resonance in the middle of the hBN Reststrahlen band, yielding a topological transition from hyperbolic to elliptical dispersion. We further visualize and characterize the spatial evolution of a deeply subwavelength canalization mode near the transition frequency, which is a collimated polariton that is the basis for hyperlensing and diffraction-less propagation. Our results provide fundamental insights into the role of polaritonic near-field coupling in metasurfaces for creating topological transitions and polariton canalization.",0.2222222222],["random matrices with continuous entries are divided by columns. the number of columns is","Balancing Gaussian vectors in high dimension","summarize: Motivated by problems in controlled experiments, we study the discrepancy of random matrices with continuous entries where the number of columns ",0.0],["algorithm predicts the final grade of each student in a class. it issues a","Predicting Grades","summarize: To increase efficacy in traditional classroom courses as well as in Massive Open Online Courses , automated systems supporting the instructor are needed. One important problem is to automatically detect students that are going to do poorly in a course early enough to be able to take remedial actions. Existing grade prediction systems focus on maximizing the accuracy of the prediction while overseeing the importance of issuing timely and personalized predictions. This paper proposes an algorithm that predicts the final grade of each student in a class. It issues a prediction for each student individually, when the expected accuracy of the prediction is sufficient. The algorithm learns online what is the optimal prediction and time to issue a prediction based on past history of students' performance in a course. We derive a confidence estimate for the prediction accuracy and demonstrate the performance of our algorithm on a dataset obtained based on the performance of approximately 700 UCLA undergraduate students who have taken an introductory digital signal processing over the past 7 years. We demonstrate that for 85% of the students we can predict with 76% accuracy whether they are going do well or poorly in the class after the 4th course week. Using data obtained from a pilot course, our methodology suggests that it is effective to perform early in-class assessments such as quizzes, which result in timely performance prediction for each student, thereby enabling timely interventions by the instructor when necessary.",0.1071428571],["we establish sharp exponential deviation estimates of the information content. we also establish a sharp bound","Concentration of information content for convex measures","summarize: We establish sharp exponential deviation estimates of the information content as well as a sharp bound on the varentropy for the class of convex measures on Euclidean spaces. This generalizes a similar development for log-concave measures in the recent work of Fradelizi, Madiman and Wang . In particular, our results imply that convex measures in high dimensions are concentrated in an annulus between two convex sets despite their possibly having much heavier tails. Various tools and consequences are developed, including a sharp comparison result for R\\'enyi entropies, inequalities of Kahane-Khinchine type for convex measures that extend those of Koldobsky, Pajor and Yaskin for log-concave measures, and an extension of Berwald's inequality .",0.1935483871],["correlation-OTDR is a multi-processor system on chip and a","Fiber as a temperature sensor with portable Correlation-OTDR as interrogator","summarize: In this paper, we report on the integration of a Correlation-OTDR into a portable unit , based on a multi-processor system on chip and a small formfactor pluggable 2.5G transceiver. Going beyond telecommunication applications, this system is demonstrated for temperature measurements, based on the change of propagation delay with temperature in a short section of optical fiber. The temperature measurement accuracy is investigated as a function of the fiber length from 4 to 25 meters.",0.5033471157],["the information freshness is measured as the age of information whose maximum AoI is","Taming the Tail of Maximal Information Age in Wireless Industrial Networks","summarize: In wireless industrial networks, the information of time-sensitive control systems needs to be transmitted in an ultra-reliable and low-latency manner. This letter studies the resource allocation problem in finite blocklength transmission, in which the information freshness is measured as the age of information whose maximal AoI is characterized using extreme value theory . The considered system design is to minimize the sensors' transmit power and transmission blocklength subject to constraints on the maximal AoI's tail behavior. The studied problem is solved using Lyapunov stochastic optimization, and a dynamic reliability and age-aware policy for resource allocation and status updates is proposed. Simulation results validate the effectiveness of using EVT to characterize the maximal AoI. It is shown that sensors need to send larger-size data with longer transmission blocklength at lower transmit power. Moreover, the maximal AoI's tail decays faster at the expense of higher average information age.",0.1428571429],["we discuss three examples of PDE-constrained optimization problems. the first is image registration","PDE-constrained optimization in medical image analysis","summarize: PDE-constrained optimization problems find many applications in medical image analysis, for example, neuroimaging, cardiovascular imaging, and oncological imaging. We review related literature and give examples on the formulation, discretization, and numerical solution of PDE-constrained optimization problems for medical imaging. We discuss three examples. The first one is image registration. The second one is data assimilation for brain tumor patients, and the third one data assimilation in cardiovascular imaging. The image registration problem is a classical task in medical image analysis and seeks to find pointwise correspondences between two or more images. The data assimilation problems use a PDE-constrained formulation to link a biophysical model to patient-specific data obtained from medical images. The associated optimality systems turn out to be sets of nonlinear, multicomponent PDEs that are challenging to solve in an efficient way. The ultimate goal of our work is the design of inversion methods that integrate complementary data, and rigorously follow mathematical and physical principles, in an attempt to support clinical decision making. This requires reliable, high-fidelity algorithms with a short time-to-solution. This task is complicated by model and data uncertainties, and by the fact that PDE-constrained optimization problems are ill-posed in nature, and in general yield high-dimensional, severely ill-conditioned systems after discretization. These features make regularization, effective preconditioners, and iterative solvers that, in many cases, have to be implemented on distributed-memory architectures to be practical, a prerequisite. We showcase state-of-the-art techniques in scientific computing to tackle these challenges.",0.2666666667],["a MCMC estimator is based on an approximate reversible chain and","Importance sampling correction versus standard averages of reversible MCMCs in terms of the asymptotic variance","summarize: We establish an ordering criterion for the asymptotic variances of two consistent Markov chain Monte Carlo estimators: an importance sampling estimator, based on an approximate reversible chain and subsequent IS weighting, and a standard MCMC estimator, based on an exact reversible chain. Essentially, we relax the criterion of the Peskun type covariance ordering by considering two different invariant probabilities, and obtain, in place of a strict ordering of asymptotic variances, a bound of the asymptotic variance of IS by that of the direct MCMC. Simple examples show that IS can have arbitrarily better or worse asymptotic variance than Metropolis-Hastings and delayed-acceptance MCMC. Our ordering implies that IS is guaranteed to be competitive up to a factor depending on the supremum of the IS weight. We elaborate upon the criterion in case of unbiased estimators as part of an auxiliary variable framework. We show how the criterion implies asymptotic variance guarantees for IS in terms of pseudo-marginal and DA corrections, essentially if the ratio of exact and approximate likelihoods is bounded. We also show that convergence of the IS chain can be less affected by unbounded high-variance unbiased estimators than PM and DA chains.",0.5122113157],["a new paper identifies an intertwining relationship between squared Bessel processes","On a gateway between continuous and discrete Bessel and Laguerre processes","summarize: By providing instances of approximation of linear diffusions by birth-death processes, Feller , has offered an original path from the discrete world to the continuous one. In this paper, by identifying an intertwining relationship between squared Bessel processes and some linear birth-death processes, we show that this connection is in fact more intimate and goes in the two directions. As by-products, we identify some properties enjoyed by the birth-death family that are inherited from squared Bessel processes. For instance, these include a discrete self-similarity property and a discrete analogue of the beta-gamma algebra. We proceed by explaining that the same gateway identity also holds for the corresponding ergodic Laguerre semi-groups. It follows again that the continuous and discrete versions are more closely related than thought before, and this enables to pass information from one semi-group to the other one.",0.6],["system helps employees to unlock entrance door via face recognition. the system is constituted by two","Cloud-Based Face and Speech Recognition for Access Control Applications","summarize: This paper describes the implementation of a system to recognize employees and visitors wanting to gain access to a physical office through face images and speech-to-text recognition. The system helps employees to unlock the entrance door via face recognition without the need of tag-keys or cards. To prevent spoofing attacks and increase security, a randomly generated code is sent to the employee, who then has to type it into the screen. On the other hand, visitors and delivery persons are provided with a speech-to-text service where they utter the name of the employee that they want to meet, and the system then sends a notification to the right employee automatically. The hardware of the system is constituted by two Raspberry Pi, a 7-inch LCD-touch display, a camera, and a sound card with a microphone and speaker. To carry out face recognition and speech-to-text conversion, the cloud-based platforms Amazon Web Services and the Google Speech-to-Text API service are used respectively. The two-step face authentication mechanism for employees provides an increased level of security and protection against spoofing attacks without the need of carrying key-tags or access cards, while disturbances by visitors or couriers are minimized by notifying their arrival to the right employee, without disturbing other co-workers by means of ring-bells.",0.0],["fine-tuned transductively, this outperforms the current state-of","A Baseline for Few-Shot Image Classification","summarize: Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the hardness of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.",0.0],["modular Politics would enable platform operators to build bottom-up governance processes from computational components that","Modular Politics: Toward a Governance Layer for Online Communities","summarize: Governance in online communities is an increasingly high-stakes challenge, and yet many basic features of offline governance legacies--juries, political parties, term limits, and formal debates, to name a few--are not in the feature-sets of the software most community platforms use. Drawing on the paradigm of Institutional Analysis and Development, this paper proposes a strategy for addressing this lapse by specifying basic features of a generalizable paradigm for online governance called Modular Politics. Whereas classical governance typologies tend to present a choice among wholesale ideologies, such as democracy or oligarchy, Modular Politics would enable platform operators and their users to build bottom-up governance processes from computational components that are modular and composable, highly versatile in their expressiveness, portable from one context to another, and interoperable across platforms. This kind of approach could implement pre-digital governance systems as well as accelerate innovation in uniquely digital techniques. As diverse communities share and connect their components and data, governance could occur through a ubiquitous network layer. To that end, this paper proposes the development of an open standard for networked governance.",0.0588235294],["convex functions are intractable already for convex functions. we consider","On Degree Sequence Optimization","summarize: We consider the problem of finding a subgraph of a given graph which maximizes a given function evaluated at its degree sequence. While the problem is intractable already for convex functions, we show that it can be solved in polynomial time for convex multi-criteria objectives. We next consider the problem with separable objectives, which is NP-hard already when all vertex functions are the square. We consider a colored extension of the separable problem, which includes the notorious exact matching problem as a special case, and show that it can be solved in polynomial time on graphs of bounded tree-depth for any vertex functions. We mention some of the many remaining open problems.",0.0],["algorithm is used by quantum annealing community to generate weighted MAX-2-","Generating Weighted MAX-2-SAT Instances of Tunable Difficulty with Frustrated Loops","summarize: Many optimization problems can be cast into the maximum satisfiability form, and many solvers have been developed for tackling such problems. To evaluate a MAX-SAT solver, it is convenient to generate hard MAX-SAT instances with known solutions. Here, we propose a method of generating weighted MAX-2-SAT instances inspired by the frustrated-loop algorithm used by the quantum annealing community. We extend the algorithm for instances of general bipartite couplings, with the associated optimization problem being the minimization of the restricted Boltzmann machine energy over the nodal values, which is useful for effectively pre-training the RBM. The hardness of the generated instances can be tuned through a central parameter known as the frustration index. Two versions of the algorithm are presented: the random- and structured-loop algorithms. For the random-loop algorithm, we provide a thorough theoretical and empirical analysis on its mathematical properties from the perspective of frustration, and observe empirically a double phase transition behavior in the hardness scaling behavior driven by the frustration index. For the structured-loop algorithm, we show that it offers an improvement in hardness over the random-loop algorithm in the regime of high loop density, with the variation of hardness tunable through the concentration of frustrated weights.",0.0909090909],["new model-based reinforcement learning algorithm uses supervision to constrain exploration and learn efficiently while handling complex constraints","Safety Augmented Value Estimation from Demonstrations : Safe Deep Model-Based RL for Sparse Cost Robotic Tasks","summarize: Reinforcement learning for robotics is challenging due to the difficulty in hand-engineering a dense cost function, which can lead to unintended behavior, and dynamical uncertainty, which makes exploration and constraint satisfaction challenging. We address these issues with a new model-based reinforcement learning algorithm, Safety Augmented Value Estimation from Demonstrations , which uses supervision that only identifies task completion and a modest set of suboptimal demonstrations to constrain exploration and learn efficiently while handling complex constraints. We then compare SAVED with 3 state-of-the-art model-based and model-free RL algorithms on 6 standard simulation benchmarks involving navigation and manipulation and a physical knot-tying task on the da Vinci surgical robot. Results suggest that SAVED outperforms prior methods in terms of success rate, constraint satisfaction, and sample efficiency, making it feasible to safely learn a control policy directly on a real robot in less than an hour. For tasks on the robot, baselines succeed less than 5% of the time while SAVED has a success rate of over 75% in the first 50 training iterations. Code and supplementary material is available at https:\/\/tinyurl.com\/saved-rl.",0.0],["we compare abstract elementary classes of Shelah with accessible categories.","Abstract elementary classes and accessible categories","summarize: We compare abstract elementary classes of Shelah with accessible categories having directed colimits.",0.3],["sparse control inputs arise naturally in networked systems. derived conditions can","Controllability of Linear Dynamical Systems Under Input Sparsity Constraints","summarize: In this work, we consider the controllability of a discrete-time linear dynamical system with sparse control inputs. Sparsity constraints on the input arises naturally in networked systems, where activating each input variable adds to the cost of control. We derive algebraic necessary and sufficient conditions for ensuring controllability of a system with an arbitrary transfer matrix. The derived conditions can be verified in polynomial time complexity, unlike the more traditional Kalman-type rank tests. Further, we characterize the minimum number of input vectors required to satisfy the derived conditions for controllability. Finally, we present a generalized Kalman decomposition-like procedure that separates the state-space into subspaces corresponding to sparse-controllable and sparse-uncontrollable parts. These results form a theoretical basis for designing networked linear control systems with sparse inputs.",0.1666666667],["this paper studies geometric properties of the iterated Matrix Multiplication polynomial","Geometric Aspects of Iterated Matrix Multiplication","summarize: This paper studies geometric properties of the Iterated Matrix Multiplication polynomial and the hypersurface that it defines. We focus on geometric aspects that may be relevant for complexity theory such as the symmetry group of the polynomial, the dual variety and the Jacobian loci of the hypersurface, that are computed with the aid of representation theory of quivers.",0.2727272727],["the article describes the process of development and implementation of a full-scale model of noise-","Research of Stability in Ad Hoc Self-Organizated Wireless Networks","summarize: To date, there is a need for the development of efficient data and device exchange protocols that this exchange will provide, since standard protocols used in traditional networks can not fully meet the needs of a new type of network. The article describes the process of development and implementation of a full-scale model of noise-resistant and sensor network breaks. The stability of this network is achieved by building a distributed network, in which all nodes send messages to all available nodes. Wireless mobile peer-to-peer network can be configured automatically, so the nodes in it can move freely. Wireless networks do not have the complexity of infrastructure and management, which allows devices to create and join on-the-go networks - anywhere, anytime. In this paper, the theoretical part of the functioning of such networks and the field of their use is considered. After that, an initial analysis of the available equipment used for constructing such hardware solutions was conducted. The software for developing such solutions is considered in detail, as well as examples of finished models that implement the investigated functional. After that, several variants of the model of network nodes, as well as the test device for creating a payload on the network, are collected. For this purpose, third-party open solutions were used in conjunction with their own developments. The system received a series of tests that made it possible to understand the weak and strong points of such a network and draw conclusions for the further development of the project and the creation of an improved working prototype. The article presents the basic electrical circuits of devices, the list of used equipment and software used and the photographic material of prototypes of the created system.",0.2592592593],["a mixture model with online knowledge distillation can achieve better evaluation performance. the proposed distill","MOD: A Deep Mixture Model with Online Knowledge Distillation for Large Scale Video Temporal Concept Localization","summarize: In this paper, we present and discuss a deep mixture model with online knowledge distillation for large-scale video temporal concept localization, which is ranked 3rd in the 3rd YouTube-8M Video Understanding Challenge. Specifically, we find that by enabling knowledge sharing with online distillation, fintuning a mixture model on a smaller dataset can achieve better evaluation performance. Based on this observation, in our final solution, we trained and fintuned 12 NeXtVLAD models in parallel with a 2-layer online distillation structure. The experimental results show that the proposed distillation structure can effectively avoid overfitting and shows superior generalization performance. The code is publicly available at: https:\/\/github.com\/linrongc\/solution_youtube8m_v3",0.2976613134],["the resulting nonconvex problem is a spherical harmonic expansion","Variational Uncalibrated Photometric Stereo under General Lighting","summarize: Photometric stereo techniques nowadays remain constrained to an ideal laboratory setup where modeling and calibration of lighting is amenable. To eliminate such restrictions, we propose an efficient principled variational approach to uncalibrated PS under general illumination. To this end, the Lambertian reflectance model is approximated through a spherical harmonic expansion, which preserves the spatial invariance of the lighting. The joint recovery of shape, reflectance and illumination is then formulated as a single variational problem. There the shape estimation is carried out directly in terms of the underlying perspective depth map, thus implicitly ensuring integrability and bypassing the need for a subsequent normal integration. To tackle the resulting nonconvex problem numerically, we undertake a two-phase procedure to initialize a balloon-like perspective depth map, followed by a lagged block coordinate descent scheme. The experiments validate efficiency and robustness of this approach. Across a variety of evaluations, we are able to reduce the mean angular error consistently by a factor of 2-3 compared to the state-of-the-art.",0.3333333333],["specificity prediction systems predict very coarse labels. the aim of this work is to generalize","Domain Agnostic Real-Valued Specificity Prediction","summarize: Sentence specificity quantifies the level of detail in a sentence, characterizing the organization of information in discourse. While this information is useful for many downstream applications, specificity prediction systems predict very coarse labels and are trained on and tailored toward specific domains . The goal of this work is to generalize specificity prediction to domains where no labeled data is available and output more nuanced real-valued specificity ratings. We present an unsupervised domain adaptation system for sentence specificity prediction, specifically designed to output real-valued estimates from binary training labels. To calibrate the values of these predictions appropriately, we regularize the posterior distribution of the labels towards a reference distribution. We show that our framework generalizes well to three different domains with 50%~68% mean absolute error reduction than the current state-of-the-art system trained for news sentence specificity. We also demonstrate the potential of our work in improving the quality and informativeness of dialogue generation systems.",0.0],["billiard in the plane endowed with symmetric .","Cries and whispers in wind-tree forests","summarize: We study billiard in the plane endowed with symmetric \\",0.7547169811],["flexibly tunable band gaps cover a wide range of the solar spectrum.","Enhanced visible light absorption in ZnO\/GaN heterostructured nanofilms","summarize: ZnO\/GaN alloys exhibit exceptional photocatalyst applications owing to the flexibly tunable band gaps that cover a wide range of the solar spectrum, and thus have attracted extensive attentions over the past few years. In this study, first-principles calculations were employed to investigate structural stabilities and electronic properties of and ZnO\/GaN heterostructured nanofilms. The effects of nanofilm thickness and GaN ratio were explored. It was found that all studied heterostructured nanofilms were less stable than the corresponding pure ZnO film but more stable than pure GaN one, exhibiting a much thicker film with better stability. Electronic band structures displayed that both two types of and heterostructured nanofilms were semiconductors with band gaps strongly depending on the GaN ratios as well as the thicknesses. Of particular interesting is that the band gaps decreased firstly, and then increased with the increasing GaN ratio. Furthermore, electronic contribution to the valence band maximum and the conduction band minimum, and optical absorption were discussed. Our results of ZnO\/GaN heterostructured nanofilms with spatial separation of electrons and holes, and flexibly tunable band gaps hold great promise for applications in visible-photovoltaic field.",0.2941176471],["the original uncertainty relation does not hold when the number of particles in the environment is small.","Position-Momentum Uncertainty Relation for an Open Macroscopic Quantum System","summarize: In this study, we explore the validity of the original Heisenberg position- momentum uncertainty relation for a macroscopic harmonic oscillator interacting with environmental micro particles. Our results show that, in the quasi-classical situation, the original uncertainty relation does not hold when the number of particles in the environment is small. Nonetheless, increasing the environmental degrees of freedom resolves the violation in the region of our investigation.",0.04],["Continual learning studies agents that learn from streams of tasks without forgetting previous tasks while adapt","Online Fast Adaptation and Knowledge Accumulation: a New Approach to Continual Learning","summarize: Continual learning studies agents that learn from streams of tasks without forgetting previous ones while adapting to new ones. Two recent continual-learning scenarios have opened new avenues of research. In meta-continual learning, the model is pre-trained to minimize catastrophic forgetting of previous tasks. In continual-meta learning, the aim is to train agents for faster remembering of previous tasks through adaptation. In their original formulations, both methods have limitations. We stand on their shoulders to propose a more general scenario, OSAKA, where an agent must quickly solve new tasks, while also requiring fast remembering. We show that current continual learning, meta-learning, meta-continual learning, and continual-meta learning techniques fail in this new scenario. We propose Continual-MAML, an online extension of the popular MAML algorithm as a strong baseline for this scenario. We empirically show that Continual-MAML is better suited to the new scenario than the aforementioned methodologies, as well as standard continual learning and meta-learning approaches.",0.0526315789],["a method is presented to evaluate the particle-phonon coupling corrections. in","Phonon-particle coupling effects in single-particle energies of semi-magic nuclei","summarize: A method is presented to evaluate the particle-phonon coupling corrections to the single-particle energies in semi-magic nuclei. In such nuclei always there is a collective low-lying ",0.4],["Buzzard irregularity does not have p-adic slope.","A remark on non-integral p-adic slopes for modular forms","summarize: We give a sufficient condition, namely Buzzard irregularity, for there to exist a cuspidal eigenform which does not have integral p-adic slope.",0.214707798],["the effect of a perfectly conducting planar boundary on the average linear momentum, ang","Average Linear and Angular Momentum and Power of Random Fields Near a Perfectly Conducting Boundary","summarize: The effect of a perfectly conducting planar boundary on the average linear momentum , angular , and power of a time-harmonic statistically isotropic random field is analyzed. These averages are purely imaginary and their magnitude decreases in a damped oscillatory manner with distance from the boundary. At discrete quasi-periodic distances and frequencies, the average LM and AM attain their free-space value. Implications for the optimal placement or tuning of power and field sensors are analyzed. Conservation of the flux of the mean LM and AM with respect to the difference of the average electric and magnetic energies and the radiation stresses via the Maxwell stress dyadic is demonstrated. The second-order spatial derivatives of differential radiation stress can be directly linked to the electromagnetic energy imbalance. Analytical results are supported by Monte Carlo simulation results. As an application, performance based estimates for the working volume of a reverberation chamber are obtained. In the context of multiphysics compatibility, mechanical self-stirred reverberation is proposed as an exploitation of electromagnetic stress.",0.3879428249],["quantum computation is an emerging technology that promises to be powerful tool in many areas. the development","Procedural generation using quantum computation","summarize: Quantum computation is an emerging technology that promises to be a powerful tool in many areas. Though some years likely still remain until significant quantum advantage is demonstrated, the development of the technology has led to a range of valuable resources. These include publicly available prototype quantum hardware, advanced simulators for small quantum programs and programming frameworks to test and develop quantum software. In this provocation paper we seek to demonstrate that these resources are sufficient to provide the first useful results in the field of procedural generation. This is done by introducing a proof-of-principle method: a quantum generalization of a blurring process, in which quantum interference is used to provide a unique effect. Through this we hope to show that further developments in the technology are not required before it becomes useful for procedural generation. Rather, fruitful experimentation with this new technology can begin now.",0.1818181818],["new materials are necessary for critical advances in technologies. a small electrical current engages with","Towards Electrical-Current Control of Quantum States in Spin-Orbit-Coupled Matter","summarize: Novel materials, which often exhibit surprising or even revolutionary physical properties, are necessary for critical advances in technologies. Simultaneous control of structural and physical properties via a small electrical current is of great significance both fundamentally and technologically. Recent studies demonstrate that a combination of strong spin-orbit interactions and a distorted crystal structure in magnetic Mott insulators is sufficient to attain this long-desired goal. In this Topical Review, we highlight underlying properties of this class of materials and present two representative antiferromagnetic Mott insulators, namely, 4d-electron based Ca2RuO4 and 5d-electron based Sr2IrO4, as model systems. In essence, a small, applied electrical current engages with the lattice, critically reducing structural distortions, which in turn readily suppresses the antiferromagnetic and insulating state and subsequently results in emergent new states. While details may vary in different materials, at the heart of these phenomena are current-reduced lattice distortions, which, via spin-orbit interactions, dictate physical properties. Electrical current, which joins magnetic field, electric field, pressure, light, etc. as a new external stimulus, provides a new, key dimension for materials research, and also pose a series of intriguing questions that may provide the impetus for advancing our understanding of spin-orbit-coupled matter. This Topical Review provides a brief introduction, a few hopefully informative examples and some general remarks. It is by no means an exhaustive report of the current state of studies on this topic.",0.24],["reinforcement learning is a popular paradigm for addressing sequential decision tasks. learning in many domain","Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey","summarize: Reinforcement learning is a popular paradigm for addressing sequential decision tasks in which the agent has only limited environmental feedback. Despite many advances over the past three decades, learning in many domains still requires a large amount of interaction with the environment, which can be prohibitively expensive in realistic scenarios. To address this problem, transfer learning has been applied to reinforcement learning such that experience gained in one task can be leveraged when starting to learn the next, harder task. More recently, several lines of research have explored how tasks, or data samples themselves, can be sequenced into a curriculum for the purpose of learning a problem that may otherwise be too difficult to learn from scratch. In this article, we present a framework for curriculum learning in reinforcement learning, and use it to survey and classify existing CL methods in terms of their assumptions, capabilities, and goals. Finally, we use our framework to find open problems and suggest directions for future RL curriculum learning research.",0.3235294118],["a physical theory loses its concrete applicability and thus verifiability of its","Examples of non-constructive proofs in quantum theory","summarize: Unlike mathematics, in which the notion of truth might be abstract, in physics, the emphasis must be placed on algorithmic procedures for obtaining numerical results subject to the experimental verifiability. For, a physical science is exactly that: algorithmic procedures for obtaining verifiable conclusions from a set of basic hypotheses. By admitting non-constructivist statements a physical theory loses its concrete applicability and thus verifiability of its predictions. Accordingly, the requirement of constructivism must be indispensable to any physical theory. Nevertheless, in at least some physical theories, and especially in quantum mechanics, one can find examples of non-constructive statements. The present paper demonstrates a couple of such examples dealing with macroscopic quantum states . As it is shown, in these examples the proofs of the existence of macroscopic quantum states are based on logical principles allowing one to decide the truth of predicates over an infinite number of things.",0.2105263158],["the method is a simple neuromorphic device organized by the same geometry high-dimensional electro","Implementing robust neuromodulation in neuromorphic circuits","summarize: We introduce a methodology to implement the physiological transition in electronic circuits composed of resistors, capacitors and transistors. The result is a simple neuromorphic device organized by the same geometry high-dimensional electrophysiological neuron models. experimental results highlight the robustness of the approach in real-world applications.",0.1052631579],["in this paper we prove conditions for transversal intersection of monomial ideals.","Transversal Intersection of Monomial Ideals","summarize: In this paper we prove conditions for transversal intersection of monomial ideals and derive a simplicial characterization of this phenomenon.",0.0769230769],["search engine combines visual and textual cues to retrieve items from a multimedia database","DeepStyle: Multimodal Search Engine for Fashion and Interior Design","summarize: In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario where the user looks for 'the same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by 18-21% on the tested datasets. Our search engine is commercially deployed and available through a Web-based application.",0.2272727273],["this paper investigates distributed control and incentive mechanisms. it proposes a distributed stoch","Online Stochastic Optimization of Networked Distributed Energy Resources","summarize: This paper investigates distributed control and incentive mechanisms to coordinate distributed energy resources with both continuous and discrete decision variables as well as device dynamics in distribution grids. We formulate a multi-period social welfare maximization problem, and based on its convex relaxation propose a distributed stochastic dual gradient algorithm for managing DERs. We further extend it to an online realtime setting with time-varying operating conditions, asynchronous updates by devices, and feedback being leveraged to account for nonlinear power flows as well as reduce communication overhead. The resulting algorithm provides a general online stochastic optimization algorithm for coordinating networked DERs with discrete power setpoints and dynamics to meet operational and economic objectives and constraints. We characterize the convergence of the algorithm analytically and evaluate its performance numerically.",0.1052631579],["a DCNN trained to recognize user actions can classify five actions in a collection of","Building Usage Profiles Using Deep Neural Nets","summarize: To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we consider publicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos . In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product.",0.2333333333],["we investigate the non-linear response and energy absorption in bulk silicon irradi","Ultrafast energy absorption and photoexcitation of bulk plasmon in crystalline silicon subjected to intense near-infrared ultrashort laser pulses","summarize: We investigate the non-linear response and energy absorption in bulk silicon irradiated by intense 12-fs near-infrared laser pulses. Depending on the laser intensity, we distinguish two regimes of non-linear absorption of the laser energy: for low intensities, energy deposition and photoionization involve perturbative three-photon transition through the direct bandgap of silicon. For laser intensities near and above 10",0.3465889484],["axioms for 2D TQFT formulated on cell graphs with edge","Edge contraction on dual ribbon graphs and 2D TQFT","summarize: We present a new set of axioms for 2D TQFT formulated on the category of cell graphs with edge-contraction operations as morphisms. We construct a functor from this category to the endofunctor category consisting of Frobenius algebras. Edge-contraction operations correspond to natural transformations of endofunctors, which are compatible with the Frobenius algebra structure. Given a Frobenius algebra A, every cell graph determines an element of the symmetric tensor algebra defined over the dual space A*. We show that the edge-contraction axioms make this assignment depending only on the topological type of the cell graph, but not on the graph itself. Thus the functor generates the TQFT corresponding to A.",0.3636363636],["open domain keyphrase extraction dataset contains nearly one hundred thousand web documents. BLING-","Open Domain Web Keyphrase Extraction Beyond Language Modeling","summarize: This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.",0.0],["the flow field has two distinct regions of influence. the flow field has two distinct regions of","Constraining Jupiter's internal flows using Juno magnetic and gravity measurements","summarize: Deciphering the flow below the cloud-level of Jupiter remains a critical milestone in understanding Jupiter's internal structure and dynamics. The expected high-precision Juno measurements of both the gravity field and the magnetic field might help to reach this goal. Here we propose a method that combines both fields to constrain the depth dependent flow field inside Jupiter. This method is based on a mean-field electrodynamic balance that relates the flow field to the anomalous magnetic field, and geostrophic balance that relates the flow field to the anomalous gravity field. We find that the flow field has two distinct regions of influence - an upper region in which the flow affects mostly the gravity field, and a lower region in which the flow affects mostly the magnetic field. An optimization procedure allows to reach a unified flow structure that is consistent with both the gravity and the magnetic fields.",0.0606060606],["two new series expansions for generalized Euler's constants.","Expansions of generalized Euler's constants into the series of polynomials in ","summarize: In this work, two new series expansions for generalized Euler's constants ",0.3032653299],["the legacy imaging surveys for the dark energy spectroscopic instrument project provides multiplecolor photo","New Determination of Fundamental Properties of Palomar 5 Using Deep DESI Imaging Data","summarize: The legacy imaging surveys for the Dark Energy Spectroscopic Instrument project provides multiplecolor photometric data, which are about 2 mag deeper than the SDSS. In this study, we redetermine the fundamental properties for an old halo globular cluster of Palomar 5 based on these new imaging data, including structure parameters, stellar population parameters, and luminosity and mass functions. These characteristics, together with its tidal tails, are key for dynamical studies of the cluster and constraining the mass model of the Milky Way. By fitting the King model to the radial surface density profile of Palomar 5, we derive the core radius of ",0.0],["nanostructure strategies focus on increasing phonon scattering and reducing mean-free-path","Heat current anticorrelation effects leading to thermal conductivity reduction in nanoporous Si","summarize: Prevailing nanostructuring strategies focus on increasing phonon scattering and reducing the mean-free-path of phonons across the spectrum. In nanoporous Si materials, for example, boundary scattering reduces thermal conductivity drastically. In this work, we identify an unusual anticorrelated specular phonon scattering effect which can result in additional reductions in thermal conductivity of up to ~ 80% for specific nanoporous geometries. We further find evidence that this effect has its origin in heat trapping between large pores with narrow necks. As the heat becomes trapped between the pores, phonons undergo multiple specular reflections such that their contribution to the thermal conductivity is partly undone. We find this effect to be wave-vector dependent at low temperatures. We use large-scale molecular dynamics simulations, wave packet analysis, as well as an analytical model to illustrate the anticorrelation effect, evaluate its impact on thermal conductivity, and detail how it can be controlled to manipulate phonon transport in nanoporous materials.",0.2046826883],["the proposed formulation achieves even greater domains of attraction. the proposed formulation achieves even","Harmonic based model predictive control for set-point tracking","summarize: This paper presents a novel model predictive control formulation for set-point tracking. Stabilizing predictive controllers based on terminal ingredients may exhibit stability and feasibility issues in the event of a reference change for small to moderate prediction horizons. In the MPC for tracking formulation, these issues are solved by the addition of an artificial equilibrium point as a new decision variable, providing a significantly enlarged domain of attraction and guaranteeing recursive feasibility for any reference change. However, it may suffer from performance issues if the prediction horizon is not large enough. This paper presents an extension of this formulation where a harmonic artificial reference is used in place of the equilibrium point. The proposed formulation achieves even greater domains of attraction and can significantly outperform other MPC formulations when the prediction horizon is small. We prove the asymptotic stability and recursive feasibility of the proposed controller, as well as provide guidelines for the design of its main ingredients. Finally, we highlight its advantages with a case study of a ball and plate system.",0.0],["uncertainty relations for non-commutative space are computed. we obtain a better","Probing Uncertainty Relations in Non-Commutative Space","summarize: In this paper, we compute uncertainty relations for non-commutative space and obtain a better lower bound than the standard one obtained from Heisenberg's uncertainty relation. We also derive the reverse uncertainty relation for product and sum of uncertainties of two incompatible variables for one linear and another non-linear model of the harmonic oscillator. The non-linear model in non-commutating space yields two different expressions for Schr\\odinger and Heisenberg uncertainty relation. This distinction does not arise in commutative space, and even in the linear model of non-commutative space.",0.2352941176],["the Noble Element Simulation Technique. a model reproduces the scintillation","Improved Modeling of ","summarize: We report here methods and techniques for creating and improving a model that reproduces the scintillation and ionization response of a dual-phase liquid and gaseous xenon time-projection chamber. Starting with the recent release of the Noble Element Simulation Technique , electronic recoil data from the ",0.0],["the resonant frequency analysis is done numerically through the determination of the eigen","Resonant frequency analysis of dental implants","summarize: Dental implant stability influences the decision on the determination of the duration between implant insertion and loading, This work investigates the resonant frequency analysis by means of a numerical model. The investigation is done numerically through the determination of the eigenfrequencies and performing a steady state response analyses using a commercial finite element package. A peri-implant interface, of simultaneously varying stiffness and layer thickness is introduced in the numerical 3D model in order to probe the sensitivity of the eigenfrequencies and steady state response to an evolving weakened layer, in an attempt to identify the bone reconstruction around the implant. For the first two modes, the resonant frequency is somewhat insensitive to the healing process, unless the weakened layer is rather large and compliant, like in the very early stages of the implantation. A Normalized Healing Factor is devised in the spirit of the Implant Stability Quotient, which can identify the healing process especially at the early stages after implantation. The sensitivity of the RFA to changes of mechanical properties of periprosthetic bone tissue seems relatively weak. Another indicator considering the amplitude as well as the resonance frequency might be more adapted to bone healing estimations. However, these results need to be verified experimentally as well as clinically.",0.2],["dynamic regime literature aims to map characteristics of a unit to a action tailored to maximize","Estimation of Personalized Effects Associated With Causal Pathways","summarize: The goal of personalized decision making is to map a unit's characteristics to an action tailored to maximize the expected outcome for that unit. Obtaining high-quality mappings of this type is the goal of the dynamic regime literature. In healthcare settings, optimizing policies with respect to a particular causal pathway may be of interest as well. For example, we may wish to maximize the chemical effect of a drug given data from an observational study where the chemical effect of the drug on the outcome is entangled with the indirect effect mediated by differential adherence. In such cases, we may wish to optimize the direct effect of a drug, while keeping the indirect effect to that of some reference treatment. shows how to combine mediation analysis and dynamic treatment regime ideas to defines policies associated with causal pathways and counterfactual responses to these policies. In this paper, we derive a variety of methods for learning high quality policies of this type from data, in a causal model corresponding to a longitudinal setting of practical importance. We illustrate our methods via a dataset of HIV patients undergoing therapy, gathered in the Nigerian PEPFAR program.",0.3571428571],["uncertainty relations for non-commutative space are computed. we obtain a better","Probing Uncertainty Relations in Non-Commutative Space","summarize: In this paper, we compute uncertainty relations for non-commutative space and obtain a better lower bound than the standard one obtained from Heisenberg's uncertainty relation. We also derive the reverse uncertainty relation for product and sum of uncertainties of two incompatible variables for one linear and another non-linear model of the harmonic oscillator. The non-linear model in non-commutating space yields two different expressions for Schr\\odinger and Heisenberg uncertainty relation. This distinction does not arise in commutative space, and even in the linear model of non-commutative space.",0.2352941176],["the calculated filling factors for a feature reflect the fraction of the solar disc covered by that","Filling Factors of Sunspots in SODISM Images","summarize: Received: 1st December 2018; Accepted: 18th February 2019; Published: 1st April 2019 Abstract: The calculated filling factors for a feature reflect the fraction of the solar disc covered by that feature, and the assignment of reference synthetic spectra. In this paper, the FFs, specified as a function of radial position on the solar disc, are computed for each image in a tabular form. The filling factor is an important parameter and is defined as the fraction of area in a pixel covered with the magnetic field, whereas the rest of the area in the pixel is field-free. However, this does not provide extensive information about the experiments conducted on tens or hundreds of such images. This is the first time that filling factors for SODISM images have been catalogued in tabular formation. This paper presents a new method that provides the means to detect sunspots on full-disk solar images recorded by the Solar Diameter Imager and Surface Mapper on the PICARD satellite. The method is a totally automated detection process that achieves a sunspot recognition rate of 97.6%. The number of sunspots detected by this method strongly agrees with the NOAA catalogue. The sunspot areas calculated by this method have a 99% correlation with SOHO over the same period, and thus help to calculate the filling factor for wavelength 607nm.",0.1],["the validity of the measurement has been double-checked in the well-mixed","Moran-evolution of cooperation: From well-mixed to heterogeneous complex networks","summarize: Configurational arrangement of network architecture and interaction character of individuals are two most influential factors on the mechanisms underlying the evolutionary outcome of cooperation, which is explained by the well-established framework of evolutionary game theory. In the current study, not only qualitatively but also quantitatively, we measure Moran-evolution of cooperation to support an analytical agreement based on the consequences of the replicator equation in a finite population. The validity of the measurement has been double-checked in the well-mixed network by the Langevin stochastic differential equation and the Gillespie-algorithmic version of Moran-evolution, while in a structured network, the measurement of accuracy is verified by the standard numerical simulation. Considering the Birth-Death and Death-Birth updating rules through diffusion of individuals, the investigation is carried out in the wide range of game environments those relate to the various social dilemmas where we are able to draw a new rigorous mathematical track to tackle the heterogeneity of complex networks. The set of modified criteria reveals the exact fact about the emergence and maintenance of cooperation in the structured population. We find that in general, nature promotes the environment of coexistent traits.",0.1176470588],["morphology of smooth deposits covering parts of the surface suggests liquid extrusions.","Cryomagma ascent on Europa","summarize: Europa's surface exhibits morphological features which, associated with a low crater density, might be interpreted to have formed as a result of recent cryovolcanic activity. In particular, the morphology of smooth deposits covering parts of the surface, and their relationship to the surrounding terrains, suggest that they result from liquid extrusions. Furthermore, recent literature suggests that the emplacement of liquid-related features, such as double ridges, lenticulae and chaos could result from the presence of liquid reservoirs beneath the surface. We model the ascent of liquid water through a fracture or a pipe-like conduit from a subsurface reservoir to Europa\\textquoteright s surface and calculate the eruption time-scale and the total volume extruded during the eruption, as a function of the reservoir volume and depth. We also estimate the freezing time of a subsurface reservoir necessary to trigger an eruption. Our model is derived for pure liquid water and for a briny mixture outlined by Kargel : 81 wt% H",0.0],["learned models can achieve significant performance gains over traditional methods. this special issue covers the state of","Editorial: Introduction to the Issue on Deep Learning for Image\/Video Restoration and Compression","summarize: Recent works have shown that learned models can achieve significant performance gains, especially in terms of perceptual quality measures, over traditional methods. Hence, the state of the art in image restoration and compression is getting redefined. This special issue covers the state of the art in learned image\/video restoration and compression to promote further progress in innovative architectures and training methods for effective and efficient networks for image\/video restoration and compression.",0.0526315789],["sequence of the primes.","The size of the primes obstructing the existence of rational points","summarize: The sequence of the primes ",0.1303304576],["a novel and general task-agnostic search space is proposed. the","MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning","summarize: We propose to incorporate neural architecture search into general-purpose multi-task learning . Existing NAS methods typically define different search spaces according to different tasks. In order to adapt to different task combinations , we disentangle the GP-MTL networks into single-task backbones , and a hierarchical and layerwise features sharing\/fusing scheme across them. This enables us to design a novel and general task-agnostic search space, which inserts cross-task edges into fixed single-task network backbones. Moreover, we also propose a novel single-shot gradient-based search algorithm that closes the performance gap between the searched architectures and the final evaluation architecture. This is realized with a minimum entropy regularization on the architecture weights during the search phase, which makes the architecture weights converge to near-discrete values and therefore achieves a single model. As a result, our searched model can be directly used for evaluation without training from scratch. We perform extensive experiments using different single-task backbones on various task sets, demonstrating the promising performance obtained by exploiting the hierarchical and layerwise features, as well as the desirable generalizability to different i) task sets and ii) single-task backbones. The code of our paper is available at https:\/\/github.com\/bhpfelix\/MTLNAS.",0.4375],["deep neural networks have been quite successful in solving complex learning problems. but the complex learning parameters","Compressed Learning of Deep Neural Networks for OpenCL-Capable Embedded Systems","summarize: Deep neural networks have been quite successful in solving many complex learning problems. However, DNNs tend to have a large number of learning parameters, leading to a large memory and computation requirement. In this paper, we propose a model compression framework for efficient training and inference of deep neural networks on embedded systems. Our framework provides data structures and kernels for OpenCL-based parallel forward and backward computation in a compressed form. In particular, our method learns sparse representations of parameters using ",0.0416666667],["nano-based nano-based devices are able to be deposited on metal surfaces.","Molecular anchoring stabilizes low valence NiTPP on copper against thermally induced chemical changes","summarize: Many applications of molecular layers deposited on metal surfaces, ranging from single-atom catalysis to on-surface magnetochemistry and biosensing, rely on the use of thermal cycles to regenerate the pristine properties of the system. Thus, understanding the microscopic origin behind the thermal stability of organic\/metal interfaces is fundamental for engineering reliable organic-based devices. Here, we study nickel porphyrin molecules on a copper surface as an archetypal system containing a metal center whose oxidation state can be controlled through the interaction with the metal substrate. We demonstrate that the strong molecule-surface interaction, followed by charge transfer at the interface, plays a fundamental role in the thermal stability of the layer by rigidly anchoring the porphyrin to the substrate. Upon thermal treatment, the molecules undergo an irreversible transition at 420 K, which is associated with an increase of the charge transfer from the substrate, mostly localized on the phenyl substituents, and a downward tilting of the latters without any chemical modification",0.0641348399],["the tetrahedron is a special case of four triangular faces with","The stray- and demagnetizing field from a homogeneously magnetized tetrahedron","summarize: The stray- and demagnetization tensor field for a homogeneously magnetized tetrahedron is found analytically. The tetrahedron is a special case of four triangular faces with constant magnetization-charge surface density, for which we also determine the tensor field. The tensor field is implemented in the open source micromagnetic and magnetostatic simulation framework MagTense and compared with the obtained magnetic field from an FEM solution, showing excellent agreement. This result is important for modeling magnetostatics in general and for micromagnetism in particular as the demagnetizing field of an arbitrary body discretized using conventional meshing techniques is significantly simplified with this approach.",0.4117647059],["the balance function measures the correlation between opposite sign charge pairs. the study of the balance function","Reaction Plane and Beam Energy Dependence Of The Balance Function at RHIC","summarize: The balance function, which measures the correlation between opposite sign charge pairs, is sensitive to the mechanisms of charge formation and the subsequent relative diffusion of the balancing charges. The study of the balance function can provide information about charge creation time as well as the subsequent collective behavior of particles. In this paper, we present a reaction-plane-dependent balance function study for Au+Au collisions at ",0.0],["multishot magnetic resonance imaging has gained popularity as it accelerates the MRI data acquisition process without","Automating Motion Correction in Multishot MRI Using Generative Adversarial Networks","summarize: Multishot Magnetic Resonance Imaging has recently gained popularity as it accelerates the MRI data acquisition process without compromising the quality of final MR image. However, it suffers from motion artifacts caused by patient movements which may lead to misdiagnosis. Modern state-of-the-art motion correction techniques are able to counter small degree motion, however, their adoption is hindered by their time complexity. This paper proposes a Generative Adversarial Network for reconstructing motion free high-fidelity images while reducing the image reconstruction time by an impressive two orders of magnitude.",0.05],["a physics-based recurrent neural network model is designed to learn dynamics of linear","DynNet: Physics-based neural architecture design for linear and nonlinear structural response modeling and prediction","summarize: Data-driven models for predicting dynamic responses of linear and nonlinear systems are of great importance due to their wide application from probabilistic analysis to inverse problems such as system identification and damage diagnosis. In this study, a physics-based recurrent neural network model is designed that is able to learn the dynamics of linear and nonlinear multiple degrees of freedom systems given a ground motion. The model is able to estimate a complete set of responses, including displacement, velocity, acceleration, and internal forces. Compared to the most advanced counterparts, this model requires a smaller number of trainable variables while the accuracy of predictions is higher for long trajectories. In addition, the architecture of the recurrent block is inspired by differential equation solver algorithms and it is expected that this approach yields more generalized solutions. In the training phase, we propose multiple novel techniques to dramatically accelerate the learning process using smaller datasets, such as hardsampling, utilization of trajectory loss function, and implementation of a trust-region approach. Numerical case studies are conducted to examine the strength of the network to learn different nonlinear behaviors. It is shown that the network is able to capture different nonlinear behaviors of dynamic systems with very high accuracy and with no need for prior information or very large datasets.",0.4629805393],["the Hamiltonian Cycle polynomial is not a monotone subexponential","Monotone Projection Lower Bounds from Extended Formulation Lower Bounds","summarize: In this short note, we reduce lower bounds on monotone projections of polynomials to lower bounds on extended formulations of polytopes. Applying our reduction to the seminal extended formulation lower bounds of Fiorini, Massar, Pokutta, Tiwari, & de Wolf and Rothvoss , we obtain the following interesting consequences. 1. The Hamiltonian Cycle polynomial is not a monotone subexponential-size projection of the permanent; this both rules out a natural attempt at a monotone lower bound on the Boolean permanent, and shows that the permanent is not complete for non-negative polynomials in VNP",0.1428571429],["the classical methods to treat quasi-periodic scattering problems no longer work.","A High Order Numerical Method for Scattering from Locally Perturbed Periodic Surfaces","summarize: In this paper, we will introduce a high order numerical method to solve the scattering problems with non-periodic incident fields and periodic surfaces. For the problems we are considering, the classical methods to treat quasi-periodic scattering problems no longer work, while a Bloch transform based numerical method was proposed in . This numerical method, on one hand, is able to solve this kind of problems convergently; on the other hand, it takes up a lot of time and memory during the computation. The motivation of this paper is to improve this numerical method, from the regularity results of the Bloch transform of the total field, which have been studied in . As the set of the singularities of the total field is discrete in ",0.0],["the aim of the present work is to provide the theoretical fundamentals needed to monitor power grids","Smart Grid Monitoring Using Power Line Modems: Effect of Anomalies on Signal Propagation","summarize: The aim of the present work is to provide the theoretical fundamentals needed to monitor power grids using high frequency sensors. In our context, network monitoring refers to the harvesting of different kinds of information: topology of the grid, load changes, presence of faults and cable degradation. We rely on transmission line theory to carry out a thorough analysis of how high frequency signals, such those produced by power line modems, propagate through multi-conductor power networks. We also consider the presence of electrical anomalies on the network and analyze how they affect the signal propagation. In this context, we propose two models that rely on reflectometric and end-to-end measurements to extrapolate information about possible anomalies. A thorough discussion is carried out to explain the properties of each model and measurement method, in order to enable the development of appropriate anomaly detection and location algorithms.",0.1],["LINC-NIRVANA is a high resolution, near infrared","Alignment and preliminary outcomes of an ELT-size instrument to a very large telescope: LINC-NIRVANA at LBT","summarize: LINC-NIRVANA is a high resolution, near infrared imager that uses a multiple field-of-view, layer-oriented, multi-conjugate AO system, consisting of four multi-pyramid wavefront sensors . The system employs up to 40 star probes, looking at up to 20 natural guide stars simultaneously. Its final goal is to perform Fizeau interferometric imaging, thereby achieving ELT-like spatial resolution . For this reason, LN is also equipped with a fringe tracker, a beam combiner and a NIR science camera, for a total of more than 250 optical components and an overall size of approximately 6x4x4.5 meters. This paper describes the tradeoffs evaluated in order to achieve the alignment of the system to the telescope. We note that LN is comparable in size to planned ELT instrumentation. The impact of such alignment strategies will be compared and the selected procedure, where the LBT telescope is, in fact, aligned to the instrument, will be described. Furthermore, results coming from early night-time commissioning of the system will be presented.",0.1228680207],["generating minors are encoded in a hypergraph. we can describe algebraic","Prime splittings of Determinantal Ideals","summarize: We consider determinantal ideals, where the generating minors are encoded in a hypergraph. We study when the generating minors form a Gr\\obner basis. In this case, the ideal is radical, and we can describe algebraic and numerical invariants of these ideals in terms of combinatorial data of their hypergraphs, such as the clique decomposition. In particular, we can construct a minimal free resolution as a tensor product of the minimal free resolution of their cliques. For several classes of hypergraphs we find a combinatorial description of the minimal primes in terms of a prime splitting. That is, we write the determinantal ideal as a sum of smaller determinantal ideals such that each minimal prime is a sum of minimal primes of the summands.",0.2631578947],["network geometry is a great tool in a variety of practical applications. it is also","Network Geometry","summarize: Real networks are finite metric spaces. Yet the geometry induced by shortest path distances in a network is definitely not its only geometry. Other forms of network geometry are the geometry of latent spaces underlying many networks, and the effective geometry induced by dynamical processes in networks. These three approaches to network geometry are all intimately related, and all three of them have been found to be exceptionally efficient in discovering fractality, scale-invariance, self-similarity, and other forms of fundamental symmetries in networks. Network geometry is also of great utility in a variety of practical applications, ranging from the understanding how the brain works, to routing in the Internet. Here, we review the most important theoretical and practical developments dealing with these approaches to network geometry in the last two decades, and offer perspectives on future research directions and challenges in this novel frontier in the study of complexity.",0.0],["spiking representation model is a challenge for a spiking neural network layer","Representation Learning using Event-based STDP","summarize: Although representation learning methods developed within the framework of traditional neural networks are relatively mature, developing a spiking representation model remains a challenging problem. This paper proposes an event-based method to train a feedforward spiking neural network layer for extracting visual features. The method introduces a novel spike-timing-dependent plasticity learning rule and a threshold adjustment rule both derived from a vector quantization-like objective function subject to a sparsity constraint. The STDP rule is obtained by the gradient of a vector quantization criterion that is converted to spike-based, spatio-temporally local update rules in a spiking network of leaky, integrate-and-fire neurons. Independence and sparsity of the model are achieved by the threshold adjustment rule and by a softmax function implementing inhibition in the representation layer consisting of WTA-thresholded spiking neurons. Together, these mechanisms implement a form of spike-based, competitive learning. Two sets of experiments are performed on the MNIST and natural image datasets. The results demonstrate a sparse spiking visual representation model with low reconstruction loss comparable with state-of-the-art visual coding approaches, yet our rule is local in both time and space, thus biologically plausible and hardware friendly.",0.25],["Siegel proved that every totally positive element of a number field K is the sum of four","A p-adic analogue of Siegel's Theorem on sums of squares","summarize: Siegel proved that every totally positive element of a number field K is the sum of four squares, so in particular the Pythagoras number is uniformly bounded across number fields. The p-adic Kochen operator provides a p-adic analogue of squaring, and a certain localisation of the ring generated by this operator consists of precisely the totally p-integral elements of K. We use this to formulate and prove a p-adic analogue of Siegel's theorem, by introducing the p-Pythagoras number of a general field, and showing that this number is uniformly bounded across number fields. We also generally study fields with finite p-Pythagoras number and show that the growth of the p-Pythagoras number in finite extensions is bounded.",0.4285714286],["the special fiber is described using the so called Bruhat-Tits stratification.","On the Bruhat-Tits stratification of a quaternionic unitary Shimura variety","summarize: In this note we study the special fiber of the Rapoport-Zink space attached to a quaternionic unitary group. The special fiber is described using the so called Bruhat-Tits stratification and is intimately related to the Bruhat-Tits building of a split symplectic group. As an application we describe the supersingular locus of the related Shimura variety.",0.3076923077],["moving active particle is observed to be surrounded by localized topological defects. such","Dressed Active Particles in Spherical Crystals","summarize: We investigate the dynamics of an active particle in two-dimensional spherical crystals, which provide an ideal environment to illustrate the interplay of active particle and crystallographic defects. A moving active particle is observed to be surrounded by localized topological defects, becoming a dressed active particle. Such a physical picture characterizes both the lattice distortion around the moving particle and the healing of the distorted lattice in its trajectory. We find that the dynamical behaviors of an active particle in both random and ballistic motions uniformly conform to this featured scenario, whether the particle is initially a defect or not. We further observe that the defect pattern around a dressed ballistic active particle randomly oscillates between two well-defined wing-like defect motifs regardless of its speed. The established physical picture of dressed active particles in this work partially deciphers the complexity of the intriguing nonequilibrium behaviors in active crystals, and opens the promising possibility of introducing the activity to engineer defects, which has strong connections with the design of materials.",0.0],["engineers can improve response times by up to a thousand. advanced engineered drive techniques can","High speed control of electro-mechanical transduction Advanced Drive Techniques for Optimized Step-and-Settle Response of MEMS Micromirrors","summarize: Micro\/Nano Electro Mechanical Systems provide the engineer with a powerful set of solutions to a wide variety of technical challenges. However, because they are mechanical systems, response times can be a limitation. In some situations, advanced engineered drive techniques can improve response times by as much as a thousand, significantly opening up the application space for MEMS\/NEMS solutions.",0.2126152239],["training deep neural networks to learn such data-driven partial differential operators requires extensive spatiotemporal","Linking Machine Learning with Multiscale Numerics: Data-Driven Discovery of Homogenized Equations","summarize: The data-driven discovery of partial differential equations consistent with spatiotemporal data is experiencing a rebirth in machine learning research. Training deep neural networks to learn such data-driven partial differential operators requires extensive spatiotemporal data. For learning coarse-scale PDEs from computational fine-scale simulation data, the training data collection process can be prohibitively expensive. We propose to transformatively facilitate this training data collection process by linking machine learning with modern multiscale scientific computation . These equation-free techniques operate over sparse collections of small, appropriately coupled, space-time subdomains , parsimoniously producing the required macro-scale training data. Our illustrative example involves the discovery of effective homogenized equations in one and two dimensions, for problems with fine-scale material property variations. The approach holds promise towards making the discovery of accurate, macro-scale effective materials PDE models possible by efficiently summarizing the physics embodied in the best fine-scale simulation models available.",0.0],["experiments show a massive acceleration of the annealing of a monolayer of passive","Activity-controlled Annealing of Colloidal Monolayers","summarize: Molecular motors are essential to the living, they generate additional fluctuations that boost transport and assist assembly. Self-propelled colloids, that consume energy to move, hold similar potential for the man-made assembly of microparticles. Yet, experiments showing their use as a powerhouse in materials science lack. Our work explores the design of man-made materials controlled by fluctuations, arising from the internal forces generated by active colloids. Here we show a massive acceleration of the annealing of a monolayer of passive beads by moderate addition of self-propelled microparticles. We rationalize our observations with a model of collisions that drive active fluctuations to overcome kinetic barriers and activate the annealing. The experiment is quantitatively compared with Brownian dynamic simulations that further unveil a dynamical transition in the mechanism of annealing. Active dopants travel uniformly in the system or co-localize at the grain boundaries as a result of the persistence of their motion. Our findings uncover the potential of man-made materials controlled by internal activity and lay the groundwork for the rise of materials science beyond equilibrium.",0.2571428571],["Moir'e superlattices are quantum-active interfaces where non-tri","Twistronics: A turning point in 2D quantum materials","summarize: Moir\\'e superlattices - periodic orbital overlaps and lattice-reconstruction between sites of high atomic registry in vertically-stacked 2D layered materials - are quantum-active interfaces where non-trivial quantum phases on novel phenomena can emerge from geometric arrangements of 2D materials, which are not intrinsic to the parent materials. Unexpected distortions in band-structure and topology lead to long-range correlations, charge-ordering, and several other fascinating quantum phenomena hidden within the physical space between the parent materials. Stacking, twisting, gate-modulating, and optically-exciting these superlattices open up a new field for seamlessly exploring physics from the weak to strong correlations limit within a many-body and topological framework. It is impossible to capture it all, and the aim of this review is to highlight some of the important recent developments in synthesis, experiments, and potential applications of these materials.",0.0],["Let us know what you think about it!","Graded Coherence of Certain Extensions of Graded Algebras","summarize: Let ",0.0],["algorithm is a fast new probabilistic algorithm for solving this problem optimally. algorithm requires","Optimal ancilla-free Clifford+T approximation of z-rotations","summarize: We consider the problem of approximating arbitrary single-qubit z-rotations by ancilla-free Clifford+T circuits, up to given epsilon. We present a fast new probabilistic algorithm for solving this problem optimally, i.e., for finding the shortest possible circuit whatsoever for the given problem instance. The algorithm requires a factoring oracle . Even in the absence of a factoring oracle, the algorithm is still near-optimal under a mild number-theoretic hypothesis. In this case, the algorithm finds a solution of T-count m + O)), where m is the T-count of the second-to-optimal solution. In the typical case, this yields circuit approximations of T-count 3log_2 + O)). Our algorithm is efficient in practice, and provably efficient under the above-mentioned number-theoretic hypothesis, in the sense that its expected runtime is O).",0.25],["RDCL 3D is open source and designed with a modular approach. framework allows","RDCL 3D, a Model Agnostic Web Framework for the Design and Composition of NFV Services","summarize: We present RDCL 3D, a model agnostic web framework for the design and composition of NFV services and components. The framework allows editing and validating the descriptors of services and components both textually and graphically and supports the interaction with external orchestrators or with deployment and execution environments. RDCL 3D is open source and designed with a modular approach, allowing developers to plug in the support for new models. We describe several advances with respect to the NFV state of the art, which have been implemented with RDCL 3D. We have integrated in the platform the latest ETSI NFV ISG model specifications for which no parsers\/validators were available. We have also included in the platform the support for OASIS TOSCA models, reusing existing parsers. Then we have considered the modelling of components in a modular software router , which goes beyond the traditional scope of NFV. We have further developed this approach by combining traditional NFV components and Click elements in a single model. Finally, we have considered the support of this solution using the Unikernels virtualization technology.",0.2707591324],["the singularity at the origin is a coordinate singularity. the tolman formula","Distributional Schwarzschild Geometry from nonsmooth regularization via Horizon. Disributional Rindler spacetime with disributional Levi-Civit\\`a connection induced vacuum dominance.Unruh effect revisited","summarize: In this paper we leave the neighborhood of the singularity at the origin and turn to the singularity at the horizon. Using nonlinear superdistributional geometry and supergeneralized functions it seems possible to show that the horizon singularity is not only a coordinate singularity without leaving Schwarzschild coordinates. However the Tolman formula for the total energy ",0.1711390397],["Cremona group in 2 dimension is not simple, over any field. some elements satisfy","Non simplicit\\'e du groupe de Cremona sur tout corps","summarize: Using a theorem of Dahmani, Guirardel and Osin we prove that the Cremona group in 2 dimension is not simple, over any field. More precisely, we show that some elements of this group satisfy a weakened WPD property which is equivalent in our particular context to the Bestvina and Fujiwara's one.",0.1333333333],["incorporating ImageNet did not help much. a similar approach was proposed to combine scene","Scene recognition with CNNs: objects, scales and dataset bias","summarize: Since scenes are composed in part of objects, accurate recognition of scenes requires knowledge about both scenes and objects. In this paper we address two related problems: 1) scale induced dataset bias in multi-scale convolutional neural network architectures, and 2) how to combine effectively scene-centric and object-centric knowledge in CNNs. An earlier attempt, Hybrid-CNN, showed that incorporating ImageNet did not help much. Here we propose an alternative method taking the scale into account, resulting in significant recognition gains. By analyzing the response of ImageNet-CNNs and Places-CNNs at different scales we find that both operate in different scale ranges, so using the same network for all the scales induces dataset bias resulting in limited performance. Thus, adapting the feature extractor to each particular scale is crucial to improve recognition, since the objects in the scenes have their specific range of scales. Experimental results show that the recognition accuracy highly depends on the scale, and that simple yet carefully chosen multi-scale combinations of ImageNet-CNNs and Places-CNNs, can push the state-of-the-art recognition accuracy in SUN397 up to 66.26% .",0.25],["we establish sharp exponential deviation estimates of the information content. we also establish a sharp bound","Concentration of information content for convex measures","summarize: We establish sharp exponential deviation estimates of the information content as well as a sharp bound on the varentropy for the class of convex measures on Euclidean spaces. This generalizes a similar development for log-concave measures in the recent work of Fradelizi, Madiman and Wang . In particular, our results imply that convex measures in high dimensions are concentrated in an annulus between two convex sets despite their possibly having much heavier tails. Various tools and consequences are developed, including a sharp comparison result for R\\'enyi entropies, inequalities of Kahane-Khinchine type for convex measures that extend those of Koldobsky, Pajor and Yaskin for log-concave measures, and an extension of Berwald's inequality .",0.1935483871],["ARM big.LITTLE architecture is at the heart of prevalent commercial edge devices.","High-Throughput CNN Inference on Embedded ARM big.LITTLE Multi-Core Processors","summarize: IoT Edge intelligence requires Convolutional Neural Network inference to take place in the edge devices itself. ARM big.LITTLE architecture is at the heart of prevalent commercial edge devices. It comprises of single-ISA heterogeneous cores grouped into multiple homogeneous clusters that enable power and performance trade-offs. All cores are expected to be simultaneously employed in inference to attain maximal throughput. However, high communication overhead involved in parallelization of computations from convolution kernels across clusters is detrimental to throughput. We present an alternative framework called Pipe-it that employs pipelined design to split convolutional layers across clusters while limiting parallelization of their respective kernels to the assigned cluster. We develop a performance-prediction model that utilizes only the convolutional layer descriptors to predict the execution time of each layer individually on all permitted core configurations . Pipe-it then exploits the predictions to create a balanced pipeline using an efficient design space exploration algorithm. Pipe-it on average results in a 39% higher throughput than the highest antecedent throughput.",0.1666666667],["Kuwert and Schatzle showed in 2001 that the Willmore flow converges","Asymptotic estimates for the Willmore flow with small energy","summarize: Kuwert and Sch\\atzle showed in 2001 that the Willmore flow converges to a standard round sphere, if the initial energy is small. In this situation, we prove stability estimates for the barycenter and the quadratic moment of the surface. Moreover, in codimension one we obtain stability bounds for the enclosed volume and averaged mean curvature. As direct applications, we recover a quasi-rigidity estimate due to De Lellis and M\\uller and an estimate for the isoperimetric deficit by R\\oger and Sch\\atzle , whose original proofs used different methods.",0.2727272727],["joint resource allocation and power control for energy efficient devices-to-device communications are investigated","Energy Efficient Joint Resource Allocation and Power Control for D2D Communications","summarize: In this paper, joint resource allocation and power control for energy efficient device-to-device communications underlaying cellular networks are investigated. The resource and power are optimized for maximization of the energy efficiency of D2D communications. Exploiting the properties of fractional programming, we transform the original nonconvex optimization problem in fractional form into an equivalent optimization problem in subtractive form. Then, an efficient iterative resource allocation and power control scheme is proposed. In each iteration, part of the constraints of the EE optimization problem is removed by exploiting the penalty function approach. We further propose a novel two-layer approach which allows to find the optimum at each iteration by decoupling the EE optimization problem of joint resource allocation and power control into two separate steps. In the first layer, the optimal power values are obtained by solving a series of maximization problems through root-finding with or without considering the loss of cellular users' rates. In the second layer, the formulated optimization problem belongs to a classical resource allocation problem with single allocation format which admits a network flow formulation so that it can be solved to optimality. Simulation results demonstrate the remarkable improvements in terms of EE by using the proposed iterative resource allocation and power control scheme.",0.1538461538],["the Novikov equation is an analogue of the Camassa-Ho","An Application of Pfaffians to multipeakons of the Novikov equation and the finite Toda lattice of BKP type","summarize: The Novikov equation is an integrable analogue of the Camassa-Holm equation with a cubic nonlinear term. Both these equations support a special family of weak solutions called multipeakon solutions. In this paper, an approach involving Pfaffians is applied to study multipeakons of the Novikov equation. First, we show that the Novikov peakon ODEs describe an isospectral flow on the manifold cut out by certain Pfaffian identities. Then, a link between the Novikov peakons and the finite Toda lattice of BKP type is established based on the use of Pfaffians. Finally, certain generalizations of the Novikov equation and the finite B-Toda lattice are proposed together with special solutions. To our knowledge, it is the first time that the peakon problem is interpreted in terms of Pfaffians.",0.2759095809],["the current practice in neural network optimization is to rely on the stochastic gradient descent algorithm","Deep Frank-Wolfe For Neural Network Optimization","summarize: Learning a deep neural network requires solving a challenging optimization problem: it is a high-dimensional, non-convex and non-smooth minimization problem with a large number of terms. The current practice in neural network optimization is to rely on the stochastic gradient descent algorithm or its adaptive variants. However, SGD requires a hand-designed schedule for the learning rate. In addition, its adaptive variants tend to produce solutions that generalize less well on unseen data than SGD with a hand-designed schedule. We present an optimization method that offers empirically the best of both worlds: our algorithm yields good generalization performance while requiring only one hyper-parameter. Our approach is based on a composite proximal framework, which exploits the compositional nature of deep neural networks and can leverage powerful convex optimization algorithms by design. Specifically, we employ the Frank-Wolfe algorithm for SVM, which computes an optimal step-size in closed-form at each time-step. We further show that the descent direction is given by a simple backward pass in the network, yielding the same computational cost per iteration as SGD. We present experiments on the CIFAR and SNLI data sets, where we demonstrate the significant superiority of our method over Adam, Adagrad, as well as the recently proposed BPGrad and AMSGrad. Furthermore, we compare our algorithm to SGD with a hand-designed learning rate schedule, and show that it provides similar generalization while converging faster. The code is publicly available at https:\/\/github.com\/oval-group\/dfw.",0.05],["two indicators are classically used to evaluate the quality of rule-based classification systems. predictive","On Evaluating the Quality of Rule-Based Classification Systems","summarize: Two indicators are classically used to evaluate the quality of rule-based classification systems: predictive accuracy, i.e. the system's ability to successfully reproduce learning data and coverage, i.e. the proportion of possible cases for which the logical rules constituting the system apply. In this work, we claim that these two indicators may be insufficient, and additional measures of quality may need to be developed. We theoretically show that classification systems presenting good predictive accuracy and coverage can, nonetheless, be trivially improved and illustrate this proposition with examples.",0.1333333333],["nano-based nano-based devices are able to be deposited on metal surfaces.","Molecular anchoring stabilizes low valence NiTPP on copper against thermally induced chemical changes","summarize: Many applications of molecular layers deposited on metal surfaces, ranging from single-atom catalysis to on-surface magnetochemistry and biosensing, rely on the use of thermal cycles to regenerate the pristine properties of the system. Thus, understanding the microscopic origin behind the thermal stability of organic\/metal interfaces is fundamental for engineering reliable organic-based devices. Here, we study nickel porphyrin molecules on a copper surface as an archetypal system containing a metal center whose oxidation state can be controlled through the interaction with the metal substrate. We demonstrate that the strong molecule-surface interaction, followed by charge transfer at the interface, plays a fundamental role in the thermal stability of the layer by rigidly anchoring the porphyrin to the substrate. Upon thermal treatment, the molecules undergo an irreversible transition at 420 K, which is associated with an increase of the charge transfer from the substrate, mostly localized on the phenyl substituents, and a downward tilting of the latters without any chemical modification",0.0641348399],["we explicitly construct modular forms on a modular basis.","Inverse period mappings of ","summarize: We explicitly construct modular forms on a ",0.0666666667],["global positioning system derived precipitable water vapor is widely being used in atmospheric remote sens","Predicting GPS-based PWV Measurements Using Exponential Smoothing","summarize: Global Positioning System derived precipitable water vapor is extensively being used in atmospheric remote sensing for applications like rainfall prediction. Many applications require PWV values with good resolution and without any missing values. In this paper, we implement an exponential smoothing method to accurately predict the missing PWV values. The method shows good performance in terms of capturing the seasonal variability of PWV values. We report a root mean square error of 0.1~mm for a lead time of 15 minutes, using past data of 30 hours measured at 5-minute intervals.",0.1764705882],["a two-dimensional model is used for solid liquid crystalline plates. the model equation","A plate theory for nematic liquid crystalline solids","summarize: We derive a F\\ppl-von K\\'rm\\'n-type constitutive model for solid liquid crystalline plates where the nematic director may or may not rotate freely relative to the elastic network. To obtain the reduced two-dimensional model, we rely on the deformation decomposition of a nematic solid into an elastic deformation and a natural shape change. The full solution to the resulting equilibrium equations consists of both the deformation displacement and stress fields. The model equations are applicable to a wide range of thin nematic bodies subject to optothermal stimuli and mechanical loads. For illustration, we consider certain reversible natural shape changes in simple systems which are stress free, and their counterparts, where the natural deformations are blocked and internal stresses appear. More general problems can be addressed within the same framework.",0.4210526316],["graph-theoretical approach can detect which equations of a delay differential-al","The Pantelides algorithm for delay differential-algebraic equations","summarize: We present a graph-theoretical approach that can detect which equations of a delay differential-algebraic equation need to be differentiated or shifted to construct a solution of the DDAE. Our approach exploits the observation that differentiation and shifting are very similar from a structural point of view, which allows us to generalize the Pantelides algorithm for differential-algebraic equations to the DDAE setting. The primary tool for the extension is the introduction of equivalence classes in the graph of the DDAE, which also allows us to derive a necessary and sufficient criterion for the termination of the new algorithm.",0.5263157895],["differential privacy has been widely applied in AI. but no study has documented which mechanisms can or","More Than Privacy: Applying Differential Privacy in Key Areas of Artificial Intelligence","summarize: Artificial Intelligence has attracted a great deal of attention in recent years. However, alongside all its advancements, problems have also emerged, such as privacy violations, security issues and model fairness. Differential privacy, as a promising mathematical model, has several attractive properties that can help solve these problems, making it quite a valuable tool. For this reason, differential privacy has been broadly applied in AI but to date, no study has documented which differential privacy mechanisms can or have been leveraged to overcome its issues or the properties that make this possible. In this paper, we show that differential privacy can do more than just privacy preservation. It can also be used to improve security, stabilize learning, build fair models, and impose composition in selected areas of AI. With a focus on regular machine learning, distributed machine learning, deep learning, and multi-agent systems, the purpose of this article is to deliver a new view on many possibilities for improving AI performance with differential privacy techniques.",0.1052631579],["translucency is neither linked to any measurable physical nor perceptual quantity. reproduc","Redefining A in RGBA: Towards a Standard for Graphical 3D Printing","summarize: Advances in multimaterial 3D printing have the potential to reproduce various visual appearance attributes of an object in addition to its shape. Since many existing 3D file formats encode color and translucency by RGBA textures mapped to 3D shapes, RGBA information is particularly important for practical applications. In contrast to color , which is specified by the object's reflectance, selected viewing conditions and a standard observer, translucency is neither linked to any measurable physical nor perceptual quantity. Thus, reproducing translucency encoded by A is open for interpretation. In this paper, we propose a rigorous definition for A suitable for use in graphical 3D printing, which is independent of the 3D printing hardware and software, and which links both optical material properties and perceptual uniformity for human observers. By deriving our definition from the absorption and scattering coefficients of virtual homogeneous reference materials with an isotropic phase function, we achieve two important properties. First, a simple adjustment of A is possible, which preserves the translucency appearance if an object is re-scaled for printing. Second, determining the value of A for a real material, can be achieved by minimizing a distance function between light transport measurements of this material and simulated measurements of the reference materials. Such measurements can be conducted by commercial spectrophotometers used in graphic arts. Finally, we conduct visual experiments employing the method of constant stimuli, and derive from them an embedding of A into a nearly perceptually uniform scale of translucency for the reference materials.",0.0],["astrocytes are a spiking neuronal-astrocytic","Introducing Astrocytes on a Neuromorphic Processor: Synchronization, Local Plasticity and Edge of Chaos","summarize: While there is still a lot to learn about astrocytes and their neuromodulatory role in the spatial and temporal integration of neuronal activity, their introduction to neuromorphic hardware is timely, facilitating their computational exploration in basic science questions as well as their exploitation in real-world applications. Here, we present an astrocytic module that enables the development of a spiking Neuronal-Astrocytic Network into Intel's Loihi neuromorphic chip. The basis of the Loihi module is an end-to-end biophysically plausible compartmental model of an astrocyte that simulates the intracellular activity in response to the synaptic activity in space and time. To demonstrate the functional role of astrocytes in SNAN, we describe how an astrocyte may sense and induce activity-dependent neuronal synchronization, switch on and off spike-time-dependent plasticity to introduce single-shot learning, and monitor the transition between ordered and chaotic activity at the synaptic space. Our module may serve as an extension for neuromorphic hardware, by either replicating or exploring the distinct computational roles that astrocytes have in forming biological intelligence.",0.1121647322],["algorithm predicts the final grade of each student in a class. it issues a","Predicting Grades","summarize: To increase efficacy in traditional classroom courses as well as in Massive Open Online Courses , automated systems supporting the instructor are needed. One important problem is to automatically detect students that are going to do poorly in a course early enough to be able to take remedial actions. Existing grade prediction systems focus on maximizing the accuracy of the prediction while overseeing the importance of issuing timely and personalized predictions. This paper proposes an algorithm that predicts the final grade of each student in a class. It issues a prediction for each student individually, when the expected accuracy of the prediction is sufficient. The algorithm learns online what is the optimal prediction and time to issue a prediction based on past history of students' performance in a course. We derive a confidence estimate for the prediction accuracy and demonstrate the performance of our algorithm on a dataset obtained based on the performance of approximately 700 UCLA undergraduate students who have taken an introductory digital signal processing over the past 7 years. We demonstrate that for 85% of the students we can predict with 76% accuracy whether they are going do well or poorly in the class after the 4th course week. Using data obtained from a pilot course, our methodology suggests that it is effective to perform early in-class assessments such as quizzes, which result in timely performance prediction for each student, thereby enabling timely interventions by the instructor when necessary.",0.1071428571],["Habibullin emph proposed an approach to construct Lax pairs of a","An upper order bound of the invariant manifold in Lax pairs of a nonlinear evolution partial differential equation","summarize: In \\cite, Habibullin \\emph proposed an approach to construct Lax pairs of a nonlinear integrable partial differential equation , where one is the linearized equation of the studied PDE and the other is the invariant manifold of the linearized equation. In this paper, we show that the invariant manifold is the characteristic of a generalized conditional symmetry of the system composed of the studied PDE and its linearized PDE. Then we give an upper order bound of the invariant manifold which provides a theoretical basis for a complete classification of such type of invariant manifold. Moreover, we suggest a modified method to construct Lax pair of the KdV equation which can not be obtained by the original method in \\cite.",0.3424321621],["the spectral graph theory provides an algebraical approach to investigate the characteristics of weighted","The Wigner's Semicircle Law of Weighted Random Networks","summarize: The spectral graph theory provides an algebraical approach to investigate the characteristics of weighted networks using the eigenvalues and eigenvectors of a matrix that represents the structure of the network. However, it is difficult for large-scale and complex networks to represent their structure as a matrix correctly. If there is a universality that the eigenvalues are independent of the detailed structure in large-scale and complex network, we can avoid the difficulty. In this paper, we clarify the Wigner's Semicircle Law for weighted networks as such a universality. The law indicates that the eigenvalues of the normalized Laplacian matrix for weighted networks can be calculated from the a few network statistics when the weighted networks satisfy the sufficient condition of the node degrees and the link weights.",0.1111111111],["fields in which addition requires three summands are shown to be isomorphic","Structure of unital 3-fields","summarize: We investigate fields in which addition requires three summands. These ternary fields are shown to be isomorphic to the set of invertible elements in a local ring ",0.0833333333],["playback detection systems use a playback detector to filter out playback attacks. the","Transforming acoustic characteristics to deceive playback spoofing countermeasures of speaker verification systems","summarize: Automatic speaker verification systems use a playback detector to filter out playback attacks and ensure verification reliability. Since current playback detection models are almost always trained using genuine and played-back speech, it may be possible to degrade their performance by transforming the acoustic characteristics of the played-back speech close to that of the genuine speech. One way to do this is to enhance speech stolen from the target speaker before playback. We tested the effectiveness of a playback attack using this method by using the speech enhancement generative adversarial network to transform acoustic characteristics. Experimental results showed that use of this enhanced stolen speech method significantly increases the equal error rates for the baseline used in the ASVspoof 2017 challenge and for a light convolutional neural network-based method. The results also showed that its use degrades the performance of a Gaussian mixture model-universal background model-based ASV system. This type of attack is thus an urgent problem needing to be solved.",0.5],["a network of two nodes separated by a noisy channel with two-sided state information","Strong coordination of signals and actions over noisy channels with two-sided state information","summarize: We consider a network of two nodes separated by a noisy channel with two-sided state information, in which the input and output signals have to be coordinated with the source and its reconstruction. In the case of non-causal encoding and decoding, we propose a joint source-channel coding scheme and develop inner and outer bounds for the strong coordination region. While the inner and outer bounds do not match in general, we provide a complete characterization of the strong coordination region in three particular cases: i) when the channel is perfect; ii) when the decoder is lossless; and iii) when the random variables of the channel are independent from the random variables of the source. Through the study of these special cases, we prove that the separation principle does not hold for joint source-channel strong coordination. Finally, in the absence of state information, we show that polar codes achieve the best known inner bound for the strong coordination region, which therefore offers a constructive alternative to random binning and coding proofs.",0.7857142857],["variance-reduced incremental methods are based on SAGA and SVRG.","Proximal Splitting Meets Variance Reduction","summarize: Despite the rise to fame of incremental variance-reduced methods in recent years, their use in nonsmooth optimization is still limited to few simple cases. This is due to the fact that existing methods require to evaluate the proximity operator for the nonsmooth terms, which can be a costly operation for complex penalties. In this work we introduce two variance-reduced incremental methods based on SAGA and SVRG that can efficiently take into account complex penalties which can be expressed as a sum of proximal terms. This includes penalties such as total variation, group lasso with overlap and trend filtering, to name a few. Furthermore, we also develop sparse variants of the proposed algorithms which can take advantage of sparsity in the input data. Like other incremental methods, it only requires to evaluate the gradient of a single sample per iteration, and so is ideally suited for large scale applications. We provide a convergence rate analysis for the proposed methods and show that they converge with a fixed step-size, achieving in some cases the same asymptotic rate as their full gradient variants. Empirical benchmarks on 3 different datasets illustrate the practical advantages of the proposed methods.",0.1111111111],["complexity of many of these systems poses accountability challenges. data provenance methods show much promise as","Decision Provenance: Harnessing data flow for accountable systems","summarize: Demand is growing for more accountability regarding the technological systems that increasingly occupy our world. However, the complexity of many of these systems - often systems-of-systems - poses accountability challenges. A key reason for this is because the details and nature of the information flows that interconnect and drive systems, which often occur across technical and organisational boundaries, tend to be invisible or opaque. This paper argues that data provenance methods show much promise as a technical means for increasing the transparency of these interconnected systems. Specifically, given the concerns regarding ever-increasing levels of automated and algorithmic decision-making, and so-called 'algorithmic systems' in general, we propose decision provenance as a concept showing much promise. Decision provenance entails using provenance methods to provide information exposing decision pipelines: chains of inputs to, the nature of, and the flow-on effects from the decisions and actions taken throughout systems. This paper introduces the concept of decision provenance, and takes an interdisciplinary exploration into its potential for assisting accountability in algorithmic systems. We argue that decision provenance can help facilitate oversight, audit, compliance, risk mitigation, and user empowerment, and we also indicate the implementation considerations and areas for research necessary for realising its vision. More generally, we make the case that considerations of data flow, and systems more broadly, are important to discussions of accountability, and complement the considerable attention already given to algorithmic specifics.",0.1111111111],["the infrared nebula L1527 traces outflows emanat","Spitzer IRAC Colors of Nebulae Associated with Star-Forming Regions","summarize: Star-forming regions are often associated with nebulosity. In this study, we investigated infrared diffuse emission in Spitzer IRAC images. The infrared nebula L1527 traces outflows emanating from a low-mass protostar. The nebular color is consistent with the color of a stellar photosphere with large extinction. Nebulae around the HII region W5-East are bright in the infrared. These colors are consistent with the model color of dust containing polycyclic aromatic hydrocarbon . The strength of ultraviolet irradiation of the nebulae and the small dust fraction were deduced from the infrared colors of the nebulae. We found that the edges of the nebulae are irradiated by strong ultraviolet radiation and have abundant small dust. Dust at the surface of the molecular cloud is thought to be destroyed by ultraviolet radiation from an early-type star.",0.0],["artificial neural networks are among the most used artificial intelligence methods. we propose sparse evolutionary","Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science","summarize: Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks , we argue that artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.",0.0],["the web 2.0 paradigm has changed the way businesses are run all around the world. e","Evaluation of Business-Oriented Performance Metrics in e-Commerce using Web-based Simulation","summarize: The Web 2.0 paradigm has radically changed the way businesses are run all around the world. Moreover, e-Commerce has overcome in daily shopping activities. For management teams, the assessment, evaluation, and forecasting of online incomes and other business-oriented performance measures have become 'a holy grail', the ultimate question imposing their current and future e-Commerce projects. Within the paper, we describe the development of a Web-based simulation model, suitable for their estimation, taking into account multiple operation profiles and scenarios. Specifically, we put focus on introducing specific classes of e-Customers, as well as the workload characterization of an arbitrary e-Commerce website. On the other hand, we employ and embed the principles of the system thinking approach and the system dynamics into the proposed solution. As a result, a complete simulation model has been developed, available online. The model, which includes numerous adjustable input variables, can be successfully utilized in making 'what-if'-like insights into a plethora of business-oriented performance metrics for an arbitrary e-Commerce website. This project is, also, a great example of the power delivered by InsightMaker, free-of-charge Web-based software, suitable for a collaborative online development of models following the systems thinking paradigm.",0.3],["we consider two different variational models of transport networks. the so-called branched","Optimal micropatterns in 2D transport networks and their relation to image inpainting","summarize: We consider two different variational models of transport networks, the so-called branched transport problem and the urban planning problem. Based on a novel relation to Mumford-Shah image inpainting and techniques developed in that field, we show for a two-dimensional situation that both highly non-convex network optimization tasks can be transformed into a convex variational problem, which may be very useful from analytical and numerical perspectives. As applications of the convex formulation, we use it to perform numerical simulations , and we prove the lower bound of an energy scaling law which helps better understand optimal networks and their minimal energies.",0.3076923077],["in 1964, he began working in the Physics Department of the Advanced Studies Center of the National","Brief Notes and History Computing in Mexico during 50 years","summarize: The history of computing in Mexico can not be thought without the name of Prof. Harold V. McIntosh . For almost 50 years, in Mexico he contributed to the development of computer science with wide international recognition. Approximately in 1964, McIntosh began working in the Physics Department of the Advanced Studies Center of the National Polytechnic Institute , now called CINVESTAV. In 1965, at the National Center of Calculus , he was a founding member of the Master in Computing, first in Latin America. With the support of Mario Baez Camargo and Enrique Melrose, McIntosh continues his research of Martin-Baltimore Computer Center and University of Florida at IBM 709.",0.1875],["artificial intelligence has been applied to control players' decisions in board games for over half a century","Collaborative Agent Gameplay in the Pandemic Board Game","summarize: While artificial intelligence has been applied to control players' decisions in board games for over half a century, little attention is given to games with no player competition. Pandemic is an exemplar collaborative board game where all players coordinate to overcome challenges posed by events occurring during the game's progression. This paper proposes an artificial agent which controls all players' actions and balances chances of winning versus risk of losing in this highly stochastic environment. The agent applies a Rolling Horizon Evolutionary Algorithm on an abstraction of the game-state that lowers the branching factor and simulates the game's stochasticity. Results show that the proposed algorithm can find winning strategies more consistently in different games of varying difficulty. The impact of a number of state evaluation metrics is explored, balancing between optimistic strategies that favor winning and pessimistic strategies that guard against losing.",0.3076923077],["two sets of sets of sets of sets of sets of sets of sets of sets of sets of","A positive fraction mutually avoiding sets theorem","summarize: Two sets ",0.0552147239],["Gavruta introduces the new gavruta.","Weaving K-frames in Hilbert Spaces","summarize: Gavruta introduced ",0.0],["dynamic statistical model learns and adapts processing and parsing rules. we use this","Flexible Log File Parsing using Hidden Markov Models","summarize: We aim to model unknown file processing. As the content of log files often evolves over time, we established a dynamic statistical model which learns and adapts processing and parsing rules. First, we limit the amount of unstructured text by focusing only on those frequent patterns which lead to the desired output table similar to Vaarandi . Second, we transform the found frequent patterns and the output stating the parsed table into a Hidden Markov Model . We use this HMM as a specific, however, flexible representation of a pattern for log file processing. With changes in the raw log file distorting learned patterns, we aim the model to adapt automatically in order to maintain high quality output. After training our model on one system type, applying the model and the resulting parsing rule to a different system with slightly different log file patterns, we achieve an accuracy over 99%.",0.0],["the modular system for shelves and coasts. the system is tailored to the coup","Modular System for Shelves and Coasts - a flexible and multi-component framework for coupled coastal ocean ecosystem modelling","summarize: Shelf and coastal sea processes extend from the atmosphere through the water column and into the sea bed. These processes are driven by physical, chemical, and biological interactions at local scales, and they are influenced by transport and cross strong spatial gradients. The linkages between domains and many different processes are not adequately described in current model systems. Their limited integration level in part reflects lacking modularity and flexibility; this shortcoming hinders the exchange of data and model components and has historically imposed supremacy of specific physical driver models. We here present the Modular System for Shelves and Coasts , a novel domain and process coupling system tailored---but not limited--- to the coupling challenges of and applications in the coastal ocean. MOSSCO builds on the existing coupling technology Earth System Modeling Framework and on the Framework for Aquatic Biogeochemical Models, thereby creating a unique level of modularity in both domain and process coupling; the new framework adds rich metadata, flexible scheduling, configurations that allow several tens of models to be coupled, and tested setups for coastal coupled applications. That way, MOSSCO addresses the technology needs of a growing marine coastal Earth System community that encompasses very different disciplines, numerical tools, and research questions.",0.1707902939],["nonylphenol ethoxylate is a non ionic surfactant","Degradation of Nonylphenol Ethoxylate-10 by Mediated Electrochemical Oxidation Technology","summarize: Nonylphenol ethoxylate is a non ionic surfactant which is synthesized from alkylphenol ethoxylate. The accumulation of NPE-10 in wastewater will endanger the ecosystem as well as human being. At present, by an advancement of technology NPE 10 can be degraded indirectly by using an electrochemically treatment. Thus, this study aimed to evaluate the potential electrodegradation of NPE 10 by mediated electrochemical oxidation using Ce ionic mediator. In addition, the influence of Ag ionic catalyst in the performance of MEO for degradation of NPE 10 was also observed. The potency of MEO technology in degradation NPE 10 was evaluated by voltammetry technique and confirmed by titrimetry and LC-MS analyses. The results showed that in the absence of Ag ionic catalyst, the degradation of NPE 10 by MEO was 85.93 %. Furthermore, when the Ag ionic catalyst was applied, the performance of MEO in degradation of NPE 10 was improved to 95.12 %. The back titration using Ba2 confirmed the formation of CO2 by 46.79 %. Whereas the redox titration shows the total of degradation organic compounds by 42.50 %, which was emphasized by formation of two new peaks in LC-MS chromatogram. In summary, our results confirm the potential of MEO technology for NPE-10 degradation.",0.2732644702],["good atlases are defined for effective orbifolds. a spark complex","Spark complexes on good effective orbifold atlases categorically","summarize: Good atlases are defined for effective orbifolds, and a spark complex is constructed on each good atlas. It is proved that this process is 2-functorial with compatible systems playing as morphisms between good atlases, and that the spark character 2-functor factors through this 2-functor.",0.6428571429],["a sequence of the general form is a sequence of the general form.","On the monoid generated by a Lucas sequence","summarize: A Lucas sequence is a sequence of the general form ",0.3448275862],["this paper introduces a novel neural network architecture designed to enforce asymmetry and transitive","Hypernym Detection Using Strict Partial Order Networks","summarize: This paper introduces Strict Partial Order Networks , a novel neural network architecture designed to enforce asymmetry and transitive properties as soft constraints. We apply it to induce hypernymy relations by training with is-a pairs. We also present an augmented variant of SPON that can generalize type information learned for in-vocabulary terms to previously unseen ones. An extensive evaluation over eleven benchmarks across different tasks shows that SPON consistently either outperforms or attains the state of the art on all but one of these benchmarks.",0.1],["pseudoachromatic index of finite affine space. pseudoachromatic index","On chromatic indices of finite affine spaces","summarize: The pseudoachromatic index of the finite affine space ",0.3333333333],["the balance function measures the correlation between opposite sign charge pairs. the study of the balance function","Reaction Plane and Beam Energy Dependence Of The Balance Function at RHIC","summarize: The balance function, which measures the correlation between opposite sign charge pairs, is sensitive to the mechanisms of charge formation and the subsequent relative diffusion of the balancing charges. The study of the balance function can provide information about charge creation time as well as the subsequent collective behavior of particles. In this paper, we present a reaction-plane-dependent balance function study for Au+Au collisions at ",0.0],["a dual-criteria time-stepping method is proposed to improve computational efficiency of particle hydro","Dual-criteria time stepping for weakly compressible smoothed particle hydrodynamics","summarize: Implementing particle-interaction configuration and time integration are performance intensive essentials of particle-based methods. In this paper, a dual-criteria time-stepping method is proposed to improve the computational efficiency of the weakly-compressible smoothed particle hydrodynamic method for modeling incompressible flows. The key idea is to introduce an advection time criterion, which is based on fluid velocity field, for recreating the particle-interaction configuration. Within this time criterion, several steps of pressure relaxation determined by the acoustic time criterion, based on the artificial speed of sound, can be carried out without updating the particle interaction configuration and much larger time-step sizes compared with the conventional counterpart. The method has shown optimized computational performance through CPU cost analysis. Good accuracy and performance is obtained for the presented benchmarks implying promising potential of the proposed method for incompressible flow and fluid-structure interaction simulations.",0.3888888889],["the physical domain of the system can be a bounded region of the system.","Local thermal equilibrium for certain stochastic models of heat transport","summarize: This paper is about nonequilibrium steady states of a class of stochastic models in which particles exchange energy with their local environments rather than directly with one another. The physical domain of the system can be a bounded region of ",0.3333333333],["NL3, G3 and IU-FSU parameters are used to study thermal effects on","Warm dense matter and cooling of supernovae remnants","summarize: We study the thermal effects on the nuclear matter properties such as binding energy, incompressibility, free symmetry energy and its coefficients using NL3, G3 and IU-FSU parameter sets of relativistic mean-field models. These models being consistent with the properties of cold NM, have also been used to study the effect of temperature by incorporating the Fermi function. The critical temperature for the liquid-gas phase transition in the symmetric NM is found to be 14.60, 15.37 and 14.50 MeV for NL3, G3 and IU-FSU parameter sets respectively, which is in excellent agreement with previous theoretical and experimental studies. We inspect that the properties related to second differential coefficient of the binding energy and free symmetry energy at saturation density and Q sym,0) exhibit the contrary effects for NL3 and G3 parameters as the temperature increases. We find that the prediction of saturated curvature parameter for G3 equation of state at finite temperature favour the combined analysis of K sym,0 for the existence of massive pulsars, gravitational waves from GW170817 and NICER observations of PSR J0030+0451. Further, we investigate the cooling mechanism of newly born stars through neutrino emissivity controlled by direct Urca process and instate some interesting remarks about neutrino emissivity. We also deliberate the effect of temperature on the M-R profile of Proto-Neutron star.",0.0833333333],["filtering for hidden Markov models is linked to the notion of duality. the filter","Optimal filtering and the dual process","summarize: We link optimal filtering for hidden Markov models to the notion of duality for Markov processes. We show that when the signal is dual to a process that has two components, one deterministic and one a pure death process, and with respect to functions that define changes of measure conjugate to the emission density, the filtering distributions evolve in the family of finite mixtures of such measures and the filter can be computed at a cost that is polynomial in the number of observations. Special cases of our framework include the Kalman filter, and computable filters for the Cox-Ingersoll-Ross process and the one-dimensional Wright-Fisher process, which have been investigated before. The dual we obtain for the Cox-Ingersoll-Ross process appears to be new in the literature.",0.2352941176],["we investigated the typical environment and physical properties of red discs and blue bulges.","NoSOCS in SDSS. V. Red Disc and Blue Bulge Galaxies Across Different Environments","summarize: We investigated the typical environment and physical properties of red discs and blue bulges, comparing those to the normal objects in the blue cloud and red sequence. Our sample is composed of cluster members and field galaxies at ",0.125],["the present work is aimed at finding suitable exchange conditions for saturated fluid flow in a por","A Discussion on the Transmission Conditions for Saturated Fluid Flow Through Porous Media With Fractal Microstructure","summarize: The present work is aimed to find suitable exchange conditions for saturated fluid flow in a porous medium, when a fractal microstructure is embedded in the porous matrix. Two different deterministic models are introduced and rigorously analyzed. Also, numerical experiments for each of them are presented to verify the theoretically predicted behavior of the phenomenon and some probabilistic versions are explored numerically, to gain further insight on the phenomenon.",0.4074074074],["Given","Exponential sum approximations for ","summarize: Given ",0.0],["van der Waals materials have shown thickness-dependent characteristics. they have been explored by numerous","Hard magnetic properties in nanoflake van der Waals Fe3GeTe2","summarize: Two dimensional van der Waals materials have demonstrated fascinating optical, electrical and thickness-dependent characteristics. These have been explored by numerous authors but reports on magnetic properties and spintronic applications of 2D vdW materials are scarce by comparison. By performing anomalous Hall effect transport measurements, we have characterised the thickness dependent magnetic properties of single crystalline vdW Fe3GeTe2. The nanoflakes of this vdW metallic material exhibit a single hard magnetic phase with a near square-shaped magnetic loop, large coercivity , a Curie temperature near 200 K and strong perpendicular magnetic anisotropy. Using criticality analysis, we confirmed the existence of magnetic coupling between vdW atomic layers and obtained an estimated coupling length of ~ 5 vdW layers in Fe3GeTe2. Furthermore, the hard magnetic behaviour of Fe3GeTe2 can be well described by a proposed model. The magnetic properties of Fe3GeTe2 highlight its potential for integration into vdW magnetic heterostructures, paving the way for spintronic research and applications based on these devices.",0.1875],["we document the evolution of Gab since its inception until a user carried out the most deadly","From Welcome New Gabbers to the Pittsburgh Synagogue Shooting: The Evolution of Gab","summarize: Gab, an online social media platform with very little content moderation, has recently come to prominence as an alt-right community and a haven for hate speech. We document the evolution of Gab since its inception until a Gab user carried out the most deadly attack on the Jewish community in US history. We investigate Gab language use, study how topics evolved over time, and find that the shooters' posts were among the most consistently anti-Semitic on Gab, but that hundreds of other users were even more extreme.",0.3181818182],["anti-SAT scheme is vulnerable to the later AppSAT and removal attacks. a variety","Generalized SAT-Attack-Resistant Logic Locking","summarize: Logic locking is used to protect integrated circuits from piracy and counterfeiting. An encrypted IC implements the correct function only when the right key is input. Many existing logic-locking methods are subject to the powerful satisfiability -based attack. Recently, an Anti-SAT scheme has been developed. By adopting two complementary logic blocks that consist of AND\/NAND trees, it makes the number of iterations needed by the SAT attack exponential to the number of input bits. Nevertheless, the Anti-SAT scheme is vulnerable to the later AppSAT and removal attacks. This paper proposes a generalized Anti-SAT scheme. Different from the Anti-SAT scheme, a variety of complementary or non-complementary functions can be adopted for the two blocks in our G-Anti-SAT scheme. The Anti-SAT scheme is just a special case of our proposed design. Our design can achieve higher output corruptibility, which is also tunable, so that better resistance to the AppSAT and removal attacks is achieved. Meanwhile, unlike existing AppSAT-resilient designs, our design does not sacrifice the resistance to the SAT attack.",0.1904761905],["the Hamiltonian Cycle polynomial is not a monotone subexponential","Monotone Projection Lower Bounds from Extended Formulation Lower Bounds","summarize: In this short note, we reduce lower bounds on monotone projections of polynomials to lower bounds on extended formulations of polytopes. Applying our reduction to the seminal extended formulation lower bounds of Fiorini, Massar, Pokutta, Tiwari, & de Wolf and Rothvoss , we obtain the following interesting consequences. 1. The Hamiltonian Cycle polynomial is not a monotone subexponential-size projection of the permanent; this both rules out a natural attempt at a monotone lower bound on the Boolean permanent, and shows that the permanent is not complete for non-negative polynomials in VNP",0.1428571429],["gravity field maps of the satellite gravimetry missions GRACE and GRACE follow-On are","Revisiting the Light Time Correction in Gravimetric Missions Like GRACE and GRACE Follow-On","summarize: The gravity field maps of the satellite gravimetry missions GRACE and GRACE Follow-On are derived by means of precise orbit determination. The key observation is the biased inter-satellite range, which is measured primarily by a K-Band Ranging system in GRACE and GRACE Follow-On. The GRACE Follow-On satellites are additionally equipped with a Laser Ranging Interferometer , which provides measurements with lower noise compared to the KBR. The biased range of KBR and LRI needs to be converted for gravity field recovery into an instantaneous range, i.e. the biased Euclidean distance between the satellites' center-of-mass at the same time. One contributor to the difference between measured and instantaneous range arises due to the non-zero travel time of electro-magnetic waves between the spacecraft. We revisit the calculation of the light time correction from first principles considering general relativistic effects and state-of-the-art models of Earth's potential field. The novel analytical expressions for the LTC of KBR and LRI can circumvent numerical limitations of the classical approach. The dependency of the LTC on geopotential models and on the parameterization is studied, and afterwards the results are compared against the LTC provided in the official datasets of GRACE and GRACE Follow-On. It is shown that the new approach has a significantly lower noise, well below the instrument noise of current instruments, especially relevant for the LRI, and even if used with kinematic orbit products. This allows calculating the LTC accurate enough even for the next generation of gravimetric missions.",0.4],["we introduce the concepts of free, projective, injective and flat Rota-Ba","Rota-Baxter modules toward derived functors","summarize: In this paper we study Rota-Baxter modules with emphasis on the role played by the Rota-Baxter operators and resulting difference between Rota-Baxter modules and the usual modules over an algebra. We introduce the concepts of free, projective, injective and flat Rota-Baxter modules. We give the construction of free modules and show that there are enough projective, injective and flat Rota-Baxter modules to provide the corresponding resolutions for derived functor.",0.0909090909],["the majority of text is stored in UTF-8, which must be validated on ingestion","Validating UTF-8 In Less Than One Instruction Per Byte","summarize: The majority of text is stored in UTF-8, which must be validated on ingestion. We present the lookup algorithm, which outperforms UTF-8 validation routines used in many libraries and languages by more than 10 times using commonly available SIMD instructions. To ensure reproducibility, our work is freely available as open source software.",0.125],["our aim is to establish new integral representations for the Fox--Wright function.","New integral representations for the Fox-Wright functions and its applications II","summarize: In this paper our aim is to establish new integral representations for the Fox--Wright function ",0.3846153846],["the much-used trace distance of coherence was shown to be not a proper measure","The modified trace distance of coherence is constant on most pure states","summarize: Recently, the much-used trace distance of coherence was shown to not be a proper measure of coherence, so a modification of it was proposed. We derive an explicit formula for this modified trace distance of coherence on pure states. Our formula shows that, despite satisfying the axioms of proper coherence measures, it is likely not a good measure to use, since it is maximal on all except for an exponentially-small fraction of pure states.",0.4444444444],["X-ray source, Suzaku J1305, was found in a trans","Discovery of a transient X-ray source Suzaku J1305-4930 in NGC 4945","summarize: We report the serendipitous discovery of a transient X-ray source, Suzaku J1305",0.4927614787],["new Python software for the parametric design of stabilizing feedback laws with time delays. the","Partial Pole Placement via Delay Action: A Python Software for Delayed Feedback Stabilizing Design","summarize: This paper presents a new Python software for the parametric design of stabilizing feedback laws with time delays, called Partial Pole Placement via Delay Action . After an introduction recalling recent theoretical results on the multiplicity-induced-dominancy and coexisting real roots-induced-dominancy properties and their use for the feedback stabilization of control systems operating under time delays, the paper presents the current version of P3",0.1666666667],["electrocatalysts can catalyze ORR and OER.","Doping of Self-Standing CNT Fibers: Promising Flexible Air-Cathodes for High Energy Density Structural Zn-air batteries","summarize: Finding proper electrocatalysts capable of efficient catalyzing both ORR and OER is of great importance for metal-air batteries. With increasing inclination towards structural and flexible devices, developing a high-performance self-standing air-cathode is highly demanded and challenging, as most of oxygen catalysts are powder and need to be further processed. Here, we construct highly bifunctional air catalyst from macroscopic CNT fibers through direct CVD spinning followed by hydrothermal method. The electrocatalytic properties of the samples were tuned by altering nitrogen-doping and defect densities readily adjusted at different hydrothermal reaction temperatures. The treated CNTfs showed excellent bifunctional activity and demonstrated exceptional performance as carbon-based self-standing air-cathodes in liquid and solid-state rechargeable Zn-air batteries, with high capacity of 698 mAh g-1 and ultrahigh energy density of 838 Wh kg-1. The rechargeable Zn-air batteries exhibit a low discharge-charge overpotential and excellent stability. This work provides novel simply achieved self-standing air electrodes with exceptional performance for structural Zn-air batteries.",0.03718836],["local conformally symplectic structures are classified on four-dimensional Lie","Structure of locally conformally symplectic Lie algebras and solvmanifolds","summarize: We obtain structure results for locally conformally symplectic Lie algebras. We classify locally conformally symplectic structures on four-dimensional Lie algebras and construct locally conformally symplectic structures on compact quotients of all four-dimensional connected and simply connected solvable Lie groups.",0.4545454545],["Paul ion trap design with integrated optical fibre cavity. the trap is designed to be","Precise positioning of an ion in an integrated Paul trap-cavity system using radiofrequency signals","summarize: We report a novel miniature Paul ion trap design with an integrated optical fibre cavity which can serve as a building block for a fibre-linked quantum network. In such cavity quantum electrodynamic set-ups, the optimal coupling of the ions to the cavity mode is of vital importance and this is achieved by moving the ion relative to the cavity mode. The trap presented herein features an endcap-style design complemented with extra electrodes on which additional radiofrequency voltages are applied to fully control the pseudopotential minimum in three dimensions. This method lifts the need to use three-dimensional translation stages for moving the fibre cavity with respect to the ion and achieves high integrability, mechanical rigidity and scalability. Not based on modifying the capacitive load of the trap, this method leads to precise control of the pseudopotential minimum allowing the ion to be moved with precisions limited only by the ion's position spread. We demonstrate this by coupling the ion to the fibre cavity and probing the cavity mode profile.",0.3888888889],["we calculate the shape and velocity of a bubble rising in an infinitely large and closed He","Theoretical analysis for flattening of a rising bubble in a Hele-Shaw cell","summarize: We calculate the shape and the velocity of a bubble rising in an infinitely large and closed Hele-Shaw cell using Park and Homsy's boundary condition which accounts for the change of the three dimensional structure in the perimeter zone. We first formulate the problem in the form of a variational problem, and discuss the shape change assuming that the bubble takes elliptic shape. We calculate the shape and the velocity of the bubble as a function of the bubble size, gap distance and the inclination angle of the cell. We show that the bubble is flattened as it rises. This result is in agreement with experiments for large Hele-Shaw cells.",0.4838709677],["threshold circuits are a super-linear gate lower bounds and a super-","Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-Two and Depth-Three Threshold Circuits","summarize: In order to formally understand the power of neural computing, we first need to crack the frontier of threshold circuits with two and three layers, a regime that has been surprisingly intractable to analyze. We prove the first super-linear gate lower bounds and the first super-quadratic wire lower bounds for depth-two linear threshold circuits with arbitrary weights, and depth-three majority circuits computing an explicit function. ",0.4926721789],["tin-selenide and tin-sulfide classes undergo","Electronic, vibrational, and electron-phonon coupling properties in SnSe","summarize: The tin-selenide and tin-sulfide classes of materials undergo multiple structural transitions under high pressure leading to periodic lattice distortions, superconductivity, and topologically non-trivial phases, yet a number of controversies exist regarding the structural transformations in these systems. We perform first-principles calculations within the framework of density functional theory and a careful comparison of our results with available experiments on SnSe",0.1097623272],["the method is based on the Laplace-Fourier transform technique. it allows","Alternative Formulation of the Induced Surface and Curvature Tensions Approach","summarize: We develop a novel method to analyze the excluded volume of the multicomponent mixtures of classical hard spheres in the grand canonical ensemble. The method is based on the Laplace-Fourier transform technique and allows one to account for the fluctuations of the particle number density for the induced surface and curvature tensions equation of state. As a result one can go beyond the Van der Waals approximation by obtaining the suppression of the induced surface and curvature tensions coefficients at moderate and high packing fractions. In contrast to the standard induced surface and curvature tensions equation of state the suppression of these coefficients is not an exponential one, but a power-like one. The obtained alternative equation of state is further generalized to account for higher virial coefficients. This result is straightforwardly generalized to the case of quantum statistics.",0.2307692308],["convection caused by Joule heating of electrolyte during charging or dischar","Thermal convection in a liquid metal battery","summarize: Generation of thermal convection flow in the liquid metal battery, a device recently proposed as a promising solution for the problem of the short-term energy storage, is analyzed using a numerical model. It is found that convection caused by Joule heating of electrolyte during charging or discharging is virtually unavoidable. It exists in laboratory prototypes larger than a few cm in size and should become much stronger in larger-scale batteries. The phenomenon needs further investigation in view of its positive and negative effects.",0.0909090909],["more intricate privacy goals emerged and more detailed bounds on the minimum overhead necessary to achieve them were","SoK on Performance Bounds in Anonymous Communication","summarize: Communicating anonymously comes at a cost - and large communities have been in a constant tug-of-war between the development of faster protocols, and the improvement of security analyses. Thereby more intricate privacy goals emerged and more detailed bounds on the minimum overhead necessary to achieve them were proven. The entanglement of requirements, scenarios, and protocols complicates analysis, and the published results are hardly comparable, due to deviating, yet specific choices of assumptions and goals . In this paper, we systematize the field by harmonizing the models, comparing the proven performance bounds, and contextualizing these theoretical results in a broad set of proposed and implemented systems. By identifying inaccuracies, we demonstrate that the attacks, on which the results are based, indeed break much weaker privacy goals than postulated, and tighten the bounds along the way. We further show the equivalence of two seemingly alternative bounds. Finally, we argue how several assumptions and requirements of the papers likely are of limited applicability in reality and suggest relaxations for future work.",0.0476190476],["proposed schemes are based on the L1 discretization for the time fractional derivative.","Highly efficient schemes for time fractional Allen-Cahn equation using extended SAV approach","summarize: In this paper, we propose and analyze high order efficient schemes for the time fractional Allen-Cahn equation. The proposed schemes are based on the L1 discretization for the time fractional derivative and the extended scalar auxiliary variable approach developed very recently to deal with the nonlinear terms in the equation. The main contributions of the paper consist in: 1) constructing first and higher order unconditionally stable schemes for different mesh types, and proving the unconditional stability of the constructed schemes for the uniform mesh; 2) carrying out numerical experiments to verify the efficiency of the schemes and to investigate the coarsening dynamics governed by the time fractional Allen-Cahn equation. Particularly, the influence of the fractional order on the coarsening behavior is carefully examined. Our numerical evidence shows that the proposed schemes are more robust than the existing methods, and their efficiency is less restricted to particular forms of the nonlinear potentials.",0.3529411765],["a special class of standard gaussian Autoregressive Hilbertian processes of","Classical and bayesian componentwise predictors for non-compact correlated ARH processes","summarize: A special class of standard Gaussian Autoregressive Hilbertian processes of order one processes), with bounded linear autocorrelation operator, which does not satisfy the usual Hilbert-Schmidt assumption, is considered. To compensate the slow decay of the diagonal coefficients of the autocorrelation operator, a faster decay velocity of the eigenvalues of the trace autocovariance operator of the innovation process is assumed. As usual, the eigenvectors of the autocovariance operator of the ARH process are considered for projection, since, here, they are assumed to be known. Diagonal componentwise classical and bayesian estimation of the autocorrelation operator is studied for prediction. The asymptotic efficiency and equivalence of both estimators is proved, as well as of their associated componentwise ARH plugin predictors. A simulation study is undertaken to illustrate the theoretical results derived.",0.4210526316],["quantum data hiding is known as quantum data hiding. it is assumed that noise does not affect","Quantum data hiding in the presence of noise","summarize: When classical or quantum information is broadcast to separate receivers, there exist codes that encrypt the encoded data such that the receivers cannot recover it when performing local operations and classical communication, but they can decode reliably if they bring their systems together and perform a collective measurement. This phenomenon is known as quantum data hiding and hitherto has been studied under the assumption that noise does not affect the encoded systems. With the aim of applying the quantum data hiding effect in practical scenarios, here we define the data-hiding capacity for hiding classical information using a quantum channel. Using this notion, we establish a regularized upper bound on the data hiding capacity of any quantum broadcast channel, and we prove that coherent-state encodings have a strong limitation on their data hiding rates. We then prove a lower bound on the data hiding capacity of channels that map the maximally mixed state to the maximally mixed state and argue how to extend this bound to generic channels and to more than two receivers.",0.25],["anisotropy in the dark energy pressure is found to evolve with cosmic expansion at least","Bianchi-V String Cosmological model with Dark Energy Anisotropy","summarize: The role of anisotropic components on the dark energy and the dynamics of the universe is investigated. An anisotropic dark energy fluid with different pressures along different spatial directions is assumed to incorporate the effect of anisotropy. One dimensional cosmic strings aligned along x-direction supplement some kind of anisotropy. Anisotropy in the dark energy pressure is found to evolve with cosmic expansion at least at late times. At an early phase, the anisotropic effect due to the cosmic strings substantially affect the dynamics of the accelerating universe.",0.1875],["a discrete LCT is based on the well-known CM-CC-","Fast Discrete Linear Canonical Transform Based on CM-CC-CM Decomposition and FFT","summarize: In this paper, a discrete LCT irrelevant to the sampling periods and without oversampling operation is developed. This DLCT is based on the well-known CM-CC-CM decomposition, that is, implemented by two discrete chirp multiplications and one discrete chirp convolution . This decomposition doesn't use any scaling operation which will change the sampling period or cause the interpolation error. Compared with previous works, DLCT calculated by direct summation and DLCT based on center discrete dilated Hermite functions , the proposed method implemented by FFTs has much lower computational complexity. The relation between the proposed DLCT and the continuous LCT is also derived to approximate the samples of the continuous LCT. Simulation results show that the proposed method somewhat outperforms the CDDHFs-based method in the approximation accuracy. Besides, the proposed method has approximate additivity property with error as small as the CDDHFs-based method. Most importantly, the proposed method has perfect reversibility, which doesn't hold in many existing DLCTs. With this property, it is unnecessary to develop the inverse DLCT additionally because it can be replaced by the forward DLCT.",0.3639715468],["the dark asteroid Bennu studied by NASAtextquoteright s OS","Modeling optical roughness and first-order scattering processes from OSIRIS-REx color images of the rough surface of asteroid Bennu","summarize: The dark asteroid Bennu studied by NASA\\textquoteright s OSIRIS-REx mission has a boulder-rich and apparently dust-poor surface, providing a natural laboratory to investigate the role of single-scattering processes in rough particulate media. Our goal is to define optical roughness and other scattering parameters that may be useful for the laboratory preparation of sample analogs, interpretation of imaging data, and analysis of the sample that will be returned to Earth. We rely on a semi-numerical statistical model aided by digital terrain model shadow ray-tracing to obtain scattering parameters at the smallest surface element allowed by the DTM . Using a Markov Chain Monte Carlo technique, we solved the inversion problem on all four-band images of the OSIRIS-REx mission\\textquoteright s top four candidate sample sites, for which high-precision laser altimetry DTMs are available. We reconstructed the \\emph probability distribution for each parameter and distinguished primary and secondary solutions. Through the photometric image correction, we found that a mixing of low and average roughness slope best describes Bennu's surface for up to ",0.2341050989],["quantitative droplets emerge from the competitive interplay of two Hamiltonian terms. the mean","Modulational instability, inter-component asymmetry and formation of quantum droplets in one-dimensional binary Bose gases","summarize: Quantum droplets are ultradilute liquid states which emerge from the competitive interplay of two Hamiltonian terms, the mean-field energy and beyond-mean-field correction, in a weakly interacting binary Bose gas. We relate the formation of droplets in symmetric and asymmetric two-component one-dimensional boson systems to the modulational instability of a spatially uniform state driven by the beyond-mean-field term. Asymmetry between the components may be caused by their unequal populations or unequal intra-component interaction strengths. Stability of both symmetric and asymmetric droplets is investigated. Robustness of the symmetric solutions against symmetry-breaking perturbations is confirmed.",0.1234614772],["independent vector analysis has emerged as an extension of independent component analysis into multiple sets of mixtures","The Extended Sequentially Drilled Joint Congruence Transformation and its Application in Gaussian Independent Vector Analysis","summarize: Independent Vector Analysis has emerged in recent years as an extension of Independent Component Analysis into multiple sets of mixtures, where the source signals in each set are independent, but may depend on source signals in the other sets. In a semi-blind IVA framework, information regarding the probability distributions of the sources may be available, giving rise to Maximum Likelihood separation. In recent work we have shown that under the multivariate Gaussian model, with arbitrary temporal covariance matrices of the source signals, ML separation requires the solution of a Sequentially Drilled Joint Congruence transformation of a set of matrices, which is reminiscent of classical joint diagonalization. In this paper we extend our results to the IVA problem, showing how the ML solution for the Gaussian model takes the form of an extended SeDJoCo problem. We formulate the extended problem, derive a condition for the existence of a solution, and propose two iterative solution algorithms. In addition, we derive the induced Cram\\'er-Rao Lower Bound on the resulting Interference-to-Source Ratios matrices, and demonstrate by simulation how the ML separation obtained by solving the extended SeDJoCo problem indeed attains the iCRLB , as opposed to other separation approaches, which cannot exploit prior knowledge regarding the sources' distributions.",0.1153846154],["Gavruta introduces the new gavruta.","Weaving K-frames in Hilbert Spaces","summarize: Gavruta introduced ",0.0],["Let us know what you think about it!","Verma modules for rank two Heisenberg-Virasoro algebra","summarize: Let ",0.0],["Snorkel DryBell builds on the Snorkel framework. it includes flexible,","Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale","summarize: Labeling training data is one of the most costly bottlenecks in developing machine learning-based applications. We present a first-of-its-kind study showing how existing knowledge resources from across an organization can be used as weak supervision in order to bring development time and cost down by an order of magnitude, and introduce Snorkel DryBell, a new weak supervision management system for this setting. Snorkel DryBell builds on the Snorkel framework, extending it in three critical aspects: flexible, template-based ingestion of diverse organizational knowledge, cross-feature production serving, and scalable, sampling-free execution. On three classification tasks at Google, we find that Snorkel DryBell creates classifiers of comparable quality to ones trained with tens of thousands of hand-labeled examples, converts non-servable organizational resources to servable models for an average 52% performance improvement, and executes over millions of data points in tens of minutes.",0.272910251],["a graph of the graph shows the sex of the sex of the","Perfect State Transfer in Laplacian Quantum Walk","summarize: For a graph ",0.0810810811],["we experimentally quantify the Raman scattering from individual carbyne chains confined in","Raman Scattering Cross Section of Confined Carbyne","summarize: We experimentally quantify the Raman scattering from individual carbyne chains confined in double-walled carbon nanotubes. We find that the resonant differential Raman cross section of confined carbyne is on the order of ",0.1875],["the main problem is that self-awareness cannot be observed from an outside perspective.","Will we ever have Conscious Machines?","summarize: The question of whether artificial beings or machines could become self-aware or consciousness has been a philosophical question for centuries. The main problem is that self-awareness cannot be observed from an outside perspective and the distinction of whether something is really self-aware or merely a clever program that pretends to do so cannot be answered without access to accurate knowledge about the mechanism's inner workings. We review the current state-of-the-art regarding these developments and investigate common machine learning approaches with respect to their potential ability to become self-aware. We realise that many important algorithmic steps towards machines with a core consciousness have already been devised. For human-level intelligence, however, many additional techniques have to be discovered.",0.0],["proposed method simplifies SDPs with no strictly feasible solution. proposed method could be","Partial facial reduction: simplified, equivalent SDPs via approximations of the PSD cone","summarize: We develop a practical semidefinite programming facial reduction procedure that utilizes computationally efficient approximations of the positive semidefinite cone. The proposed method simplifies SDPs with no strictly feasible solution by solving a sequence of easier optimization problems and could be a useful pre-processing technique for SDP solvers. We demonstrate effectiveness of the method on SDPs arising in practice, and describe our publicly-available software implementation. We also show how to find maximum rank matrices in our PSD cone approximations , and we give a post-processing procedure for dual solution recovery that generally applies to facial-reduction-based pre-processing techniques. Finally, we show how approximations can be chosen to preserve problem sparsity.",0.0588235294],["the largest body in the Main Belt is characterized by a large abundance of water ice","Thermal convection in the crust of the dwarf planet Ceres","summarize: Ceres is the largest body in the Main Belt, and it is characterized by a large abundance of water ice in its interior. This feature is suggested by its relatively low bulk density , while its partial differentiation into a rocky core and icy crust is suggested by several geological and geochemical features: minerals and salts produced by aqueous alteration, icy patches on the surface, lobate morphology interpreted as surface flows. In this work we explore how the composition can influence the characteristics of thermal convection in the crust of Ceres. Our results suggest that the onset of thermal convection is difficult and when it occurs it is short lived and this could imply that Ceres preserved deep liquid until present, as recent suggested by the work of Castillo-Rogez et al.. Moreover, cryovolcanism could be driven by diapirism rather than thermal convection.",0.3214285714],["data augmentation is a key element in training high-dimensional models. we apply pre","Dreaming More Data: Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation","summarize: Data augmentation is a key element in training high-dimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g.~new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. With an eye towards true end-to-end learning, we suggest learning the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. We then learn a class-specific probabilistic generative models of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manually-specified augmentation schemes. Our code and augmented datasets are available online.",0.4545454545],["the Argonne v18 and Urbana IX nuclear potentials are constructed for core","Nuclear equation of state for core-collapse supernova simulations with realistic nuclear forces","summarize: A new table of the nuclear equation of state based on realistic nuclear potentials is constructed for core-collapse supernova numerical simulations. Adopting the EOS of uniform nuclear matter constructed by two of the present authors with the cluster variational method starting from the Argonne v18 and Urbana IX nuclear potentials, the Thomas-Fermi calculation is performed to obtain the minimized free energy of a Wigner-Seitz cell in non-uniform nuclear matter. As a preparation for the Thomas-Fermi calculation, the EOS of uniform nuclear matter is modified so as to remove the effects of deuteron cluster formation in uniform matter at low densities. Mixing of alpha particles is also taken into account following the procedure used by Shen et al. . The critical densities with respect to the phase transition from non-uniform to uniform phase with the present EOS are slightly higher than those with the Shen EOS at small proton fractions. The critical temperature with respect to the liquid-gas phase transition decreases with the proton fraction in a more gradual manner than in the Shen EOS. Furthermore, the mass and proton numbers of nuclides appearing in non-uniform nuclear matter with small proton fractions are larger than those of the Shen EOS. These results are consequences of the fact that the density derivative coefficient of the symmetry energy of our EOS is smaller than that of the Shen EOS.",0.25],["a small batch-size or large learning rate is used to train neural networks. the","On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length","summarize: Stochastic Gradient Descent based training of neural networks with a large learning rate or a small batch-size typically ends in well-generalizing, flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. However, the curvature along the SGD trajectory is poorly understood. An empirical investigation shows that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. When studying the SGD dynamics in relation to the sharpest directions in this initial phase, we find that the SGD step is large compared to the curvature and commonly fails to minimize the loss along the sharpest directions. Furthermore, using a reduced learning rate along these directions can improve training speed while leading to both sharper and better generalizing solutions compared to vanilla SGD. In summary, our analysis of the dynamics of SGD in the subspace of the sharpest directions shows that they influence the regions that SGD steers to , the overall training speed, and the generalization ability of the final model.",0.1692841418],["unified learning-based technique uses both types of cues for depth inference.","Deep Eyes: Binocular Depth-from-Focus on Focal Stack Pairs","summarize: Human visual system relies on both binocular stereo cues and monocular focusness cues to gain effective 3D perception. In computer vision, the two problems are traditionally solved in separate tracks. In this paper, we present a unified learning-based technique that simultaneously uses both types of cues for depth inference. Specifically, we use a pair of focal stacks as input to emulate human perception. We first construct a comprehensive focal stack training dataset synthesized by depth-guided light field rendering. We then construct three individual networks: a Focus-Net to extract depth from a single focal stack, a EDoF-Net to obtain the extended depth of field image from the focal stack, and a Stereo-Net to conduct stereo matching. We show how to integrate them into a unified BDfF-Net to obtain high-quality depth maps. Comprehensive experiments show that our approach outperforms the state-of-the-art in both accuracy and speed and effectively emulates human vision systems.",0.0],["pipelined Krylov subspace methods achieve significantly improved parallel scalability on present-","Numerically Stable Variants of the Communication-hiding Pipelined Conjugate Gradients Algorithm for the Parallel Solution of Large Scale Symmetric Linear Systems","summarize: By reducing the number of global synchronization bottlenecks per iteration and hiding communication behind useful computational work, pipelined Krylov subspace methods achieve significantly improved parallel scalability on present-day HPC hardware. However, this typically comes at the cost of a reduced maximal attainable accuracy. This paper presents and compares several stabilized versions of the communication-hiding pipelined Conjugate Gradients method. The main novel contribution of this work is the reformulation of the multi-term recurrence pipelined CG algorithm by introducing shifts in the recursions for specific auxiliary variables. These shifts reduce the amplification of local rounding errors on the residual. The stability analysis presented in this work provides a rigorous method for selection of the optimal shift value in practice. It is shown that, given a proper choice for the shift parameter, the resulting shifted pipelined CG algorithm restores the attainable accuracy and displays nearly identical robustness to local rounding error propagation compared to classical CG. Numerical results on a variety of SPD benchmark problems compare different stabilization techniques for the pipelined CG algorithm, showing that the shifted pipelined CG algorithm is able to attain a high accuracy while displaying excellent parallel performance.",0.0401121062],["black hole solution is a new charge for black hole solution. the topological charge is","The Last Lost Charge And Phase Transition In Schwarzschild AdS Minimally Coupled to a Cloud of Strings","summarize: In this paper we study the Schwarzschild AdS black hole with a cloud of string background in an extended phase space and investigate a new phase transition related to the topological charge. By treating the topological charge as a new charge for black hole solution we study its thermodynamics in this new extended phase space. We treat by two approaches to study the phase transition behavior via both ",0.1750346638],["a robust and stable relationship between a country's productive structure and its economic growth has","The Impact of Services on Economic Complexity: Service Sophistication as Route for Economic Growth","summarize: Economic complexity reflects the amount of knowledge that is embedded in the productive structure of an economy. By combining tools from network science and econometrics, a robust and stable relationship between a country's productive structure and its economic growth has been established. Here we report that not only goods but also services are important for predicting the rate at which countries will grow. By adopting a terminology which classifies manufactured goods and delivered services as products, we investigate the influence of services on the country's productive structure. In particular, we provide evidence that complexity indices for services are in general higher than those for goods, which is reflected in a general tendency to rank countries with developed service sector higher than countries with economy centred on manufacturing of goods. By focusing on country dynamics based on experimental data, we investigate the impact of services on the economic complexity of countries measured in the product space . Importantly, we show that diversification of service exports and its sophistication can provide an additional route for economic growth in both developing and developed countries.",0.2068965517],["a hybrid low order\/high order discretisation is proposed for the high order entrop","A provably entropy stable subcell shock capturing approach for high order split form DG for the compressible Euler Equations","summarize: The main result in this paper is a provably entropy stable shock capturing approach for the high order entropy stable DGSEM based on a hybrid blending with a subcell low order variant. Since it is possible to rewrite a high order SBP operator into an equivalent conservative finite volume form, we were able to design a low order scheme directly with the LGL nodes that is compatible to the discrete entropy analysis used for the proof of the entropy stable DGSEM. Furthermore, we present a hybrid low order\/high order discretisation where it is possible to seamlessly blend between the two approaches, while still being provably entropy stable. We are able to extend the approach to three spatial dimensions on unstructured curvilinear hexahedral meshes. We validate our theoretical findings and demonstrate convergence order for smooth problems, conservation of the primary quantities and discrete entropy stability for an arbitrary blending on curvilinear grids. In practical simulations, we connect the blending factor to a local troubled element indicator that provides the control of the amount of low order dissipation injected into the high order scheme. We modified an existing shock indicator, which is based on the modal polynomial representation, to our provably stable hybrid scheme. The aim is to reduce the impact of the parameters as good as possible. We describe our indicator in detail and demonstrate its robustness in combination with the hybrid scheme, as it is possible to compute all the different test cases without changing the indicator. The test cases include e.g. the double Mach reflection setup, forward and backward facing steps with shock Mach numbers up to 100. The proposed approach is relatively straight forward to implement in an existing entropy stable DGSEM code as only modifications local to an element are necessary.",0.2401193092],["the classical theory of enzymatic inhibition aims to quantitatively describe the effect of certain","Single-molecule theory of enzymatic inhibition predicts the emergence of inhibitor-activator duality","summarize: The classical theory of enzymatic inhibition aims to quantitatively describe the effect of certain molecules -- called inhibitors -- on the progression of enzymatic reactions, but growing signs indicate that it must be revised to keep pace with the single-molecule revolution that is sweeping through the sciences. Here, we take the single enzyme perspective and rebuild the theory of enzymatic inhibition from the bottom up. We find that accounting for multi-conformational enzyme structure and intrinsic randomness cannot undermine the validity of classical results in the case of competitive inhibition; but that it should strongly change our view on the uncompetitive and mixed modes of inhibition. There, stochastic fluctuations on the single-enzyme level could give rise to inhibitor-activator duality -- a phenomenon in which, under some conditions, the introduction of a molecule whose binding shuts down enzymatic catalysis will counter intuitively work to facilitate product formation. We state -- in terms of experimentally measurable quantities -- a mathematical condition for the emergence of inhibitor-activator duality, and propose that it could explain why certain molecules that act as inhibitors when substrate concentrations are high elicit a non-monotonic dose response when substrate concentrations are low. The fundamental and practical implications of our findings are thoroughly discussed.",0.6],["a study of the relation between the charge density at a point on a conducting surface","On the Dependence of Charge Density on Surface Curvature of an Isolated Conductor","summarize: A study of the relation between the electrostatic charge density at a point on a conducting surface and the curvature of the surface is presented. Two major scientific literature on this topic are reviewed and the apparent discrepancy between them is resolved. Hence, a step is taken towards obtaining a general analytic formula for relating the charge density with surface curvature of conductors. The merit of this formula and its limitations are discussed.",0.5641025641],["TESs are used as very sensitive thermometers in microcalorimeters.","AC\/DC characterization of a Ti\/Au TES with Au\/Bi absorber for X-ray detection","summarize: Transition-edge sensors are used as very sensitive thermometers in microcalorimeters aimed at detection of different wavelengths. In particular, for soft X-ray astrophysics, science goals require very high resolution microcalorimeters which can be achieved with TESs coupled to suitable absorbers. For many applications there is also need for a high number of pixels which typically requires multiplexing in the readout stage. Frequency Domain Multiplexing is a common scheme and is the baseline proposed for the ATHENA mission. FDM requires biasing the TES in AC at MHz frequencies. Recently there has been reported degradation in performances under AC with respect to DC bias. In order to assess the performances of TESs to be used with FDM, it is thus of great interest to compare the performances of the same device both under AC and DC bias. This requires two different measurement setups with different processes for making the characterization. We report in this work the preliminary results of a single pixel characterization performed on a TiAu TES under AC and afterwards under DC bias in different facilities. Extraction of dynamical parameters and noise performances are compared in both cases as a first stage for further AC\/DC comparison of these devices.",0.0],["a method for analyzing chemical bonds and their energy distributions in a two-dimensional","Observing the 3D chemical bond and its energy distribution in a projected space","summarize: Our curiosity-driven desire to see chemical bonds dates back at least one-hundred years, perhaps to antiquity. Sweeping improvements in the accuracy of measured and predicted electron charge densities, alongside our largely bondcentric understanding of molecules and materials, heighten this desire with means and significance. Here we present a method for analyzing chemical bonds and their energy distributions in a two-dimensional projected space called the condensed charge density. Bond silhouettes in the condensed charge density can be reverse-projected to reveal precise three-dimensional bonding regions we call bond bundles. We show that delocalized metallic bonds and organic covalent bonds alike can be objectively analyzed, the formation of bonds observed, and that the crystallographic structure of simple metals can be rationalized in terms of bond bundle structure. Our method also reproduces the expected results of organic chemistry, enabling the recontextualization of existing bond models from a charge density perspective.",0.5],["the propensity score is a common tool for estimating the causal effect of a","Estimation of causal effects with multiple treatments: a review and new ideas","summarize: The propensity score is a common tool for estimating the causal effect of a binary treatment in observational data. In this setting, matching, subclassification, imputation, or inverse probability weighting on the propensity score can reduce the initial covariate bias between the treatment and control groups. With more than two treatment options, however, estimation of causal effects requires additional assumptions and techniques, the implementations of which have varied across disciplines. This paper reviews current methods, and it identifies and contrasts the treatment effects that each one estimates. Additionally, we propose possible matching techniques for use with multiple, nominal categorical treatments, and use simulations to show how such algorithms can yield improved covariate similarity between those in the matched sets, relative the pre-matched cohort. To sum, this manuscript provides a synopsis of how to notate and use causal methods for categorical treatments.",0.5416666667],["three publicly available Hungarian corpora are evaluated. perplexity values comparable to","emLam -- a Hungarian Language Modeling baseline","summarize: This paper aims to make up for the lack of documented baselines for Hungarian language modeling. Various approaches are evaluated on three publicly available Hungarian corpora. Perplexity values comparable to models of similar-sized English corpora are reported. A new, freely downloadable Hungar- ian benchmark corpus is introduced.",0.0909090909],["approach was developed to describe the first passage time in multistep processes. the approach is an","First passage time in multi-step stochastic processes with applications to dust charging","summarize: An approach was developed to describe the first passage time in multistep stochastic processes with discrete states governed by a master equation . The approach is an extension of the totally absorbing boundary approach given for calculation of FPT in one-step processes to include multistep processes where jumps are not restricted to adjacent sites. In addition, a Fokker-Planck equation was derived from the multistep ME, assuming the continuity of the state variable. The developed approach and an FPE based approach were used to find the mean first passage time of the transition between the negative and positive stable macrostates of dust grain charge when the charging process was bistable. The dust was in a plasma and charged by collecting ions and electrons, and emitting secondary electrons. The MFPTs for the transitioning of grain charge from one macrostate to the other were calculated by the two approaches for a range of grain sizes. Both approaches produced very similar results for the same grain except for when it was very small. The difference between MFPTs of two approaches for very small grains was attributed to the failure of the charge continuity assumption in the FPE description. For a given grain, the MFPT for a transition from the negative macrostate to the positive one was substantially larger than that for a transition in a reverse order. The normalized MFPT for a transition from the positive to the negative macrostate showed little sensitivity to the grain radius. For a reverse transition, with the increase of the grain radius, it dropped first and then increased. The probability density function of FPT was substantially wider for a transition from the positive to the negative macrostate, as compared to a reverse transition.",0.2380952381],["horizontal FL is a new method of perturbed local embedding. it is","VAFL: a Method of Vertical Asynchronous Federated Learning","summarize: Horizontal Federated learning handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochastic gradient algorithms without coordination with other clients, so it is suitable for intermittent connectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.",0.3125],["Buzzard irregularity does not have p-adic slope.","A remark on non-integral p-adic slopes for modular forms","summarize: We give a sufficient condition, namely Buzzard irregularity, for there to exist a cuspidal eigenform which does not have integral p-adic slope.",0.214707798],["straggler stars were detected in the young globular cluster NGC 2173","The double blue straggler sequence in NGC 2173: a field contamination artefact?","summarize: Li et al. detected two apparently distinct populations of blue straggler stars in the young globular cluster NGC 2173, a similar feature as observed in numerous Galactic globular clusters . Recently, Dalessandro et al. compared the observed color--magnitude diagrams of both the cluster and a nearby reference field . They conclude that the bifurcated pattern of BSSs in NGC 2173 observed by L18 is a field contamination artefact. In this note, we explore the central concentration properties of the removed `field stars' identified by D18. Our purpose is to examine if these `field stars' are spatially homogeneously distributed. Employing a Monte Carlo-based approach, we have carefully studied the probability that any such central concentration may be caused by small number statistics. We find that, in most cases of, the `field stars' removed by D18 exhibit a clear central concentration, which cannot be explained on the basis of small number statistics alone. Therefore, we suggest that D18 may well have overestimated the field contamination level, implying that the bifurcated BSS pattern in NGC 2173 cannot, in fact, be explained by field contamination.",0.3320366241],["quantum reading is a process used to read out classical information stored in a read-only","Quantum reading capacity: General definition and bounds","summarize: Quantum reading refers to the task of reading out classical information stored in a read-only memory device. In any such protocol, the transmitter and receiver are in the same physical location, and the goal of such a protocol is to use these devices , coupled with a quantum strategy, to read out as much information as possible from a memory device, such as a CD or DVD. As a consequence of the physical setup of quantum reading, the most natural and general definition for quantum reading capacity should allow for an adaptive operation after each call to the channel, and this is how we define quantum reading capacity in this paper. We also establish several bounds on quantum reading capacity, and we introduce an environment-parametrized memory cell with associated environment states, delivering second-order and strong converse bounds for its quantum reading capacity. We calculate the quantum reading capacities for some exemplary memory cells, including a thermal memory cell, a qudit erasure memory cell, and a qudit depolarizing memory cell. We finally provide an explicit example to illustrate the advantage of using an adaptive strategy in the context of zero-error quantum reading capacity.",0.4444444444],["the loss of synchronizability at large coupling strength is of major concern.","Enhancement of Network Synchronizability via Two Oscillatory System","summarize: The loss of synchronizability at large coupling strength is of major concern especially in the fields of secure communication and complex systems. Because theoretically, the coupling mode that can surely stabilize the chaotic\/hyperchaotic synchronized state is vector coupling which is in contrast to the practical demand of information exchange using lesser number of coordinates . In the present work, we propose that if the node dynamics are given by a pair of oscillators rather than by a conventional way of single oscillator , then the information exchange via a single coordinate could be sufficient to stabilize the chaotic\/hyperchaotic synchronization manifold at large coupling strength. The frameworks of drive-response system and Master Stability Function have been used to study the TOS effect by varying TOS parameters with and without feedback . The TOS effect has been found numerically both in the chaotic and hyperchaotic systems. However, since threshold also increases as a side effect of TOS, the extent of ",0.2142857143],["PHANTOM extracts five measures from Git logs. each measure is transformed","PHANTOM: Curating GitHub for engineered software projects using time-series clustering","summarize: Context: Within the field of Mining Software Repositories, there are numerous methods employed to filter datasets in order to avoid analysing low-quality projects. Unfortunately, the existing filtering methods have not kept up with the growth of existing data sources, such as GitHub, and researchers often rely on quick and dirty techniques to curate datasets. Objective: The objective of this study is to develop a method capable of filtering large quantities of software projects in a resource-efficient way. Method: This study follows the Design Science Research methodology. The proposed method, PHANTOM, extracts five measures from Git logs. Each measure is transformed into a time-series, which is represented as a feature vector for clustering using the k-means algorithm. Results: Using the ground truth from a previous study, PHANTOM was shown to be able to rediscover the ground truth on the training dataset, and was able to identify engineered projects with up to 0.87 Precision and 0.94 Recall on the validation dataset. PHANTOM downloaded and processed the metadata of 1,786,601 GitHub repositories in 21.5 days using a single personal computer, which is over 33% faster than the previous study which used a computer cluster of 200 nodes. The possibility of applying the method outside of the open-source community was investigated by curating 100 repositories owned by two companies. Conclusions: It is possible to use an unsupervised approach to identify engineered projects. PHANTOM was shown to be competitive compared to the existing supervised approaches while reducing the hardware requirements by two orders of magnitude.",0.1666666667],["we consider two-body and quasi-two-body decays of the type.","Polarization in two-body decays and new physics","summarize: We consider two-body and quasi-two-body decays of the type ",0.3],["we examine and compare two well-motivated approaches to contextuality. one approach is","Measuring Observable Quantum Contextuality","summarize: Contextuality is a central property in comparative analysis of classical, quantum, and supercorrelated systems. We examine and compare two well-motivated approaches to contextuality. One approach is based on the idea that one and the same physical property measured under different conditions is represented by different random variables. The other approach is based on the idea that while a physical property is represented by a single random variable irrespective of its context, the joint distributions of the random variables describing the system can involve negative probabilities. We show that in the Leggett-Garg and EPR-Bell systems, the two measures essentially coincide.",0.0],["glycinium oxalate is the simplest amino acid- carboxy","Hydrogen Bond Symmetrization in Glycinium Oxalate under Pressure","summarize: We report here the evidences of hydrogen bond symmetrization in the simplest amino acid- carboxylic acid complex, glycinium oxalate, at moderate pressures of 8 GPa using in-situ infrared and Raman spectroscopic investigations combined with first-principles simulations. The protonation of the semioxalate units through dynamic proton movement results in infinite oxalate chains. At pressures above 12 GPa, the glycine units systematically reorient with pressure to form hydrogen bonded supramolecular assemblies held together by these chains.",0.0],["FD-NILSS does not require tangent solvers, and can be","Sensitivity analysis on chaotic dynamical systems by Finite Difference Non-Intrusive Least Squares Shadowing ","summarize: We present the Finite Difference Non-Intrusive Least Squares Shadowing algorithm for computing sensitivities of long-time averaged quantities in chaotic dynamical systems. FD-NILSS does not require tangent solvers, and can be implemented with little modification to existing numerical simulation software. We also give a formula for solving the least-squares problem in FD-NILSS, which can be applied in NILSS as well. Finally, we apply FD-NILSS for sensitivity analysis of a chaotic flow over a 3-D cylinder at Reynolds number 525, where FD-NILSS computes accurate sensitivities and the computational cost is in the same order as the numerical simulation.",0.0],["we classify incompressible, boundary-incompressible, nonorientable surfaces","Nonorientable, incompressible surfaces in punctured-torus bundles over S^1","summarize: We classify incompressible, boundary-incompressible, nonorientable surfaces in punctured-torus bundles over ",0.1023616158],["VSSS-RL is a traditional league in the Latin American robotics competition.","Learning to Play Soccer by Reinforcement and Applying Sim-to-Real to Compete in the Real World","summarize: This work presents an application of Reinforcement Learning for the complete control of real soccer robots of the IEEE Very Small Size Soccer , a traditional league in the Latin American Robotics Competition . In the VSSS league, two teams of three small robots play against each other. We propose a simulated environment in which continuous or discrete control policies can be trained, and a Sim-to-Real method to allow using the obtained policies to control a robot in the real world. The results show that the learned policies display a broad repertoire of behaviors that are difficult to specify by hand. This approach, called VSSS-RL, was able to beat the human-designed policy for the striker of the team ranked 3rd place in the 2018 LARC, in 1-vs-1 matches.",0.3271265545],["system consists of a Willow Garage PR2 and specialized autonomous behaviors for scooping and","Towards Assistive Feeding with a General-Purpose Mobile Manipulator","summarize: General-purpose mobile manipulators have the potential to serve as a versatile form of assistive technology. However, their complexity creates challenges, including the risk of being too difficult to use. We present a proof-of-concept robotic system for assistive feeding that consists of a Willow Garage PR2, a high-level web-based interface, and specialized autonomous behaviors for scooping and feeding yogurt. As a step towards use by people with disabilities, we evaluated our system with 5 able-bodied participants. All 5 successfully ate yogurt using the system and reported high rates of success for the system's autonomous behaviors. Also, Henry Evans, a person with severe quadriplegia, operated the system remotely to feed an able-bodied person. In general, people who operated the system reported that it was easy to use, including Henry. The feeding system also incorporates corrective actions designed to be triggered either autonomously or by the user. In an offline evaluation using data collected with the feeding system, a new version of our multimodal anomaly detection system outperformed prior versions.",0.2173913043],["the non-commutative black hole can reduce to the Schwarzschild black hole.","Shadow cast of non-commutative black holes in Rastall gravity","summarize: We study the shadow and energy emission rate of a spherically symmetric non-commutative black hole in Rastall gravity. Depending on the model parameters, the non-commutative black hole can reduce to the Schwarzschild black hole. Since the non-vanishing non-commutative parameter affects the formation of event horizon, the visibility of the resulting shadow depends on the non-commutative parameter in Rastall gravity. The obtained sectional shadows respect the unstable circular orbit condition, which is crucial for physical validity of the black hole image model.",0.2941176471],["this paper focuses on a traditional relation extraction task. we explore this task with","A bag-of-concepts model improves relation extraction in a narrow knowledge domain with limited data","summarize: This paper focuses on a traditional relation extraction task in the context of limited annotated data and a narrow knowledge domain. We explore this task with a clinical corpus consisting of 200 breast cancer follow-up treatment letters in which 16 distinct types of relations are annotated. We experiment with an approach to extracting typed relations called window-bounded co-occurrence , which uses an adjustable context window around entity mentions of a relevant type, and compare its performance with a more typical intra-sentential co-occurrence baseline. We further introduce a new bag-of-concepts approach to feature engineering based on the state-of-the-art word embeddings and word synonyms. We demonstrate the competitiveness of BoC by comparing with methods of higher complexity, and explore its effectiveness on this small dataset.",0.5],["a new approach can be used to remove irrelevant samples from real-world web images.","Exploiting Web Images for Fine-Grained Visual Recognition by Eliminating Noisy Samples and Utilizing Hard Ones","summarize: Labeling objects at a subordinate level typically requires expert knowledge, which is not always available when using random annotators. As such, learning directly from web images for fine-grained recognition has attracted broad attention. However, the presence of label noise and hard examples in web images are two obstacles for training robust fine-grained recognition models. Therefore, in this paper, we propose a novel approach for removing irrelevant samples from real-world web images during training, while employing useful hard examples to update the network. Thus, our approach can alleviate the harmful effects of irrelevant noisy web images and hard examples to achieve better performance. Extensive experiments on three commonly used fine-grained datasets demonstrate that our approach is far superior to current state-of-the-art web-supervised methods.",0.3103542599],["evolution of extragalactic radio sources has been a problem in the study of active ga","Changing-look AGNs or short-lived radio sources?","summarize: The evolution of extragalactic radio sources has been a fundamental problem in the study of active galactic nuclei for many years. A standard evolutionary model has been created based on observations of a wide range of radio sources. In the general scenario of the evolution, the younger and smaller Gigahertz-Peaked Spectrum and Compact Steep Spectrum sources become large-scale FRI and FRII objects. However, a growing number of observations of low power radio sources suggests that the model cannot explain all their properties and there are still some aspects of the evolutionary path that remain unclear. There are indications, that some sources may be short-lived objects on timescales of ",0.2],["","Adaptive multi-channel event segmentation and feature extraction for monitoring health outcomes","summarize: ",0.0000453999],["the non-triviality for the holonomy groups is shown. the","Losik classes for codimension-one foliations","summarize: Following Losik's approach to Gelfand's formal geometry, certain characteristic classes for codimension-one foliations coming from the Gelfand-Fuchs cohomology are considered. Sufficient conditions for non-triviality in terms of dynamical properties of generators of the holonomy groups are found. The non-triviality for the Reeb foliations is shown; this is in contrast with some classical theorems on the Godbillon-Vey class, e.g, the Mizutani-Morita-Tsuboi Theorem about triviality of the Godbillon-Vey class of foliations almost without holonomy is not true for the classes under consideration. It is shown that the considered classes are trivial for a large class of foliations without holonomy. The question of triviality is related to ergodic theory of dynamical systems on the circle and to the problem of smooth conjugacy of local diffeomorphisms. Certain classes are obstructions for the existence of transverse affine and projective connections.",0.0666666667],["we present the novel approach to mathematical modeling of information processes in biosystems. it explore","Quantum-like modeling in biology with open quantum systems and instruments","summarize: We present the novel approach to mathematical modeling of information processes in biosystems. It explores the mathematical formalism and methodology of quantum theory, especially quantum measurement theory. This approach is known as and it should be distinguished from study of genuine quantum physical processes in biosystems . It is based on quantum information representation of biosystem's state and modeling its dynamics in the framework of theory of open quantum systems. This paper starts with the non-physicist friendly presentation of quantum measurement theory, from the original von Neumann formulation to modern theory of quantum instruments. Then, latter is applied to model combinations of cognitive effects and gene regulation of glucose\/lactose metabolism in Escherichia coli bacterium. The most general construction of quantum instruments is based on the scheme of indirect measurement, in that measurement apparatus plays the role of the environment for a biosystem. The biological essence of this scheme is illustrated by quantum formalization of Helmholtz sensation-perception theory. Then we move to open systems dynamics and consider quantum master equation, with concentrating on quantum Markov processes. In this framework, we model functioning of biological functions such as psychological functions and epigenetic mutation.",0.2777777778],["consensus protocol is proposed based on the sign of innovations. each agent only requires single-","Single-Bit Consensus with Finite-Time Convergence: Theory and Applications","summarize: In this brief paper, a new consensus protocol based on the sign of innovations is proposed. Based on this protocol each agent only requires single-bit of information about its relative state to its neighboring agents. This is significant in real-time applications, since it requires less computation and\/or communication load on agents. Using Lyapunov stability theorem the convergence is proved for networks having a spanning tree. Further, the convergence is shown to be in finite-time, which is significant as compared to most asymptotic protocols in the literature. Time-variant network topologies are also considered in this paper, and final consensus value is derived for undirected networks. Applications of the proposed consensus protocol in 2D\/3D rendezvous task, distributed estimation, distributed optimization, and formation control are considered and significance of applying this protocol is discussed. Numerical simulations are provided to compare the protocol with the existing protocols in the literature.",0.1666666667],["foamy systems require a dedicated study to increase our understanding of elasticity. they have","Coupled elasticity in soft solid foams","summarize: Elasticity of soft materials can be greatly influenced by the presence of air bubbles. Such a capillary effect is expected for a wide range of materials, from polymer gels to concentrated emulsions and colloidal suspensions. Whereas experimental results and theory exist for describing the elasto-capillary behavior of bubbly materials , foamy systems still require a dedicated study in order to increase our understanding of elasticity in aerated materials over the full range of gas volume fractions. Here we elaborate well-controlled foams with concentrated emulsion and we measure their shear elastic modulus as a function of gas fraction, bubble size and elastic modulus of the emulsion. Such complex foams possess the elastic features of both the bubble assembly and the interstitial matrix. Moreover, their elastic modulus is shown to be governed by two parameters, namely the gas volume fraction and the elasto-capillary number, defined as the ratio of the emulsion modulus with the bubble capillary pressure. We connect our results for foams with existing data for bubbly systems and we provide a general view for the effect of gas bubbles in soft elastic media. Finally, we suggest that our results could be useful for estimating the shear modulus of aqueous foams and emulsions with multimodal size distributions.",0.2],["the SEROA signal is proportional to the magnetic polarizability of the substrate","Strongly Enhanced Raman Optical Activity in Molecules by Magnetic Response of Nanoparticles","summarize: An analytical theory for the surface-enhanced Raman optical activity with the magnetic response of the substrate particle has been presented. We have demonstrated that the SEROA signal is proportional to the magnetic polarizability of the substrate particle, which can be significantly enhanced due to the existence of the magnetic response. At the same time, a large circular intensity difference for the SEROA can also be achieved in the presence of the magnetic response. Taking Si nanoparticles as examples, we have found that the CID enhanced by a Si nanoparticle is 10 times larger than that of Au. Furthermore, when the molecule is located in the hotspot of a Si dimer, CID can be 60 times larger. The phenomena originate from large magnetic fields concentrated near the nanoparticle and boosted magnetic dipole emission of the molecule. The symmetric breaking of the electric fields caused by the magnetic dipole response of the nanoparticle also plays an important role. Our findings provide a new way to tailor theRaman optical activity by designing metamaterials with the strong magnetic response.",0.0555555556],["nanostructure strategies focus on increasing phonon scattering and reducing mean-free-path","Heat current anticorrelation effects leading to thermal conductivity reduction in nanoporous Si","summarize: Prevailing nanostructuring strategies focus on increasing phonon scattering and reducing the mean-free-path of phonons across the spectrum. In nanoporous Si materials, for example, boundary scattering reduces thermal conductivity drastically. In this work, we identify an unusual anticorrelated specular phonon scattering effect which can result in additional reductions in thermal conductivity of up to ~ 80% for specific nanoporous geometries. We further find evidence that this effect has its origin in heat trapping between large pores with narrow necks. As the heat becomes trapped between the pores, phonons undergo multiple specular reflections such that their contribution to the thermal conductivity is partly undone. We find this effect to be wave-vector dependent at low temperatures. We use large-scale molecular dynamics simulations, wave packet analysis, as well as an analytical model to illustrate the anticorrelation effect, evaluate its impact on thermal conductivity, and detail how it can be controlled to manipulate phonon transport in nanoporous materials.",0.2046826883],["we compare the variance of work output per cycle to an average work output. the variable of","Work and power fluctuations in a critical heat engine","summarize: We investigate fluctuations of output work for a class of Stirling heat engines with working fluid composed of interacting units and compare these fluctuations to an average work output. In particular, we focus on engine performance close to a critical point where Carnot's efficiency may be attained at a finite power as reported in . We show that the variance of work output per cycle scales with the same critical exponent as the heat capacity of the working fluid. As a consequence, the relative work fluctuation diverges unless the output work obeys a rather strict scaling condition, which would be very hard to fulfill in practice. Even under this condition, the fluctuations of work and power do not vanish in the infinite system size limit. Large fluctuations of output work thus constitute inseparable and dominant element in performance of the macroscopic heat engines close to a critical point.",0.0769230769],["NL3, G3 and IU-FSU parameters are used to study thermal effects on","Warm dense matter and cooling of supernovae remnants","summarize: We study the thermal effects on the nuclear matter properties such as binding energy, incompressibility, free symmetry energy and its coefficients using NL3, G3 and IU-FSU parameter sets of relativistic mean-field models. These models being consistent with the properties of cold NM, have also been used to study the effect of temperature by incorporating the Fermi function. The critical temperature for the liquid-gas phase transition in the symmetric NM is found to be 14.60, 15.37 and 14.50 MeV for NL3, G3 and IU-FSU parameter sets respectively, which is in excellent agreement with previous theoretical and experimental studies. We inspect that the properties related to second differential coefficient of the binding energy and free symmetry energy at saturation density and Q sym,0) exhibit the contrary effects for NL3 and G3 parameters as the temperature increases. We find that the prediction of saturated curvature parameter for G3 equation of state at finite temperature favour the combined analysis of K sym,0 for the existence of massive pulsars, gravitational waves from GW170817 and NICER observations of PSR J0030+0451. Further, we investigate the cooling mechanism of newly born stars through neutrino emissivity controlled by direct Urca process and instate some interesting remarks about neutrino emissivity. We also deliberate the effect of temperature on the M-R profile of Proto-Neutron star.",0.0833333333],["a new technique is proposed to analyze the representations that enable such success. we use","Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models","summarize: Neural language models perform well on tasks that require sensitivity to syntactic structure. Drawing on the syntactic priming paradigm from psycholinguistics, we propose a novel technique to analyze the representations that enable such success. By establishing a gradient similarity metric between structures, this technique allows us to reconstruct the organization of the LMs' syntactic representational space. We use this technique to demonstrate that LSTM LMs' representations of different types of sentences with relative clauses are organized hierarchically in a linguistically interpretable manner, suggesting that the LMs track abstract properties of the sentence.",0.4],["reinforcement learning algorithms combine deep neural networks and tree search. algorithm is not directly applicable to single","Ranked Reward: Enabling Self-Play Reinforcement Learning for Combinatorial Optimization","summarize: Adversarial self-play in two-player games has delivered impressive results when used with reinforcement learning algorithms that combine deep neural networks and tree search. Algorithms like AlphaZero and Expert Iteration learn tabula-rasa, producing highly informative training data on the fly. However, the self-play training strategy is not directly applicable to single-player games. Recently, several practically important combinatorial optimisation problems, such as the travelling salesman problem and the bin packing problem, have been reformulated as reinforcement learning problems, increasing the importance of enabling the benefits of self-play beyond two-player games. We present the Ranked Reward algorithm which accomplishes this by ranking the rewards obtained by a single agent over multiple games to create a relative performance metric. Results from applying the R2 algorithm to instances of a two-dimensional and three-dimensional bin packing problems show that it outperforms generic Monte Carlo tree search, heuristic algorithms and integer programming solvers. We also present an analysis of the ranked reward mechanism, in particular, the effects of problem instances with varying difficulty and different ranking thresholds.",0.0555555556],["the current expansion of theory and research on artificial intelligence has revitalized the theory and research on","Augmenting Organizational Decision-Making with Deep Learning Algorithms: Principles, Promises, and Challenges","summarize: The current expansion of theory and research on artificial intelligence in management and organization studies has revitalized the theory and research on decision-making in organizations. In particular, recent advances in deep learning algorithms promise benefits for decision-making within organizations, such as assisting employees with information processing, thereby augment their analytical capabilities and perhaps help their transition to more creative work.",0.1818181818],["the brownian web and net is a collection of one-dimensional coalescing brown","The Brownian web, the Brownian net, and their universality","summarize: The Brownian web is a collection of one-dimensional coalescing Brownian motions starting from everywhere in space and time, and the Brownian net is a generalization that also allows branching. They appear in the diffusive scaling limits of many one-dimensional interacting particle systems with branching and coalescence. This article gives an introduction to the Brownian web and net, and how they arise in the scaling limits of various one-dimensional models, focusing mainly on coalescing random walks and random walks in i.i.d. space-time random environments. We will also briefly survey models and results connected to the Brownian web and net, including alternative topologies, population genetic models, true self-repelling motion, planar aggregation, drainage networks, oriented percolation, black noise and critical percolation. Some open questions are discussed at the end.",0.4705882353],["microservices are an essential ingredient to deploy machine learning models. traditional applications are becoming increasingly","Minerva: A Portable Machine Learning Microservice Framework for Traditional Enterprise SaaS Applications","summarize: In traditional SaaS enterprise applications, microservices are an essential ingredient to deploy machine learning models successfully. In general, microservices result in efficiencies in software service design, development, and delivery. As they become ubiquitous in the redesign of monolithic software, with the addition of machine learning, the traditional applications are also becoming increasingly intelligent. Here, we propose a portable ML microservice framework Minerva as an efficient way to modularize and deploy intelligent microservices in traditional legacy SaaS applications suite, especially in the enterprise domain. We identify and discuss the needs, challenges and architecture to incorporate ML microservices in such applications. Minervas design for optimal integration with legacy applications using microservices architecture leveraging lightweight infrastructure accelerates deploying ML models in such applications.",0.0],["positive maps for two positive maps.","Merging of positive maps: a construction of various classes of positive maps on matrix algebras","summarize: For two positive maps ",0.1785041281],["we leverage longitudinal data from 56 conspiracy communities on Reddit. we first identify 30K","What Makes People Join Conspiracy Communities?: Role of Social Factors in Conspiracy Engagement","summarize: Widespread conspiracy theories, like those motivating anti-vaccination attitudes or climate change denial, propel collective action and bear society-wide consequences. Yet, empirical research has largely studied conspiracy theory adoption as an individual pursuit, rather than as a socially mediated process. What makes users join communities endorsing and spreading conspiracy theories? We leverage longitudinal data from 56 conspiracy communities on Reddit to compare individual and social factors determining which users join the communities. Using a quasi-experimental approach, we first identify 30K future conspiracists- and 30K matched non-conspiracists-. We then provide empirical evidence of importance of social factors across six dimensions relative to the individual factors by analyzing 6 million Reddit comments and posts. Specifically in social factors, we find that dyadic interactions with members of the conspiracy communities and marginalization outside of the conspiracy communities, are the most important social precursors to conspiracy joining-even outperforming individual factor baselines. Our results offer quantitative backing to understand social processes and echo chamber effects in conspiratorial engagement, with important implications for democratic institutions and online communities.",0.1111111111],["Sieve-SDP inspects the constraints of the problem to detect lack of strict feasibility","Sieve-SDP: a simple facial reduction algorithm to preprocess semidefinite programs","summarize: We introduce Sieve-SDP, a simple facial reduction algorithm to preprocess semidefinite programs . Sieve-SDP inspects the constraints of the problem to detect lack of strict feasibility, deletes redundant rows and columns, and reduces the size of the variable matrix. It often detects infeasibility. It does not rely on any optimization solver: the only subroutine it needs is Cholesky factorization, hence it can be implemented in a few lines of code in machine precision. We present extensive computational results on several problem collections from the literature, with many SDPs coming from polynomial optimization.",0.1176470588],["entropy solutions are uniformly bounded with respect to space and time variables","Large time behavior of entropy solutions to one-dimensional unipolar hydrodynamic model for semiconductor devices","summarize: We are concerned with the global existence and large time behavior of entropy solutions to the one dimensional unipolar hydrodynamic model for semiconductors in the form of Euler-Poisson equations in a bounded interval. In this paper, we first prove the global existence of entropy solution by vanishing viscosity and compensated compactness framework. In particular, the solutions are uniformly bounded with respect to space and time variables by introducing modified Riemann invariants and the theory of invariant region. Based on the uniform estimates of density, we further show that the entropy solution converges to the corresponding unique stationary solution exponentially in time. No any smallness condition is assumed on the initial data and doping profile. Moreover, the novelty in this paper is about the unform bound with respect to time for the weak solutions of the isentropic Euler-Possion system.",0.282160575],["the method is general and largely game-independent. it can be applied to","A Generic Metaheuristic Approach to Sequential Security Games","summarize: The paper introduces a generic approach to solving Sequential Security Games which utilizes Evolutionary Algorithms. Formulation of the method is general and largely game-independent, which allows for its application to a wide range of SGs with just little adjustments addressing game specificity. Comprehensive experiments performed on 3 different types of games demonstrate robustness and stability of EASG, manifested by repeatable achieving optimal or near-optimal solutions in the vast majority of the cases. The main advantage of EASG is time efficiency. The method scales visibly better than state-of-the-art approaches and consequently can be applied to SG instances which are beyond capabilities of the existing methods. Furthermore, due to anytime characteristics, EASG is very well suited for time-critical applications, as the method can be terminated at any moment and still provide a reasonably good solution - the best one found so far.",0.25],["every graph of average degree more than average degree.","Erdos-Gallai Stability Theorem for Linear Forests","summarize: The Erd\\Hs-Gallai Theorem states that every graph of average degree more than ",0.0],["phishing attacks have become increasingly necessary. we propose a deep learning based data","HTMLPhish: Enabling Phishing Web Page Detection by Applying Deep Learning Techniques on HTML Analysis","summarize: Recently, the development and implementation of phishing attacks require little technical skills and costs. This uprising has led to an ever-growing number of phishing attacks on the World Wide Web. Consequently, proactive techniques to fight phishing attacks have become extremely necessary. In this paper, we propose HTMLPhish, a deep learning based data-driven end-to-end automatic phishing web page classification approach. Specifically, HTMLPhish receives the content of the HTML document of a web page and employs Convolutional Neural Networks to learn the semantic dependencies in the textual contents of the HTML. The CNNs learn appropriate feature representations from the HTML document embeddings without extensive manual feature engineering. Furthermore, our proposed approach of the concatenation of the word and character embeddings allows our model to manage new features and ensure easy extrapolation to test data. We conduct comprehensive experiments on a dataset of more than 50,000 HTML documents that provides a distribution of phishing to benign web pages obtainable in the real-world that yields over 93 percent Accuracy and True Positive Rate. Also, HTMLPhish is a completely language-independent and client-side strategy which can, therefore, conduct web page phishing detection regardless of the textual language.",0.1683565598],["neural architecture for wide-coverage natural language understanding in Spoken Dialogue Systems. we","Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU","summarize: We present a new neural architecture for wide-coverage Natural Language Understanding in Spoken Dialogue Systems. We develop a hierarchical multi-task architecture, which delivers a multi-layer representation of sentence meaning . The architecture is a hierarchy of self-attention mechanisms and BiLSTM encoders followed by CRF tagging layers. We describe a variety of experiments, showing that our approach obtains promising results on a dataset annotated with Dialogue Acts and Frame Semantics. Moreover, we demonstrate its applicability to a different, publicly available NLU dataset annotated with domain-specific intents and corresponding semantic roles, providing overall performance higher than state-of-the-art tools such as RASA, Dialogflow, LUIS, and Watson. For example, we show an average 4.45% improvement in entity tagging F-score over Rasa, Dialogflow and LUIS.",0.2307692308],["endoscopic endonasal surgery is a minimal invasive surgery that has been used","Continuous and Pulsed Experiments with Numerical Simulation to Dissect Pituitary Gland Tumour by Using Liquid Jet","summarize: Endoscopic endonasal surgery is a minimal invasive surgery that has been used to dissect pituitary gland tumour via curettes with the help of endoscope. However, this type of surgery has a high risk of failure because curettes may cause damages to blood vessels and optical nerves that lead to more complication for the patient. The aim of this study is to develop a new technique to dissect the tumour by using liquid jet. A series of experimental tests have been performed on animal tissue to study the effect of liquid pressure and nozzle diameter on dissecting and cutting the tissue. Continuous \/ pulsed liquid jet used with variable nozzle diameters, distances, pressures and angles. The study concluded with promising results on liquid jet to dissect hypophysis tumour and preserving fine blood vessels and optical nerve located near the pituitary gland. Index Terms-Endoscopic endonasal surgery, tumour removal, liquid jet, continuous and pulsed jet",0.2149593932],["the experiments were performed in a rectangular bubble column heated from one side wall and cooled from","Experimental investigation of heat transport in inhomogeneous bubbly flow","summarize: In this work we study the heat transport in inhomogeneous bubbly flow. The experiments were performed in a rectangular bubble column heated from one side wall and cooled from the other, with millimetric bubbles introduced through one half of the injection section . We characterise the global heat transport while varying two parameters: the gas volume fraction ",0.2083333333],["modelers have a tendency to label the fine-grained components. the modeler","Learning to Group and Label Fine-Grained Shape Components","summarize: A majority of stock 3D models in modern shape repositories are assembled with many fine-grained components. The main cause of such data form is the component-wise modeling process widely practiced by human modelers. These modeling components thus inherently reflect some function-based shape decomposition the artist had in mind during modeling. On the other hand, modeling components represent an over-segmentation since a functional part is usually modeled as a multi-component assembly. Based on these observations, we advocate that labeled segmentation of stock 3D models should not overlook the modeling components and propose a learning solution to grouping and labeling of the fine-grained components. However, directly characterizing the shape of individual components for the purpose of labeling is unreliable, since they can be arbitrarily tiny and semantically meaningless. We propose to generate part hypotheses from the components based on a hierarchical grouping strategy, and perform labeling on those part groups instead of directly on the components. Part hypotheses are mid-level elements which are more probable to carry semantic information. A multiscale 3D convolutional neural network is trained to extract context-aware features for the hypotheses. To accomplish a labeled segmentation of the whole shape, we formulate higher-order conditional random fields to infer an optimal label assignment for all components. Extensive experiments demonstrate that our method achieves significantly robust labeling results on raw 3D models from public shape repositories. Our work also contributes the first benchmark for component-wise labeling.",0.2941176471],["high resolution infrared spectrographs now available are reaching the high precision of","High precision radial velocities with GIANO spectra","summarize: Radial velocities measured from near-infrared spectra are a potentially excellent tool to search for extrasolar planets around cool or active stars. High resolution infrared spectrographs now available are reaching the high precision of visible instruments, with a constant improvement over time. GIANO is an infrared echelle spectrograph at the Telescopio Nazionale Galileo and it is a powerful tool to provide high resolution spectra for accurate RV measurements of exoplanets and for chemical and dynamical studies of stellar or extragalactic objects. No other high spectral resolution IR instrument has GIANO's capability to cover the entire NIR wavelength range in a single exposure. In this paper we describe the ensemble of procedures that we have developed to measure high precision RVs on GIANO spectra acquired during the Science Verification run, using the telluric lines as wavelength reference. We used the Cross Correlation Function method to determine the velocity for both the star and the telluric lines. For this purpose, we constructed two suitable digital masks that include about 2000 stellar lines, and a similar number of telluric lines. The method is applied to various targets with different spectral type, from K2V to M8 stars. We reached different precisions mainly depending on the H -magnitudes: for H ~ 5 we obtain an rms scatter of ~ 10 m s-1, while for H ~ 9 the standard deviation increases to ~ 50 - 80 m s-1. The corresponding theoretical error expectations are ~4 m s-1 and 30 m s-1, respectively. Finally we provide the RVs measured with our procedure for the targets observed during GIANO Science Verification.",0.0666666667],["the attenuation theory assumes the UCAs oscillate linearly at sufficiently low","Theoretical Estimation of Attenuation Coefficient of Resonant Ultrasound Contrast Agents","summarize: Acoustic characterization of ultrasound contrast agents relies on the attenuation theory that assumes the UCAs oscillate linearly at sufficiently low excitation pressures. Effective shell parameters of the UCAs can be estimated by fitting a theoretical attenuation curve to experimentally measured attenuation data. Depending on the excitation frequency and properties of the shell, however, an UCA may oscillate nonlinearly even at sufficiently low excitation pressures violating the assumption in the linear attenuation theory. Notably, the concern on the estimation of the attenuation coefficient of a microbubble at resonance using linearized approximation has long been addressed. In this article, we investigated the attenuation phenomenon through analyzing the energy dissipation of a single UCA and propagating waves in an UCA suspension, both of which employed a nonlinear Rayleigh-Plesset equation. Analytical formulae capable of estimating the attenuation coefficient due to the weakly nonlinear oscillations of the UCA were obtained with a relatively rigorous mathematical analysis. The computed results that were verified by numerical simulations showed the attenuation coefficient of the UCA at resonance was pressure-dependent and could be significantly smaller than that predicted by the linear attenuation theory. Polydispersity of the UCA population enlarged the difference in the estimation of attenuation between the linear and present second-order nonlinear theories.",0.1111111111],["we present the novel approach to mathematical modeling of information processes in biosystems. it explore","Quantum-like modeling in biology with open quantum systems and instruments","summarize: We present the novel approach to mathematical modeling of information processes in biosystems. It explores the mathematical formalism and methodology of quantum theory, especially quantum measurement theory. This approach is known as and it should be distinguished from study of genuine quantum physical processes in biosystems . It is based on quantum information representation of biosystem's state and modeling its dynamics in the framework of theory of open quantum systems. This paper starts with the non-physicist friendly presentation of quantum measurement theory, from the original von Neumann formulation to modern theory of quantum instruments. Then, latter is applied to model combinations of cognitive effects and gene regulation of glucose\/lactose metabolism in Escherichia coli bacterium. The most general construction of quantum instruments is based on the scheme of indirect measurement, in that measurement apparatus plays the role of the environment for a biosystem. The biological essence of this scheme is illustrated by quantum formalization of Helmholtz sensation-perception theory. Then we move to open systems dynamics and consider quantum master equation, with concentrating on quantum Markov processes. In this framework, we model functioning of biological functions such as psychological functions and epigenetic mutation.",0.2777777778],["the modular system for shelves and coasts. the system is tailored to the coup","Modular System for Shelves and Coasts - a flexible and multi-component framework for coupled coastal ocean ecosystem modelling","summarize: Shelf and coastal sea processes extend from the atmosphere through the water column and into the sea bed. These processes are driven by physical, chemical, and biological interactions at local scales, and they are influenced by transport and cross strong spatial gradients. The linkages between domains and many different processes are not adequately described in current model systems. Their limited integration level in part reflects lacking modularity and flexibility; this shortcoming hinders the exchange of data and model components and has historically imposed supremacy of specific physical driver models. We here present the Modular System for Shelves and Coasts , a novel domain and process coupling system tailored---but not limited--- to the coupling challenges of and applications in the coastal ocean. MOSSCO builds on the existing coupling technology Earth System Modeling Framework and on the Framework for Aquatic Biogeochemical Models, thereby creating a unique level of modularity in both domain and process coupling; the new framework adds rich metadata, flexible scheduling, configurations that allow several tens of models to be coupled, and tested setups for coastal coupled applications. That way, MOSSCO addresses the technology needs of a growing marine coastal Earth System community that encompasses very different disciplines, numerical tools, and research questions.",0.1707902939],["the goal of every LMS implementation is to ensure the use of the system by instructors and students","How to Survive a Learning Management System Implementation? A Stakeholder Analysis Approach","summarize: To survive a learning management system implementation an understanding of the needs of the various stakeholders is necessary. The goal of every LMS implementation is to ensure the use of the system by instructors and students to enhance teaching and communication thereby enhancing learning outcomes of the students. If the teachers and students do not use the system, the system is useless. This research is motivated by the importance of identifying and understanding various stakeholders involved in the LMS implementation process in order to anticipate possible challenges and identify critical success factors essential for the effective implementation and adoption of a new LMS system. To this end, we define the term stakeholder. We conducted a stakeholder analysis to identify the key stakeholders in an LMS implementation process. We then analyze their goals and needs, and how they collaborate in the implementation process. The findings of this work will provide institutions of higher learning an overview of the implementation process and useful insights into the needs of the stakeholders, which will in turn ensure an increase in the level of success achieved when implementing a LMS.",0.0740740741],["13 musicians\/developer teams, a total of 61 users, needed to co-","AI Song Contest: Human-AI Co-Creation in Songwriting","summarize: Machine learning is challenging the way we make music. Although research in deep generative models has dramatically improved the capability and fluency of music models, recent work has shown that it can be challenging for humans to partner with this new class of algorithms. In this paper, we present findings on what 13 musician\/developer teams, a total of 61 users, needed when co-creating a song with AI, the challenges they faced, and how they leveraged and repurposed existing characteristics of AI to overcome some of these challenges. Many teams adopted modular approaches, such as independently running multiple smaller models that align with the musical building blocks of a song, before re-combining their results. As ML models are not easily steerable, teams also generated massive numbers of samples and curated them post-hoc, or used a range of strategies to direct the generation, or algorithmically ranked the samples. Ultimately, teams not only had to manage the flare and focus aspects of the creative process, but also juggle them with a parallel process of exploring and curating multiple ML models and outputs. These findings reflect a need to design machine learning-powered music interfaces that are more decomposable, steerable, interpretable, and adaptive, which in return will enable artists to more effectively explore how AI can extend their personal expression.",0.1333333333],["model was developed to investigate how the presence of Seyfert activity relates to their environment","Is the cluster environment quenching the Seyfert activity in elliptical and spiral galaxies?","summarize: We developed a hierarchical Bayesian model to investigate how the presence of Seyfert activity relates to their environment, herein represented by the galaxy cluster mass, ",0.2777777778],["the Hopf bifurcation is a unique feature of the multi-compartment","Oscillations in a white blood cell production model with multiple differentiation stages","summarize: In this work we prove occurrence of a super-critical Hopf bifurcation in a model of white blood cell formation structured by three maturation stages. We provide an explicit analytical expression for the bifurcation point depending on model parameters. The Hopf bifurcation is a unique feature of the multi-compartment structure as it does not exist in the corresponding two-compartment model. It appears for a parameter set different from the parameters identified for healthy hematopoiesis and requires changes in at least two cell properties. Model analysis allows identifying a range of biologically plausible parameter sets that can explain persistent oscillations of white blood cell counts observed in some hematopoietic diseases. Relating the identified parameter sets to recent experimental and clinical findings provides insights into the pathological mechanisms leading to oscillating blood cell counts.",0.2183282008],["learning speaker-specific features is vital in many applications like speaker recognition, diarization and speech","Neural Predictive Coding using Convolutional Neural Networks towards Unsupervised Learning of Speaker Characteristics","summarize: Learning speaker-specific features is vital in many applications like speaker recognition, diarization and speech recognition. This paper provides a novel approach, we term Neural Predictive Coding , to learn speaker-specific characteristics in a completely unsupervised manner from large amounts of unlabeled training data that even contain many non-speech events and multi-speaker audio streams. The NPC framework exploits the proposed short-term active-speaker stationarity hypothesis which assumes two temporally-close short speech segments belong to the same speaker, and thus a common representation that can encode the commonalities of both the segments, should capture the vocal characteristics of that speaker. We train a convolutional deep siamese network to produce speaker embeddings by learning to separate `same' vs `different' speaker pairs which are generated from an unlabeled data of audio streams. Two sets of experiments are done in different scenarios to evaluate the strength of NPC embeddings and compare with state-of-the-art in-domain supervised methods. First, two speaker identification experiments with different context lengths are performed in a scenario with comparatively limited within-speaker channel variability. NPC embeddings are found to perform the best at short duration experiment, and they provide complementary information to i-vectors for full utterance experiments. Second, a large scale speaker verification task having a wide range of within-speaker channel variability is adopted as an upper-bound experiment where comparisons are drawn with in-domain supervised methods.",0.1875],["convolutional sparse representations are a form of sparse representation with","Convolutional Dictionary Learning: A Comparative Review and New Algorithms","summarize: Convolutional sparse representations are a form of sparse representation with a dictionary that has a structure that is equivalent to convolution with a set of linear filters. While effective algorithms have recently been developed for the convolutional sparse coding problem, the corresponding dictionary learning problem is substantially more challenging. Furthermore, although a number of different approaches have been proposed, the absence of thorough comparisons between them makes it difficult to determine which of them represents the current state of the art. The present work both addresses this deficiency and proposes some new approaches that outperform existing ones in certain contexts. A thorough set of performance comparisons indicates a very wide range of performance differences among the existing and proposed methods, and clearly identifies those that are the most effective.",0.3157894737],["Thomassen conjectures triangle-free planar graphs have an exponential number of exponential","Do triangle-free planar graphs have exponentially many 3-colorings?","summarize: Thomassen conjectured that triangle-free planar graphs have an exponential number of ",0.6],["black hole candidate 1E 1740.7-2942 is one of the strongest hard X","Tandem Swift and INTEGRAL Data to Revisit the Orbital and Superorbital Periods of 1E 1740.7-2942","summarize: The black hole candidate 1E 1740.7-2942 is one of the strongest hard X-ray sources in the Galactic Center region. No counterparts in longer wavelengths have been identified for this object yet. The presence of characteristic timing signatures in the flux history of X-ray sources has been shown to be an important diagnostic tool for the properties of these systems. Using simultaneous data from NASA's Swift and ESA's INTEGRAL missions, we have found two periodic signatures at 12.61 ",0.3245003263],["ambiguity aversion and Ellsberg-type preferences have been widely studied in decision","From Ambiguity Aversion to a Generalized Expected Utility. Modeling Preferences in a Quantum Probabilistic Framework","summarize: Ambiguity and ambiguity aversion have been widely studied in decision theory and economics both at a theoretical and an experimental level. After Ellsberg's seminal studies challenging subjective expected utility theory , several approaches have been put forward to reproduce ambiguity aversion and Ellsberg-type preferences. However, Machina and other authors have pointed out some fundamental difficulties of these generalizations of SEUT to cope with some variants of Ellsberg's thought experiments, which has recently been experimentally confirmed. Starting from our quantum modeling approach to human cognition, we develop here a general probabilistic framework to model human decisions under uncertainty. We show that our quantum theoretical model faithfully represents different sets of data collected on both the Ellsberg and the Machina paradox situations, and is flexible enough to describe different subjective attitudes with respect to ambiguity. Our approach opens the way toward a quantum-based generalization of expected utility theory , where subjective probabilities depend on the state of the conceptual entity at play and its interaction with the decision-maker, while preferences between acts are determined by the maximization of this 'state-dependent expected utility'.",0.0631949026],["polarization memory effects in single-crystal CuFeO2. magnetic","Polarization memory in the nonpolar magnetic ground state of multiferroic CuFeO2","summarize: We investigate polarization memory effects in single-crystal CuFeO2, which has a magnetically-induced ferroelectric phase at low temperatures and applied B fields between 7.5 and 13 T. Following electrical poling of the ferroelectric phase, we find that the nonpolar collinear antiferromagnetic ground state at B = 0 T retains a strong memory of the polarization magnitude and direction, such that upon re-entering the ferroelectric phase a net polarization of comparable magnitude to the initial polarization is recovered in the absence of external bias. This memory effect is very robust: in pulsed-magnetic-field measurements, several pulses into the ferroelectric phase with reverse bias are required to switch the polarization direction, with significant switching only seen after the system is driven out of the ferroelectric phase and ground state either magnetically or thermally. The memory effect is also largely insensitive to the magnetoelastic domain composition, since no change in the memory effect is observed for a sample driven into a single-domain state by application of stress in the direction. On the basis of Monte Carlo simulations of the ground state spin configurations, we propose that the memory effect is due to the existence of helical domain walls within the nonpolar collinear antiferromagnetic ground state, which would retain the helicity of the polar phase for certain magnetothermal histories.",0.2117692958],["the design of an experiment can be always considered implicitly Bayesian. prior knowledge can","Bayesian design of experiments for generalised linear models and dimensional analysis with industrial and scientific application","summarize: The design of an experiment can be always be considered at least implicitly Bayesian, with prior knowledge used informally to aid decisions such as the variables to be studied and the choice of a plausible relationship between the explanatory variables and measured responses. Bayesian methods allow uncertainty in these decisions to be incorporated into design selection through prior distributions that encapsulate information available from scientific knowledge or previous experimentation. Further, a design may be explicitly tailored to the aim of the experiment through a decision-theoretic approach using an appropriate loss function. We review the area of decision-theoretic Bayesian design, with particular emphasis on recent advances in computational methods. For many problems arising in industry and science, experiments result in a discrete response that is well described by a member of the class of generalised linear models. We describe how Gaussian process emulation, commonly used in computer experiments, can play an important role in facilitating Bayesian design for realistic problems. A main focus is the combination of Gaussian process regression to approximate the expected loss with cyclic descent optimisation algorithms to allow optimal designs to be found for previously infeasible problems. We also present the first optimal design results for statistical models formed from dimensional analysis, a methodology widely employed in the engineering and physical sciences to produce parsimonious and interpretable models. Using the famous paper helicopter experiment, we show the potential for the combination of Bayesian design, generalised linear models and dimensional analysis to produce small but informative experiments.",0.3650012209],["bounded curvature path is a continuously differentiable piecewise.","The classification of homotopy classes of bounded curvature paths","summarize: A bounded curvature path is a continuously differentiable piecewise ",0.5147898598],["static analysis can predict uses of invokedynamic while also cooperating with extra rules to handle runtime","Deep Static Modeling of invokedynamic","summarize: Java 7 introduced programmable dynamic linking in the form of the invokedynamic framework. Static analysis of code containing programmable dynamic linking has often been cited as a significant source of unsoundness in the analysis of Java programs. For example, Java lambdas, introduced in Java 8, are a very popular feature, which is, however, resistant to static analysis, since it mixes invokedynamic with dynamic code generation. These techniques invalidate static analysis assumptions: programmable linking breaks reasoning about method resolution while dynamically generated code is, by definition, not available statically. In this paper, we show that a static analysis can predictively model uses of invokedynamic while also cooperating with extra rules to handle the runtime code generation of lambdas. Our approach plugs into an existing static analysis and helps eliminate all unsoundness in the handling of lambdas and generic invokedynamic uses. We evaluate our technique on a benchmark suite of our own and on third-party benchmarks, uncovering all code previously unreachable due to unsoundness, highly efficiently.",0.125],["the lid moves parallel to the cube sidewall or parallel to the diagonal plane. the","Linear instability of the lid-driven flow in a cubic cavity","summarize: Primary instability of the lid-driven flow in a cube is studied by a comprehensive linear stability approach. Two cases, in which the lid moves parallel to the cube sidewall or parallel to the diagonal plane, are considered. The SIMPLE procedure is applied for evaluation of the Krylov vectors needed for application of the Newton and Arnoldi iteration methods. The finite volume grid is gradually refined from 1003 to 2563 nodes. The computations result in the grid convergent values of the critical Reynolds number and oscillation frequency. Patterns of the most unstable perturbation are reported. Finally, some new arguments supporting the assumption that the centrifugal mechanism triggers instability in both cases are given.",0.1612903226],["constant Q contours for passive circuits form circle arcs on a coax","3D Smith chart constant quality factor semi-circles contours for positive and negative resistance circuits","summarize: The article proves first that the constant quality factor contours for passive circuits, while represented on a 2D Smith chart, form circle arcs on a coaxal circle family. Furthermore, these circle arcs represent semi-circles families in the north hemisphere while represented on a 3D Smith chart. On the contrary we show that the constant Q contours for active circuits with negative resistance form complementary circle arcs on the same family of coaxal circles in the exterior of the 2D Smith chart. Also, we find out that these constant Q contours represent complementary semi-circles in the south hemisphere while represented on the 3D Smith chart for negative resistance circuits. The constant Q - computer aided design implementation of the Q semi-circles on the 3D Smith chart is then successfully used to evaluate the quality factor variations of newly fabricated Vanadium dioxide inductors first, directly from their reflection coefficient, as the temperature is increased from room temperature to 50 degrees Celsius . Thus, a direct multi-parameter frequency dependent analysis is proposed including Q, inductance and reflection coefficient for inductors. Then, quality factor direct analysis is used for two tunnel diode small signal equivalent circuits analysis, allowing for the first time the Q and input impedance direct analysis on Smith chart representation of a circuit, including negative resistance",0.5346200368],["detecting user community in such heterogeneous graphs is an essential task to uncover user","Detecting User Community in Sparse Domain via Cross-Graph Pairwise Learning","summarize: Cyberspace hosts abundant interactions between users and different kinds of objects, and their relations are often encapsulated as bipartite graphs. Detecting user community in such heterogeneous graphs is an essential task to uncover user information needs and to further enhance recommendation performance. While several main cyber domains carrying high-quality graphs, unfortunately, most others can be quite sparse. However, as users may appear in multiple domains , their high-quality activities in the main domains can supply community detection in the sparse ones, e.g., user behaviors on Google can help thousands of applications to locate his\/her local community when s\/he uses Google ID to login those applications. In this paper, our model, Pairwise Cross-graph Community Detection , is proposed to cope with the sparse graph problem by involving external graph knowledge to learn user pairwise community closeness instead of detecting direct communities. Particularly in our model, to avoid taking excessive propagated information, a two-level filtering module is utilized to select the most informative connections through both community and node level filters. Subsequently, a Community Recurrent Unit is designed to estimate pairwise user community closeness. Extensive experiments on two real-world graph datasets validate our model against several strong alternatives. Supplementary experiments also validate its robustness on graphs with varied sparsity scales.",0.1764705882],["we provide a process on the space of coalescing cadlag stable paths.","A Construction of the Stable Web","summarize: We provide a process on the space of coalescing cadlag stable paths and show convergence in the appropriate topology for coalescing stable random walks on the integer lattice.",0.2222222222],["deterministic incremental aggregated gradient method is based on a deterministic order","On the Convergence Rate of Incremental Aggregated Gradient Algorithms","summarize: Motivated by applications to distributed optimization over networks and large-scale data processing in machine learning, we analyze the deterministic incremental aggregated gradient method for minimizing a finite sum of smooth functions where the sum is strongly convex. This method processes the functions one at a time in a deterministic order and incorporates a memory of previous gradient values to accelerate convergence. Empirically it performs well in practice; however, no theoretical analysis with explicit rate results was previously given in the literature to our knowledge, in particular most of the recent efforts concentrated on the randomized versions. In this paper, we show that this deterministic algorithm has global linear convergence and characterize the convergence rate. We also consider an aggregated method with momentum and demonstrate its linear convergence. Our proofs rely on a careful choice of a Lyapunov function that offers insight into the algorithm's behavior and simplifies the proofs considerably.",0.25],["a clique in a link stream is defined as a set of nodes and","Enumerating maximal cliques in link streams with durations","summarize: Link streams model interactions over time, and a clique in a link stream is defined as a set of nodes and a time interval such that all pairs of nodes in this set interact permanently during this time interval. This notion was introduced recently in the case where interactions are instantaneous. We generalize it to the case of interactions with durations and show that the instantaneous case actually is a particular case of the case with durations. We propose an algorithm to detect maximal cliques that improves our previous one for instantaneous link streams, and performs better than the state of the art algorithms in several cases of interest.",0.6774193548],["chapter presents foundations of computing paradigms for realizing emerging IoT applications.","Internet of Things and New Computing Paradigms","summarize: The chapter presents foundations of computing paradigms for realizing emerging IoT applications, especially fog and edge computing, their background, characteristics, architectures and open challenges.",0.0909090909],["we define an assembly map in relative geometric geometric. we define an assembly map in relative geometric","Relative geometric assembly and mapping cones, Part I: The geometric model and applications","summarize: Inspired by an analytic construction of Chang, Weinberger and Yu, we define an assembly map in relative geometric ",0.4],["the method is very simple and intuitive. it is based on sampling a function at","Global Complex Roots and Poles Finding Algorithm Based on Phase Analysis for Propagation and Radiation Problems","summarize: A flexible and effective algorithm for complex roots and poles finding is presented. A wide class of analytic functions can be analyzed, and any arbitrarily shaped search region can be considered. The method is very simple and intuitive. It is based on sampling a function at the nodes of a regular mesh, and on the analysis of the function phase. As a result, a set of candidate regions is created and then the roots\/poles are verified using a discretized Cauchy's argument principle. The accuracy of the results can be improved by the application of a self-adaptive mesh. The effectiveness of the presented technique is supported by numerical tests involving different types of structures, where electromagnetic waves are guided and radiated. The results are verified, and the computational efficiency of the method is examined.",0.4880906009],["in the present contribution, we introduce a wireless optical communication-based system architecture which is shown","Optical Wireless Cochlear Implants","summarize: In the present contribution, we introduce a wireless optical communication-based system architecture which is shown to significantly improve the reliability and the spectral and power efficiency of the transcutaneous link in cochlear implants . We refer to the proposed system as optical wireless cochlear implant .In order to provide a quantified understanding of its design parameters, we establish a theoretical framework that takes into account the channel particularities, the integration area of the internal unit, the transceivers misalignment, and the characteristics of the optical units. To this end, we derive explicit expressions for the corresponding average signal-to-noise-ratio, outage probability, ergodic spectral efficiency and capacity of the transcutaneous optical link . These expressions are subsequently used to assess the dependence of the TOL's communication quality on the transceivers design parameters and the corresponding channels characteristics. The offered analytic results are corroborated with respective results from Monte Carlo simulations. Our findings reveal that OWCI is a particularly promising architecture that drastically increases the reliability and effectiveness of the CI TOL, whilst it requires considerably lower transmit power compared to the corresponding widely-used radio frequency solution.",0.15],["we study the leading coefficient in the formula asymptotical formula. we study","Generic asymptotics of resonance counting function for Schr\\odinger point interactions","summarize: We study the leading coefficient in the asymptotical formula ",0.0952380952],["Paul ion trap design with integrated optical fibre cavity. the trap is designed to be","Precise positioning of an ion in an integrated Paul trap-cavity system using radiofrequency signals","summarize: We report a novel miniature Paul ion trap design with an integrated optical fibre cavity which can serve as a building block for a fibre-linked quantum network. In such cavity quantum electrodynamic set-ups, the optimal coupling of the ions to the cavity mode is of vital importance and this is achieved by moving the ion relative to the cavity mode. The trap presented herein features an endcap-style design complemented with extra electrodes on which additional radiofrequency voltages are applied to fully control the pseudopotential minimum in three dimensions. This method lifts the need to use three-dimensional translation stages for moving the fibre cavity with respect to the ion and achieves high integrability, mechanical rigidity and scalability. Not based on modifying the capacitive load of the trap, this method leads to precise control of the pseudopotential minimum allowing the ion to be moved with precisions limited only by the ion's position spread. We demonstrate this by coupling the ion to the fibre cavity and probing the cavity mode profile.",0.3888888889],["deep neural networks can beat many man-made algorithms. deep neural networks are capable of learning","Two Applications of Deep Learning in the Physical Layer of Communication Systems","summarize: Deep learning has proved itself to be a powerful tool to develop data-driven signal processing algorithms for challenging engineering problems. By learning the key features and characteristics of the input signals, instead of requiring a human to first identify and model them, learned algorithms can beat many man-made algorithms. In particular, deep neural networks are capable of learning the complicated features in nature-made signals, such as photos and audio recordings, and use them for classification and decision making. The situation is rather different in communication systems, where the information signals are man-made, the propagation channels are relatively easy to model, and we know how to operate close to the Shannon capacity limits. Does this mean that there is no role for deep learning in the development of future communication systems?",0.0476190476],["phantom and patient studies were performed to test the automated technique. the method automatically","Automated target tracking in kilovoltage images using dynamic templates of fiducial marker clusters","summarize: Purpose: Implanted fiducial markers are often used in radiotherapy to facilitate accurate visualization and localization of tumors. Typically, such markers are used to aid daily patient positioning and to verify the target's position during treatment. This work introduces a novel, automated method for identifying fiducial markers in planar x-ray imaging. Methods: In brief, the method consists of automated filtration and reconstruction steps that generate 3D templates of marker positions. The normalized cross-correlation was the used to identify fiducial markers in projection images. To quantify the accuracy of the technique, a phantom study was performed. 75 pre-treatment CBCT scans of 15 pancreatic cancer patients were analyzed to test the automated technique under real life conditions, including several challenging scenarios for tracking fiducial markers. Results: In phantom and patient studies, the method automatically tracked visible marker clusters in 100% of projection images. For scans in which a phantom exhibited 0D, 1D, and 3D motion, the automated technique showed median errors of 39 ",0.0526315789],["the resonant frequency analysis is done numerically through the determination of the eigen","Resonant frequency analysis of dental implants","summarize: Dental implant stability influences the decision on the determination of the duration between implant insertion and loading, This work investigates the resonant frequency analysis by means of a numerical model. The investigation is done numerically through the determination of the eigenfrequencies and performing a steady state response analyses using a commercial finite element package. A peri-implant interface, of simultaneously varying stiffness and layer thickness is introduced in the numerical 3D model in order to probe the sensitivity of the eigenfrequencies and steady state response to an evolving weakened layer, in an attempt to identify the bone reconstruction around the implant. For the first two modes, the resonant frequency is somewhat insensitive to the healing process, unless the weakened layer is rather large and compliant, like in the very early stages of the implantation. A Normalized Healing Factor is devised in the spirit of the Implant Stability Quotient, which can identify the healing process especially at the early stages after implantation. The sensitivity of the RFA to changes of mechanical properties of periprosthetic bone tissue seems relatively weak. Another indicator considering the amplitude as well as the resonance frequency might be more adapted to bone healing estimations. However, these results need to be verified experimentally as well as clinically.",0.2],["entropy solutions are uniformly bounded with respect to space and time variables","Large time behavior of entropy solutions to one-dimensional unipolar hydrodynamic model for semiconductor devices","summarize: We are concerned with the global existence and large time behavior of entropy solutions to the one dimensional unipolar hydrodynamic model for semiconductors in the form of Euler-Poisson equations in a bounded interval. In this paper, we first prove the global existence of entropy solution by vanishing viscosity and compensated compactness framework. In particular, the solutions are uniformly bounded with respect to space and time variables by introducing modified Riemann invariants and the theory of invariant region. Based on the uniform estimates of density, we further show that the entropy solution converges to the corresponding unique stationary solution exponentially in time. No any smallness condition is assumed on the initial data and doping profile. Moreover, the novelty in this paper is about the unform bound with respect to time for the weak solutions of the isentropic Euler-Possion system.",0.282160575],["initialization of weights in backpropagation neural net is heavily affected by initialization of","A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition","summarize: Convergence rate of training algorithms for neural networks is heavily affected by initialization of weights. In this paper, an original algorithm for initialization of weights in backpropagation neural net is presented with application to character recognition. The initialization method is mainly based on a customization of the Kalman filter, translating it into Bayesian statistics terms. A metrological approach is used in this context considering weights as measurements modeled by mutually dependent normal random variables. The algorithm performance is demonstrated by reporting and discussing results of simulation trials. Results are compared with random weights initialization and other methods. The proposed method shows an improved convergence rate for the backpropagation training algorithm.",0.4178540304],["text mining network-based approach extrapolates information about characters and places. we extract the","Biblical names' relationships in the Gospel of Matthew, Mark, Luke, John and Acts of Apostles","summarize: In this paper we extrapolate the information about Bible's characters and places, and their interrelationships, by using text mining network-based approach. We study the narrative structure of the WEB version of 5 books: the Gospel of Matthew, Mark, Luke, John and Acts of the Apostles. The main focus is the protagonists' names interrelationships in an analytical way, namely using various network-based methods and descriptors. This corpus is processed for creating a network: we download the names of people and places from Wikipedia's list of biblical names, then we look for their co-occurrences in each verse and, at the end of this process, we get N co-occurred names. The strength of the link between two names is defined as the sum of the times that these occur together in all the verses, in this way we obtain 5 adjacency matrices of N by N couples of names. After this pre-processing phase, for each of the 5 analysed books we calculate the main network centrality measures , the network vulnerability and we run the Community Detection algorithm to highlight the role of Messiah inside the overall networks and his groups . We have found that the proposed approach is suitable for highlighting the structures of the names co-occurrences. The found frameworks' structures are useful for interpreting the characters' plots under a structural point of view.",0.1319082953],["a network of two nodes separated by a noisy channel with two-sided state information","Strong coordination of signals and actions over noisy channels with two-sided state information","summarize: We consider a network of two nodes separated by a noisy channel with two-sided state information, in which the input and output signals have to be coordinated with the source and its reconstruction. In the case of non-causal encoding and decoding, we propose a joint source-channel coding scheme and develop inner and outer bounds for the strong coordination region. While the inner and outer bounds do not match in general, we provide a complete characterization of the strong coordination region in three particular cases: i) when the channel is perfect; ii) when the decoder is lossless; and iii) when the random variables of the channel are independent from the random variables of the source. Through the study of these special cases, we prove that the separation principle does not hold for joint source-channel strong coordination. Finally, in the absence of state information, we show that polar codes achieve the best known inner bound for the strong coordination region, which therefore offers a constructive alternative to random binning and coding proofs.",0.7857142857],["spectral signature observed at 682 cm.","Study of Titan's fall southern stratospheric polar cloud composition with Cassini\/CIRS: detection of benzene ice","summarize: We report the detection of a spectral signature observed at 682 cm",0.0318757372],["deep Reinforcement Learning has been applied to multi-agent problems. no previous work","Deep Multi-Agent Reinforcement Learning with Discrete-Continuous Hybrid Action Spaces","summarize: Deep Reinforcement Learning has been applied to address a variety of cooperative multi-agent problems with either discrete action spaces or continuous action spaces. However, to the best of our knowledge, no previous work has ever succeeded in applying DRL to multi-agent problems with discrete-continuous hybrid action spaces which is very common in practice. Our work fills this gap by proposing two novel algorithms: Deep Multi-Agent Parameterized Q-Networks and Deep Multi-Agent Hierarchical Hybrid Q-Networks . We follow the centralized training but decentralized execution paradigm: different levels of communication between different agents are used to facilitate the training process, while each agent executes its policy independently based on local observations during execution. Our empirical results on several challenging tasks show that both Deep MAPQN and Deep MAHHQN are effective and significantly outperform existing independent deep parameterized Q-learning method.",0.1666666667],["multiplication is arithmetic operation that is often encountered in microprocessing and digital","Indicating Asynchronous Array Multipliers","summarize: Multiplication is an important arithmetic operation that is frequently encountered in microprocessing and digital signal processing applications, and multiplication is physically realized using a multiplier. This paper discusses the physical implementation of many indicating asynchronous array multipliers, which are inherently elastic and modular and are robust to timing, process and parametric variations. We consider the physical realization of many indicating asynchronous array multipliers using a 32\/28nm CMOS technology. The weak-indication array multipliers comprise strong-indication or weak-indication full adders, and strong-indication 2-input AND functions to realize the partial products. The multipliers were synthesized in a semi-custom ASIC design style using standard library cells including a custom-designed 2-input C-element. 4x4 and 8x8 multiplication operations were considered for the physical implementations. The 4-phase return-to-zero and the 4-phase return-to-one handshake protocols were utilized for data communication, and the delay-insensitive dual-rail code was used for data encoding. Among several weak-indication array multipliers, a weak-indication array multiplier utilizing a biased weak-indication full adder and the strong-indication 2-input AND function is found to have reduced cycle time and power-cycle time product with respect to RTZ and RTO handshaking for 4x4 and 8x8 multiplications. Further, the 4-phase RTO handshaking is found to be preferable to the 4-phase RTZ handshaking for achieving enhanced optimizations of the design metrics.",0.0666666667],["multi-scroll circuit is complex due to irregular breakpoints and slopes.","Generating Multi-Scroll Chua's Attractors via Simplified Piecewise-Linear Chua's Diode","summarize: High implementation complexity of multi-scroll circuit is a bottleneck problem in real chaos-based communication. Especially, in multi-scroll Chua's circuit, the simplified implementation of piecewise-linear resistors with multiple segments is difficult due to their intricate irregular breakpoints and slopes. To solve the challenge, this paper presents a systematic scheme for synthesizing a Chua's diode with multi-segment piecewise-linearity, which is achieved by cascading even-numbered passive nonlinear resistors with odd-numbered ones via a negative impedance converter. The traditional voltage mode op-amps are used to implement nonlinear resistors. As no extra DC bias voltage is employed, the scheme can be implemented by much simpler circuits. The voltage-current characteristics of the obtained Chua's diode are analyzed theoretically and verified by numerical simulations. Using the Chua's diode and a second-order active Sallen-Key high-pass filter, a new inductor-free Chua's circuit is then constructed to generate multi-scroll chaotic attractors. Different number of scrolls can be generated by changing the number of passive nonlinear resistor cells or adjusting two coupling parameters. Besides, the system can be scaled by using different power supplies, satisfying the low-voltage low-power requirement of integrated circuit design. The circuit simulations and hardware experiments both confirmed the feasibility of the designed system.",0.2],["research works across multiple social networks attract more and more attention from researchers. community detection is an","Community Detection Across Multiple Social Networks based on Overlapping Users","summarize: With the rapid development of Internet technology, online social networks have got fast development and become increasingly popular. Meanwhile, the research works across multiple social networks attract more and more attention from researchers, and community detection is an important one across OSNs for online security problems, such as the user behavior analysis and abnormal community discovery. In this paper, a community detection method is proposed across multiple social networks based on overlapping users. First, the concept of overlapping users is defined, then an algorithm CMN NMF is designed to discover the stub communities from overlapping users based on the social relevance. After that, we extend each stub community in different social networks by adding the users with strong similarity, and in the end different communities are excavated out across networks. Experimental results show the advantage on effectiveness of our method over other methods under real data sets.",0.0454545455],["in this paper, we extend bottleneck stability to setting of one dimensional constructible persistences","Bottleneck Stability for Generalized Persistence Diagrams","summarize: In this paper, we extend bottleneck stability to the setting of one dimensional constructible persistences module valued in any small abelian category.",0.0],["in this paper, we propose to interpolate between these two extremes. we propose","Sparse matrix factorizations for fast linear solvers with application to Laplacian systems","summarize: In solving a linear system with iterative methods, one is usually confronted with the dilemma of having to choose between cheap, inefficient iterates over sparse search directions , or expensive iterates in well-chosen search directions . In this paper, we propose to interpolate between these two extremes, and show how to perform cheap iterations along non-sparse search directions, provided that these directions can be extracted from a new kind of sparse factorization. For example, if the search directions are the columns of a hierarchical matrix, then the cost of each iteration is typically logarithmic in the number of variables. Using some graph-theoretical results on low-stretch spanning trees, we deduce as a special case a nearly-linear time algorithm to approximate the minimal norm solution of a linear system ",0.1],["the instrument onboard the InSight mission to Mars is the critical instrument. it is","Evaluating the wind-induced mechanical noise on the InSight seismometers","summarize: The SEIS instrument onboard the InSight mission to Mars is the critical instrument for determining the interior structure of Mars, the current level of tectonic activity and the meteorite flux. Meeting the performance requirements of the SEIS instrument is vital to successfully achieve these mission objectives. Here we analyse in-situ wind measurements from previous Mars space missions to understand the wind environment that we are likely to encounter on Mars, and then we use an elastic ground deformation model to evaluate the mechanical noise contributions on the SEIS instrument due to the interaction between the Martian winds and the InSight lander. Lander mechanical noise maps that will be used to select the best deployment site for SEIS once the InSight lander arrives on Mars are also presented. We find the lander mechanical noise may be a detectable signal on the InSight seismometers. However, for the baseline SEIS deployment position, the noise is expected to be below the total noise requirement >97% of the time and is, therefore, not expected to endanger the InSight mission objectives.",0.4074074074],["parallelization program, called MPI_XSTAR, has been developed and implemented in the C","MPI_XSTAR: MPI-based Parallelization of the XSTAR Photoionization Program","summarize: We describe a program for the parallel implementation of multiple runs of XSTAR, a photoionization code that is used to predict the physical properties of an ionized gas from its emission and\/or absorption lines. The parallelization program, called MPI_XSTAR, has been developed and implemented in the C++ language by using the Message Passing Interface protocol, a conventional standard of parallel computing. We have benchmarked parallel multiprocessing executions of XSTAR, using MPI_XSTAR, against a serial execution of XSTAR, in terms of the parallelization speedup and the computing resource efficiency. Our experience indicates that the parallel execution runs significantly faster than the serial execution, however, the efficiency in terms of the computing resource usage decreases with increasing the number of processors used in the parallel computing.",0.0833333333],["a new interpretation of the WAY theorem is based on a relation","A relational perspective on the Wigner-Araki-Yanase theorem","summarize: We present a novel interpretation of the Wigner-Araki-Yanase theorem based on a relational view of quantum mechanics. Several models are analysed in detail, backed up by general considerations, which serve to illustrate that the moral of the WAY theorem may be that in the presence of symmetry, a measuring apparatus must fulfil the dual purpose of both reflecting the statistical behaviour of the system under investigation, and acting as a physical reference system serving to define those quantities which must be understood as relative.",0.6956521739],["playback detection systems use a playback detector to filter out playback attacks. the","Transforming acoustic characteristics to deceive playback spoofing countermeasures of speaker verification systems","summarize: Automatic speaker verification systems use a playback detector to filter out playback attacks and ensure verification reliability. Since current playback detection models are almost always trained using genuine and played-back speech, it may be possible to degrade their performance by transforming the acoustic characteristics of the played-back speech close to that of the genuine speech. One way to do this is to enhance speech stolen from the target speaker before playback. We tested the effectiveness of a playback attack using this method by using the speech enhancement generative adversarial network to transform acoustic characteristics. Experimental results showed that use of this enhanced stolen speech method significantly increases the equal error rates for the baseline used in the ASVspoof 2017 challenge and for a light convolutional neural network-based method. The results also showed that its use degrades the performance of a Gaussian mixture model-universal background model-based ASV system. This type of attack is thus an urgent problem needing to be solved.",0.5],["we introduce a novel RGB-D patch descriptor designed for SLAM reconstruction","PlaneMatch: Patch Coplanarity Prediction for Robust RGB-D Reconstruction","summarize: We introduce a novel RGB-D patch descriptor designed for detecting coplanar surfaces in SLAM reconstruction. The core of our method is a deep convolutional neural net that takes in RGB, depth, and normal information of a planar patch in an image and outputs a descriptor that can be used to find coplanar patches from other images.We train the network on 10 million triplets of coplanar and non-coplanar patches, and evaluate on a new coplanarity benchmark created from commodity RGB-D scans. Experiments show that our learned descriptor outperforms alternatives extended for this new task by a significant margin. In addition, we demonstrate the benefits of coplanarity matching in a robust RGBD reconstruction formulation.We find that coplanarity constraints detected with our method are sufficient to get reconstruction results comparable to state-of-the-art frameworks on most scenes, but outperform other methods on standard benchmarks when combined with a simple keypoint method.",0.3333333333],["entanglement and resonance energy transfer between two-level quantum emitters are typically limited to","Resonance Energy Transfer and Quantum Entanglement Mediated by Epsilon-Near-Zero and Other Plasmonic Waveguide Systems","summarize: The entanglement and resonance energy transfer between two-level quantum emitters are typically limited to sub-wavelength distances due to the inherently short-range nature of the dipole-dipole interactions. Moreover, the entanglement of quantum systems is hard to preserve for a long time period due to decoherence and dephasing mainly caused by radiative and nonradiative losses. In this work, we outperform the aforementioned limitations by presenting efficient long-range inter-emitter entanglement and large enhancement of resonance energy transfer between two optical qubits mediated by epsilon-near-zero and other plasmonic waveguide types, such as V-shaped grooves and cylindrical nanorods. More importantly, we explicitly demonstrate that the ENZ waveguide resonant energy transfer and entanglement performance drastically outperforms the other waveguide systems. Only the excited ENZ mode has an infinite phase velocity combined with a strong and homogeneous electric field distribution, which leads to a giant energy transfer and efficient entanglement independent to the emitters separation distances and nanoscale positions in the ENZ nanowaveguide, an advantageous feature that can potentially accommodate multi-qubit entanglement. Moreover, the transient entanglement can be further improved and become almost independent of the detrimental decoherence effect when an optically active medium is embedded inside the ENZ waveguide. We also present that efficient steady-state entanglement can be achieved by using a coherent external pumping scheme. Finally, we report a practical way to detect the steady-state entanglement by computing the second-order correlation function. The presented findings stress the importance of plasmonic ENZ waveguides in the design of the envisioned on-chip quantum communication and information processing plasmonic nanodevices.",0.0712277753],["new theory of massive gravity with only two propagating degrees of freedom. we will perform","Minimal theory of massive gravity","summarize: We propose a new theory of massive gravity with only two propagating degrees of freedom. After defining the theory in the unitary gauge in the vielbein language, we shall perform a Hamiltonian analysis to count the number of physical degrees of freedom, and then study some phenomenologies. While the homogeneous and isotropic background cosmology and the tensor linear perturbations around it are described by exactly the same equations as those in the de Rham-Gabadadze-Tolley massive gravity, the scalar and vector gravitational degrees of freedom are absent in the new theory at the fully nonlinear level. Hence the new theory provides a stable nonlinear completion of the self-accelerating cosmological solution that was originally found in the dRGT theory.",0.2941176471],["a new approach to search for first order invariants of rational second order ordinary differential equation","Dealing with Rational Second Order Ordinary Differential Equations where both Darboux and Lie Find It Difficult: The ","summarize: Here we present a new approach to search for first order invariants of rational second order ordinary differential equations. This method is an alternative to the Darbouxian and symmetry approaches. Our procedure can succeed in many cases where these two approaches fail. We also present here a Maple implementation of the theoretical results and methods, hereby introduced, in a computational package -- . The package is designed, apart from materializing the algorithms presented, to provide a set of tools to allow the user to analyse the intermediary steps of the process.",0.2521419722],["state-of-the-art approaches perform semantic segmentation or refine object bounding boxes obtained","Instance Segmentation of Biomedical Images with an Object-aware Embedding Learned with Local Constraints","summarize: Automatic instance segmentation is a problem that occurs in many biomedical applications. State-of-the-art approaches either perform semantic segmentation or refine object bounding boxes obtained from detection methods. Both suffer from crowded objects to varying degrees, merging adjacent objects or suppressing a valid object. In this work, we assign an embedding vector to each pixel through a deep neural network. The network is trained to output embedding vectors of similar directions for pixels from the same object, while adjacent objects are orthogonal in the embedding space, which effectively avoids the fusion of objects in a crowd. Our method yields state-of-the-art results even with a light-weighted backbone network on a cell segmentation and a leaf segmentation data set . The code and model weights are public available.",0.0],["we have developed ad-hoc social network application for android devices. this is","An ad-hoc social network generation approach","summarize: The use of social networks is still confined to infrastructure-based networks such as the Internet. However, many situations may require the implementation and rapid deployment of an ad-hoc application for disseminating information: we call this type of application, Ad-hoc Social Network. These applications are necessarily distributed, deployable on mobile units, etc. They therefore inevitably share the same characteristics as those inherent in ad-hoc mobile networks and make them good candidates for their deployment. In this paper, by using techniques from the field of generative programming, we propose an approach to produce environments for generating such applications from their specifications in a domain-specific language. By applying this approach, we have developed SMGenerator, an environment for generating mobile ad-hoc social network applications for Android devices. Moreover by using SMGenerator, we easily generated the ConfInfo application: an ad-hoc social network application for disseminating information to participants in a scientific manifestation.",0.2307692308],["the choice of turbulence model, real gas model, and chemical kinetics model","Numerical Investigation of Coaxial GCH4\/LOx Combustion at Supercritical Pressures","summarize: This article aims to numerically investigate the combustion phenomenon of coaxial gaseous CH4 LOx at supercritical pressures. The choice of turbulence model, real gas model, and chemical kinetics model are the critical parameters in numerical simulations of cryogenic combustion at high pressure. At this supercritical operating pressure, the ideal gas law does not remain valid for such cases. Therefore, we have systematically carried out a comparative study to analyze the importance of real gas models, turbulence parameters, and chemical kinetics at such conditions. The comparison of real gas models with the NIST database reveals better conformity of SRK ) model predictions with the database. Further, the computed results indicate that the Standard k-e turbulence model with modified constant captures the better flame shape and temperature peak position compared to other RANS based turbulence models while invoking the non-premixed steady b-PDF flamelet model for simulating the combustion process. Furthermore, a comparative study comparing two different chemical kinetics models indicates that the reduced Jones Lindstedt mechanism can accurately predict the flame characteristics with the least computational cost. Finally, we have studied the effect of chamber pressure and LOx inlet temperature on the flame characteristics. The flame characteristics exhibit a strong sensitivity towards the chamber pressure due to the weakening of the pseudo-boiling effect with an increase in pressure. As a consequence of lower turbulent rates of energy and mass transfer through the transcritical mixing layer, the flame spreading becomes narrower at elevated pressure and temperature, thereby yielding an increased flame length at transcritical conditions.",0.0625],["moving active particle is observed to be surrounded by localized topological defects. such","Dressed Active Particles in Spherical Crystals","summarize: We investigate the dynamics of an active particle in two-dimensional spherical crystals, which provide an ideal environment to illustrate the interplay of active particle and crystallographic defects. A moving active particle is observed to be surrounded by localized topological defects, becoming a dressed active particle. Such a physical picture characterizes both the lattice distortion around the moving particle and the healing of the distorted lattice in its trajectory. We find that the dynamical behaviors of an active particle in both random and ballistic motions uniformly conform to this featured scenario, whether the particle is initially a defect or not. We further observe that the defect pattern around a dressed ballistic active particle randomly oscillates between two well-defined wing-like defect motifs regardless of its speed. The established physical picture of dressed active particles in this work partially deciphers the complexity of the intriguing nonequilibrium behaviors in active crystals, and opens the promising possibility of introducing the activity to engineer defects, which has strong connections with the design of materials.",0.0],["we present an approach to tackle the speaker recognition problem using Triplet Neural Networks.","Latent space representation for multi-target speaker detection and identification with a sparse dataset using Triplet neural networks","summarize: We present an approach to tackle the speaker recognition problem using Triplet Neural Networks. Currently, the ",0.2882563382],["self-dual gravity is a diffeomorphism invariant theory in","Self-Dual Gravity","summarize: Self-dual gravity is a diffeomorphism invariant theory in four dimensions that describes two propagating polarisations of the graviton and has a negative mass dimension coupling constant. Nevertheless, this theory is not only renormalisable but quantum finite, as we explain. We also collect various facts about self-dual gravity that are scattered across the literature.",0.1428571429],["a quadratic objective function is used to formulate calibration as a quadratic program","Certifiably Globally Optimal Extrinsic Calibration from Per-Sensor Egomotion","summarize: We present a certifiably globally optimal algorithm for determining the extrinsic calibration between two sensors that are capable of producing independent egomotion estimates. This problem has been previously solved using a variety of techniques, including local optimization approaches that have no formal global optimality guarantees. We use a quadratic objective function to formulate calibration as a quadratically constrained quadratic program . By leveraging recent advances in the optimization of QCQPs, we are able to use existing semidefinite program solvers to obtain a certifiably global optimum via the Lagrangian dual problem. Our problem formulation can be globally optimized by existing general-purpose solvers in less than a second, regardless of the number of measurements available and the noise level. This enables a variety of robotic platforms to rapidly and robustly compute and certify a globally optimal set of calibration parameters without a prior estimate or operator intervention. We compare the performance of our approach with a local solver on extensive simulations and multiple real datasets. Finally, we present necessary observability conditions that connect our approach to recent theoretical results and analytically support the empirical performance of our system.",0.2857142857],["the Schrodinger equation violates local causality. it causes instantan","Nonlocality and local causality in the Schr\\odinger Equation with time-dependent boundary conditions","summarize: We investigate the nonlocal dynamics of a single particle placed in an infinite well with moving walls. It is shown that in this situation, the Schr\\odinger equation violates local causality by causing instantaneous changes in the probability current everywhere inside the well. This violation is formalized by designing a gedanken faster-than-light communication device which uses an ensemble of long narrow cavities and weak measurements to resolve the weak value of the momentum far away from the movable wall. Our system is free from the usual features causing nonphysical violations of local causality when using the SE, such as instantaneous changes in potentials or states involving arbitrarily high energies or velocities. We explore in detail several possible artifacts that could account for the failure of the SE to respect local causality for systems involving time-dependent boundary conditions.",0.3582656553],["phylogeny is the field of modelling the temporal discrete dynamics of speciation","Modelling trait dependent speciation with Approximate Bayesian Computation","summarize: Phylogeny is the field of modelling the temporal discrete dynamics of speciation. Complex models can nowadays be studied using the Approximate Bayesian Computation approach which avoids likelihood calculations. The field's progression is hampered by the lack of robust software to estimate the numerous parameters of the speciation process. In this work we present an R package, pcmabc, based on Approximate Bayesian Computations, that implements three novel phylogenetic algorithms for trait-dependent speciation modelling. Our phylogenetic comparative methodology takes into account both the simulated traits and phylogeny, attempting to estimate the parameters of the processes generating the phenotype and the trait. The user is not restricted to a predefined set of models and can specify a variety of evolutionary and branching models. We illustrate the software with a simulation-reestimation study focused around the branching Ornstein-Uhlenbeck process, where the branching rate depends non-linearly on the value of the driving Ornstein-Uhlenbeck process. Included in this work is a tutorial on how to use the software.",0.0588235294],["the second law is consistent with the second law.","Thermodynamical consistency of the Dual Phase Lag heat conduction equation","summarize: Dual phase lag equation for heat conduction is analyzed from the point of view of non-equilibrium thermodynamics. Its first order Taylor series expansion is consistent with the second law as long as the two relaxation times are not negative.",0.1677823719],["the interlayer vdW coupling determines the properties of 2D multi-layer","Polytypism and Unexpected Strong Interlayer Coupling of two-Dimensional Layered ReS2","summarize: The anisotropic two-dimensional van der Waals layered materials, with both scientific interest and potential application, have one more dimension to tune the properties than the isotropic 2D materials. The interlayer vdW coupling determines the properties of 2D multi-layer materials by varying stacking orders. As an important representative anisotropic 2D materials, multilayer rhenium disulfide was expected to be random stacking and lack of interlayer coupling. Here, we demonstrate two stable stacking orders of N layer ReS2 from ultralow-frequency and high-frequency Raman spectroscopy, photoluminescence spectroscopy and first-principles density functional theory calculation. Two interlayer shear modes are observed in aa-stacked NL-ReS2 while only one interlayer shear mode appears in a-b-stacked NL-ReS2, suggesting anisotropic-like and isotropic-like stacking orders in aa- and a-b-stacked NL-ReS2, respectively. The frequency of the interlayer shear and breathing modes reveals unexpected strong interlayer coupling in aa- and a-b-NL-ReS2, the force constants of which are 55-90% to those of multilayer MoS2. The observation of strong interlayer coupling and polytypism in multi-layer ReS2 stimulate future studies on the structure, electronic and optical properties of other 2D anisotropic materials.",0.0833333333],["initialization of weights in backpropagation neural net is heavily affected by initialization of","A Bayesian approach for initialization of weights in backpropagation neural net with application to character recognition","summarize: Convergence rate of training algorithms for neural networks is heavily affected by initialization of weights. In this paper, an original algorithm for initialization of weights in backpropagation neural net is presented with application to character recognition. The initialization method is mainly based on a customization of the Kalman filter, translating it into Bayesian statistics terms. A metrological approach is used in this context considering weights as measurements modeled by mutually dependent normal random variables. The algorithm performance is demonstrated by reporting and discussing results of simulation trials. Results are compared with random weights initialization and other methods. The proposed method shows an improved convergence rate for the backpropagation training algorithm.",0.4178540304],["a fluid flow can be described via a time-reversible equation.","Reversible viscosity and Navier--Stokes fluids","summarize: Exploring the possibility of describing a fluid flow via a time-reversible equation and its relevance for the fluctuations statistics in stationary turbulent incompressible Navier-Stokes flows.",0.2631578947],["continuous delivery is becoming popular in application software development. these domains require thorough analysis regarding safety","Keeping Continuous Deliveries Safe","summarize: Allowing swift release cycles, Continuous Delivery has become popular in application software development and is starting to be applied in safety-critical domains such as the automotive industry. These domains require thorough analysis regarding safety constraints, which can be achieved by formal verification and the execution of safety tests resulting from a safety analysis on the product. With continuous delivery in place, such tests need to be executed with every build to ensure the latest software still fulfills all safety requirements. Even more though, the safety analysis has to be updated with every change to ensure the safety test suite is still up-to-date. We thus propose that a safety analysis should be treated no differently from other deliverables such as source-code and dependencies, formulate guidelines on how to achieve this and advert areas where future research is needed.",0.0952380952],["image forensic plays a crucial role in both criminal investigations and civil litigation. there are","A Survey of Machine Learning Techniques in Adversarial Image Forensics","summarize: Image forensic plays a crucial role in both criminal investigations and civil litigation . Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial examples, with real-world consequences . Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.",0.32],["horizontal FL is a new method of perturbed local embedding. it is","VAFL: a Method of Vertical Asynchronous Federated Learning","summarize: Horizontal Federated learning handles multi-client data that share the same set of features, and vertical FL trains a better predictor that combine all the features from different clients. This paper targets solving vertical FL in an asynchronous fashion, and develops a simple FL method. The new method allows each client to run stochastic gradient algorithms without coordination with other clients, so it is suitable for intermittent connectivity of clients. This method further uses a new technique of perturbed local embedding to ensure data privacy and improve communication efficiency. Theoretically, we present the convergence rate and privacy level of our method for strongly convex, nonconvex and even nonsmooth objectives separately. Empirically, we apply our method to FL on various image and healthcare datasets. The results compare favorably to centralized and synchronous FL methods.",0.3125],["the power distribution network modelling with the R language is used to represent the network and support computation of","Distribution Power Network Reconfiguration in the Smart Grid","summarize: The power network reconfiguration algorithm with an R modeling approach evaluates its behavior in computing new reconfiguration topologies for the power grid in the context of the Smart Grid. The power distribution network modelling with the R language is used to represent the network and support computation of different algorithm configurations for the evaluation of new reconfiguration topologies. This work presents a reconfiguration solution of distribution networks, with a construction of an algorithm that receiving the network configuration data and the nodal measurements and from these data build a radial network, after this and using a branch exchange algorithm And verifying the best configuration of the network through artificial intelligence, so that there are no unnecessary changes during the operation, and applied an algorithm that analyses the load levels, to suggest changes in the network.",0.1785714286],["the two approaches each have unique advantages but are hard to reconcile. applying a similar","Interpreting Frame Transformations as Diagonalization of Harmonic Transfer Functions","summarize: Analysis of ac electrical systems can be performed via frame transformations in the time-domain or via harmonic transfer functions in the frequency-domain. The two approaches each have unique advantages but are hard to reconcile because the coupling effect in the frequency-domain leads to infinite dimensional HTF matrices that need to be truncated. This paper explores the relation between the two representations and shows that applying a similarity transformation to an HTF matrix creates a direct equivalence to a frame transformation on the input-output signals. Under certain conditions, such similarity transformations have a diagonalizing effect which, essentially, reduces the HTF matrix order from infinity to two or one, making the matrix tractable mathematically without truncation or approximation. This theory is applied to a droop-controlled voltage source inverter as an illustrative example. A stability criterion is derived in the frequency-domain which agrees with the conventional state-space model but offers greater insights into the mechanism of instability in terms of the negative damping under droop control. The paper not only establishes a unified view in theory but also offers an effective practical tool for stability assessment.",0.3461538462],["a framework for generating time series data is aimed at developing, evaluating, and","Data Generating Process to Evaluate Causal Discovery Techniques for Time Series Data","summarize: Going beyond correlations, the understanding and identification of causal relationships in observational time series, an important subfield of Causal Discovery, poses a major challenge. The lack of access to a well-defined ground truth for real-world data creates the need to rely on synthetic data for the evaluation of these methods. Existing benchmarks are limited in their scope, as they either are restricted to a static selection of data sets, or do not allow for a granular assessment of the methods' performance when commonly made assumptions are violated. We propose a flexible and simple to use framework for generating time series data, which is aimed at developing, evaluating, and benchmarking time series causal discovery methods. In particular, the framework can be used to fine tune novel methods on vast amounts of data, without overfitting them to a benchmark, but rather so they perform well in real-world use cases. Using our framework, we evaluate prominent time series causal discovery methods and demonstrate a notable degradation in performance when their assumptions are invalidated and their sensitivity to choice of hyperparameters. Finally, we propose future research directions and how our framework can support both researchers and practitioners.",0.6],["entropy models are optimized for rate-distortion performance. ANN","Image-Dependent Local Entropy Models for Learned Image Compression","summarize: The leading approach for image compression with artificial neural networks is to learn a nonlinear transform and a fixed entropy model that are optimized for rate-distortion performance. We show that this approach can be significantly improved by incorporating spatially local, image-dependent entropy models. The key insight is that existing ANN-based methods learn an entropy model that is shared between the encoder and decoder, but they do not transmit any side information that would allow the model to adapt to the structure of a specific image. We present a method for augmenting ANN-based image coders with image-dependent side information that leads to a 17.8% rate reduction over a state-of-the-art ANN-based baseline model on a standard evaluation set, and 70-98% reductions on images with low visual complexity that are poorly captured by a fixed, global entropy model.",0.1111111111],["bio-inspired workpiece structural optimization approach is presented in this paper. aim of this method","Bio-inspired method based on bone architecture to optimize the structure of mechanical workspieces","summarize: Nowadays, additive manufacturing processes greatly simplify the production of openwork workpiece providing new opportunities for workpieces design. Based on Nature knowledge, a new bio-inspired workpiece structural optimization approach is presented in this paper. This approach is derived from bones structure. The aim of this method is to reduce the workpiece weight maintaining an acceptable resistance. Like in bones, the porosity of the part to optimize was controlled by a bio-inspired method as function of the local stress field. Shape, size and orientation of the porosities were derived from bone structure; two main strategies were used: one inspired of avian species and other inspired of terrestrial mammalian. Subsequently, to validate this method, an experimental test was carried out for comparing a topological optimization and the proposed bio-inspired designs. This test was conducted on a beam part in 2.5D subjected to a static three-point bending with 65% of density. Three beams were manufactured by 3D metal printing: two bio-inspired beams and the last designed using a topological optimization method. Experimental test results demonstrated the usefulness of the proposed method. This bio-inspired structural optimization approach opens up new prospects in design of openwork workpiece.",0.1578947368],["a variant of chip-firing is introduced on a line where the chips are given","Root system chip-firing I: Interval-firing","summarize: Jim Propp recently introduced a variant of chip-firing on a line where the chips are given distinct integer labels. Hopkins, McConville, and Propp showed that this process is confluent from some initial configurations of chips. We recast their set-up in terms of root systems: labeled chip-firing can be seen as a root-firing process which allows the moves ",0.1363636364],["equiatomic FeCo layers are intercalated under graphene grown on Ir","Narrowing of d bands of FeCo layers intercalated under graphene","summarize: We report on the electronic properties of an artificial system obtained by the intercalation of equiatomic FeCo layers under graphene grown on Ir. Upon intercalation, the FeCo film grows epitaxially on Ir, resulting in a lattice-mismatched system. By performing Density Functional Theory calculations, we show that the intercalated FeCo layer leads to a pronounced corrugation of the graphene film. At the same time, the FeCo intercalated layers induce a clear transition from a nearly undisturbed to a strongly hybridized graphene -band, as measured by angle-resolved photoemission spectroscopy. A comparison of experimental results with the computed band structure and the projected density of states unveils a spin-selective hybridization between the band of graphene and FeCo-3d states. Our results demonstrate that the reduced dimensionality, as well as the hybridization within the FeCo layers, induce a narrowing and a clear splitting of Fe 3d-up and Fe 3d-down spin bands of the confined FeCo layers with respect to bulk Fe and Co.",0.5],["Glioma heterogeneous nature makes segmentation difficult. a","3D Semantic Segmentation of Brain Tumor for Overall Survival Prediction","summarize: Glioma, the malignant brain tumor, requires immediate treatment to improve the survival of patients. Gliomas heterogeneous nature makes the segmentation difficult, especially for sub-regions like necrosis, enhancing tumor, non-enhancing tumor, and Edema. Deep neural networks like full convolution neural networks and ensemble of fully convolution neural networks are successful for Glioma segmentation. The paper demonstrates the use of a 3D fully convolution neural network with a three layer encoder decoder approach for layer arrangement. The encoder blocks include the dense modules, and decoder blocks include convolution modules. The input to the network is 3D patches. The loss function combines dice loss and focal loss functions. The validation set dice score of the network is 0.74, 0.88, and 0.73 for enhancing tumor, whole tumor, and tumor core, respectively. The Random Forest Regressor uses shape, volumetric, and age features extracted from ground truth for overall survival prediction. The regressor achieves an accuracy of 44.8% on the validation set.",0.2961086625],["new method of multi-cell tracking is based on a study of brain-wide 4","SPF-CellTracker: Tracking multiple cells with strongly-correlated moves using a spatial particle filter","summarize: Tracking many cells in time-lapse 3D image sequences is an important challenging task of bioimage informatics. Motivated by a study of brain-wide 4D imaging of neural activity in C. elegans, we present a new method of multi-cell tracking. Data types to which the method is applicable are characterized as follows: cells are imaged as globular-like objects, it is difficult to distinguish cells based only on shape and size, the number of imaged cells ranges in several hundreds, moves of nearly-located cells are strongly correlated and cells do not divide. We developed a tracking software suite which we call SPF-CellTracker. Incorporating dependency on cells' moves into prediction model is the key to reduce the tracking errors: cell-switching and coalescence of tracked positions. We model target cells' correlated moves as a Markov random field and we also derive a fast computation algorithm, which we call spatial particle filter. With the live-imaging data of nuclei of C. elegans neurons in which approximately 120 nuclei of neurons are imaged, we demonstrate an advantage of the proposed method over the standard particle filter and a method developed by Tokunaga et al. .",0.2777777778],["we demonstrate up to 12 km, 56 Gb\/s DMT transmission using high-speed","56 Gb\/s DMT Transmission with VCSELs in 1.5 um Wavelength Range over up to 12 km for DWDM Intra-Data Center Connects","summarize: We demonstrate up to 12 km, 56 Gb\/s DMT transmission using high-speed VCSELs in the 1.5 um wavelength range for future 400Gb\/s intra-data center connects, enabled by vestigial sideband filtering of the transmit signal.",0.2361832764],["the analysis is performed in a 4-level vee+ladder system.","Sub- and super-luminal light propagation using a Rydberg state","summarize: We present a theoretical study to investigate sub- and super-luminal light propagation in a rubidium atomic system consisting of a Rydberg state by using density matrix formalism. The analysis is performed in a 4-level vee+ladder system interacting with a weak probe, and strong control and switching fields. The dispersion and absorption profiles are shown for stationary atoms as well as for moving atoms by carrying out Doppler averaging at room temperature. We also present the group index variation with control Rabi frequency and observe that a transparent medium can be switched from sub- to super-luminal propagation in the presence of switching field. Finally, the transient response of the medium is discussed, which shows that the considered 4-level scheme has potential applications in absorptive optical switching.",0.4166666667],["free space optical communication has gained significant importance in recent years. it uses the optical carrier in","Optical Communication in Space: Challenges and Mitigation Techniques","summarize: In recent years, free space optical communication has gained significant importance owing to its unique features: large bandwidth, license-free spectrum, high data rate, easy and quick deployability, less power and low mass requirements. FSO communication uses the optical carrier in the near infrared band to establish either terrestrial links within the Earth's atmosphere or inter-satellite or deep space links or ground-to-satellite or satellite-to-ground links. However, despite the great potential of FSO communication, its performance is limited by the adverse effects viz., absorption, scattering, and turbulence of the atmospheric channel. This paper presents a comprehensive survey on various challenges faced by FSO communication system for ground-to-satellite or satellite-to-ground and inter-satellite links. It also provides details of various performance mitigation techniques in order to have high link availability and reliability. The first part of the paper will focus on various types of impairments that pose a serious challenge to the performance of optical communication system for ground-to-satellite or satellite-to-ground and inter-satellite links. The latter part of the paper will provide the reader with an exhaustive review of various techniques both at physical layer as well as at the other layers i.e., link, network or transport layer to combat the adverse effects of the atmosphere. It also uniquely presents a recently developed technique using orbital angular momentum for utilizing the high capacity advantage of the optical carrier in case of space-based and near-Earth optical communication links. This survey provides the reader with comprehensive details on the use of space-based optical backhaul links in order to provide high-capacity and low-cost backhaul solutions.",0.1304347826],["the optimal strategy is derived analytically under the worst-case scenario with or without derivative trading","Robust portfolio optimization with multi-factor stochastic volatility","summarize: This paper studies a robust portfolio optimization problem under the multi-factor volatility model introduced by Christoffersen et al. . The optimal strategy is derived analytically under the worst-case scenario with or without derivative trading. To illustrate the effects of ambiguity, we compare our optimal robust strategy with some strategies that ignore the information of uncertainty, and provide the corresponding welfare analysis. The effects of derivative trading to the optimal portfolio selection are also discussed by considering alternative strategies. Our study is further extended to the cases with jump risks in asset price and correlated volatility factors, respectively. Numerical experiments are provided to demonstrate the behavior of the optimal portfolio and utility loss.",0.1578947368],["Let us know what you think about it!","Serre's problem on the density of isotropic fibres in conic bundles","summarize: Let ",0.0],["the attenuation theory assumes the UCAs oscillate linearly at sufficiently low","Theoretical Estimation of Attenuation Coefficient of Resonant Ultrasound Contrast Agents","summarize: Acoustic characterization of ultrasound contrast agents relies on the attenuation theory that assumes the UCAs oscillate linearly at sufficiently low excitation pressures. Effective shell parameters of the UCAs can be estimated by fitting a theoretical attenuation curve to experimentally measured attenuation data. Depending on the excitation frequency and properties of the shell, however, an UCA may oscillate nonlinearly even at sufficiently low excitation pressures violating the assumption in the linear attenuation theory. Notably, the concern on the estimation of the attenuation coefficient of a microbubble at resonance using linearized approximation has long been addressed. In this article, we investigated the attenuation phenomenon through analyzing the energy dissipation of a single UCA and propagating waves in an UCA suspension, both of which employed a nonlinear Rayleigh-Plesset equation. Analytical formulae capable of estimating the attenuation coefficient due to the weakly nonlinear oscillations of the UCA were obtained with a relatively rigorous mathematical analysis. The computed results that were verified by numerical simulations showed the attenuation coefficient of the UCA at resonance was pressure-dependent and could be significantly smaller than that predicted by the linear attenuation theory. Polydispersity of the UCA population enlarged the difference in the estimation of attenuation between the linear and present second-order nonlinear theories.",0.1111111111],["high-throughput computational screening has emerged as a critical component of materials discovery.","Predicting Electronic Structure Properties of Transition Metal Complexes with Neural Networks","summarize: High-throughput computational screening has emerged as a critical component of materials discovery. Direct density functional theory simulation of inorganic materials and molecular transition metal complexes is often used to describe subtle trends in inorganic bonding and spin-state ordering, but these calculations are computationally costly and properties are sensitive to the exchange-correlation functional employed. To begin to overcome these challenges, we trained artificial neural networks to predict quantum-mechanically-derived properties, including spin-state ordering, sensitivity to Hartree-Fock exchange, and spin- state specific bond lengths in transition metal complexes. Our ANN is trained on a small set of inorganic-chemistry-appropriate empirical inputs that are both maximally transferable and do not require precise three-dimensional structural information for prediction. Using these descriptors, our ANN predicts spin-state splittings of single-site transition metal complexes at arbitrary amounts of Hartree-Fock exchange to within 3 kcal\/mol accuracy of DFT calculations. Our exchange-sensitivity ANN enables improved predictions on a diverse test set of experimentally-characterized transition metal complexes by extrapolation from semi-local DFT to hybrid DFT. The ANN also outperforms other machine learning models , demonstrating particularly improved performance in transferability, as measured by prediction errors on the diverse test set. We establish the value of new uncertainty quantification tools to estimate ANN prediction uncertainty in computational chemistry, and we provide additional heuristics for identification of when a compound of interest is likely to be poorly predicted by the ANN.",0.2],["pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from","Pseudo-Outcrop Visualization of Borehole Images and Core Scans","summarize: A pseudo-outcrop visualization is demonstrated for borehole and full-diameter rock core images to augment the ubiquitous unwrapped cylinder view and thereby to assist non-specialist interpreters. The pseudo-outcrop visualization is equivalent to a nonlinear projection of the image from borehole to earth frame of reference that creates a solid volume sliced longitudinally to reveal two or more faces in which the orientations of geological features indicate what is observed in the subsurface. A proxy for grain size is used to modulate the external dimensions of the plot to mimic profiles seen in real outcrops. The volume is created from a mixture of geological boundary elements and texture, the latter being the residue after the sum of boundary elements is subtracted from the original data. In the case of measurements from wireline microresistivity tools, whose circumferential coverage is substantially less than 100%, the missing circumferential data is first inpainted using multiscale directional transforms, which decompose the image into its elemental building structures, before reconstructing the full image. The pseudo-outcrop view enables direct observation of the angular relationships between features and aids visual comparison between borehole and core images, especially for the interested non-specialist.",0.3888888889],["bone conduction is the transmission of acoustic energy to the inner ear","Hand bone conduction sound study by using the DSP Logger MX 300","summarize: Bone conduction is the transmission of acoustic energy to the inner ear by different paths involving the bones of the skull. In this work, we use the path the hand provides in order to transmit the sound coming from the cell phone using Bluetooth system. The aim of this work was to study the vibrations produced by a sound transmitted through bone conduction between a mobile phone and the hand analyzed with the DSP Logger MX equipment.",0.2666666667],["our CACHEFIX framework verifies the cache side-channel freedom of a program","Symbolic Verification of Cache Side-channel Freedom","summarize: Cache timing attacks allow third-party observers to retrieve sensitive information from program executions. But, is it possible to automatically check the vulnerability of a program against cache timing attacks and then, automatically shield program executions against these attacks? For a given program, a cache configuration and an attack model, our CACHEFIX framework either verifies the cache side-channel freedom of the program or synthesizes a series of patches to ensure cache side-channel freedom during program execution. At the core of our framework is a novel symbolic verification technique based on automated abstraction refinement of cache semantics. The power of such a framework is to allow symbolic reasoning over counterexample traces and to combine it with runtime monitoring for eliminating cache side channels during program execution. Our evaluation with routines from OpenSSL, libfixedtimefixedpoint, GDK and FourQlib libraries reveals that our CACHEFIX approach proves cache sidechannel freedom within an average of 75 seconds. Besides, in all except one case, CACHEFIX synthesizes all patches within 20 minutes to ensure cache side-channel freedom of the respective routines during execution.",0.2666666667],["the hydroxyl groups have been introduced on the nanorods surface. the dye-","The effect of hydroxyl on dye-sensitized solar cells assembled with TiO2 nanorods","summarize: TiO2 nanorods have been prepared on ITO substrates by dc reactive magnetron sputtering technique. The hydroxyl groups have been introduced on the nanorods surface. The structure and the optical properties of these nanorods have been studied. The dye-sensitized solar cells have been assembled using these TiO2 nanorods as photoelectrode. And the effect of the hydroxyl groups on the properties of the photoelectric conversion of the DSSCs has been studied.",0.2222222222],["cite has proposed the use of the maximum loss over random structured outputs. this","Structured Prediction: From Gaussian Perturbations to Linear-Time Principled Algorithms","summarize: Margin-based structured prediction commonly uses a maximum loss over all possible structured outputs \\cite. In natural language processing, recent work \\cite has proposed the use of the maximum loss over random structured outputs sampled independently from some proposal distribution. This method is linear-time in the number of random structured outputs and trivially parallelizable. We study this family of loss functions in the PAC-Bayes framework under Gaussian perturbations \\cite. Under some technical conditions and up to statistical accuracy, we show that this family of loss functions produces a tighter upper bound of the Gibbs decoder distortion than commonly used methods. Thus, using the maximum loss over random structured outputs is a principled way of learning the parameter of structured prediction models. Besides explaining the experimental success of \\cite, our theoretical results show that more general techniques are possible.",0.0],["the strategy is based on methods developed to analyze social networks. we illustrate the use of","Using Data Mining to Explore Calmodulin Bibliography","summarize: In this chapter, we present a strategy and the technics to approach a scientific field from a set of articles gathered from the bibliographic database, Web of Science. The strategy is based on methods developed to analyze social networks. We illustrate the use of such strategy in studying the calmodulin field. Such method allows to structure a huge number of articles when writing a review, to detect the key opinion leaders in a given field and to locate his own research topic in the landscape of the themes deciphered by our own community. We show that the free software VosViewer may be used without knowledge in computing science and with a short learning period. iii.",0.0555555556],["mmWave signals suffer from significant path loss due to high directivity and sensitivity","RIS-Assisted Coverage Enhancement in Millimeter-Wave Cellular Networks","summarize: The use of millimeter-wave bandwidth is one key enabler to achieve the high data rates in the fifth-generation cellular systems. However, mmWave signals suffer from significant path loss due to high directivity and sensitivity to blockages, limiting its adoption within small-scale deployments. To enhance the coverage of mmWave communication in 5G and beyond, it is promising to deploy a large number of reconfigurable intelligent surfaces that passively reflect mmWave signals towards desired directions. With this motivation, in this work we study the coverage of an RIS-assisted large-scale mmWave cellular network using stochastic geometry, and derive the peak reflection power expression of an RIS and the downlink signal-to-interference ratio coverage expression in closed forms. These analytic results clarify the effectiveness of deploying RISs in the mmWave SIR coverage enhancement, while unveiling the major role of the density ratio between active base stations and passive RISs. Furthermore, the results show that deploying passive reflectors is as effective as equipping BSs with more active antennas in the mmWave coverage enhancement. Simulation results confirm the tightness of the closed form expressions, corroborating our major findings based on the derived expressions.",0.0],["the minimum-time speed law satisfies kinematic and dynamic constraints","A solution of the minimum-time velocity planning problem based on lattice theory","summarize: For a vehicle on an assigned path, we find the minimum-time speed law that satisfies kinematic and dynamic constraints, related to maximum speed and maximum tangential and transversal acceleration. We present a necessary and sufficient condition for the feasibility of the problem and a simple operator, based on the solution of two ordinary differential equations, which computes the optimal solution. Theoretically, we show that the problem feasible set, if not empty, is a lattice, whose supremum element corresponds to the optimal solution.",0.1592291801],["we propose a new design principle for incrementally growing robust networks. the networks are self","A new design principle of robust onion-like networks self-organized in growth","summarize: Today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading efficiency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures.",0.4210526316],["the original lemma states that concentration of the Laplace spectrum implies combinatorial expansion in","Mixing in high-dimensional expanders","summarize: We prove a generalization of the Expander Mixing Lemma for arbitrary simplicial complexes. The original lemma states that concentration of the Laplace spectrum of a graph implies combinatorial expansion . Recently, an analogue of this Lemma was proved for simplicial complexes of arbitrary dimension, provided that the skeleton of the complex is complete. More precisely, it was shown that a concentrated spectrum of the simplicial Hodge Laplacian implies a similar type of expansion as in graphs. In this paper we remove the assumption of a complete skeleton, showing that concentration of the Laplace spectra in all dimensions implies combinatorial expansion in any complex. As applications we show that spectral concentration implies Gromov's geometric overlap property, and can be used to bound the chromatic number of a complex.",0.1111111111],["constraints on scaling relations of galaxy cluster X-ray luminosity, temperature and gas mass","Weighing the Giants V: Galaxy Cluster Scaling Relations","summarize: We present constraints on the scaling relations of galaxy cluster X-ray luminosity, temperature and gas mass with mass and redshift, employing masses from robust weak gravitational lensing measurements. These are the first such results obtained from an analysis that simultaneously accounts for selection effects and the underlying mass function, and directly incorporates lensing data to constrain total masses. Our constraints on the scaling relations and their intrinsic scatters are in good agreement with previous studies, and reinforce a picture in which departures from self-similar scaling laws are primarily limited to cluster cores. However, the data are beginning to reveal new features that have implications for cluster astrophysics and provide new tests for hydrodynamical simulations. We find a positive correlation in the intrinsic scatters of luminosity and temperature at fixed mass, which is related to the dynamical state of the clusters. While the evolution of the nominal scaling relations over the redshift range ",0.0666666667],["Let us know what you think about it!","Unitary Subgroups of commutative group algebras of characteristic two","summarize: Let ",0.0],["the goal is to achieve the lowest possible distortion at any given bit rate. but in recent","Rethinking Lossy Compression: The Rate-Distortion-Perception Tradeoff","summarize: Lossy compression algorithms are typically designed and analyzed through the lens of Shannon's rate-distortion theory, where the goal is to achieve the lowest possible distortion at any given bit rate. However, in recent years, it has become increasingly accepted that low distortion is not a synonym for high perceptual quality, and in fact optimization of one often comes at the expense of the other. In light of this understanding, it is natural to seek for a generalization of rate-distortion theory which takes perceptual quality into account. In this paper, we adopt the mathematical definition of perceptual quality recently proposed by Blau & Michaeli , and use it to study the three-way tradeoff between rate, distortion, and perception. We show that restricting the perceptual quality to be high, generally leads to an elevation of the rate-distortion curve, thus necessitating a sacrifice in either rate or distortion. We prove several fundamental properties of this triple-tradeoff, calculate it in closed form for a Bernoulli source, and illustrate it visually on a toy MNIST example.",0.1818181818],["text analysis techniques can be used to uncover unstructured information from text. a 3-","From Review to Rating: Exploring Dependency Measures for Text Classification","summarize: Various text analysis techniques exist, which attempt to uncover unstructured information from text. In this work, we explore using statistical dependence measures for textual classification, representing text as word vectors. Student satisfaction scores on a 3-point scale and their free text comments written about university subjects are used as the dataset. We have compared two textual representations: a frequency word representation and term frequency relationship to word vectors, and found that word vectors provide a greater accuracy. However, these word vectors have a large number of features which aggravates the burden of computational complexity. Thus, we explored using a non-linear dependency measure for feature selection by maximizing the dependence between the text reviews and corresponding scores. Our quantitative and qualitative analysis on a student satisfaction dataset shows that our approach achieves comparable accuracy to the full feature vector, while being an order of magnitude faster in testing. These text analysis and feature reduction techniques can be used for other textual data applications such as sentiment analysis.",0.25],["the main purpose of present paper is to determine some lower bounds for the quotient of","Partial sums of Hyper-Bessel function with applications","summarize: The main purpose of present paper is to determine some lower bounds for the quotient of the normalized hyper-Bessel function and its partial sum, as well as for the quotient of the derivative of normalized hyper-Bessel function and its partial sum. In addition, some applications related to obtained results are also given.",0.1],["state-of-the-art approaches perform semantic segmentation or refine object bounding boxes obtained","Instance Segmentation of Biomedical Images with an Object-aware Embedding Learned with Local Constraints","summarize: Automatic instance segmentation is a problem that occurs in many biomedical applications. State-of-the-art approaches either perform semantic segmentation or refine object bounding boxes obtained from detection methods. Both suffer from crowded objects to varying degrees, merging adjacent objects or suppressing a valid object. In this work, we assign an embedding vector to each pixel through a deep neural network. The network is trained to output embedding vectors of similar directions for pixels from the same object, while adjacent objects are orthogonal in the embedding space, which effectively avoids the fusion of objects in a crowd. Our method yields state-of-the-art results even with a light-weighted backbone network on a cell segmentation and a leaf segmentation data set . The code and model weights are public available.",0.0],["the spectral graph theory provides an algebraical approach to investigate the characteristics of weighted","The Wigner's Semicircle Law of Weighted Random Networks","summarize: The spectral graph theory provides an algebraical approach to investigate the characteristics of weighted networks using the eigenvalues and eigenvectors of a matrix that represents the structure of the network. However, it is difficult for large-scale and complex networks to represent their structure as a matrix correctly. If there is a universality that the eigenvalues are independent of the detailed structure in large-scale and complex network, we can avoid the difficulty. In this paper, we clarify the Wigner's Semicircle Law for weighted networks as such a universality. The law indicates that the eigenvalues of the normalized Laplacian matrix for weighted networks can be calculated from the a few network statistics when the weighted networks satisfy the sufficient condition of the node degrees and the link weights.",0.1111111111],["we give applications of these structures results. we give applications of these structures results.","On the structure of graded commutative exponential funtors","summarize: We investigate the structure of graded commutative exponential functors. We give applications of these structure results, including computations of the homology of the symmetric groups and of extensions in the category of strict polynomial functors.",0.0714285714],["resistivity and uniform magnetic susceptibility anisotropies of in-plane resistivity","Dichotomy between in-plane magnetic susceptibility and resistivity anisotropies in extremely strained ","summarize: The in-plane resistivity and uniform magnetic susceptibility anisotropies of ",0.4559744704],["image forensic plays a crucial role in both criminal investigations and civil litigation. there are","A Survey of Machine Learning Techniques in Adversarial Image Forensics","summarize: Image forensic plays a crucial role in both criminal investigations and civil litigation . Increasingly, machine learning approaches are also utilized in image forensics. However, there are also a number of limitations and vulnerabilities associated with machine learning-based approaches, for example how to detect adversarial examples, with real-world consequences . Therefore, with a focus on image forensics, this paper surveys techniques that can be used to enhance the robustness of machine learning-based binary manipulation detectors in various adversarial scenarios.",0.32],["clinical dermoscopic features may indicate melanoma. a neural network architecture","Fully Convolutional Neural Networks to Detect Clinical Dermoscopic Features","summarize: The presence of certain clinical dermoscopic features within a skin lesion may indicate melanoma, and automatically detecting these features may lead to more quantitative and reproducible diagnoses. We reformulate the task of classifying clinical dermoscopic features within superpixels as a segmentation problem, and propose a fully convolutional neural network to detect clinical dermoscopic features from dermoscopy skin lesion images. Our neural network architecture uses interpolated feature maps from several intermediate network layers, and addresses imbalanced labels by minimizing a negative multi-label Dice-F",0.2222222222],["the spatial resolution is 025-028. the high resolution allowed us to resolve highly filamentary","An ALMA view of molecular filaments in the Large Magellanic Cloud I: The formation of high-mass stars and pillars in the N159E-Papillon Nebula triggered by a cloud-cloud collision","summarize: We present the ALMA observations of CO isotopes and 1.3 mm continuum emission toward the N159E-Papillon Nebula in the Large Magellanic Cloud . The spatial resolution is 025-028 , which is a factor of 3 higher than the previous ALMA observations in this region. The high resolution allowed us to resolve highly filamentary CO distributions with typical widths of ",0.1161724551],["the fractal Julia sets is a mathematical model of the power used in electrical systems","Topological properties of fractal Julia sets related to the signs and magnitudes of the real and reactive powers","summarize: In AC electrical systems, the power depends on the real power due to resistive elements and the reactive power due to the inductive and capacitive elements, which are commonly studied by using phasor and scalar methods. Thus, this paper focuses on applying the fractal Julia sets to observe the topological properties related to the signs and magnitudes of the real and reactive powers consumed or supplied by an electrical circuit. To perform this, different power combinations were used to represent the fractal diagrams with an algorithm that considers the mathematical model of Julia sets. The study considers three type of loads: the first study considers the change of real power when the reactive power is fixed; the second study deals with the change of the reactive power when the real power is fixed; and finally, the third study contemplates that both real and reactive powers change. Furthermore, the fractal diagrams of the power in the four quadrants of the complex plane are studied to identify the topological properties that each sign and magnitude represent. A qualitative analysis of the diagrams helps to identify that the complex power loads present some fractal graphic patterns, with respect to the signs and magnitudes considered in the different quadrants of the complex planes. The diagrams represented in the complex planes save a relation in the forms and structure with other points studied, concluding that the power is related to other figures in other quadrants. Thus, this result allows a new study of the behavior of the power in an electrical circuit, showing a clear relation of the different fractal diagrams that the Julia sets obtained.",0.5353239539],["the calculated filling factors for a feature reflect the fraction of the solar disc covered by that","Filling Factors of Sunspots in SODISM Images","summarize: Received: 1st December 2018; Accepted: 18th February 2019; Published: 1st April 2019 Abstract: The calculated filling factors for a feature reflect the fraction of the solar disc covered by that feature, and the assignment of reference synthetic spectra. In this paper, the FFs, specified as a function of radial position on the solar disc, are computed for each image in a tabular form. The filling factor is an important parameter and is defined as the fraction of area in a pixel covered with the magnetic field, whereas the rest of the area in the pixel is field-free. However, this does not provide extensive information about the experiments conducted on tens or hundreds of such images. This is the first time that filling factors for SODISM images have been catalogued in tabular formation. This paper presents a new method that provides the means to detect sunspots on full-disk solar images recorded by the Solar Diameter Imager and Surface Mapper on the PICARD satellite. The method is a totally automated detection process that achieves a sunspot recognition rate of 97.6%. The number of sunspots detected by this method strongly agrees with the NOAA catalogue. The sunspot areas calculated by this method have a 99% correlation with SOHO over the same period, and thus help to calculate the filling factor for wavelength 607nm.",0.1],["acyclic graphs are a class of graphs. a t","Identifying causal effects in maximally oriented partially directed acyclic graphs","summarize: We develop a necessary and sufficient causal identification criterion for maximally oriented partially directed acyclic graphs . MPDAGs as a class of graphs include directed acyclic graphs , completed partially directed acyclic graphs , and CPDAGs with added background knowledge. As such, they represent the type of graph that can be learned from observational data and background knowledge under the assumption of no latent variables. Our identification criterion can be seen as a generalization of the g-formula of Robins . We further obtain a generalization of the truncated factorization formula and compare our criterion to the generalized adjustment criterion of Perkovi\\'c et al. which is sufficient, but not necessary for causal identification.",0.6614029733],["gradient-flow algorithm finds a good global minimum despite spurious local minima.","Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model","summarize: Gradient-based algorithms are effective for many machine learning tasks, but despite ample recent effort and some progress, it often remains unclear why they work in practice in optimising high-dimensional non-convex functions and why they find good minima instead of being trapped in spurious ones. Here we present a quantitative theory explaining this behaviour in a spiked matrix-tensor model. Our framework is based on the Kac-Rice analysis of stationary points and a closed-form analysis of gradient-flow originating from statistical physics. We show that there is a well defined region of parameters where the gradient-flow algorithm finds a good global minimum despite the presence of exponentially many spurious local minima. We show that this is achieved by surfing on saddles that have strong negative direction towards the global minima, a phenomenon that is connected to a BBP-type threshold in the Hessian describing the critical points of the landscapes.",0.2606789731],["magnetic materials with strong perpendicular anisotropy offer stability and high recording density","New highly-anisotropic Rh-based Heusler compound for magnetic recording","summarize: The development of high-density magnetic recording media is limited by the superparamagnetism in very small ferromagnetic crystals. Hard magnetic materials with strong perpendicular anisotropy offer stability and high recording density. To overcome the difficulty of writing media with a large coercivity, heat assisted magnetic recording has been developed, rapidly heating the media to the Curie temperature Tc before writing, followed by rapid cooling. Requirements are a suitable Tc, coupled with anisotropic thermal conductivity and hard magnetic properties. Here we introduce Rh2CoSb as a new hard magnet with potential for thin film magnetic recording. A magnetocrystalline anisotropy of 3.6 MJm-3 is combined with a saturation magnetization of 0Ms = 0.52 T at 2 K . The magnetic hardness parameter of 3.7 at room temperature is the highest observed for any rare-earth free hard magnet. The anisotropy is related to an unquenched orbital moment of 0.42 B on Co, which is hybridized with neighbouring Rh atoms with a large spin-orbit interaction. Moreover, the pronounced temperature-dependence of the anisotropy that follows from its Tc of 450 K, together with a high thermal conductivity of 20 Wm-1K-1, makes Rh2CoSb a candidate for development for heat assisted writing with a recording density in excess of 10 Tb\/in2.",0.25],["CAN messages lack a common mapping of functions to commands. CAN messages are","Exploiting the Shape of CAN Data for In-Vehicle Intrusion Detection","summarize: Modern vehicles rely on scores of electronic control units broadcasting messages over a few controller area networks . Bereft of security features, in-vehicle CANs are exposed to cyber manipulation and multiple researches have proved viable, life-threatening cyber attacks. Complicating the issue, CAN messages lack a common mapping of functions to commands, so packets are observable but not easily decipherable. We present a transformational approach to CAN IDS that exploits the geometric properties of CAN data to inform two novel detectors--one based on distance from a learned, lower dimensional manifold and the other on discontinuities of the manifold over time. Proof-of-concept tests are presented by implementing a potential attack approach on a driving vehicle. The initial results suggest that the first detector requires additional refinement but does hold promise; the second detector gives a clear, strong indicator of the attack; and the algorithms keep pace with high-speed CAN messages. As our approach is data-driven it provides a vehicle-agnostic IDS that eliminates the need to reverse engineer CAN messages and can be ported to an after-market plugin.",0.2608695652],["superconductivity in Nd has been observed in recent years.","Doping evolution of the Mott-Hubbard landscape in infinite-layer nickelates","summarize: The recent observation of superconductivity in Nd",0.3636363636],["we investigate the Schrdinger operators.","2D Schr\\dinger operators with singular potentials concentrated near curves","summarize: We investigate the Schr\\dinger operators ",0.0898657928],["commutator subgroup of wreath product cite is a permutational","Minimal generating set and structure of wreath product of groups with non-faithful action, comutator subgroup of wreath product and the fundamental group of orbit of Morse function ","summarize: Given a permutational wreath product sequence of cyclic groups we investigate its minimal generating set, minimal generating set for its commutator and some properties of its commutator subgroup. We strengthen the result of author \\cite and construct minimal generating set for wreath product of finite and infinite cyclic groups and direct product of such groups. We generalize results of Meldrum about commutator subgroup of wreath product \\cite because we take in consideration as regular wreath product as well as no regular . Also commutator of such group and its minimal generating set. Also center of such products was investigated. Also fundamental group of orbits of a Morse function ",0.0838407686],["trajectories of a Hamiltonian system are elastically reflected after collision","Degenerate billiards in celestial mechanics","summarize: In an ordinary billiard trajectories of a Hamiltonian system are elastically reflected after a collision with a hypersurface . If the scatterer is a submanifold of codimension more than one, we say that the billiard is degenerate. Degenerate billiards appear as limits of systems with singularities in celestial mechanics. We prove the existence of trajectories of such systems shadowing trajectories of the corresponding degenerate billiards. This research is motivated by the problem of second species solutions of Poincar\\'e.",0.2352941176],["cloning algorithm reduces to the determination of the growth rate of a population","Discreteness Effects in Population Dynamics","summarize: We analyse numerically the effects of small population size in the initial transient regime of a simple example population dynamics. These effects play an important role for the numerical determination of large deviation functions of additive observables for stochastic processes. A method commonly used in order to determine such functions is the so-called cloning algorithm which in its non-constant population version essentially reduces to the determination of the growth rate of a population, averaged over many realizations of the dynamics. However, the averaging of populations is highly dependent not only on the number of realizations of the population dynamics, and on the initial population size but also on the cut-off time considered to stop their numerical evolution. This may result in an over-influence of discreteness effects at initial times, caused by small population size. We overcome these effects by introducing a time delay in the evolution of populations, additional to the discarding of the initial transient regime of the population growth where these discreteness effects are strong. We show that the improvement in the estimation of the large deviation function comes precisely from these two main contributions.",0.0952380952],["european leaders doubt waiving intellectual property rights for vaccines. proposal recently supported by the","EU leaders raise doubts over U.S. plan to waive Covid vaccine patents","summarize: European leaders have doubts that waiving intellectual property rights for Covid-19 vaccines, a proposal recently supported by the United States, is the way to go.",0.1428571429],["Worcester polytechnic institute was founded in 1865 to create and convey the latest science and engineering","wpi provide an education that balances theory with practice","summarize: Worcester Polytechnic Institute was founded in 1865 to create and convey the latest science and engineering knowledge in ways that are most beneficial to society. Today, WPI holds firm to its founding mission to provide an education that balances theory with practice.",0.0476190476]]}