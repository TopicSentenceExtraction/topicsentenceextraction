{"columns":["predict_title","actual_title","actual_abstract","bleu"],"index":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],"data":[["the paper presents the description of several key RAN enablers. the five-generation","RAN Enablers for 5G Radio Resource Management","summarize: This paper presents the description of several key RAN enablers for the radio resource management framework of the fifth generation radio access network , referred to as building blocks of the 5G RRM. In particular, the following key RAN enablers are discussed: i) interference management techniques for dense and dynamic deployments, focusing on cell-edge performance enhancement; ii) dynamic traffic steering mechanisms that aim to attain the optimum mapping of 5G services to any available resources when and where needed by considering the peculiarities of different air interface variants ; iii) resource management strategies that deal with network slices; and iv) tight interworking between novel 5G AIVs and evolved legacy AIVs such as Long-term Evolution . Evaluation results for each of these key RAN enablers are also presented.",0.0555555556],["we shed new light on the textit of optimization problems arising in prediction error parameter","On the smoothness of nonlinear system identification","summarize: We shed new light on the \\textit of optimization problems arising in prediction error parameter estimation of linear and nonlinear systems. We show that for regions of the parameter space where the model is not contractive, the Lipschitz constant and ",0.2777777778],["Molecular motors walk along filaments until they detach stochastically.","Force-dependent unbinding rate of molecular motors from stationary optical trap data","summarize: Molecular motors walk along filaments until they detach stochastically with a force-dependent unbinding rate. Here, we show that this unbinding rate can be obtained from the analysis of experimental data of molecular motors moving in stationary optical traps. Two complementary methods are presented, based on the analysis of the distribution for the unbinding forces and of the motor's force traces. In the first method, analytically derived force distributions for slip bonds, slip-ideal bonds, and catch bonds are used to fit the cumulative distributions of the unbinding forces. The second method is based on the statistical analysis of the observed force traces. We validate both methods with stochastic simulations and apply them to experimental data for kinesin-1.",0.0889708225],["dark matter in the form of macroscopically large nuggets of standard model qu","The 21cm Absorption Line and Axion Quark Nugget Dark Matter Model","summarize: We argue that dark matter in the form of macroscopically large nuggets of standard model quarks and antiquarks can help to alleviate the tension between standard model cosmology and the recent EDGES observation of a stronger than anticipated 21 cm absorption feature. The effect occurs as a result of the thermal emission from quark nugget dark matter at early times and at energies well below the peak of the CMB. Similar radiation at early times may also contribute a fraction of the GHz range excess observed by ARCADE2.",0.0666666667],["we propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving","Softwarization of Internet of Things Infrastructure for Secure and Smart Healthcare","summarize: We propose an agile softwarized infrastructure for flexible, cost effective, secure and privacy preserving deployment of Internet of Things for smart healthcare applications and services. It integrates state-of-the-art networking and virtualization techniques across IoT, fog and cloud domains, employing Blockchain, Tor and message brokers to provide security and privacy for patients and healthcare providers. We propose a novel platform using Machine-to-Machine messaging and rule-based beacons for seamless data management and discuss the role of data and decision fusion in the cloud and the fog, respectively, for smart healthcare applications and services.",0.2],["we use a multi-layer representation of Twitter diffusion networks. we compute for each layer","A multi-layer approach to disinformation detection on Twitter","summarize: We tackle the problem of classifying news articles pertaining to disinformation vs mainstream news by solely inspecting their diffusion mechanisms on Twitter. Our technique is inherently simple compared to existing text-based approaches, as it allows to by-pass the multiple levels of complexity which are found in news content . We employ a multi-layer representation of Twitter diffusion networks, and we compute for each layer a set of global network features which quantify different aspects of the sharing process. Experimental results with two large-scale datasets, corresponding to diffusion cascades of news shared respectively in the United States and Italy, show that a simple Logistic Regression model is able to classify disinformation vs mainstream networks with high accuracy , also when considering the political bias of different sources in the classification task. We also highlight differences in the sharing patterns of the two news domains which appear to be country-independent. We believe that our network-based approach provides useful insights which pave the way to the future development of a system to detect misleading and harmful information spreading on social media.",0.380952381],["entropic regularization is a method of computing entropic regularized OT","Entropic Optimal Transport between Unbalanced Gaussian Measures has a Closed Form","summarize: Although optimal transport problems admit closed form solutions in a very few notable cases, e.g. in 1D or between Gaussians, these closed forms have proved extremely fecund for practitioners to define tools inspired from the OT geometry. On the other hand, the numerical resolution of OT problems using entropic regularization has given rise to many applications, but because there are no known closed-form solutions for entropic regularized OT problems, these approaches are mostly algorithmic, not informed by elegant closed forms. In this paper, we propose to fill the void at the intersection between these two schools of thought in OT by proving that the entropy-regularized optimal transport problem between two Gaussian measures admits a closed form. Contrary to the unregularized case, for which the explicit form is given by the Wasserstein-Bures distance, the closed form we obtain is differentiable everywhere, even for Gaussians with degenerate covariance matrices. We obtain this closed form solution by solving the fixed-point equation behind Sinkhorn's algorithm, the default method for computing entropic regularized OT. Remarkably, this approach extends to the generalized unbalanced case -- where Gaussian measures are scaled by positive constants. This extension leads to a closed form expression for unbalanced Gaussians as well, and highlights the mass transportation \/ destruction trade-off seen in unbalanced optimal transport. Moreover, in both settings, we show that the optimal transportation plans are Gaussians and provide analytical formulas of their parameters. These formulas constitute the first non-trivial closed forms for entropy-regularized optimal transport, thus providing a ground truth for the analysis of entropic OT and Sinkhorn's algorithm.",0.2412899781],["previous paper analyzed the difficulties associated with the application of numerical methods to the approximate kernel for the","On the Application of Numerical Methods to Hallen's Equation: The Case of a Lossy Medium","summarize: A previous paper analyzed in detail the difficulties associated with the application of numerical methods to Hallen's integral equation with the approximate kernel for the case of a lossless surrounding medium. The present paper extends to the case where the medium is conducting and points out similarities and differences between the two cases. Our main device is an analytical\/asymptotic study of the antenna of infinite length.",0.2],["cuneiform text from the series Enuma Anu Enlil reports on","The Double Eclipse at the Downfall of Old Babylon","summarize: For many decades scholars converse how to correctly include the Old Babylonian Empire into the absolute timeline of history. A cuneiform text from the series Enuma Anu Enlil reports on the destruction of Babylon after a lunar and solar eclipse. Eclipses provide a great tool for examining historic events, and this account will be our basis for the investigation of eclipse pairs to be fitted into the various chronologies proposed. We consider three interpretations of that text: literal understanding, the inverted sequence of the two eclipse types, and a relocation of the setting to Akkad. All variants show imperfections. The least complications emerge when the account draws upon the third option, i.e. if it is allotted to 2161 BCE, as the year would mark the end of the Gutian rule in Akkad. But in any case the account seems to support rather the Long Chronology than any other. When dealing with eclipses such far back in time, we also allow for the shift of the visibility zone in regard to the irregular deceleration of Earth's rotation.",0.2],["in this paper we give new examples of QCH Kahler surfaces. the opposite is","QCH Kahler surfaces II","summarize: In this paper we give new examples of QCH Kahler surfaces whose opposite almost Hermitian strucure is Hermitian and not locally conformally Kahler. In this way we give also a large class of examples of Hermitian surfaces with J-invariant Ricci tensor which are not l.c.k.",0.2],["dynamic landscape models are proposed for prisoner's Dilemma and Snowdrift games","Dynamic landscape models of coevolutionary games","summarize: Players of coevolutionary games may update not only their strategies but also their networks of interaction. Based on interpreting the payoff of players as fitness, dynamic landscape models are proposed. The modeling procedure is carried out for Prisoner's Dilemma and Snowdrift games that both use either birth--death or death--birth strategy updating. The main focus is on using dynamic fitness landscapes as a mathematical model of coevolutionary game dynamics. Hence, an alternative tool for analyzing coevolutionary games becomes available, and landscape measures such as modality, ruggedness and information content can be computed and analyzed. In addition, fixation properties of the games and quantifiers characterizing the interaction networks are calculated numerically. Relations are established between landscape properties expressed by landscape measures and quantifiers of coevolutionary game dynamics such as fixation probabilities, fixation times and network properties.",0.3333333333],["recent ALMA observations have resolved the obscuring torus in the nearest galaxy, NGC","Interpretation of ALMA velocity map for the obscuring torus in NGC1068","summarize: Recent ALMA observations have resolved the obscuring torus in the nearest Sy2 galaxy, NGC1068, in the millimeter band. These observations have confirmed the presence of a geometrically thick torus with an orbital motion of its matter and the velocity distribution which can reflect the clumpy structure. In the framework of N-body simulations we consider a dynamical model of an obscuring torus which accounts for the gravitational interaction between the clouds moving in the field of the central mass. In considered model, clouds are orbiting around the central mass exhibiting a spread in inclination and eccentricity. The self-gravity of the torus induces the velocity distribution of clouds with a global orbital motion which mimics the ALMA data for NGC1068.",0.5],["the Predicting Media Memorability task in MediaEval 2020 aims to address the","Leveraging Audio Gestalt to Predict Media Memorability","summarize: Memorability determines what evanesces into emptiness, and what worms its way into the deepest furrows of our minds. It is the key to curating more meaningful media content as we wade through daily digital torrents. The Predicting Media Memorability task in MediaEval 2020 aims to address the question of media memorability by setting the task of automatically predicting video memorability. Our approach is a multimodal deep learning-based late fusion that combines visual, semantic, and auditory features. We used audio gestalt to estimate the influence of the audio modality on overall video memorability, and accordingly inform which combination of features would best predict a given video's memorability scores.",0.25],["classical novae are thermonuclear explosions that occur on the surfaces of white","Direct evidence for shock-powered optical emission in a nova","summarize: Classical novae are thermonuclear explosions that occur on the surfaces of white dwarf stars in interacting binary systems . It has long been thought that the luminosity of classical novae is powered by continued nuclear burning on the surface of the white dwarf after the initial runaway . However, recent observations of GeV ",0.0666666667],["we show that any type of a person is a sexy person.","Singular MASAs in type III factors and Connes' Bicentralizer Property","summarize: We show that any type ",0.45],["this chapter offers a detailed survey on intrinsically localized frames. we re-","A Guide to Localized Frames and Applications to Galerkin-like Representations of Operators","summarize: This chapter offers a detailed survey on intrinsically localized frames and the corresponding matrix representation of operators. We re-investigate the properties of localized frames and the associated Banach spaces in full detail. We investigate the representation of operators using localized frames in a Galerkin-type scheme. We show how the boundedness and the invertibility of matrices and operators are linked and give some sufficient and necessary conditions for the boundedness of operators between the associated Banach spaces.",0.4117647059],["Lie algebras were introduced by Kaplan as a class of real Lie algebras","On complex H-type Lie algebras","summarize: H-type Lie algebras were introduced by Kaplan as a class of real Lie algebras generalizing the familiar Heisenberg Lie algebra ",0.2413793103],["we prove the existence of a Galois closure for towers of torsors under","Nori fundamental gerbe of essentially finite covers and Galois closure of towers of torsors","summarize: We prove the existence of a Galois closure for towers of torsors under finite group schemes over a proper, geometrically connected and geometrically reduced algebraic stack ",0.5991512862],["cosmological constraints on the scalar spectral index of primordial fluctuations.","Bayesian Evidence against Harrison-Zel'dovich spectrum in tension cosmology","summarize: Current cosmological constraints on the scalar spectral index of primordial fluctuations ",0.1666666667],["open domain keyphrase extraction dataset contains nearly one hundred thousand web documents. BLING-","Open Domain Web Keyphrase Extraction Beyond Language Modeling","summarize: This paper studies keyphrase extraction in real-world scenarios where documents are from diverse domains and have variant content quality. We curate and release OpenKP, a large scale open domain keyphrase extraction dataset with near one hundred thousand web documents and expert keyphrase annotations. To handle the variations of domain and content quality, we develop BLING-KPE, a neural keyphrase extraction model that goes beyond language understanding using visual presentations of documents and weak supervision from search queries. Experimental results on OpenKP confirm the effectiveness of BLING-KPE and the contributions of its neural architecture, visual features, and search log weak supervision. Zero-shot evaluations on DUC-2001 demonstrate the improved generalization ability of learning from the open domain data compared to a specific domain.",0.0],["2DCCA is incapable of extracting sufficient discriminatory representations. this is a","A Complete Discriminative Tensor Representation Learning for Two-Dimensional Correlation Analysis","summarize: As an effective tool for two-dimensional data analysis, two-dimensional canonical correlation analysis is not only capable of preserving the intrinsic structural information of original two-dimensional data, but also reduces the computational complexity effectively. However, due to the unsupervised nature, 2DCCA is incapable of extracting sufficient discriminatory representations, resulting in an unsatisfying performance. In this letter, we propose a complete discriminative tensor representation learning method based on linear correlation analysis for analyzing 2D signals . This letter shows that the introduction of the complete discriminatory tensor representation strategy provides an effective vehicle for revealing, and extracting the discriminant representations across the 2D data sets, leading to improved results. Experimental results show that the proposed CDTRL outperforms state-of-the-art methods on the evaluated data sets.",0.4545454545],["we presented the Bayesian portfolio selection strategy via the.","Sparse Portfolio selection via Bayesian Multiple testing","summarize: We presented Bayesian portfolio selection strategy, via the ",0.2727272727],["Convolutional Neural Network based trackers have achieved state-of-the-art","Robust Visual Tracking via Statistical Positive Sample Generation and Gradient Aware Learning","summarize: In recent years, Convolutional Neural Network based trackers have achieved state-of-the-art performance on multiple benchmark datasets. Most of these trackers train a binary classifier to distinguish the target from its background. However, they suffer from two limitations. Firstly, these trackers cannot effectively handle significant appearance variations due to the limited number of positive samples. Secondly, there exists a significant imbalance of gradient contributions between easy and hard samples, where the easy samples usually dominate the computation of gradient. In this paper, we propose a robust tracking method via Statistical Positive sample generation and Gradient Aware learning to address the above two limitations. To enrich the diversity of positive samples, we present an effective and efficient statistical positive sample generation algorithm to generate positive samples in the feature space. Furthermore, to handle the issue of imbalance between easy and hard samples, we propose a gradient sensitive loss to harmonize the gradient contributions between easy and hard samples. Extensive experiments on three challenging benchmark datasets including OTB50, OTB100 and VOT2016 demonstrate that the proposed SPGA performs favorably against several state-of-the-art trackers.",0.0],["kagome materials are ideal system to realize QSL. they are a geometric","From Claringbullite to a new spin liquid candidate Cu","summarize: The search for quantum spin liquid materials has attracted significant attention in the field of condensed matter physics in recent years, but until now only a handful of them are considered as candidates hosting QSL ground state. Owning to their geometrically frustrated structure, Kagome materials are ideal system to realize QSL. In this study, we synthesized the kagome structured material Claringbullite ",0.2380952381],["ab initio methods are used to study absorption energy of atomic hydrogen at rotated graph","Selective Hydrogen Adsoprtion in Graphene Rotated Bilayers","summarize: The absorption energy of atomic hydrogen at rotated graphene bilayers is studied using ab initio methods based on the density functional theory including van der Waals interactions. We find that, due to the surface corrugation induced by the underneath rotated layer and the perturbation of the electronic density of states near the Fermi energy, the atoms with an almost AA stacking are the preferential ones for hydrogen chemisorption. The adsorption energy difference between different atoms can be as large as 80 meV. In addition, we find that, due to the logarithmic van Hove singularities in the electronic density of states at energies close to the Dirac point, the adsorption energy of either electron or hole doped samples is substantially increased. We also find that the adsorption energy increases with the decrease of the rotated angle between the layers. Finally, the large zero point energy of the C-H bond suggests adsorption and desorption of atomic hydrogen and deuterium should behave differently.",0.0526315789],["the approach relies on Monte Carlo Simulations of the fine-grained CDF equation","An Efficient Solver for Cumulative Density Function-based Solutions of Uncertain Kinematic Wave Models","summarize: We develop a numerical framework to implement the cumulative density function method for obtaining the probability distribution of the system state described by a kinematic wave model. The approach relies on Monte Carlo Simulations of the fine-grained CDF equation of system state, as derived by the CDF method. This fine-grained CDF equation is solved via the method of characteristics. Each method of characteristics solution is far more computationally efficient than the direct solution of the kinematic wave model, and the MCS estimator of the CDF converges relatively quickly. We verify the accuracy and robustness of our procedure via comparison with direct MCS of a particular kinematic wave system, the Saint-Venant equation.",0.1623607791],["we introduce two types of variational quantum circuits for knowledge graph embedding. we","Variational Quantum Circuit Model for Knowledge Graphs Embedding","summarize: In this work, we propose the first quantum Ans\\atze for the statistical relational learning on knowledge graphs using parametric quantum circuits. We introduce two types of variational quantum circuits for knowledge graph embedding. Inspired by the classical representation learning, we first consider latent features for entities as coefficients of quantum states, while predicates are characterized by parametric gates acting on the quantum states. For the first model, the quantum advantages disappear when it comes to the optimization of this model. Therefore, we introduce a second quantum circuit model where embeddings of entities are generated from parameterized quantum gates acting on the pure quantum state. The benefit of the second method is that the quantum embeddings can be trained efficiently meanwhile preserving the quantum advantages. We show the proposed methods can achieve comparable results to the state-of-the-art classical models, e.g., RESCAL, DistMult. Furthermore, after optimizing the models, the complexity of inductive inference on the knowledge graphs might be reduced with respect to the number of entities.",0.0666666667],["GPs are useful in many applications, including Bayesian optimization. GPs","Scaling Gaussian Process Regression with Derivatives","summarize: Gaussian processes with derivatives are useful in many applications, including Bayesian optimization, implicit surface reconstruction, and terrain reconstruction. Fitting a GP to function values and derivatives at ",0.0714285714],["for any countable homogeneous ordered graph, we show that for any countable homo","Conjugacy for homogeneous ordered graphs","summarize: We show that for any countable homogeneous ordered graph ",0.25],["a smaller line of work considers physical adversarial attacks. in all cases these","Adversarial camera stickers: A physical camera-based attack on deep learning systems","summarize: Recent work has documented the susceptibility of deep learning systems to adversarial examples, but most such attacks directly manipulate the digital input to a classifier. Although a smaller line of work considers physical adversarial attacks, in all cases these involve manipulating the object of interest, e.g., putting a physical sticker on an object to misclassify it, or manufacturing an object specifically intended to be misclassified. In this work, we consider an alternative question: is it possible to fool deep classifiers, over all perceived objects of a certain type, by physically manipulating the camera itself? We show that by placing a carefully crafted and mainly-translucent sticker over the lens of a camera, one can create universal perturbations of the observed images that are inconspicuous, yet misclassify target objects as a different class. To accomplish this, we propose an iterative procedure for both updating the attack perturbation , and the threat model itself . For example, we show that we can achieve physically-realizable attacks that fool ImageNet classifiers in a targeted fashion 49.6% of the time. This presents a new class of physically-realizable threat models to consider in the context of adversarially robust machine learning. Our demo video can be viewed at: https:\/\/youtu.be\/wUVmL33Fx54",0.5],["a self-supervised learning method for denoising DWI data is introduced. patch","Patch2Self: Denoising Diffusion MRI with Self-Supervised Learning","summarize: Diffusion-weighted magnetic resonance imaging is the only noninvasive method for quantifying microstructure and reconstructing white-matter pathways in the living human brain. Fluctuations from multiple sources create significant additive noise in DWI data which must be suppressed before subsequent microstructure analysis. We introduce a self-supervised learning method for denoising DWI data, Patch2Self, which uses the entire volume to learn a full-rank locally linear denoiser for that volume. By taking advantage of the oversampled q-space of DWI data, Patch2Self can separate structure from noise without requiring an explicit model for either. We demonstrate the effectiveness of Patch2Self via quantitative and qualitative improvements in microstructure modeling, tracking and model estimation relative to other unsupervised methods on real and simulated data.",0.2352941176],["Regulatory compliance is a well-studied area, including research on how to model,","Compliance Requirements in Large-Scale Software Development: An Industrial Case Study","summarize: Regulatory compliance is a well-studied area, including research on how to model, check, analyse, enact, and verify compliance of software. However, while the theoretical body of knowledge is vast, empirical evidence on challenges with regulatory compliance, as faced by industrial practitioners particularly in the Software Engineering domain, is still lacking. In this paper, we report on an industrial case study which aims at providing insights into common practices and challenges with checking and analysing regulatory compliance, and we discuss our insights in direct relation to the state of reported evidence. Our study is performed at Ericsson AB, a large telecommunications company, which must comply to both locally and internationally governing regulatory entities and standards such as GDPR. The main contributions of this work are empirical evidence on challenges experienced by Ericsson that complement the existing body of knowledge on regulatory compliance.",0.3333333333],["a proof of concept is a proof of concept implementation of an Incremental CAD","Towards Incremental Cylindrical Algebraic Decomposition in Maple","summarize: Cylindrical Algebraic Decomposition is an important tool within computational real algebraic geometry, capable of solving many problems for polynomial systems over the reals. It has long been studied by the Symbolic Computation community and has found recent interest in the Satisfiability Checking community. The present report describes a proof of concept implementation of an Incremental CAD algorithm in Maple, where CADs are built and then refined as additional polynomial constraints are added. The aim is to make CAD suitable for use as a theory solver for SMT tools who search for solutions by continually reformulating logical formula and querying whether a logical solution is admissible. We describe experiments for the proof of concept, which clearly display the computational advantages compared to iterated re-computation. In addition, the project implemented this work under the recently verified Lazard projection scheme .",0.2894736842],["affine CapsNets are more robust than CNNs to affine transformations","Improving the Robustness of Capsule Networks to Image Affine Transformations","summarize: Convolutional neural networks achieve translational invariance by using pooling operations. However, the operations do not preserve the spatial relationships in the learned representations. Hence, CNNs cannot extrapolate to various geometric transformations of inputs. Recently, Capsule Networks have been proposed to tackle this problem. In CapsNets, each entity is represented by a vector and routed to high-level entity representations by a dynamic routing algorithm. CapsNets have been shown to be more robust than CNNs to affine transformations of inputs. However, there is still a huge gap between their performance on transformed inputs compared to untransformed versions. In this work, we first revisit the routing procedure by rolling its forward and backward passes. Our investigation reveals that the routing procedure contributes neither to the generalization ability nor to the affine robustness of the CapsNets. Furthermore, we explore the limitations of capsule transformations and propose affine CapsNets , which are more robust to affine transformations. On our benchmark task, where models are trained on the MNIST dataset and tested on the AffNIST dataset, our Aff-CapsNets improve the benchmark performance by a large margin , without using any routing mechanism.",0.0833333333],["exposure to engagement metrics increases the vulnerability of users to misinformation. this finding has important implications","Exposure to Social Engagement Metrics Increases Vulnerability to Misinformation","summarize: News feeds in virtually all social media platforms include engagement metrics, such as the number of times each post is liked and shared. We find that exposure to these social engagement signals increases the vulnerability of users to misinformation. This finding has important implications for the design of social media interactions in the misinformation age. To reduce the spread of misinformation, we call for technology platforms to rethink the display of social engagement metrics. Further research is needed to investigate whether and how engagement metrics can be presented without amplifying the spread of low-credibility information.",0.2222222222],["Jordan algebras were first introduced in an effort to restructure quantum mechanics purely in","The geometry of physical observables","summarize: Jordan algebras were first introduced in an effort to restructure quantum mechanics purely in terms of physical observables. In this paper we explain why, if one attempts to reformulate the internal structure of the standard model of particle physics geometrically, one arrives naturally at a discrete internal geometry that is coordinatized by a Jordan algebra.",0.0],["the maximum inequalities for diffusion processes have drawn increasing attention in recent years. the","Moderate maximal inequalities for the Ornstein-Uhlenbeck process","summarize: The maximal inequalities for diffusion processes have drawn increasing attention in recent years. However, the existing proof of the ",0.3157894737],["linear regression in linear regression. linear regression in linear regression.","Fast, Provably convergent IRLS Algorithm for p-norm Linear Regression","summarize: Linear regression in ",0.0454545455],["a new framework and methodology is proposed to analyze data from certain image-based biochemical as","Cell Detection by Functional Inverse Diffusion and Non-negative Group Sparsity","summarize: In this two-part paper, we present a novel framework and methodology to analyze data from certain image-based biochemical assays, e.g., ELISPOT and Fluorospot assays. In this first part, we start by presenting a physical partial differential equations model up to image acquisition for these biochemical assays. Then, we use the PDEs' Green function to derive a novel parametrization of the acquired images. This parametrization allows us to propose a functional optimization problem to address inverse diffusion. In particular, we propose a non-negative group-sparsity regularized optimization problem with the goal of localizing and characterizing the biological cells involved in the said assays. We continue by proposing a suitable discretization scheme that enables both the generation of synthetic data and implementable algorithms to address inverse diffusion. We end Part I by providing a preliminary comparison between the results of our methodology and an expert human labeler on real data. Part II is devoted to providing an accelerated proximal gradient algorithm to solve the proposed problem and to the empirical validation of our methodology.",0.1851851852],["a time series model is developed for time-series. the LSTM-","Development and Evaluation of Recurrent Neural Network based Models for Hourly Traffic Volume and AADT Prediction","summarize: The prediction of high-resolution hourly traffic volumes of a given roadway is essential for transportation planning. Traditionally, Automatic Traffic Recorders are used to collect this hourly volume data. These large datasets are time series data characterized by long-term temporal dependencies and missing values. Regarding the temporal dependencies, all roadways are characterized by seasonal variations that can be weekly, monthly or yearly, depending on the cause of the variation. Regarding the missing data in a time-series sequence, traditional time series forecasting models perform poorly under the influence of seasonal variations. To address this limitation, robust, Recurrent Neural Network based, multi-step ahead forecasting models are developed for time-series in this study. The simple RNN, the Gated Recurrent Unit and the Long Short-Term Memory units are used to develop the model and evaluate its performance. Two approaches are used to address the missing value issue: masking and imputation, in conjunction with the RNN models. Six different imputation algorithms are then used to identify the best model. The analysis indicates that the LSTM model performs better than simple RNN and GRU models, and imputation performs better than masking to predict future traffic volume. Based on analysis using 92 ATRs, the LSTM-Median model is deemed the best model in all scenarios for hourly traffic volume and AADT prediction, with an average RMSE of 274 and MAPE of 18.91% for hourly traffic volume prediction and average RMSE of 824 and MAPE of 2.10% for AADT prediction.",0.091468606],["DFM rules are a list of recommended rules which aim to improve design margins for better","In Design DFM Rule Scoring and Fixing Method using ICV","summarize: As compared to DRC rules, DFM rules are a list of selected recommended rules which aim to improve the design margins for better manufacturability. In GLOBALFOUNDRIES, we use DFM scoring methodology as an effective technique to analyze design quality in terms of manufacturability. Physical design engineers can perform our Manufacturability Check Deck to asset their design quality during the sign-off stage. In the past, Synopsys users have to convert their design though milkyway database to GDSII format and execute the verification through the third party EDA tools. This method is costly and time-consuming for our Synopsys users. Today, we propose a new and easy-to-use integrated flow which leverages on the ICV engine to provide DFM scoring and in-design fixing techniques. The new methodology address DFM violations early in the design flow and achieve DFM compliance design during sign-off phase.",0.0952380952],["a new paper explores the limitations of our original work. we address issues about the","Cluster Failure Revisited: Impact of First Level Design and Data Quality on Cluster False Positive Rates","summarize: Methodological research rarely generates a broad interest, yet our work on the validity of cluster inference methods for functional magnetic resonance imaging created intense discussion on both the minutia of our approach and its implications for the discipline. In the present work, we take on various critiques of our work and further explore the limitations of our original work. We address issues about the particular event-related designs we used, considering multiple event types and randomisation of events between subjects. We consider the lack of validity found with one-sample permutation tests, investigating a number of approaches to improve the false positive control of this widely used procedure. We found that the combination of a two-sided test and cleaning the data using ICA FIX resulted in nominal false positive rates for all datasets, meaning that data cleaning is not only important for resting state fMRI, but also for task fMRI. Finally, we discuss the implications of our work on the fMRI literature as a whole, estimating that at least 10% of the fMRI studies have used the most problematic cluster inference method , and how individual studies can be interpreted in light of our findings. These additional results underscore our original conclusions, on the importance of data sharing and thorough evaluation of statistical methods on realistic null data.",0.2976613134],["we analyze the fastest way to process graphs. we consider 11 graph algorithms, 3 programming","To Push or To Pull: On Reducing Communication and Synchronization in Graph Computations","summarize: We reduce the cost of communication and synchronization in graph processing by analyzing the fastest way to process graphs: pushing the updates to a shared state or pulling the updates to a private state.We investigate the applicability of this push-pull dichotomy to various algorithms and its impact on complexity, performance, and the amount of used locks, atomics, and reads\/writes. We consider 11 graph algorithms, 3 programming models, 2 graph abstractions, and various families of graphs. The conducted analysis illustrates surprising differences between push and pull variants of different algorithms in performance, speed of convergence, and code complexity; the insights are backed up by performance data from hardware counters.We use these findings to illustrate which variant is faster for each algorithm and to develop generic strategies that enable even higher speedups. Our insights can be used to accelerate graph processing engines or libraries on both massively-parallel shared-memory machines as well as distributed-memory systems.",0.0],["the master equation is a very useful tool to study the decoherence of a","Alternative Derivation of the Hu-Paz-Zhang Master Equation for Quantum Brownian Motion","summarize: Hu, Paz and Zhang have derived an exact master equation for quantum Brownian motion in a general environment via path integral techniques. Their master equation provides a very useful tool to study the decoherence of a quantum system due to the interaction with its environment. In this paper, we give an alternative and elementary derivation of the Hu-Paz-Zhang master equation, which involves tracing the evolution equation for the Wigner function. We also discuss the master equation in some special cases.",0.4782608696],["magnetic reconnection provides the primary source for explosive energy release, plasma heating and particle acceleration in","Fast Magnetic Reconnection: Secondary Tearing Instability and Role of the Hall Term","summarize: Magnetic reconnection provides the primary source for explosive energy release, plasma heating and particle acceleration in many astrophysical environments. The last years witnessed a revival of interest in the MHD tearing instability as a driver for efficient reconnection. It has been established that, provided the current sheet aspect ratio becomes small enough , reconnection occurs on ideal Alfv\\'en timescales and becomes independent on ",0.1764705882],["direct nanoimprinting of crystalline metals is a simple and high-throughput","One-step fabrication of metal nanostructures by high-throughput imprinting","summarize: Direct nanoimprinting provides a simple and high-throughput route for producing uniform nanopatterns at great precision and at low costs. However, applying this technique to crystalline metals has been considered as impossible due to intrinsic limitation from grain size effect. Here we demonstrate direct superplastic nanoimprinting of crystalline metals well below their melting temperatures , generating ordered nanowire arrays with aspect ratio up to ~2000. Our investigations of replicating metal hierarchical nanostructures show the capability of imprinting features as small as 8 nm, far smaller than the grain size of bulk metals. Most surprisingly, the prepared metal hierarchical nanostructures were found possessing perfect monocrystalline structures. These findings indicate that nanoimprinting of crystalline metals below Tm might be from lattice diffusion. SPNI as a one-step and highly controlled high-throughput fabrication method, could facilitate the applications of metal nanostructures in bio-sensing, diagnostic imaging, catalysis, food industry and environmental conservation.",0.4285714286],["the optimal open-loop solution passes by the optimal steady-state for consecutive time instants","Stability and performance in transient average constrained economic MPC without terminal constraints","summarize: In this paper, we investigate system theoretic properties of transient average constrained economic model predictive control without terminal constraints. We show that the optimal open-loop solution passes by the optimal steady-state for consecutive time instants. Using this turnpike property and suitable controllability conditions, we provide closed-loop performance bounds. Furthermore, stability is proved by combining the rotated value function with an input-to-state Lyapunov function of an extended state related to the transient average constraints. The results are illustrated with a numerical example.",0.0588235294],["forensic techniques look for traces that are mostly ineffective when dealing with strongly compressed or low","Efficient video integrity analysis through container characterization","summarize: Most video forensic techniques look for traces within the data stream that are, however, mostly ineffective when dealing with strongly compressed or low resolution videos. Recent research highlighted that useful forensic traces are also left in the video container structure, thus offering the opportunity to understand the life-cycle of a video file without looking at the media stream itself. In this paper we introduce a container-based method to identify the software used to perform a video manipulation and, in most cases, the operating system of the source device. As opposed to the state of the art, the proposed method is both efficient and effective and can also provide a simple explanation for its decisions. This is achieved by using a decision-tree-based classifier applied to a vectorial representation of the video container structure. We conducted an extensive validation on a dataset of 7000 video files including both software manipulated contents , and videos exchanged through social media platforms . This dataset has been made available to the research community. The proposed method achieves an accuracy of 97.6% in distinguishing pristine from tampered videos and classifying the editing software, even when the video is cut without re-encoding or when it is downscaled to the size of a thumbnail. Furthermore, it is capable of correctly identifying the operating system of the source device for most of the tampered videos.",0.0],["the ZTF alert stream distribution and processing system uses open-source technologies developed in industry.","The Zwicky Transient Facility Alert Distribution System","summarize: The Zwicky Transient Facility survey generates real-time alerts for optical transients, variables, and moving objects discovered in its wide-field survey. We describe the ZTF alert stream distribution and processing system. The system uses existing open-source technologies developed in industry: Kafka, a real-time streaming platform, and Avro, a binary serialization format. The technologies used in this system provide a number of advantages for the ZTF use case, including built-in replication, scalability, and stream rewind for the distribution mechanism; structured messages with strictly enforced schemas and dynamic typing for fast parsing; and a Python-based stream processing interface that is similar to batch for a familiar and user-friendly plug-in filter system, all in a modular, primarily containerized system. The production deployment has successfully supported streaming up to 1.2 million alerts or roughly 70 GB of data per night, with each alert available to a consumer within about 10 s of alert candidate production. Data transfer rates of about 80,000 alerts\/minute have been observed. In this paper, we discuss this alert distribution and processing system, the design motivations for the technology choices for the framework, performance in production, and how this system may be generally suitable for other alert stream use cases, including the upcoming Large Synoptic Survey Telescope.",0.0],["this paper investigates the generalized rank weights. we study rank metric codes over","On defining generalized rank weights","summarize: This paper investigates the generalized rank weights, with a definition implied by the study of the generalized rank weight enumerator. We study rank metric codes over ",0.25],["a&R uses two ideas: latent variable augmentation and stochastic variational","Augment and Reduce: Stochastic Inference for Large Categorical Distributions","summarize: Categorical distributions are ubiquitous in machine learning, e.g., in classification, language models, and recommendation systems. However, when the number of possible outcomes is very large, using categorical distributions becomes computationally expensive, as the complexity scales linearly with the number of outcomes. To address this problem, we propose augment and reduce , a method to alleviate the computational complexity. A&R uses two ideas: latent variable augmentation and stochastic variational inference. It maximizes a lower bound on the marginal likelihood of the data. Unlike existing methods which are specific to softmax, A&R is more general and is amenable to other categorical models, such as multinomial probit. On several large-scale classification problems, we show that A&R provides a tighter bound on the marginal likelihood and has better predictive performance than existing approaches.",0.1],["multigrid method is based on a stable splitting of the spline space","Robust Multigrid for Isogeometric Analysis Based on Stable Splittings of Spline Spaces","summarize: We present a robust and efficient multigrid method for single-patch isogeometric discretizations using tensor product B-splines of maximum smoothness. Our method is based on a stable splitting of the spline space into a large subspace of interior splines which satisfy a robust inverse inequality, as well as one or several smaller subspaces which capture the boundary effects responsible for the spectral outliers which occur in Isogeometric Analysis. We then construct a multigrid smoother based on an additive subspace correction approach, applying a different smoother to each of the subspaces. For the interior splines, we use a mass smoother, whereas the remaining components are treated with suitably chosen Kronecker product smoothers or direct solvers. We prove that the resulting multigrid method exhibits iteration numbers which are robust with respect to the spline degree and the mesh size. Furthermore, it can be efficiently realized for discretizations of problems in arbitrarily high geometric dimension. Some numerical examples illustrate the theoretical results and show that the iteration numbers also scale relatively mildly with the problem dimension.",0.4666666667],["generic object recognition is a challenging problem because all possible appearances of objects cannot be registered","Construction of Latent Descriptor Space and Inference Model of Hand-Object Interactions","summarize: Appearance-based generic object recognition is a challenging problem because all possible appearances of objects cannot be registered, especially as new objects are produced every day. Function of objects, however, has a comparatively small number of prototypes. Therefore, function-based classification of new objects could be a valuable tool for generic object recognition. Object functions are closely related to hand-object interactions during handling of a functional object; i.e., how the hand approaches the object, which parts of the object and contact the hand, and the shape of the hand during interaction. Hand-object interactions are helpful for modeling object functions. However, it is difficult to assign discrete labels to interactions because an object shape and grasping hand-postures intrinsically have continuous variations. To describe these interactions, we propose the interaction descriptor space which is acquired from unlabeled appearances of human hand-object interactions. By using interaction descriptors, we can numerically describe the relation between an object's appearance and its possible interaction with the hand. The model infers the quantitative state of the interaction from the object image alone. It also identifies the parts of objects designed for hand interactions such as grips and handles. We demonstrate that the proposed method can unsupervisedly generate interaction descriptors that make clusters corresponding to interaction types. And also we demonstrate that the model can infer possible hand-object interactions.",0.2222222222],["entropic regularization is a method of computing entropic regularized OT","Entropic Optimal Transport between Unbalanced Gaussian Measures has a Closed Form","summarize: Although optimal transport problems admit closed form solutions in a very few notable cases, e.g. in 1D or between Gaussians, these closed forms have proved extremely fecund for practitioners to define tools inspired from the OT geometry. On the other hand, the numerical resolution of OT problems using entropic regularization has given rise to many applications, but because there are no known closed-form solutions for entropic regularized OT problems, these approaches are mostly algorithmic, not informed by elegant closed forms. In this paper, we propose to fill the void at the intersection between these two schools of thought in OT by proving that the entropy-regularized optimal transport problem between two Gaussian measures admits a closed form. Contrary to the unregularized case, for which the explicit form is given by the Wasserstein-Bures distance, the closed form we obtain is differentiable everywhere, even for Gaussians with degenerate covariance matrices. We obtain this closed form solution by solving the fixed-point equation behind Sinkhorn's algorithm, the default method for computing entropic regularized OT. Remarkably, this approach extends to the generalized unbalanced case -- where Gaussian measures are scaled by positive constants. This extension leads to a closed form expression for unbalanced Gaussians as well, and highlights the mass transportation \/ destruction trade-off seen in unbalanced optimal transport. Moreover, in both settings, we show that the optimal transportation plans are Gaussians and provide analytical formulas of their parameters. These formulas constitute the first non-trivial closed forms for entropy-regularized optimal transport, thus providing a ground truth for the analysis of entropic OT and Sinkhorn's algorithm.",0.2412899781],["Let us know what you think about it!","The hit problem of five variables in the generic degree and its application","summarize: Let ",0.0],["efficient algorithm of solution is proposed. the algorithm can be easily adapted to circular or","An efficient algorithm of solution for the flow of generalized Newtonian fluid in channels of simple geometries","summarize: In this paper a problem of stationary flow of generalized Newtonian fluid in a thin channel is considered. An efficient algorithm of solution is proposed that includes a flexible procedure for a continuous approximation of the apparent viscosity by means of elementary functions combined with analytical integration of the governing equations. The algorithm can be easily adapted to circular or elliptic conduits. The accuracy and efficiency of computations are analyzed using an example of the Carreau fluid. The proposed computational scheme proves to be highly efficient and versatile providing excellent accuracy of solution at a very low computational cost.",0.4145557827],["the feasible region can be employed for the selection of feasible footholds and CoM tra","Feasible Region: an Actuation-Aware Extension of the Support Region","summarize: In legged locomotion the projection of the robot Center of Mass being inside the convex hull of the contact points is a commonly accepted sufficient condition to achieve static balancing. However, some of these configurations cannot be realized because the joint torques required to sustain them would be above their limits . In this manuscript we rule out such configurations and define the Feasible Region, a revisited support region that guarantees both global static stability in the sense of tipover and slippage avoidance and of existence of a set of joint-torques that are able to sustain the robot body weight. We show that the feasible region can be employed for the selection of feasible footholds and CoM trajectories to achieve static locomotion on rough terrains, also in presence of load intensive tasks. Key results of our approach include the efficiency in the computation of the feasible region thanks to an Iterative Projection algorithm. This allowed us to carry out successful experiments on the HyQ robot, that was able to negotiate obstacles of moderate dimensions while carrying an extra 10 kg payload.",0.1578947368],["threshold functions for homogeneous random intersection graphs have to be modified and extended.","The coupling method for inhomogeneous random intersection graphs","summarize: We present new results concerning threshold functions for a wide family of random intersection graphs. To this end we apply the coupling method used for establishing threshold functions for homogeneous random intersection graphs introduced by Karo\\'nski, Scheinerman, and Singer--Cohen. In the case of inhomogeneous random intersection graphs the method has to be considerably modified and extended. By means of the altered method we are able to establish threshold functions for a general random intersection graph for such properties as ",0.4285714286],["this paper generalizes the definition of a multilinear map to arbitrary groups","Multilinear Cryptography using Nilpotent Groups","summarize: In this paper we generalize the definition of a multilinear map to arbitrary groups and develop a novel idea of multilinear cryptosystem using nilpotent group identities.",0.1666666667],["the standard approach to MCMC involves constructing discrete-time reversible Mar","Limit theorems for the Zig-Zag process","summarize: Markov chain Monte Carlo methods provide an essential tool in statistics for sampling from complex probability distributions. While the standard approach to MCMC involves constructing discrete-time reversible Markov chains whose transition kernel is obtained via the Metropolis- Hastings algorithm, there has been recent interest in alternative schemes based on piecewise deterministic Markov processes . One such approach is based on the Zig-Zag process, introduced in Bierkens and Roberts , which proved to provide a highly scalable sampling scheme for sampling in the big data regime ). In this paper we study the performance of the Zig-Zag sampler, focusing on the one-dimensional case. In particular, we identify conditions under which a Central limit theorem holds and characterize the asymptotic variance. Moreover, we study the influence of the switching rate on the diffusivity of the Zig-Zag process by identifying a diffusion limit as the switching rate tends to infinity. Based on our results we compare the performance of the Zig-Zag sampler to existing Monte Carlo methods, both analytically and through simulations.",0.1],["silico results show that single-hormone and dual-hormone delivery strategies achieve good","Basal Glucose Control in Type 1 Diabetes using Deep Reinforcement Learning: An In Silico Validation","summarize: People with Type 1 diabetes require regular exogenous infusion of insulin to maintain their blood glucose concentration in a therapeutically adequate target range. Although the artificial pancreas and continuous glucose monitoring have been proven to be effective in achieving closed-loop control, significant challenges still remain due to the high complexity of glucose dynamics and limitations in the technology. In this work, we propose a novel deep reinforcement learning model for single-hormone and dual-hormone delivery. In particular, the delivery strategies are developed by double Q-learning with dilated recurrent neural networks. For designing and testing purposes, the FDA-accepted UVA\/Padova Type 1 simulator was employed. First, we performed long-term generalized training to obtain a population model. Then, this model was personalized with a small data-set of subject-specific data. In silico results show that the single and dual-hormone delivery strategies achieve good glucose control when compared to a standard basal-bolus therapy with low-glucose insulin suspension. Specifically, in the adult cohort , percentage time in target range mg\/dL improved from 77.6% to 80.9% with single-hormone control, and to ",0.0],["a novel method for gradient-based optimization of black-box simulators. we propose","Black-Box Optimization with Local Generative Surrogates","summarize: We propose a novel method for gradient-based optimization of black-box simulators using differentiable local surrogate models. In fields such as physics and engineering, many processes are modeled with non-differentiable simulators with intractable likelihoods. Optimization of these forward models is particularly challenging, especially when the simulator is stochastic. To address such cases, we introduce the use of deep generative models to iteratively approximate the simulator in local neighborhoods of the parameter space. We demonstrate that these local surrogates can be used to approximate the gradient of the simulator, and thus enable gradient-based optimization of simulator parameters. In cases where the dependence of the simulator on the parameter space is constrained to a low dimensional submanifold, we observe that our method attains minima faster than baseline methods, including Bayesian optimization, numerical optimization, and approaches using score function gradient estimators.",0.3125],["the fractional derivative creates substantial difficulties compared with the case of a classical parabo","Regularity theory for time-fractional advection-diffusion-reaction equations","summarize: We investigate the behavior of the time derivatives of the solution to a linear time-fractional, advection-diffusion-reaction equation, allowing space- and time-dependent coefficients as well as initial data that may have low regularity. Our focus is on proving estimates that are needed for the error analysis of numerical methods. The nonlocal nature of the fractional derivative creates substantial difficulties compared with the case of a classical parabolic PDE. In our analysis, we rely on novel energy methods in combination with a fractional Gronwall inequality and certain properties of fractional integrals.",0.3214285714],["medical first responders are trained to deal with emergencies more effectively. this would require real-","Effects of Voice-Based Synthetic Assistant on Performance of Emergency Care Provider in Training","summarize: As part of a perennial project, our team is actively engaged in developing new synthetic assistant technologies to assist in training combat medics and medical first responders. It is critical that medical first responders are well trained to deal with emergencies more effectively. This would require real-time monitoring and feedback for each trainee. Therefore, we introduced a voice-based SA to augment the training process of medical first responders and enhance their performance in the field. The potential benefits of SAs include a reduction in training costs and enhanced monitoring mechanisms. Despite the increased usage of voice-based personal assistants in day-to-day life, the associated effects are commonly neglected for a study of human factors. Therefore, this paper focuses on performance analysis of the developed voice-based SA in emergency care provider training for a selected emergency treatment scenario. The research discussed in this paper follows design science in developing proposed technology; at length, we discussed architecture and development and presented working results of voice-based SA. The empirical testing was conducted on two groups as user studies using statistical analysis tools, one trained with conventional methods and the other with the help of SA. The statistical results demonstrated the amplification in training efficacy and performance of medical responders powered by SA. Furthermore, the paper also discusses the accuracy and time of task execution and concludes with the guidelines for resolving the identified problems.",0.0666666667],["tight continuity bounds obtained for quantum mutual information and relative entropy of entang","Lower bounds on distances between a given quantum channel and certain classes of channels","summarize: The tight, in a sense, lower estimates of diamond-norm distance from a given quantum channel to the sets of degradable, antidegradable and entanglement-breaking channels are obtained via the tight continuity bounds for quantum mutual information and for relative entropy of entanglement in finite-dimensional case. As an auxiliary result there are established lower bounds of trace-norm distance from a given bipartite state to the set of all separable states.",0.2645603082],["observations suggest that the underlying water significantly affects the morphogenesis of leaves","Water affects morphogenesis of growing aquatic plant leaves","summarize: Lotus leaves floating on water usually experience short-wavelength edge wrinkling that decays toward the center, while the leaves growing above water normally morph into a global bending cone shape with long rippled waves near the edge. Observations suggest that the underlying water significantly affects the morphogenesis of leaves. To understand the biophysical mechanism under such phenomena, we develop mathematical models that can effectively account for inhomogeneous differential growth of floating and free-standing leaves, to quantitatively predict formation and evolution of their morphology. We find, both theoretically and experimentally, that the short-wavelength buckled configuration is energetically favorable for growing membranes lying on liquid, while the global buckling shape is more preferable for suspended ones. Other influencing factors such as stem\/vein, heterogeneity and dimension are also investigated. Our results provide a fundamental insight into a variety of plant morphogenesis affected by water foundation and suggest that such surface instabilities can be harnessed for morphology control of biomimetic deployable structures using substrate or edge actuation.",0.2857142857],["we propose a decentralized algorithm, based on a non-cooperative cost-","Cost Sharing Games for Energy-Efficient Multi-Hop Broadcast in Wireless Networks","summarize: We study multi-hop broadcast in wireless networks with one source node and multiple receiving nodes. The message flow from the source to the receivers can be modeled as a tree-graph, called broadcast-tree. The problem of finding the minimum-power broadcast-tree is NP-complete. Unlike most of the existing centralized approaches, we propose a decentralized algorithm, based on a non-cooperative cost-sharing game. In this game, every receiving node, as a player, chooses another node of the network as its respective transmitting node for receiving the message. Consequently, a cost is assigned to the receiving node based on the power imposed on its chosen transmitting node. In our model, the total required power at a transmitting node consists of the transmit power and the circuitry power needed for communication hardware modules. We develop our algorithm using the marginal contribution cost-sharing scheme and show that the optimum broadcast-tree is always a Nash equilibrium of the game. Simulation results demonstrate that our proposed algorithm outperforms conventional algorithms for the MPBT problem. Besides, we show that the circuitry power, which is usually ignored by existing algorithms, significantly impacts the energy-efficiency of the network.",0.380952381],["the automated generation of Japanese fonts is in high demand. a Japanese font requires over","Automatic Generation of Typographic Font from a Small Font Subset","summarize: This paper addresses the automatic generation of a typographic font from a subset of characters. Specifically, we use a subset of a typographic font to extrapolate additional characters. Consequently, we obtain a complete font containing a number of characters sufficient for daily use. The automated generation of Japanese fonts is in high demand because a Japanese font requires over 1,000 characters. Unfortunately, professional typographers create most fonts, resulting in significant financial and time investments for font generation. The proposed method can be a great aid for font creation because designers do not need to create the majority of the characters for a new font. The proposed method uses strokes from given samples for font generation. The strokes, from which we construct characters, are extracted by exploiting a character skeleton dataset. This study makes three main contributions: a novel method of extracting strokes from characters, which is applicable to both standard fonts and their variations; a fully automated approach for constructing characters; and a selection method for sample characters. We demonstrate our proposed method by generating 2,965 characters in 47 fonts. Objective and subjective evaluations verify that the generated characters are similar to handmade characters.",0.2307692308],["the advent of fullerenes, nanotubes and graphene has opened new","Carbyne: from the elusive allotrope to stable carbon atom wires","summarize: Besides graphite and diamond, the solid allotropes of carbon in sp2 and sp3 hybridization, the possible existence of a third allotrope based on the sp-carbon linear chain, the Carbyne, has stimulated researchers for a long time. The advent of fullerenes, nanotubes and graphene has opened new opportunities and nurtured the interest in novel carbon allotropes including linear structures. The efforts made in this direction produced a number of interesting sp-hybridized carbon molecules and nanostructures in the form of carbon-atom wires. We here discuss some of the new perspectives opened by the recent advancements in the research on sp-carbon systems.",0.1],["multigrid method is based on a stable splitting of the spline space","Robust Multigrid for Isogeometric Analysis Based on Stable Splittings of Spline Spaces","summarize: We present a robust and efficient multigrid method for single-patch isogeometric discretizations using tensor product B-splines of maximum smoothness. Our method is based on a stable splitting of the spline space into a large subspace of interior splines which satisfy a robust inverse inequality, as well as one or several smaller subspaces which capture the boundary effects responsible for the spectral outliers which occur in Isogeometric Analysis. We then construct a multigrid smoother based on an additive subspace correction approach, applying a different smoother to each of the subspaces. For the interior splines, we use a mass smoother, whereas the remaining components are treated with suitably chosen Kronecker product smoothers or direct solvers. We prove that the resulting multigrid method exhibits iteration numbers which are robust with respect to the spline degree and the mesh size. Furthermore, it can be efficiently realized for discretizations of problems in arbitrarily high geometric dimension. Some numerical examples illustrate the theoretical results and show that the iteration numbers also scale relatively mildly with the problem dimension.",0.4666666667],["we investigate the propagating profiles of a degenerate chemotaxis model","Propagating Profiles of a Chemotaxis Model with Degenerate Diffusion: Initial Shrinking, Eventual Smoothness and Expanding","summarize: We investigate the propagating profiles of a degenerate chemotaxis model describing the bacteria chemotaxis and consumption of oxygen by aerobic bacteria, in particular, the effect of the initial attractant distribution on bacterial clustering. We prove that the compact support of solutions may shrink if the signal concentration satisfies a special structure, and show the finite speed propagating property without assuming the special structure on attractant concentration, and obtain an explicit formula of the population spreading speed in terms of model parameters. The presented results suggest that bacterial cluster formation can be affected by chemotactic attractants and density-dependent dispersal.",0.3032653299],["laser power requirements render PPPP uncompetitive with laser Tomography AO.","Projected Pupil Plane Pattern with artificial Neural Networks","summarize: Focus anisoplanatism is a significant measurement error when using one single laser guide star in an Adaptive Optics system, especially for the next generation of extremely large telescopes. An alternative LGS configuration, called Projected Pupil Plane Pattern solves this problem by launching a collimated laser beam across the full pupil of the telescope. If using a linear, modal reconstructor, the high laser power requirement renders PPPP uncompetitive with Laser Tomography AO. This work discusses easing the laser power requirements by using an artificial Neural Network as a non-linear reconstructor. We find that the non-linear NN reduces the required measurement signal-to-noise ratio significantly to reduce PPPP laser power requirements to ",0.0833333333],["experimental experiment aims to distinguish two classical gases. the work that can be extracted from mixing","Gibbs mixing of partially distinguishable photons with a polarising beamsplitter membrane","summarize: For a thought experiment concerning the mixing of two classical gases, Gibbs concluded that the work that can be extracted from mixing is determined by whether or not the gases can be distinguished by a semi-permeable membrane; that is, the mixing work is a discontinuous function of how similar the gases are. Here we describe an optomechanical setup that generalises Gibbs' thought experiment to partially distinguishable quantum gases. Specifically, we model the interaction between a polarising beamsplitter, that plays the role of a semi-permeable membrane, and two photon gases of non-orthogonal polarisation. We find that the work arising from the mixing of the gases is related to the potential energy associated with the displacement of the microscopic membrane, and we derive a general quantum mixing work expression, valid for any two photon gases with the same number distribution. The quantum mixing work is found to change continuously with the distinguishability of the two polarised gases. In addition, fluctuations of the work on the microscopic membrane become important, which we calculate for Fock and thermal states of the photon gases. Our findings generalise Gibbs' mixing to the quantum regime and open the door for new quantum thermodynamic experiments with quantum gases with non-orthogonal polarisations and microscopic pistons that can distinguish orthogonal polarisations.",0.2352941176],["this article examines the structure and spatial patterns of violent political organizations in the Sahel-","Wars Without Beginning or End: Violent Political Organizations and Irregular Warfare in the Sahel-Sahara","summarize: This article examines the structure and spatial patterns of violent political organizations in the Sahel-Sahara, a region characterized by growing political instability over the last 20 years. Drawing on a public collection of disaggregated data, the article uses network science to represent alliances and conflicts of 179 organizations that were involved in violent events between 1997 and 2014. To this end, we combine two spectral embedding techniques that have previously been considered separately: one for directed graphs , and one for signed graphs . Our result show that groups that are net attackers are indistinguishable at the level of their individual behavior, but clearly separate into pro- and anti-political violence based on the groups to which they are close. The second part of the article maps a series of 389 events related to nine Trans-Saharan Islamist groups between 2004 and 2014. Spatial analysis suggests that cross-border movement has intensified following the establishment of military bases by AQIM in Mali but reveals no evidence of a border sanctuary. Owing to the transnational nature of conflict, the article shows that national management strategies and foreign military interventions have profoundly affected the movement of Islamist groups.",0.3333333333],["binary fluid mixtures are examples of complex fluids whose microstructure and flow are strongly coupled","Theories of Binary Fluid Mixtures: From Phase-Separation Kinetics to Active Emulsions","summarize: Binary fluid mixtures are examples of complex fluids whose microstructure and flow are strongly coupled. For pairs of simple fluids, the microstructure consists of droplets or bicontinuous demixed domains and the physics is controlled by the interfaces between these domains. At continuum level, the structure is defined by a composition field whose gradients which are steep near interfaces drive its diffusive current. These gradients also cause thermodynamic stresses which can drive fluid flow. Fluid flow in turn advects the composition field, while thermal noise creates additional random fluxes that allow the system to explore its configuration space and move towards the Boltzmann distribution. This article introduces continuum models of binary fluids, first covering some well-studied areas such as the thermodynamics and kinetics of phase separation, and emulsion stability. We then address cases where one of the fluid components has anisotropic structure at mesoscopic scales creating nematic liquid-crystalline order; this can be described through an additional tensor order parameter field. We conclude by outlining a thriving area of current research, namely active emulsions, in which one of the binary components consists of living or synthetic material that is continuously converting chemical energy into mechanical work.",0.0555555556],["a lexical analysis of a component is a problem. a","Identifiability of Complete Dictionary Learning","summarize: Sparse component analysis , also known as complete dictionary learning, is the following problem: Given an input matrix ",0.3714285714],["the attractive tail of the intermolecular interaction affects very weakly the structural properties of","Role of attractive forces in the relaxation dynamics of supercooled liquids","summarize: The attractive tail of the intermolecular interaction affects very weakly the structural properties of liquids, while it affects dramatically their dynamical ones. Via the numerical simulations of model systems not prone to crystallization, both in three and in two spatial dimensions, here we demonstrate that the non-perturbative dynamical effects of the attractive forces are tantamount to a rescaling of the activation energy by the glass transition temperature ",0.3636363636],["microwave window has unique property of controlling transmission and reflection. this surface can preferentially pass","Waveform Selective Surfaces","summarize: The role of frequency is very important in electromagnetics since it may significantly change how a material interacts with an incident wave if the frequency spectrum varies. Here, we demonstrate a new kind of microwave window that has the unique property of controlling transmission and reflection based not only on the frequency of an incoming wave but also on the waveform or pulse width. This surface can preferentially pass or reject different kinds of signals, such as short pulses or continuous waves, even if they occur at the same frequency. Such a structure can be used, for example, to allow long communication signals to pass through, while rejecting short radar pulses in the same frequency band. It is related to the classic frequency selective surface, but adds the new dimension of waveform selectivity, which is only possible by introducing nonlinear electronics into the surface. Thus, our study is expected to provide new solutions to both fundamental and applied electromagnetic issues ranging from traditional antenna design and wireless communications to emerging areas such as cloaking, perfect lenses, and wavefront shaping.",0.0],["Convolutional Neural Network based trackers have achieved state-of-the-art","Robust Visual Tracking via Statistical Positive Sample Generation and Gradient Aware Learning","summarize: In recent years, Convolutional Neural Network based trackers have achieved state-of-the-art performance on multiple benchmark datasets. Most of these trackers train a binary classifier to distinguish the target from its background. However, they suffer from two limitations. Firstly, these trackers cannot effectively handle significant appearance variations due to the limited number of positive samples. Secondly, there exists a significant imbalance of gradient contributions between easy and hard samples, where the easy samples usually dominate the computation of gradient. In this paper, we propose a robust tracking method via Statistical Positive sample generation and Gradient Aware learning to address the above two limitations. To enrich the diversity of positive samples, we present an effective and efficient statistical positive sample generation algorithm to generate positive samples in the feature space. Furthermore, to handle the issue of imbalance between easy and hard samples, we propose a gradient sensitive loss to harmonize the gradient contributions between easy and hard samples. Extensive experiments on three challenging benchmark datasets including OTB50, OTB100 and VOT2016 demonstrate that the proposed SPGA performs favorably against several state-of-the-art trackers.",0.0],["we introduce a definitional framework and theory that defines and characterizes a core property","Characterization of Convex Objective Functions and Optimal Expected Convergence Rates for SGD","summarize: We study Stochastic Gradient Descent with diminishing step sizes for convex objective functions. We introduce a definitional framework and theory that defines and characterizes a core property, called curvature, of convex objective functions. In terms of curvature we can derive a new inequality that can be used to compute an optimal sequence of diminishing step sizes by solving a differential equation. Our exact solutions confirm known results in literature and allows us to fully characterize a new regularizer with its corresponding expected convergence rates.",0.4375],["round functions are used as building blocks for iterated block ciphers. the","Wave-Shaped Round Functions and Primitive Groups","summarize: Round functions used as building blocks for iterated block ciphers, both in the case of Substitution-Permutation Networks and Feistel Networks, are often obtained as the composition of different layers which provide confusion and diffusion, and key additions. The bijectivity of any encryption function, crucial in order to make the decryption possible, is guaranteed by the use of invertible layers or by the Feistel structure. In this work a new family of ciphers, called wave ciphers, is introduced. In wave ciphers, round functions feature wave functions, which are vectorial Boolean functions obtained as the composition of non-invertible layers, where the confusion layer enlarges the message which returns to its original size after the diffusion layer is applied. This is motivated by the fact that relaxing the requirement that all the layers are invertible allows to consider more functions which are optimal with regard to non-linearity. In particular it allows to consider injective APN S-boxes. In order to guarantee efficient decryption we propose to use wave functions in Feistel Networks. With regard to security, the immunity from some group-theoretical attacks is investigated. In particular, it is shown how to avoid that the group generated by the round functions acts imprimitively, which represent a serious flaw for the cipher.",0.0],["the methodology of DNA-mediated computing is used to enrich decision theory. it is a","Programmable DNA-mediated decision maker","summarize: DNA-mediated computing is a novel technology that seeks to capitalize on the enormous informational capacity of DNA and has tremendous computational ability to compete with the current silicon-mediated computing, due to massive parallelism and unique characteristics inherent in DNA interaction. In this paper, the methodology of DNA-mediated computing is utilized to enrich decision theory, by demonstrating how a novel programmable DNA-mediated normative decision-making apparatus is able to capture rational choice under uncertainty.",0.3],["Riemannian manifold with strictly convex boundary can be recovered from integrals","Reconstruction of piecewise constant functions from X-ray data","summarize: We show that on a two-dimensional compact nontrapping Riemannian manifold with strictly convex boundary, a piecewise constant function can be recovered from its integrals over geodesics. We adapt the injectivity proof which uses variations through geodesics to recover the function and we improve this result when the manifold is simple and the function is constant on tiles with geodesic edges, showing that the Jacobi fields of these variations are sufficient. We give also explicit formulas for the values near the boundary. We finally study the stability of the reconstruction method.",0.0909090909],["the AF lumped-parameter model was run to simulate resting and various exercise","A Computational Study on the Relation between Resting Heart Rate and Atrial Fibrillation Hemodynamics under Exercise","summarize: Aims. Clinical data indicating a heart rate target during rate control therapy for permanent atrial fibrillation and assessing its eventual relationship with reduced exercise tolerance are lacking. The present study aims at investigating the impact of resting HR on the hemodynamic response to exercise in permanent AF patients by means of a computational cardiovascular model. Methods. The AF lumped-parameter model was run to simulate resting and various exercise conditions , considering different resting HR . To compare relative variations of cardiovascular variables upon exertion, the variation comparative index - the absolute variation between the exercise and the resting values in SHR simulations referred to the absolute variation in HHR simulations -was calculated at each exercise grade . Results. Pulmonary venous pressure underwent a greater increase in HHR compared to SHR simulations , while for systemic arterial pressure the opposite is true . Conclusions. The computational findings suggest that a slower, with respect to a higher resting HR, might be preferable in permanent AF patients, since during exercise pulmonary venous pressure undergoes a slighter increase and systemic blood pressure reveals a more appropriate increase.",0.1194218851],["hybrid MPI+threads programming is becoming an alternative to the traditional MPI everywhere'","Scalable Communication Endpoints for MPI+Threads Applications","summarize: Hybrid MPI+threads programming is gaining prominence as an alternative to the traditional MPI everywhere' model to better handle the disproportionate increase in the number of cores compared with other on-node resources. Current implementations of these two models represent the two extreme cases of communication resource sharing in modern MPI implementations. In the MPI-everywhere model, each MPI process has a dedicated set of communication resources , which is ideal for performance but is resource wasteful. With MPI+threads, current MPI implementations share a single communication endpoint for all threads, which is ideal for resource usage but is hurtful for performance. In this paper, we explore the tradeoff space between performance and communication resource usage in MPI+threads environments. We first demonstrate the two extreme cases---one where all threads share a single communication endpoint and another where each thread gets its own dedicated communication endpoint and showcase the inefficiencies in both these cases. Next, we perform a thorough analysis of the different levels of resource sharing in the context of Mellanox InfiniBand. Using the lessons learned from this analysis, we design an improved resource-sharing model to produce \\emph that can achieve the same performance as with dedicated communication resources per thread but using just a third of the resources.",0.0833333333],["the computational chemistry mailing list reveals the tensions associated with software within the community","Only the Initiates Will Have the Secrets Revealed: Computational Chemists and the Openness of Scientific Software","summarize: Computational chemistry is a scientific field within which the computer is a pivotal element. This scientific community emerged in the 1980s and was involved with two major industries: the computer manufacturers and the pharmaceutical industry, the latter becoming a potential market for the former through molecular modeling software packages. We aim to address the difficult relationships between scientific modeling methods and the software implementing these methods throughout the 1990s. Developing, using, licensing, and distributing software leads to multiple tensions among the actors in intertwined academic and industrial contexts. The Computational Chemistry mailing List , created in 1991, constitutes a valuable corpus for revealing the tensions associated with software within the community. We analyze in detail two flame wars that exemplify these tensions. We conclude that models and software must be addressed together. Interrelations between both imply that openness in computational science is complex.",0.3715190999],["delay differential equations model of leukemia is introduced. the model takes into account three","A Comparison Between the Stability Properties in a DDE Model for Leukemia and the Modified Fractional Counterpart","summarize: In this paper, a delay differential equations model of leukemia is introduced and its dynamical properties are investigated in comparison with the modified fractional-order system where the Caputo's derivative is used. The model takes into account three types of division that a stem-like cell can undergo and cell competition between healthy and leukemia cell populations. The action of the immune system on the leukemic cell populations is also considered. The stability properties of the equilibrium points are established through numerical results and the differences between the two types of approaches are discussed. Medical conclusions are drawn in view of the obtained numerical simulations.",0.1008897184],["the two-dimensional incommensurate order autonomous linear system consists of a differential","Stability properties of a two-dimensional system involving one Caputo derivative and applications to the investigation of a fractional-order Morris-Lecar neuronal model","summarize: Necessary and sufficient conditions are given for the asymptotic stability and instability of a two-dimensional incommensurate order autonomous linear system, which consists of a differential equation with a Caputo-type fractional order derivative and a classical first order differential equation. These conditions are expressed in terms of the elements of the system's matrix, as well as of the fractional order of the Caputo derivative. In this setting, we obtain a generalization of the well known Routh-Hurwitz conditions. These theoretical results are then applied to the analysis of a two-dimensional fractional-order Morris-Lecar neuronal model, focusing on stability and instability properties. This fractional order model is built up taking into account the dimensional consistency of the resulting system of differential equations. The occurrence of Hopf bifurcations is also discussed. Numerical simulations exemplify the theoretical results, revealing rich spiking behavior. The obtained results are also compared to similar ones obtained for the classical integer-order Morris-Lecar neuronal model.",0.2769870961],["we introduce a tree-like forcing notion extending some properties of the random forcing in the","A null ideal for inaccessibles","summarize: In this paper we introduce a tree-like forcing notion extending some properties of the random forcing in the context of the generalised Cantor space and study its associated ideal of null sets and notion of measurability. This issue was also addressed by Shelah and concerns the definition of a forcing which is ",0.125],["Meeker and Hong provided an extensive discussion of big data and reliability. they discussed how","Big Data and Reliability Applications: The Complexity Dimension","summarize: Big data features not only large volumes of data but also data with complicated structures. Complexity imposes unique challenges in big data analytics. Meeker and Hong provided an extensive discussion of the opportunities and challenges in big data and reliability, and described engineering systems that can generate big data that can be used in reliability analysis. Meeker and Hong focused on large scale system operating and environment data , and provided examples on how to link such data as covariates to traditional reliability responses such as time to failure, time to recurrence of events, and degradation measurements. This paper intends to extend that discussion by focusing on how to use data with complicated structures to do reliability analysis. Such data types include high-dimensional sensor data, functional curve data, and image streams. We first provide a review of recent development in those directions, and then we provide a discussion on how analytical methods can be developed to tackle the challenging aspects that arise from the complexity feature of big data in reliability applications. The use of modern statistical methods such as variable selection, functional data analysis, scalar-on-image regression, spatio-temporal data models, and machine learning techniques will also be discussed.",0.1578947368],["we simulate numerically the classical charge dynamics in a microscopic, planar, vacuum","Space-Charge Limited Current from a Finite Emitter in Nano- and Microdiodes","summarize: We simulate numerically the classical charge dynamics in a microscopic, planar, vacuum diode with a finite emitter area and a finite number of electrons in the gap. We assume electrons are emitted under space-charge limited conditions with a fixed potential applied to the diode. The Coulomb interaction between all electrons is included using the method of molecular dynamics. We compare our results to the conventional two-dimensional Child-Langmuir and explain how it is limited in applicability for sub-micron diameter emitters. Finally, we offer some simple relations for understanding space-charge limited flow from very small emitters.",0.2857142857],["we propose an adaptation of this model so that a macroscopic system can be obtained as","ODE and PDE based modeling of biological transportation networks","summarize: We study the global existence of solutions of a discrete model on a graph describing the formation of biological transportation networks, introduced by Hu and Cai. We propose an adaptation of this model so that a macroscopic system can be obtained as its formal continuum limit. We prove the global existence of weak solutions of the macroscopic PDE model. Finally, we present results of numerical simulations of the discrete model, illustrating the convergence to steady states, their non-uniqueness as well as their dependence on initial data and model parameters.",0.3846153846],["arterial pulse wave propagates along the artery. the displacement at multiple parts of the human","Signal Separation Using a Mathematical Model of Physiological Signals for the Measurement of Heart Pulse Wave Propagation With Array Radar","summarize: The arterial pulse wave, which propagates along the artery, is an important indicator of various cardiovascular diseases. By measuring the displacement at multiple parts of the human body, pulse wave velocity can be estimated from the pulse transit time. This paper proposes a technique for signal separation using an antenna array, so that pulse wave propagation can be measured in a non-contact manner. The body displacements due to the pulse wave at different body parts are highly correlated, and cannot be accurately separated using techniques that assume independent or uncorrelated signals. The proposed method formulates the signal separation as an optimization problem, based on a mathematical model of the arterial pulse wave. The objective function in the optimization comprises four terms that are derived based on a small-displacement approximation, unimodal impulse response approximation, and a causality condition. The optimization process was implemented using a genetic algorithm. The effectiveness of the proposed method is demonstrated through numerical simulations and experiments.",0.2931264452],["body composition analysis is known to be associated with many diseases including diabetes, cancers and cardiovascular diseases","Automatic segmentation of CT images for ventral body composition analysis","summarize: Purpose: Body composition is known to be associated with many diseases including diabetes, cancers and cardiovascular diseases. In this paper, we developed a fully automatic body tissue decomposition procedure to segment three major compartments that are related to body composition analysis - subcutaneous adipose tissue , visceral adipose tissue and muscle. Three additional compartments - the ventral cavity, lung and bones were also segmented during the segmentation process to assist segmentation of the major compartments. Methods: A convolutional neural network model with densely connected layers was developed to perform ventral cavity segmentation. An image processing workflow was developed to segment the ventral cavity in any patient's CT using the CNN model, then further segment the body tissue into multiple compartments using hysteresis thresholding followed by morphological operations. It is important to segment ventral cavity firstly to allow accurate separation of compartments with similar Hounsfield unit inside and outside the ventral cavity. Results: The ventral cavity segmentation CNN model was trained and tested with manually labelled ventral cavities in 60 CTs. Dice scores for ventral cavity segmentation were 0.966+\/-0.012. Tested on CT datasets with intravenous and oral contrast, the Dice scores were 0.96+\/-0.02, 0.94+\/-0.06, 0.96+\/-0.04, 0.95+\/-0.04 and 0.99+\/-0.01 for bone, VAT, SAT, muscle and lung, respectively. The respective Dice scores were 0.97+\/-0.02, 0.94+\/-0.07, 0.93+\/-0.06, 0.91+\/-0.04 and 0.99+\/-0.01 for non-contrast CT datasets. Conclusion: A body tissue decomposition procedure was developed to automatically segment multiple compartments of the ventral body. The proposed method enables fully automated quantification of 3D ventral body composition metrics from CT images.",0.2173913043],["Prediction of a sex","Cross Section Prediction for Inclusive Production of Z Boson in ","summarize: Prediction of ",0.0868869717],["a variety of pattern analysis techniques for model training in brain interfaces exploit neural feature dimensional","Information Theoretic Feature Transformation Learning for Brain Interfaces","summarize: Objective: A variety of pattern analysis techniques for model training in brain interfaces exploit neural feature dimensionality reduction based on feature ranking and selection heuristics. In the light of broad evidence demonstrating the potential sub-optimality of ranking based feature selection by any criterion, we propose to extend this focus with an information theoretic learning driven feature transformation concept. Methods: We present a maximum mutual information linear transformation , and a nonlinear transformation framework derived by a general definition of the feature transformation learning problem. Empirical assessments are performed based on electroencephalographic data recorded during a four class motor imagery brain-computer interface task. Exploiting state-of-the-art methods for initial feature vector construction, we compare the proposed approaches with conventional feature selection based dimensionality reduction techniques which are widely used in brain interfaces. Furthermore, for the multi-class problem, we present and exploit a hierarchical graphical model based BCI decoding system. Results: Both binary and multi-class decoding analyses demonstrate significantly better performances with the proposed methods. Conclusion: Information theoretic feature transformations are capable of tackling potential confounders of conventional approaches in various settings. Significance: We argue that this concept provides significant insights to extend the focus on feature selection heuristics to a broader definition of feature transformation learning in brain interfaces.",0.3333333333],["reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals. it","Brain-inspired photonic signal processor for periodic pattern generation and chaotic system emulation","summarize: Reservoir computing is a bio-inspired computing paradigm for processing time-dependent signals. Its hardware implementations have received much attention because of their simplicity and remarkable performance on a series of benchmark tasks. In previous experiments the output was uncoupled from the system and in most cases simply computed offline on a post-processing computer. However, numerical investigations have shown that feeding the output back into the reservoir would open the possibility of long-horizon time series forecasting. Here we present a photonic reservoir computer with output feedback, and demonstrate its capacity to generate periodic time series and to emulate chaotic systems. We study in detail the effect of experimental noise on system performance. In the case of chaotic systems, this leads us to introduce several metrics, based on standard signal processing techniques, to evaluate the quality of the emulation. Our work significantly enlarges the range of tasks that can be solved by hardware reservoir computers, and therefore the range of applications they could potentially tackle. It also raises novel questions in nonlinear dynamics and chaos theory.",0.2941176471],["we propose a statistical non-linear model based on the photomultiplier tube","Statistical Non-linear Model, Achievable Rates and Signal Detection for Photon-level Photomultiplier Receiver","summarize: We characterize the practical receiver in a wide range of signal intensity for optical wireless communication, from discrete pulse regime to continuous waveform regime. We first propose a statistical non-linear model based on the photomultiplier tube multi-stage amplification and Poisson channel, and then derive the optimal and tractable suboptimal duty cycle with peak-power and average-power constraints for on-off key modulation in linear regime. Subsequently, a threshold-based classifier is proposed to distinguish the PMT working regimes based on the non-linear model. Moreover, we derive the approximate performance of mean power detection with infinite sampling rate and finite over-sampling rate in the linear regime based on small dead time and central-limit theorem. We also fomulate a signal model in the non-linar regime. Furthermore, the performance of mean power detection and photon counting detection with maximum likelihood detection for different sampling rates is evaluated from both theoretical and numerical perspectives. We can conclude that the sample interval equivalent to dead time is a good choice, and lower sampling rate would significantly degrade the performance.",0.3994815634],["a new method to interpret the method is presented in the paper.","Analyzing -rays of the Galactic Center with Deep Learning","summarize: We present a new method to interpret the ",0.3333333333],["the attractive tail of the intermolecular interaction affects very weakly the structural properties of","Role of attractive forces in the relaxation dynamics of supercooled liquids","summarize: The attractive tail of the intermolecular interaction affects very weakly the structural properties of liquids, while it affects dramatically their dynamical ones. Via the numerical simulations of model systems not prone to crystallization, both in three and in two spatial dimensions, here we demonstrate that the non-perturbative dynamical effects of the attractive forces are tantamount to a rescaling of the activation energy by the glass transition temperature ",0.3636363636]]}